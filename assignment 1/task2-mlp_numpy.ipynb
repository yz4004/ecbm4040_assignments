{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qe6yTh55trpQ"
   },
   "source": [
    "# Assignment 1, Task 2: Multilayer Perceptron (MLP)\n",
    "You will get to know how to build basic fully connected neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Vs2WYIFtrpS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "# Plot configurations\n",
    "%matplotlib inline\n",
    "\n",
    "# Notebook auto reloads code. (Ref: http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3gYnTjputrpV"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "colab_type": "code",
    "id": "I31uJ6KltrpW",
    "outputId": "1a677958-43ca-422c-ec66-38e8cdff4283",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49000, 784) (1000, 784) (10000, 784) (100, 784)\n",
      "Train data shape:  (49000, 784)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 784)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (10000, 784)\n",
      "Test labels shape:  (10000,)\n",
      "Development data shape: (100, 784)\n",
      "Development data shape (100,)\n"
     ]
    }
   ],
   "source": [
    "# Load the raw Fashion-MNIST data.\n",
    "train, test = fashion_mnist.load_data()\n",
    "\n",
    "X_train_raw, y_train = train\n",
    "X_test_raw, y_test = test\n",
    "\n",
    "X_train = X_train_raw.reshape((X_train_raw.shape[0], X_train_raw.shape[1]**2))\n",
    "X_test = X_test_raw.reshape((X_test_raw.shape[0], X_test_raw.shape[1]**2))\n",
    "\n",
    "# Data organizations:\n",
    "# Train data: 49000 samples from original train set: 1~49,000\n",
    "# Validation data: 1000 samples from original train set: 49,000~50,000\n",
    "# Test data: 10000 samples from original test set: 1~10,000\n",
    "# Development data (for gradient check): 100 from the train set: 1~49,000\n",
    "num_training = 49000\n",
    "num_validation = 1000\n",
    "num_dev = 100\n",
    "\n",
    "X_val = X_train[-num_validation:, :]\n",
    "y_val = y_train[-num_validation:]\n",
    "\n",
    "mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "X_dev = X_train[mask]\n",
    "y_dev = y_train[mask]\n",
    "\n",
    "X_train = X_train[:num_training, :]\n",
    "y_train = y_train[:num_training]\n",
    "\n",
    "# Preprocessing: subtract the mean value across every dimension for training data\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "\n",
    "X_train = X_train.astype(np.float32) - mean_image.astype(np.float32)\n",
    "X_val = X_val.astype(np.float32) - mean_image\n",
    "X_test = X_test.astype(np.float32) - mean_image\n",
    "X_dev = X_dev.astype(np.float32) - mean_image\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape, X_dev.shape)\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('Development data shape:', X_dev.shape)\n",
    "print('Development data shape', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pLhMsud9trpa"
   },
   "source": [
    "## Part 1: Basic layers\n",
    "In this part, all the functions will be created from scratch using numpy for better understanding. (In the next task, you will be introduced to built in layers from TensorFlow.)\n",
    "\n",
    "### Create basic layer functions\n",
    "\n",
    "<span style=\"color:red\"><strong>TODO</strong></span>: Complete functions **affine_forward**, **affine_backward** in **./utils/layer_funcs.py**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>NOTE</strong></span>: Please do not change the code in the cell below, The cell below will run correctly if your code is right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "2AfAMrOZtrpb",
    "outputId": "af228bb7-f1eb-41bf-e9f5-c3a6de6cdbd5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is out correct? True\n",
      "Is dx correct? True\n",
      "Is dw correct? True\n",
      "Is db correct? True\n"
     ]
    }
   ],
   "source": [
    "# THE FOLLOWING IS THE VERIFICATION CODE     #\n",
    "# DO NOT CHANGE IT.                        #\n",
    "\n",
    "from utils.layer_funcs import affine_forward\n",
    "from utils.layer_funcs import affine_backward\n",
    "\n",
    "# generate data for checking\n",
    "x = X_dev\n",
    "w = np.random.rand(x.shape[1],100)\n",
    "b = np.random.rand(100)\n",
    "dout = np.ones((x.shape[0],100))\n",
    "\n",
    "## Affine function: H = W*X + b\n",
    "out = affine_forward(x, w, b)\n",
    "dx, dw, db = affine_backward(dout, x, w, b)\n",
    "\n",
    "## check your implementation using the tf.gradients_function()\n",
    "x_tf = tf.Variable(x, name='x')\n",
    "w_tf = tf.Variable(w, name='w')\n",
    "b_tf = tf.Variable(b, name='b')\n",
    "\n",
    "def affine_layer(x, w, b):\n",
    "    return tf.matmul(x, w) + b\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(w_tf)\n",
    "    out_tf = affine_layer(x_tf, w_tf, b_tf)\n",
    "    dx_tf, dw_tf, db_tf = tape.gradient(out_tf, (x_tf, w_tf, b_tf))\n",
    "\n",
    "out_check = out_tf.numpy()\n",
    "dx_check, dw_check, db_check = dx_tf.numpy(), dw_tf.numpy(), db_tf.numpy()\n",
    "\n",
    "## Print validation results\n",
    "print(\"Is out correct? {}\".format(np.allclose(out, out_check)))\n",
    "print(\"Is dx correct? {}\".format(np.allclose(dx, dx_check)))\n",
    "print(\"Is dw correct? {}\".format(np.allclose(dw, dw_check)))\n",
    "print(\"Is db correct? {}\".format(np.allclose(db, db_check)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LL0DqZ_ftrpd"
   },
   "source": [
    "<span style=\"color:red\"><strong>TODO</strong></span>: Complete functions **relu_forward**, **relu_backward** in **./utils/layer_funcs.py**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>NOTE</strong></span>: Please do not change the code in the cell below, The cell below will run correctly if your code is right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "nYDT8Idatrpe",
    "outputId": "a1db7202-1da9-45f5-bde1-ee42b623029f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is out correct? True\n",
      "Is dx correct? True\n"
     ]
    }
   ],
   "source": [
    "# THE FOLLOWING IS THE VERIFICATION CODE     #\n",
    "# DO NOT CHANGE IT.                        #\n",
    "\n",
    "from utils.layer_funcs import relu_forward\n",
    "from utils.layer_funcs import relu_backward\n",
    "\n",
    "## Activation layers -- Here we introduce ReLU activation function\n",
    "## since it is the most commonly used in computer vision problems.\n",
    "## However, you can also try to implement \n",
    "## other activation functions like sigmoid, tanh etc.\n",
    "x = X_dev\n",
    "dout = np.ones(x.shape)\n",
    "\n",
    "## ReLU\n",
    "out = relu_forward(x)\n",
    "dx = relu_backward(dout, x)\n",
    "\n",
    "## check by tf.GradientTape.gradients()\n",
    "x_tf = tf.Variable(x, name='x')\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x_tf)\n",
    "    out_tf = tf.nn.relu(x_tf)\n",
    "    grad_gt = tape.gradient(out_tf, x_tf)\n",
    "\n",
    "out_check = out_tf.numpy()\n",
    "dx_check = grad_gt.numpy()\n",
    "\n",
    "## Print validation result\n",
    "print(\"Is out correct? {}\".format(np.allclose(out, out_check)))\n",
    "print(\"Is dx correct? {}\".format(np.allclose(dx, dx_check)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LrSXJOdktrph"
   },
   "source": [
    "<span style=\"color:red\"><strong>TODO</strong></span>: Complete functions **softmax_loss** in **./utils/layer_funcs.py**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>NOTE</strong></span>: Please do not change the code in the cell below, The cell below will run correctly if your code is right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "rG2_zSsjtrpi",
    "outputId": "5b8fd3ce-0175-4aa2-c29d-b0925dcf2f42",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is loss correct? True\n",
      "Is dx correct? True\n"
     ]
    }
   ],
   "source": [
    "# THE FOLLOWING CODE IS JUST FOR CHECKING.     #\n",
    "# NO NEED TO CHANGE IT.                        #\n",
    "\n",
    "from utils.layer_funcs import softmax_loss\n",
    "\n",
    "## generate some random data for testing\n",
    "x = np.random.rand(100,20)\n",
    "y = np.argmax(x, axis=1)\n",
    "\n",
    "loss, dx = softmax_loss(x, y)\n",
    "\n",
    "## check by tf.GradientTape.gradients()\n",
    "\n",
    "x_tf = tf.Variable(x, name='x')\n",
    "y_tf = tf.Variable(y, name='y')\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x_tf)\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits= x_tf, labels=tf.one_hot(y_tf,20))\n",
    "    loss_tf = tf.reduce_mean(cross_entropy)\n",
    "    dx_tf = tape.gradient(loss_tf, x_tf)\n",
    "\n",
    "loss_check = loss_tf.numpy()\n",
    "dx_check = dx_tf.numpy()\n",
    "## Print validation result\n",
    "print(\"Is loss correct? {}\".format(np.allclose(loss, loss_check)))\n",
    "print(\"Is dx correct? {}\".format(np.allclose(dx, dx_check)))\n",
    "\n",
    "# print(dx)\n",
    "# print(dx_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iC_Q820Ltrpk"
   },
   "source": [
    "### Create a single layer\n",
    "\n",
    "Combine an affine function and a nonlinear activation function into a single fully-connected layer. Edit the code in ./utils/layer_utils.py\n",
    "\n",
    "$$\\mathbf{O} = activation(\\mathbf{W} \\times \\mathbf{X} + \\mathbf{b})$$\n",
    "\n",
    "For this assignment, you need to create two types of layers as below. You can get started with the skeleton code in ./utils/layer_utils.py. The basic class structure has been provided, and you need to fill in the \"TODO\" part(s). \n",
    "\n",
    "* DenseLayer -- Affine transform >> ReLU\n",
    "```\n",
    "Class DenseLayer:\n",
    "    Variables: weights, bias \n",
    "    Functions: \n",
    "        __init__: given (input_dim, output_dim, weight_scale)\n",
    "        feedforward: TODO\n",
    "        backforward: TODO      \n",
    "```    \n",
    "* AffineLayer -- Affine transform and the class structure is similar to DenseLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PhIgKu9ptrpl"
   },
   "source": [
    "<span style=\"color:red\"><strong>TODO</strong></span>: Complete function **AffineLayer** in **./utils/layer_utils.py**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>NOTE</strong></span>: Please do not change the code in the cell below, The cell below will run correctly if your code is right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "Se8aWo7Ktrpm",
    "outputId": "803e04f9-7654-4ad4-8050-90e0f0e6a2c5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is out correct? True\n",
      "Is dx correct? True\n",
      "Is dw correct? True\n",
      "Is db correct? True\n",
      "[100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100.] [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100.] (100,) (100,)\n"
     ]
    }
   ],
   "source": [
    "# THE FOLLOWING IS THE VERIFICATION CODE     #\n",
    "# DO NOT CHANGE IT.                        #\n",
    "\n",
    "from utils.layer_utils import AffineLayer\n",
    "\n",
    "## Affine\n",
    "test_affine = AffineLayer(input_dim=X_train.shape[1],output_dim=100)\n",
    "w, b = test_affine.params\n",
    "\n",
    "## Data for correctness check\n",
    "x = X_dev\n",
    "dout = np.ones((x.shape[0], 100))\n",
    "\n",
    "out = test_affine.feedforward(x)\n",
    "dx = test_affine.backward(dout)\n",
    "dw, db = test_affine.gradients\n",
    "\n",
    "## check by tf.GradientTape.gradients()\n",
    "x_tf = tf.Variable(x, name='x')\n",
    "w_tf = tf.Variable(w, name='w')\n",
    "b_tf = tf.Variable(b, name='b')\n",
    "\n",
    "def affine_layer(x, w, b):\n",
    "    return tf.matmul(x, w) + b\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(w_tf)\n",
    "    out_tf = affine_layer(x_tf, w_tf, b_tf)\n",
    "    dx_tf, dw_tf, db_tf = tape.gradient(out_tf, (x_tf, w_tf, b_tf))\n",
    "    \n",
    "out_check = out_tf.numpy()\n",
    "dx_check = dx_tf.numpy()\n",
    "dw_check = dw_tf.numpy()\n",
    "db_check = db_tf.numpy()\n",
    "\n",
    "## Print validation result\n",
    "print(\"Is out correct? {}\".format(np.allclose(out, out_check)))\n",
    "print(\"Is dx correct? {}\".format(np.allclose(dx, dx_check)))\n",
    "print(\"Is dw correct? {}\".format(np.allclose(dw, dw_check)))\n",
    "print(\"Is db correct? {}\".format(np.allclose(db, db_check)))\n",
    "print(db,db_check,db.shape,db_check.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L2MhDHIJtrpo"
   },
   "source": [
    "<span style=\"color:red\"><strong>TODO</strong></span>: Complete function **DenseLayer** in **./utils/layer_utils.py**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>NOTE</strong></span>: Please do not change the code in the cell below, The cell below will run correctly if your code is right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "G8MgO2Gztrpq",
    "outputId": "5047dedc-a265-437f-882e-24adfd691074",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is out correct? True\n",
      "Is dx correct? True\n",
      "Is dw correct? True\n",
      "Is db correct? True\n"
     ]
    }
   ],
   "source": [
    "# THE FOLLOWING IS THE VERIFICATION CODE     #\n",
    "# DO NOT CHANGE IT.                        #\n",
    "\n",
    "## First, let's make a dense layer\n",
    "from utils.layer_utils import DenseLayer\n",
    "\n",
    "## Affine + ReLU\n",
    "test_dense = DenseLayer(input_dim=X_train.shape[1],output_dim=100)\n",
    "w, b = test_dense.params\n",
    "\n",
    "## Data for correctness check\n",
    "x = X_dev\n",
    "dout = np.ones((x.shape[0], 100))\n",
    "\n",
    "out = test_dense.feedforward(x)\n",
    "dx = test_dense.backward(dout)\n",
    "dw, db = test_dense.gradients\n",
    "\n",
    "## check by tf.GradientTape.gradients()\n",
    "x_tf = tf.Variable(x, name='x')\n",
    "w_tf = tf.Variable(w, name='w')\n",
    "b_tf = tf.Variable(b, name='b')\n",
    "\n",
    "def dense_layer(x, w, b):\n",
    "    return tf.nn.relu(tf.matmul(x, w) + b)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(w_tf)\n",
    "    out_tf = dense_layer(x_tf, w_tf, b_tf)\n",
    "    dx_tf, dw_tf, db_tf = tape.gradient(out_tf, (x_tf, w_tf, b_tf))\n",
    "    \n",
    "out_check = out_tf.numpy()\n",
    "dx_check = dx_tf.numpy()\n",
    "dw_check = dw_tf.numpy()\n",
    "db_check = db_tf.numpy()\n",
    "\n",
    "## Print validation result\n",
    "print(\"Is out correct? {}\".format(np.allclose(out, out_check)))\n",
    "print(\"Is dx correct? {}\".format(np.allclose(dx, dx_check)))\n",
    "print(\"Is dw correct? {}\".format(np.allclose(dw, dw_check)))\n",
    "print(\"Is db correct? {}\".format(np.allclose(db, db_check)))\n",
    "\n",
    "# print(db,\"\\n\\n\",db_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y9cJLCPstrpt"
   },
   "source": [
    "## Part 2: Two Layer Network\n",
    "\n",
    "Complete the class **TwoLayerNet** in **./utils/classifiers/twolayernet.py**. Through this experiment, you will create a two-layer neural network and learn about the backpropagation mechanism. The network structure is like **input >> DenseLayer >> AffineLayer >> softmax loss >> output**. Complete \"TODO\" part(s).\n",
    "```\n",
    "Class TwoLayerNet:   \n",
    "    Functions: \n",
    "        __init__: GIVEN\n",
    "        loss: TODO - calculate cross entropy loss and gradients wst all weights and bias.\n",
    "        step: TODO - a single update all weights and bias by SGD.\n",
    "        predict: TODO - output result(classification accuracy) based on input data\n",
    "    \n",
    "    Variables:\n",
    "        layers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JBFtQx5Utrpu"
   },
   "source": [
    "<span style=\"color:red\"><strong>TODO</strong></span>: Complete class **TwoLayerNet** in **./utils/classifiers/twolayernet.py**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>NOTE</strong></span>: Please do not change the code in the cell below, The cell below will run correctly if your code is right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1QlpmRrEtrpv",
    "outputId": "2dfb94b6-1d2d-4bf6-efce-0d54c7725cc8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100) (100,) (100, 20) (20,)\n",
      "Is loss correct? True\n"
     ]
    }
   ],
   "source": [
    "# THE FOLLOWING IS THE VERIFICATION CODE     #\n",
    "# DO NOT CHANGE IT.                        #\n",
    "\n",
    "from utils.classifiers.twolayernet import TwoLayerNet\n",
    "\n",
    "## Define a model\n",
    "model = TwoLayerNet(input_dim=X_train.shape[1], hidden_dim=100, num_classes=20, reg=1e-4)\n",
    "W1, b1 = model.layer1.params\n",
    "W2, b2 = model.layer2.params\n",
    "\n",
    "print(W1.shape, b1.shape, W2.shape,b2.shape)\n",
    "\n",
    "## Backprogation -- Finish loss function and gradients calculation in TwoLayerNet\n",
    "loss = model.loss(X_dev, y_dev)\n",
    "\n",
    "## Check loss by tensorflow\n",
    "x_tf = tf.Variable(X_dev, dtype = tf.float32)\n",
    "y_tf = tf.Variable(y_dev, dtype = tf.uint8)\n",
    "\n",
    "W1_tf = tf.Variable(W1.astype('float32'))\n",
    "b1_tf = tf.Variable(b1.astype('float32'))\n",
    "W2_tf = tf.Variable(W2.astype('float32'))\n",
    "b2_tf = tf.Variable(b2.astype('float32'))\n",
    "h1_tf = tf.nn.relu(tf.matmul(x_tf, W1_tf))\n",
    "h2_tf = tf.matmul(h1_tf, W2_tf) + b2_tf\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits= h2_tf, labels=tf.one_hot(y_tf,20))\n",
    "L2_loss = tf.nn.l2_loss(W1_tf) + tf.nn.l2_loss(W2_tf)\n",
    "loss_tf = tf.reduce_mean(cross_entropy) + 1e-4 * L2_loss \n",
    "\n",
    "loss_check=loss_tf.numpy()\n",
    "    \n",
    "## Print validation result\n",
    "print(\"Is loss correct? {}\".format(np.allclose(loss, loss_check)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4WjSapZXtrpy"
   },
   "source": [
    "### Train a two-layer network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HnEd1Z9Wtrpz"
   },
   "source": [
    "#### Import functions for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "et5ZTXBktrpz"
   },
   "outputs": [],
   "source": [
    "from utils.train_funcs import train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n2Nbd9iotrp2"
   },
   "source": [
    "#### Start training\n",
    "We have provide you the **train( )** function in **./utils/train_func.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "eOIqjSUCtrp2",
    "outputId": "8cc8db83-6caa-4357-e086-87801f6fabf1",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches for training: 98\n",
      "5000/49000 loss: 2.7929669962454726\n",
      "10000/49000 loss: 2.8557622067766104\n",
      "15000/49000 loss: 2.538328482468993\n",
      "20000/49000 loss: 2.64743154924006\n",
      "25000/49000 loss: 2.112237874928677\n",
      "30000/49000 loss: 1.9892502148824458\n",
      "35000/49000 loss: 1.9913466941385798\n",
      "40000/49000 loss: 1.6460990974141316\n",
      "45000/49000 loss: 1.3692100032407932\n",
      "epoch 1: valid acc = 0.523, new learning rate = 0.000475\n",
      "5000/49000 loss: 1.2774829915455232\n",
      "10000/49000 loss: 1.140448894205564\n",
      "15000/49000 loss: 1.0811930358950188\n",
      "20000/49000 loss: 1.0389434070377683\n",
      "25000/49000 loss: 1.05307032974229\n",
      "30000/49000 loss: 0.9980220696692561\n",
      "35000/49000 loss: 1.1390166833465096\n",
      "40000/49000 loss: 0.9508307027802055\n",
      "45000/49000 loss: 0.933545472690002\n",
      "epoch 2: valid acc = 0.715, new learning rate = 0.00045125\n",
      "5000/49000 loss: 0.7948704780246241\n",
      "10000/49000 loss: 0.8127942826524689\n",
      "15000/49000 loss: 0.7470226662274086\n",
      "20000/49000 loss: 0.7194468675692463\n",
      "25000/49000 loss: 0.709286900791974\n",
      "30000/49000 loss: 0.7976671471218151\n",
      "35000/49000 loss: 0.6416416501091166\n",
      "40000/49000 loss: 0.7371433866472155\n",
      "45000/49000 loss: 0.6738507227375484\n",
      "epoch 3: valid acc = 0.758, new learning rate = 0.0004286875\n",
      "5000/49000 loss: 0.6821442715323824\n",
      "10000/49000 loss: 0.615129218089526\n",
      "15000/49000 loss: 0.5691487560514508\n",
      "20000/49000 loss: 0.6364520220071751\n",
      "25000/49000 loss: 0.6300783857612079\n",
      "30000/49000 loss: 0.6380808154839673\n",
      "35000/49000 loss: 0.576564299976714\n",
      "40000/49000 loss: 0.6040724310262195\n",
      "45000/49000 loss: 0.5959215868826406\n",
      "epoch 4: valid acc = 0.789, new learning rate = 0.00040725312499999993\n",
      "5000/49000 loss: 0.5622362659652623\n",
      "10000/49000 loss: 0.6201148835801412\n",
      "15000/49000 loss: 0.5769403366352595\n",
      "20000/49000 loss: 0.5825303249047117\n",
      "25000/49000 loss: 0.5713326229163479\n",
      "30000/49000 loss: 0.5499271662348761\n",
      "35000/49000 loss: 0.5966752130075587\n",
      "40000/49000 loss: 0.5084091914033273\n",
      "45000/49000 loss: 0.5107319971712553\n",
      "epoch 5: valid acc = 0.803, new learning rate = 0.0003868904687499999\n",
      "5000/49000 loss: 0.5397598773597573\n",
      "10000/49000 loss: 0.5526058199821633\n",
      "15000/49000 loss: 0.5701605819381106\n",
      "20000/49000 loss: 0.5092197171671708\n",
      "25000/49000 loss: 0.5072264058547995\n",
      "30000/49000 loss: 0.47780432107580184\n",
      "35000/49000 loss: 0.5017044303624855\n",
      "40000/49000 loss: 0.5611837679015638\n",
      "45000/49000 loss: 0.5602944662519893\n",
      "epoch 6: valid acc = 0.81, new learning rate = 0.0003675459453124999\n",
      "5000/49000 loss: 0.41231141559903867\n",
      "10000/49000 loss: 0.47043100300305457\n",
      "15000/49000 loss: 0.5147551780477951\n",
      "20000/49000 loss: 0.5698270148866755\n",
      "25000/49000 loss: 0.4208735171059819\n",
      "30000/49000 loss: 0.4894165806358744\n",
      "35000/49000 loss: 0.48380470372710066\n",
      "40000/49000 loss: 0.4384819639268215\n",
      "45000/49000 loss: 0.513503664537369\n",
      "epoch 7: valid acc = 0.825, new learning rate = 0.00034916864804687486\n",
      "5000/49000 loss: 0.43044453265067106\n",
      "10000/49000 loss: 0.4829602499316565\n",
      "15000/49000 loss: 0.5012131663852931\n",
      "20000/49000 loss: 0.45426287303198687\n",
      "25000/49000 loss: 0.42651195470942094\n",
      "30000/49000 loss: 0.46641592309995655\n",
      "35000/49000 loss: 0.48119867772126956\n",
      "40000/49000 loss: 0.5108441830883594\n",
      "45000/49000 loss: 0.43147726255382385\n",
      "epoch 8: valid acc = 0.836, new learning rate = 0.0003317102156445311\n",
      "5000/49000 loss: 0.49600593143281685\n",
      "10000/49000 loss: 0.4098434291484906\n",
      "15000/49000 loss: 0.4965054443883926\n",
      "20000/49000 loss: 0.47613042165969227\n",
      "25000/49000 loss: 0.4508940560979035\n",
      "30000/49000 loss: 0.4496766848643806\n",
      "35000/49000 loss: 0.45278507691829967\n",
      "40000/49000 loss: 0.4717191805808644\n",
      "45000/49000 loss: 0.42448988729916964\n",
      "epoch 9: valid acc = 0.833, new learning rate = 0.0003151247048623045\n",
      "5000/49000 loss: 0.49603553882351525\n",
      "10000/49000 loss: 0.469466956202085\n",
      "15000/49000 loss: 0.49487519316663364\n",
      "20000/49000 loss: 0.44625024582431494\n",
      "25000/49000 loss: 0.44429610102469946\n",
      "30000/49000 loss: 0.45788695444673155\n",
      "35000/49000 loss: 0.45910327164740833\n",
      "40000/49000 loss: 0.5279677056652107\n",
      "45000/49000 loss: 0.43101503330650565\n",
      "epoch 10: valid acc = 0.833, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.8286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8286"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# THE FOLLOWING IS THE VERIFICATION CODE     #\n",
    "# DO NOT CHANGE IT.                        #\n",
    "\n",
    "from utils.classifiers.twolayernet import TwoLayerNet\n",
    "\n",
    "## TODO: Use previous layers to create a two layer neural network\n",
    "## input->(affine->activation)->(affine->softmax)->output\n",
    "## The recommended activation function is ReLU. And you can \n",
    "## also make a comparison with other activation function to see\n",
    "## any difference.\n",
    "model = TwoLayerNet(input_dim=X_train.shape[1], hidden_dim=400, num_classes=20, reg=1e-4, weight_scale=1e-3)\n",
    "\n",
    "num_epoch = 10\n",
    "batch_size = 500\n",
    "lr = 5e-4\n",
    "verbose = True\n",
    "train_acc_hist, val_acc_hist = train(model, X_train, y_train, X_val, y_val, \n",
    "                  num_epoch=num_epoch, batch_size=batch_size, learning_rate=lr, verbose=verbose)\n",
    "test(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ID-4_nLqtrp5"
   },
   "source": [
    "<span style=\"color:red\"><strong>TODO</strong></span>: Plot training and validation accuracy history of each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>Solution</strong></span>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "duZxp0yntrp6",
    "outputId": "12b3338c-8ea2-4577-90ba-17851449308c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_acc_hist, label='Train')\n",
    "plt.plot(val_acc_hist, label = 'Validation' )\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "42xHlu5utrp9"
   },
   "source": [
    "#### Visulize the weight variable in the first layer.\n",
    "\n",
    "Visualization of the intermediate weights can help you get an intuitive understanding of how the network works, especially in  Convolutional Neural Networks (CNNs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gFr5Kvrmtrp9"
   },
   "outputs": [],
   "source": [
    "from utils.display_funcs import visualize_pics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "OrusGTkltrqA",
    "outputId": "6a87d009-7825-4795-b23d-95ba44a5cf8d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of feature vectors: 400\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "weights = model.layer1.params[0]\n",
    "pics = weights.reshape(1, X_train_raw.shape[1], X_train_raw.shape[2], -1).transpose(3, 1, 2, 0)\n",
    "## visualization\n",
    "visualize_pics(pics, cmap='nipy_spectral')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2s4-QzTHtrqD"
   },
   "source": [
    "### Get test accuracy greater than 80%\n",
    "\n",
    "For this part, you need to train a better two-layer net. The requirement is to get test accuracy better than 35%. If your accuracy is lower, for each 1% lower than 35%, you will lose 1 point (There are totally 10 points for this part).\n",
    "\n",
    "Here are some recommended methods for improving the performance. Feel free to try any other method as you see fit.\n",
    "\n",
    "1. Hyperparameter tuning: reg, hidden_dim, lr, learning_decay, num_epoch, batch_size, weight_scale.\n",
    "2. Adjust training strategy: Randomly select a batch of samples rather than selecting them orderly. \n",
    "3. Try new optimization methods: Now we are using SGD, you can try SGD with momentum, adam, etc.\n",
    "4. Early-stopping.\n",
    "5. Good (better) initial values for weights in the model.\n",
    "\n",
    "A comparison between SGD and SGD with momentum.\n",
    "\n",
    "* Stochastic gradient descent - SGD\n",
    "    ```\n",
    "    w = w - learning_rate * gradient \n",
    "    ```\n",
    "* SGD with momentum\n",
    "    ```\n",
    "    v = momentum*v + learning_rate * gradient\n",
    "    w = w - v\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XK0pOcTLtrqE"
   },
   "source": [
    "<span style=\"color:red\"><strong>TODO</strong></span>: See below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PfTHBopktrqE"
   },
   "outputs": [],
   "source": [
    "from utils.classifiers.twolayernet import TwoLayerNet\n",
    "# TODO: Use previous layers to create a two layer neural network.\n",
    "# Try several solutions and report the best performing one.\n",
    "# input->(affine->activation)->(affine->softmax)->output\n",
    "# The recommended activation function is ReLU. You can \n",
    "# make a comparison with other activation functions to see\n",
    "# the differences.\n",
    "#\n",
    "# You will need to execute code similar to the code below, using your parameter specs:\n",
    "#    model = TwoLayerNet(input_dim=TBD, hidden_dim=TBD, num_classes=TBD, reg=TBD, weight_scale=TBD)\n",
    "#    num_epoch = TBD\n",
    "#    batch_size = TBD\n",
    "#    lr = TBD\n",
    "#    verbose = TBD\n",
    "#    train_acc_hist, val_acc_hist = train(TBD)\n",
    "#    test(TBD, TBD, TBD)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>Solution</strong></span>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches for training: 245\n",
      "2000/49000 loss: 2.7065869963051683\n",
      "4000/49000 loss: 2.5920426445906353\n",
      "6000/49000 loss: 2.6325152703432377\n",
      "8000/49000 loss: 2.49792504246436\n",
      "10000/49000 loss: 2.3943323484361967\n",
      "12000/49000 loss: 2.141051117884743\n",
      "14000/49000 loss: 1.894299452191484\n",
      "16000/49000 loss: 1.58210088712369\n",
      "18000/49000 loss: 1.3882290848731573\n",
      "20000/49000 loss: 1.293648998737616\n",
      "22000/49000 loss: 1.2370810983833351\n",
      "24000/49000 loss: 1.114070294838058\n",
      "26000/49000 loss: 1.0084617044873816\n",
      "28000/49000 loss: 1.1660891270501745\n",
      "30000/49000 loss: 0.9385969920575883\n",
      "32000/49000 loss: 1.043270289154694\n",
      "34000/49000 loss: 1.0330290705071699\n",
      "36000/49000 loss: 0.9083824454352597\n",
      "38000/49000 loss: 0.8572594607770633\n",
      "40000/49000 loss: 0.8608978298234621\n",
      "42000/49000 loss: 0.8433380243449327\n",
      "44000/49000 loss: 0.810041782602103\n",
      "46000/49000 loss: 0.7474118529157241\n",
      "48000/49000 loss: 0.6468064274663375\n",
      "epoch 1: valid acc = 0.751, new learning rate = 0.000475\n",
      "2000/49000 loss: 0.7291881535080966\n",
      "4000/49000 loss: 0.6617381325945124\n",
      "6000/49000 loss: 0.6190162072703131\n",
      "8000/49000 loss: 0.6084849879737629\n",
      "10000/49000 loss: 0.6637504830548874\n",
      "12000/49000 loss: 0.7290129298046518\n",
      "14000/49000 loss: 0.6404217246171213\n",
      "16000/49000 loss: 0.6524621193079283\n",
      "18000/49000 loss: 0.6087288879345895\n",
      "20000/49000 loss: 0.6649314915668714\n",
      "22000/49000 loss: 0.5219007977231468\n",
      "24000/49000 loss: 0.5511948089912219\n",
      "26000/49000 loss: 0.477143841514554\n",
      "28000/49000 loss: 0.5836643552506583\n",
      "30000/49000 loss: 0.6308467110273693\n",
      "32000/49000 loss: 0.5227980114687751\n",
      "34000/49000 loss: 0.4474875944186827\n",
      "36000/49000 loss: 0.47885629005439495\n",
      "38000/49000 loss: 0.5578701040142138\n",
      "40000/49000 loss: 0.5065148422067806\n",
      "42000/49000 loss: 0.5451091910429094\n",
      "44000/49000 loss: 0.5176513568418404\n",
      "46000/49000 loss: 0.5862982517221389\n",
      "48000/49000 loss: 0.48731454778594746\n",
      "epoch 2: valid acc = 0.805, new learning rate = 0.00045125\n",
      "2000/49000 loss: 0.5209232037724775\n",
      "4000/49000 loss: 0.5778946151173874\n",
      "6000/49000 loss: 0.5951887964266596\n",
      "8000/49000 loss: 0.4437183785521869\n",
      "10000/49000 loss: 0.48353567372680734\n",
      "12000/49000 loss: 0.42486324909856404\n",
      "14000/49000 loss: 0.46803926104501137\n",
      "16000/49000 loss: 0.4526869271675694\n",
      "18000/49000 loss: 0.5105953706163491\n",
      "20000/49000 loss: 0.45473963692569175\n",
      "22000/49000 loss: 0.4915592853372848\n",
      "24000/49000 loss: 0.5018243559017921\n",
      "26000/49000 loss: 0.5386954222450704\n",
      "28000/49000 loss: 0.43558618817581934\n",
      "30000/49000 loss: 0.5114823701004577\n",
      "32000/49000 loss: 0.4527224693972029\n",
      "34000/49000 loss: 0.4598391576760628\n",
      "36000/49000 loss: 0.43504764520378575\n",
      "38000/49000 loss: 0.4761606346361649\n",
      "40000/49000 loss: 0.5709994812283179\n",
      "42000/49000 loss: 0.5306059918866612\n",
      "44000/49000 loss: 0.38387871077085445\n",
      "46000/49000 loss: 0.37708910778249444\n",
      "48000/49000 loss: 0.451758044029848\n",
      "epoch 3: valid acc = 0.839, new learning rate = 0.0004286875\n",
      "2000/49000 loss: 0.36330726944744957\n",
      "4000/49000 loss: 0.5682336032546915\n",
      "6000/49000 loss: 0.49449603325738867\n",
      "8000/49000 loss: 0.36965787367137776\n",
      "10000/49000 loss: 0.4169476214665632\n",
      "12000/49000 loss: 0.5147593751844447\n",
      "14000/49000 loss: 0.5556452041904005\n",
      "16000/49000 loss: 0.4469867471951426\n",
      "18000/49000 loss: 0.4644004968876174\n",
      "20000/49000 loss: 0.4432176196783573\n",
      "22000/49000 loss: 0.5399167253174711\n",
      "24000/49000 loss: 0.4233476236006782\n",
      "26000/49000 loss: 0.4686062873256129\n",
      "28000/49000 loss: 0.5860470604694097\n",
      "30000/49000 loss: 0.456034116931992\n",
      "32000/49000 loss: 0.42861611268478156\n",
      "34000/49000 loss: 0.3251123057417328\n",
      "36000/49000 loss: 0.45207609171165536\n",
      "38000/49000 loss: 0.418483745250737\n",
      "40000/49000 loss: 0.43188268507947547\n",
      "42000/49000 loss: 0.34170785947282706\n",
      "44000/49000 loss: 0.4456570191312072\n",
      "46000/49000 loss: 0.4208906717196982\n",
      "48000/49000 loss: 0.4288601699081122\n",
      "epoch 4: valid acc = 0.844, new learning rate = 0.00040725312499999993\n",
      "2000/49000 loss: 0.39428197453260105\n",
      "4000/49000 loss: 0.41700254494334255\n",
      "6000/49000 loss: 0.3393389329373696\n",
      "8000/49000 loss: 0.45674351816638\n",
      "10000/49000 loss: 0.43887879910260336\n",
      "12000/49000 loss: 0.38778404936520383\n",
      "14000/49000 loss: 0.3894600118420004\n",
      "16000/49000 loss: 0.4641415639861072\n",
      "18000/49000 loss: 0.425646456899062\n",
      "20000/49000 loss: 0.44322964992786107\n",
      "22000/49000 loss: 0.4152314344760172\n",
      "24000/49000 loss: 0.47695030285514556\n",
      "26000/49000 loss: 0.47716751830967186\n",
      "28000/49000 loss: 0.39136746277635454\n",
      "30000/49000 loss: 0.4403354355437891\n",
      "32000/49000 loss: 0.41273638223065257\n",
      "34000/49000 loss: 0.3707220302703446\n",
      "36000/49000 loss: 0.38109763020803206\n",
      "38000/49000 loss: 0.4855894783546641\n",
      "40000/49000 loss: 0.4384772055228274\n",
      "42000/49000 loss: 0.4501008851536913\n",
      "44000/49000 loss: 0.3957785499096506\n",
      "46000/49000 loss: 0.42116276422457105\n",
      "48000/49000 loss: 0.47068016500622595\n",
      "epoch 5: valid acc = 0.85, new learning rate = 0.0003868904687499999\n",
      "2000/49000 loss: 0.35400766654603527\n",
      "4000/49000 loss: 0.41900585521347755\n",
      "6000/49000 loss: 0.40041387107304643\n",
      "8000/49000 loss: 0.4633811256841649\n",
      "10000/49000 loss: 0.3738897402749369\n",
      "12000/49000 loss: 0.310674692936811\n",
      "14000/49000 loss: 0.4002060891558968\n",
      "16000/49000 loss: 0.41103777819393666\n",
      "18000/49000 loss: 0.4105052665020827\n",
      "20000/49000 loss: 0.4007517624962195\n",
      "22000/49000 loss: 0.3450386089203151\n",
      "24000/49000 loss: 0.39319486470214987\n",
      "26000/49000 loss: 0.31989179738226403\n",
      "28000/49000 loss: 0.3393877066805415\n",
      "30000/49000 loss: 0.4244835844573709\n",
      "32000/49000 loss: 0.5030128387193104\n",
      "34000/49000 loss: 0.46920993919907067\n",
      "36000/49000 loss: 0.3677792445851179\n",
      "38000/49000 loss: 0.3440548510313062\n",
      "40000/49000 loss: 0.3656485556812347\n",
      "42000/49000 loss: 0.40529859781117733\n",
      "44000/49000 loss: 0.44279847093249447\n",
      "46000/49000 loss: 0.41574387407323277\n",
      "48000/49000 loss: 0.38205992425370244\n",
      "epoch 6: valid acc = 0.862, new learning rate = 0.0003675459453124999\n",
      "2000/49000 loss: 0.3144471002693874\n",
      "4000/49000 loss: 0.4263783307053742\n",
      "6000/49000 loss: 0.4402056967914836\n",
      "8000/49000 loss: 0.40222066751625013\n",
      "10000/49000 loss: 0.349458630915178\n",
      "12000/49000 loss: 0.4791021788196718\n",
      "14000/49000 loss: 0.34444455854382366\n",
      "16000/49000 loss: 0.44715228987534267\n",
      "18000/49000 loss: 0.32794025658076464\n",
      "20000/49000 loss: 0.4470790491390544\n",
      "22000/49000 loss: 0.4652856221790169\n",
      "24000/49000 loss: 0.3463660405702751\n",
      "26000/49000 loss: 0.3390177941443924\n",
      "28000/49000 loss: 0.3861345895229323\n",
      "30000/49000 loss: 0.4413542160596802\n",
      "32000/49000 loss: 0.42708675978611826\n",
      "34000/49000 loss: 0.3185107496991683\n",
      "36000/49000 loss: 0.423873259786615\n",
      "38000/49000 loss: 0.31710353337238495\n",
      "40000/49000 loss: 0.2961982667604082\n",
      "42000/49000 loss: 0.3657205944068705\n",
      "44000/49000 loss: 0.37605180101998337\n",
      "46000/49000 loss: 0.397246568254198\n",
      "48000/49000 loss: 0.35665682599406784\n",
      "epoch 7: valid acc = 0.855, new learning rate = 0.00034916864804687486\n",
      "2000/49000 loss: 0.36823981889300833\n",
      "4000/49000 loss: 0.40910633499082366\n",
      "6000/49000 loss: 0.36273861299428783\n",
      "8000/49000 loss: 0.37872068534399084\n",
      "10000/49000 loss: 0.38426538522606934\n",
      "12000/49000 loss: 0.39642527113328097\n",
      "14000/49000 loss: 0.3779240240180449\n",
      "16000/49000 loss: 0.3409016409273871\n",
      "18000/49000 loss: 0.38069697048740614\n",
      "20000/49000 loss: 0.3821322049178285\n",
      "22000/49000 loss: 0.4285994955763444\n",
      "24000/49000 loss: 0.4518017885817848\n",
      "26000/49000 loss: 0.393062228436402\n",
      "28000/49000 loss: 0.3813070625932082\n",
      "30000/49000 loss: 0.4030658316436876\n",
      "32000/49000 loss: 0.3036067606894138\n",
      "34000/49000 loss: 0.326326239132758\n",
      "36000/49000 loss: 0.3984917048657391\n",
      "38000/49000 loss: 0.34177216277888406\n",
      "40000/49000 loss: 0.4234014731055434\n",
      "42000/49000 loss: 0.42705637998448787\n",
      "44000/49000 loss: 0.38284572259824784\n",
      "46000/49000 loss: 0.34967516467680293\n",
      "48000/49000 loss: 0.37639540653021747\n",
      "epoch 8: valid acc = 0.87, new learning rate = 0.0003317102156445311\n",
      "2000/49000 loss: 0.32016005787959717\n",
      "4000/49000 loss: 0.33991840246591887\n",
      "6000/49000 loss: 0.37239033420599565\n",
      "8000/49000 loss: 0.3749594090789966\n",
      "10000/49000 loss: 0.35293145128885106\n",
      "12000/49000 loss: 0.38783259118198316\n",
      "14000/49000 loss: 0.42313136447471156\n",
      "16000/49000 loss: 0.3742261626990566\n",
      "18000/49000 loss: 0.3099545454453262\n",
      "20000/49000 loss: 0.2805911387277841\n",
      "22000/49000 loss: 0.456556240530118\n",
      "24000/49000 loss: 0.36543722632715203\n",
      "26000/49000 loss: 0.36610348065263165\n",
      "28000/49000 loss: 0.33196330691690745\n",
      "30000/49000 loss: 0.4167887885291439\n",
      "32000/49000 loss: 0.32020431399992577\n",
      "34000/49000 loss: 0.343824754064446\n",
      "36000/49000 loss: 0.34545137790304703\n",
      "38000/49000 loss: 0.3257547353455252\n",
      "40000/49000 loss: 0.3318152279056114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/49000 loss: 0.37378633305499404\n",
      "44000/49000 loss: 0.36676796356850655\n",
      "46000/49000 loss: 0.342244389472868\n",
      "48000/49000 loss: 0.31128865099810077\n",
      "epoch 9: valid acc = 0.872, new learning rate = 0.0003151247048623045\n",
      "2000/49000 loss: 0.3628392816000867\n",
      "4000/49000 loss: 0.40748546833723465\n",
      "6000/49000 loss: 0.32310886803892164\n",
      "8000/49000 loss: 0.35866690290100334\n",
      "10000/49000 loss: 0.35142991599074364\n",
      "12000/49000 loss: 0.3811317905777134\n",
      "14000/49000 loss: 0.4003084732351235\n",
      "16000/49000 loss: 0.29979770100828695\n",
      "18000/49000 loss: 0.38948657948600246\n",
      "20000/49000 loss: 0.39247861713953525\n",
      "22000/49000 loss: 0.42147518659639477\n",
      "24000/49000 loss: 0.41542296889058966\n",
      "26000/49000 loss: 0.4629530709730824\n",
      "28000/49000 loss: 0.43674107327256073\n",
      "30000/49000 loss: 0.4429241264179587\n",
      "32000/49000 loss: 0.3138663939532909\n",
      "34000/49000 loss: 0.38747657861961815\n",
      "36000/49000 loss: 0.3826814997703399\n",
      "38000/49000 loss: 0.23558975004097926\n",
      "40000/49000 loss: 0.3903497161995078\n",
      "42000/49000 loss: 0.40026161952814343\n",
      "44000/49000 loss: 0.3193750055803586\n",
      "46000/49000 loss: 0.45144278768120866\n",
      "48000/49000 loss: 0.3770277397668038\n",
      "epoch 10: valid acc = 0.877, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.8730204081632653\n",
      "test acc: 0.877\n",
      "test acc: 0.8534\n",
      "number of batches for training: 245\n",
      "epoch 1: valid acc = 0.739, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.806, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.83, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.828, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.853, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.853, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.861, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.868, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.869, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.877, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.8731836734693877\n",
      "test acc: 0.877\n",
      "test acc: 0.8556\n",
      "number of batches for training: 245\n",
      "2000/49000 loss: 2.659749340058532\n",
      "4000/49000 loss: 2.7038513658157566\n",
      "6000/49000 loss: 2.5512373660138508\n",
      "8000/49000 loss: 2.4840109637623677\n",
      "10000/49000 loss: 2.283374172304146\n",
      "12000/49000 loss: 2.101189905388171\n",
      "14000/49000 loss: 1.9905262084972504\n",
      "16000/49000 loss: 1.7475863069388966\n",
      "18000/49000 loss: 1.5300846631420577\n",
      "20000/49000 loss: 1.1983151862697794\n",
      "22000/49000 loss: 1.1436307627619595\n",
      "24000/49000 loss: 1.1338101999211543\n",
      "26000/49000 loss: 1.1181526458213584\n",
      "28000/49000 loss: 1.0821722115734629\n",
      "30000/49000 loss: 1.0125943479248436\n",
      "32000/49000 loss: 0.8319288706116722\n",
      "34000/49000 loss: 0.8881419481005451\n",
      "36000/49000 loss: 0.9026129040287998\n",
      "38000/49000 loss: 0.8971126993528671\n",
      "40000/49000 loss: 0.8150341290119596\n",
      "42000/49000 loss: 0.7371364703531529\n",
      "44000/49000 loss: 0.8823506120510815\n",
      "46000/49000 loss: 0.7414454946351354\n",
      "48000/49000 loss: 0.7572792796758767\n",
      "epoch 1: valid acc = 0.751, new learning rate = 0.000475\n",
      "2000/49000 loss: 0.7389416646611698\n",
      "4000/49000 loss: 0.5924177673113681\n",
      "6000/49000 loss: 0.5210782456438312\n",
      "8000/49000 loss: 0.5659313872790118\n",
      "10000/49000 loss: 0.6281888178294355\n",
      "12000/49000 loss: 0.5978474529996521\n",
      "14000/49000 loss: 0.568490998902938\n",
      "16000/49000 loss: 0.5779461205531834\n",
      "18000/49000 loss: 0.6605921822259984\n",
      "20000/49000 loss: 0.6224443584348183\n",
      "22000/49000 loss: 0.6931281682298468\n",
      "24000/49000 loss: 0.5890763277572763\n",
      "26000/49000 loss: 0.578630182119797\n",
      "28000/49000 loss: 0.5579812658464265\n",
      "30000/49000 loss: 0.5445164658365873\n",
      "32000/49000 loss: 0.5109226736391862\n",
      "34000/49000 loss: 0.5391532347404797\n",
      "36000/49000 loss: 0.6287939707302865\n",
      "38000/49000 loss: 0.6025933644380121\n",
      "40000/49000 loss: 0.581152817750887\n",
      "42000/49000 loss: 0.46517606451900295\n",
      "44000/49000 loss: 0.4873261049393674\n",
      "46000/49000 loss: 0.4661829999042047\n",
      "48000/49000 loss: 0.5732399959244433\n",
      "epoch 2: valid acc = 0.809, new learning rate = 0.00045125\n",
      "2000/49000 loss: 0.4682260223217688\n",
      "4000/49000 loss: 0.38801759032268024\n",
      "6000/49000 loss: 0.6016157190039818\n",
      "8000/49000 loss: 0.6039588098470052\n",
      "10000/49000 loss: 0.39709037818899884\n",
      "12000/49000 loss: 0.41920189826127924\n",
      "14000/49000 loss: 0.5252828206316276\n",
      "16000/49000 loss: 0.4780873262868573\n",
      "18000/49000 loss: 0.4622707390787471\n",
      "20000/49000 loss: 0.5156177351801132\n",
      "22000/49000 loss: 0.5459388917167963\n",
      "24000/49000 loss: 0.3973998313209236\n",
      "26000/49000 loss: 0.4970931431003443\n",
      "28000/49000 loss: 0.47106357173621144\n",
      "30000/49000 loss: 0.4505881060723416\n",
      "32000/49000 loss: 0.45890642030891055\n",
      "34000/49000 loss: 0.5308224491009518\n",
      "36000/49000 loss: 0.4024053760803863\n",
      "38000/49000 loss: 0.393200307048071\n",
      "40000/49000 loss: 0.37958982121101387\n",
      "42000/49000 loss: 0.4162608481839102\n",
      "44000/49000 loss: 0.382679903143369\n",
      "46000/49000 loss: 0.4155687203737715\n",
      "48000/49000 loss: 0.26098221146208833\n",
      "epoch 3: valid acc = 0.837, new learning rate = 0.0004286875\n",
      "2000/49000 loss: 0.49269059589353276\n",
      "4000/49000 loss: 0.4523538262370512\n",
      "6000/49000 loss: 0.3401516694141656\n",
      "8000/49000 loss: 0.5153079075062492\n",
      "10000/49000 loss: 0.3649620149671901\n",
      "12000/49000 loss: 0.5000043033572098\n",
      "14000/49000 loss: 0.49125624566182396\n",
      "16000/49000 loss: 0.33091275788313784\n",
      "18000/49000 loss: 0.38514590014560734\n",
      "20000/49000 loss: 0.4159459790539617\n",
      "22000/49000 loss: 0.5814364348556077\n",
      "24000/49000 loss: 0.43338398377934295\n",
      "26000/49000 loss: 0.38565151246595786\n",
      "28000/49000 loss: 0.5081539749796392\n",
      "30000/49000 loss: 0.510937789018489\n",
      "32000/49000 loss: 0.4328683372695142\n",
      "34000/49000 loss: 0.3925460243352116\n",
      "36000/49000 loss: 0.3917398898940014\n",
      "38000/49000 loss: 0.46762909594183\n",
      "40000/49000 loss: 0.42111885594996523\n",
      "42000/49000 loss: 0.4230607841083724\n",
      "44000/49000 loss: 0.2960797158296742\n",
      "46000/49000 loss: 0.3635716695914592\n",
      "48000/49000 loss: 0.5271156980783237\n",
      "epoch 4: valid acc = 0.839, new learning rate = 0.00040725312499999993\n",
      "2000/49000 loss: 0.524841335103498\n",
      "4000/49000 loss: 0.4099033586047672\n",
      "6000/49000 loss: 0.3697491457341257\n",
      "8000/49000 loss: 0.38041627134031614\n",
      "10000/49000 loss: 0.4882362844608153\n",
      "12000/49000 loss: 0.36475537480001796\n",
      "14000/49000 loss: 0.3001700586485603\n",
      "16000/49000 loss: 0.4797928882094367\n",
      "18000/49000 loss: 0.5142564144941101\n",
      "20000/49000 loss: 0.4407049800860455\n",
      "22000/49000 loss: 0.3206276827714983\n",
      "24000/49000 loss: 0.39096014379277344\n",
      "26000/49000 loss: 0.43550652885187513\n",
      "28000/49000 loss: 0.3746622060288841\n",
      "30000/49000 loss: 0.40546868354012794\n",
      "32000/49000 loss: 0.3955127059135458\n",
      "34000/49000 loss: 0.4052754796232767\n",
      "36000/49000 loss: 0.4317280723467168\n",
      "38000/49000 loss: 0.37972967404853397\n",
      "40000/49000 loss: 0.3911793449454917\n",
      "42000/49000 loss: 0.4943800552364909\n",
      "44000/49000 loss: 0.4564310413515187\n",
      "46000/49000 loss: 0.3311486775188305\n",
      "48000/49000 loss: 0.4029043001504647\n",
      "epoch 5: valid acc = 0.855, new learning rate = 0.0003868904687499999\n",
      "2000/49000 loss: 0.3689261773024748\n",
      "4000/49000 loss: 0.40489904468846133\n",
      "6000/49000 loss: 0.4761351388877387\n",
      "8000/49000 loss: 0.4199927725811519\n",
      "10000/49000 loss: 0.43985869026745517\n",
      "12000/49000 loss: 0.4187199229696342\n",
      "14000/49000 loss: 0.46131156581057076\n",
      "16000/49000 loss: 0.3830932944064059\n",
      "18000/49000 loss: 0.3901481315076424\n",
      "20000/49000 loss: 0.36969914088937855\n",
      "22000/49000 loss: 0.4232510605601462\n",
      "24000/49000 loss: 0.3996683141232192\n",
      "26000/49000 loss: 0.37419446782667254\n",
      "28000/49000 loss: 0.33756779955625377\n",
      "30000/49000 loss: 0.4455083157911961\n",
      "32000/49000 loss: 0.3536064934191326\n",
      "34000/49000 loss: 0.4599303326981495\n",
      "36000/49000 loss: 0.43882301610156993\n",
      "38000/49000 loss: 0.43827453302036734\n",
      "40000/49000 loss: 0.4671353673436988\n",
      "42000/49000 loss: 0.4070307871999177\n",
      "44000/49000 loss: 0.3393892154170114\n",
      "46000/49000 loss: 0.4010800505758386\n",
      "48000/49000 loss: 0.4395346215002316\n",
      "epoch 6: valid acc = 0.858, new learning rate = 0.0003675459453124999\n",
      "2000/49000 loss: 0.38233998235434397\n",
      "4000/49000 loss: 0.4004646228881249\n",
      "6000/49000 loss: 0.4109406437200777\n",
      "8000/49000 loss: 0.4274265672801184\n",
      "10000/49000 loss: 0.37530519852270094\n",
      "12000/49000 loss: 0.29811844486893496\n",
      "14000/49000 loss: 0.41705901747736657\n",
      "16000/49000 loss: 0.38130596996545874\n",
      "18000/49000 loss: 0.34774346710875487\n",
      "20000/49000 loss: 0.5011001928498302\n",
      "22000/49000 loss: 0.419821827932357\n",
      "24000/49000 loss: 0.3681563060890232\n",
      "26000/49000 loss: 0.3799835356145289\n",
      "28000/49000 loss: 0.3062785952863149\n",
      "30000/49000 loss: 0.35161544600331684\n",
      "32000/49000 loss: 0.368154058319431\n",
      "34000/49000 loss: 0.33091031648752345\n",
      "36000/49000 loss: 0.3577560561261522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38000/49000 loss: 0.28105102402359333\n",
      "40000/49000 loss: 0.36343602672309\n",
      "42000/49000 loss: 0.3628298488946769\n",
      "44000/49000 loss: 0.3724630767923716\n",
      "46000/49000 loss: 0.35755695039511426\n",
      "48000/49000 loss: 0.3078462640401599\n",
      "epoch 7: valid acc = 0.856, new learning rate = 0.00034916864804687486\n",
      "2000/49000 loss: 0.414559267523379\n",
      "4000/49000 loss: 0.25760534860040557\n",
      "6000/49000 loss: 0.3705277235989159\n",
      "8000/49000 loss: 0.4388758521870053\n",
      "10000/49000 loss: 0.32148049687137903\n",
      "12000/49000 loss: 0.2997450584887097\n",
      "14000/49000 loss: 0.4020452324008127\n",
      "16000/49000 loss: 0.3516047243253015\n",
      "18000/49000 loss: 0.3784355274510141\n",
      "20000/49000 loss: 0.46104634754941404\n",
      "22000/49000 loss: 0.3639279339622073\n",
      "24000/49000 loss: 0.4092217968666388\n",
      "26000/49000 loss: 0.37671945630377407\n",
      "28000/49000 loss: 0.3707322706813852\n",
      "30000/49000 loss: 0.28444124819866445\n",
      "32000/49000 loss: 0.372830315022912\n",
      "34000/49000 loss: 0.40156356146696714\n",
      "36000/49000 loss: 0.39419236678035163\n",
      "38000/49000 loss: 0.39947366728059075\n",
      "40000/49000 loss: 0.3850209621441262\n",
      "42000/49000 loss: 0.43266507055773623\n",
      "44000/49000 loss: 0.3905306482604249\n",
      "46000/49000 loss: 0.38856924302930207\n",
      "48000/49000 loss: 0.39327728877437595\n",
      "epoch 8: valid acc = 0.861, new learning rate = 0.0003317102156445311\n",
      "2000/49000 loss: 0.41498574612547134\n",
      "4000/49000 loss: 0.35866740428651855\n",
      "6000/49000 loss: 0.29367763371332484\n",
      "8000/49000 loss: 0.3617260211580801\n",
      "10000/49000 loss: 0.39287755410068487\n",
      "12000/49000 loss: 0.3117754866671651\n",
      "14000/49000 loss: 0.34530752363984896\n",
      "16000/49000 loss: 0.32980617655644745\n",
      "18000/49000 loss: 0.3356076721619309\n",
      "20000/49000 loss: 0.33153151567033917\n",
      "22000/49000 loss: 0.38303076537044883\n",
      "24000/49000 loss: 0.33752403289328065\n",
      "26000/49000 loss: 0.4169001991420241\n",
      "28000/49000 loss: 0.3296394818377472\n",
      "30000/49000 loss: 0.39795332783272463\n",
      "32000/49000 loss: 0.3869687273380107\n",
      "34000/49000 loss: 0.4872015916373836\n",
      "36000/49000 loss: 0.4035410019022822\n",
      "38000/49000 loss: 0.4620265135579516\n",
      "40000/49000 loss: 0.361847394336366\n",
      "42000/49000 loss: 0.5264239773325405\n",
      "44000/49000 loss: 0.4017356333454904\n",
      "46000/49000 loss: 0.3541161087037751\n",
      "48000/49000 loss: 0.3390849234344957\n",
      "epoch 9: valid acc = 0.874, new learning rate = 0.0003151247048623045\n",
      "2000/49000 loss: 0.3649007474792735\n",
      "4000/49000 loss: 0.3609043075180021\n",
      "6000/49000 loss: 0.38906305000100466\n",
      "8000/49000 loss: 0.3481337672658059\n",
      "10000/49000 loss: 0.3685919076709415\n",
      "12000/49000 loss: 0.42194730415212106\n",
      "14000/49000 loss: 0.3101754816705451\n",
      "16000/49000 loss: 0.3294592657541444\n",
      "18000/49000 loss: 0.4161164607589189\n",
      "20000/49000 loss: 0.3484450775793518\n",
      "22000/49000 loss: 0.37845800524656387\n",
      "24000/49000 loss: 0.322386786127848\n",
      "26000/49000 loss: 0.4475058399774154\n",
      "28000/49000 loss: 0.39412848296225994\n",
      "30000/49000 loss: 0.34129559574378954\n",
      "32000/49000 loss: 0.30360473273030375\n",
      "34000/49000 loss: 0.45863223249835894\n",
      "36000/49000 loss: 0.39235923956659824\n",
      "38000/49000 loss: 0.3242961666050637\n",
      "40000/49000 loss: 0.4319242558838877\n",
      "42000/49000 loss: 0.3166123789141776\n",
      "44000/49000 loss: 0.4223518929262148\n",
      "46000/49000 loss: 0.3834970430555184\n",
      "48000/49000 loss: 0.35500376196027533\n",
      "epoch 10: valid acc = 0.863, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.8721428571428571\n",
      "test acc: 0.863\n",
      "test acc: 0.8549\n",
      "number of batches for training: 245\n",
      "epoch 1: valid acc = 0.748, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.802, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.831, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.846, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.852, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.852, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.871, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.867, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.872, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.87, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.8738367346938776\n",
      "test acc: 0.87\n",
      "test acc: 0.8532\n",
      "number of batches for training: 245\n",
      "2000/49000 loss: 2.639551974759756\n",
      "4000/49000 loss: 2.617623822749883\n",
      "6000/49000 loss: 2.494478332192997\n",
      "8000/49000 loss: 2.423968057741961\n",
      "10000/49000 loss: 2.324494173175951\n",
      "12000/49000 loss: 2.2414765322122583\n",
      "14000/49000 loss: 2.048215475024336\n",
      "16000/49000 loss: 1.6933402098070636\n",
      "18000/49000 loss: 1.444427661720026\n",
      "20000/49000 loss: 1.1526517914472514\n",
      "22000/49000 loss: 1.1634127714438407\n",
      "24000/49000 loss: 1.0237942150310024\n",
      "26000/49000 loss: 1.021143483863614\n",
      "28000/49000 loss: 1.1130288657134249\n",
      "30000/49000 loss: 0.9469864623121181\n",
      "32000/49000 loss: 1.1051782149056735\n",
      "34000/49000 loss: 0.8503348992832279\n",
      "36000/49000 loss: 0.9075919095377407\n",
      "38000/49000 loss: 0.8146417712082323\n",
      "40000/49000 loss: 0.861232316825966\n",
      "42000/49000 loss: 0.7735879228367322\n",
      "44000/49000 loss: 0.7083737853500301\n",
      "46000/49000 loss: 0.8143708793375658\n",
      "48000/49000 loss: 0.7180373188534005\n",
      "epoch 1: valid acc = 0.755, new learning rate = 0.000475\n",
      "2000/49000 loss: 0.6804186432739802\n",
      "4000/49000 loss: 0.7615445353122257\n",
      "6000/49000 loss: 0.7855492438692165\n",
      "8000/49000 loss: 0.528529503824516\n",
      "10000/49000 loss: 0.6056882512508313\n",
      "12000/49000 loss: 0.5968076976540873\n",
      "14000/49000 loss: 0.5877419522206241\n",
      "16000/49000 loss: 0.5056682333615636\n",
      "18000/49000 loss: 0.5900116198412442\n",
      "20000/49000 loss: 0.594686494387785\n",
      "22000/49000 loss: 0.5691748896804666\n",
      "24000/49000 loss: 0.5714100229510904\n",
      "26000/49000 loss: 0.5353836290326257\n",
      "28000/49000 loss: 0.6253197555692118\n",
      "30000/49000 loss: 0.5963322585283439\n",
      "32000/49000 loss: 0.5794489218076633\n",
      "34000/49000 loss: 0.5507321937187499\n",
      "36000/49000 loss: 0.5501908506278291\n",
      "38000/49000 loss: 0.5759101930474606\n",
      "40000/49000 loss: 0.5176932296850972\n",
      "42000/49000 loss: 0.4718991120189864\n",
      "44000/49000 loss: 0.4285704524250724\n",
      "46000/49000 loss: 0.4588257113429644\n",
      "48000/49000 loss: 0.6116549903280379\n",
      "epoch 2: valid acc = 0.816, new learning rate = 0.00045125\n",
      "2000/49000 loss: 0.47655930990171386\n",
      "4000/49000 loss: 0.5672282146800777\n",
      "6000/49000 loss: 0.4698784757952943\n",
      "8000/49000 loss: 0.6179458147784364\n",
      "10000/49000 loss: 0.5184159622483242\n",
      "12000/49000 loss: 0.39143479091155536\n",
      "14000/49000 loss: 0.41903210214605074\n",
      "16000/49000 loss: 0.5523007279162995\n",
      "18000/49000 loss: 0.4183639271508079\n",
      "20000/49000 loss: 0.49407008431195004\n",
      "22000/49000 loss: 0.4686874078076125\n",
      "24000/49000 loss: 0.4792006630733588\n",
      "26000/49000 loss: 0.4435362496405395\n",
      "28000/49000 loss: 0.4998735699200864\n",
      "30000/49000 loss: 0.43464328893969706\n",
      "32000/49000 loss: 0.4824107166895391\n",
      "34000/49000 loss: 0.3695732288993386\n",
      "36000/49000 loss: 0.4899032566090384\n",
      "38000/49000 loss: 0.4305240987817379\n",
      "40000/49000 loss: 0.42198365296365764\n",
      "42000/49000 loss: 0.4594943486060473\n",
      "44000/49000 loss: 0.486965036105864\n",
      "46000/49000 loss: 0.5142800660642827\n",
      "48000/49000 loss: 0.5746790237838894\n",
      "epoch 3: valid acc = 0.84, new learning rate = 0.0004286875\n",
      "2000/49000 loss: 0.42304330765940173\n",
      "4000/49000 loss: 0.4404311871783559\n",
      "6000/49000 loss: 0.46417057700712894\n",
      "8000/49000 loss: 0.48970542187955685\n",
      "10000/49000 loss: 0.4727700484137229\n",
      "12000/49000 loss: 0.5214038271743257\n",
      "14000/49000 loss: 0.3655284444284992\n",
      "16000/49000 loss: 0.44231283499407736\n",
      "18000/49000 loss: 0.45213123950291806\n",
      "20000/49000 loss: 0.4077376355745517\n",
      "22000/49000 loss: 0.5703557493971538\n",
      "24000/49000 loss: 0.5042373409082905\n",
      "26000/49000 loss: 0.4844766062047945\n",
      "28000/49000 loss: 0.39298000328644206\n",
      "30000/49000 loss: 0.4556056507461065\n",
      "32000/49000 loss: 0.5134151023394116\n",
      "34000/49000 loss: 0.4597619155682814\n",
      "36000/49000 loss: 0.4460351029429705\n",
      "38000/49000 loss: 0.41496547007120216\n",
      "40000/49000 loss: 0.4296248335508605\n",
      "42000/49000 loss: 0.41106729669100556\n",
      "44000/49000 loss: 0.38048115675731825\n",
      "46000/49000 loss: 0.4951733196552344\n",
      "48000/49000 loss: 0.4168145239452055\n",
      "epoch 4: valid acc = 0.846, new learning rate = 0.00040725312499999993\n",
      "2000/49000 loss: 0.44032051090913615\n",
      "4000/49000 loss: 0.3589509905674982\n",
      "6000/49000 loss: 0.4743181113110179\n",
      "8000/49000 loss: 0.39515349442498215\n",
      "10000/49000 loss: 0.4400148707542389\n",
      "12000/49000 loss: 0.3942889838718519\n",
      "14000/49000 loss: 0.3747373138602076\n",
      "16000/49000 loss: 0.4348463561618835\n",
      "18000/49000 loss: 0.450373631985204\n",
      "20000/49000 loss: 0.35685911500401124\n",
      "22000/49000 loss: 0.3752005548342673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000/49000 loss: 0.4800629080213573\n",
      "26000/49000 loss: 0.42112079652990736\n",
      "28000/49000 loss: 0.4292728347657613\n",
      "30000/49000 loss: 0.4743683363701268\n",
      "32000/49000 loss: 0.36449614692251864\n",
      "34000/49000 loss: 0.4624441827822784\n",
      "36000/49000 loss: 0.2887644356573024\n",
      "38000/49000 loss: 0.4677073467739531\n",
      "40000/49000 loss: 0.46967525955134326\n",
      "42000/49000 loss: 0.4026739527524817\n",
      "44000/49000 loss: 0.42188911253777517\n",
      "46000/49000 loss: 0.4063234285996927\n",
      "48000/49000 loss: 0.395782054968993\n",
      "epoch 5: valid acc = 0.856, new learning rate = 0.0003868904687499999\n",
      "2000/49000 loss: 0.40129604539488517\n",
      "4000/49000 loss: 0.41476249377243296\n",
      "6000/49000 loss: 0.33770318332748045\n",
      "8000/49000 loss: 0.34811041910618007\n",
      "10000/49000 loss: 0.4905191452111587\n",
      "12000/49000 loss: 0.4030580479540777\n",
      "14000/49000 loss: 0.42974149440061005\n",
      "16000/49000 loss: 0.35986191595118205\n",
      "18000/49000 loss: 0.4688497289445583\n",
      "20000/49000 loss: 0.39769048556496644\n",
      "22000/49000 loss: 0.4096900695926644\n",
      "24000/49000 loss: 0.5262807959824044\n",
      "26000/49000 loss: 0.4068342164321626\n",
      "28000/49000 loss: 0.4072544733017418\n",
      "30000/49000 loss: 0.3961789762144311\n",
      "32000/49000 loss: 0.3800366163621763\n",
      "34000/49000 loss: 0.3721877932491422\n",
      "36000/49000 loss: 0.41542584734065147\n",
      "38000/49000 loss: 0.4769049388050791\n",
      "40000/49000 loss: 0.45249692285420984\n",
      "42000/49000 loss: 0.3432265397042927\n",
      "44000/49000 loss: 0.3843790345809813\n",
      "46000/49000 loss: 0.43776076667914243\n",
      "48000/49000 loss: 0.36595631244198745\n",
      "epoch 6: valid acc = 0.865, new learning rate = 0.0003675459453124999\n",
      "2000/49000 loss: 0.40597706546073964\n",
      "4000/49000 loss: 0.3759677534284146\n",
      "6000/49000 loss: 0.36177063011131194\n",
      "8000/49000 loss: 0.5338878291232314\n",
      "10000/49000 loss: 0.3857819096081056\n",
      "12000/49000 loss: 0.3817148759974811\n",
      "14000/49000 loss: 0.4203047718539886\n",
      "16000/49000 loss: 0.38800656974254166\n",
      "18000/49000 loss: 0.5302367646949362\n",
      "20000/49000 loss: 0.39738595398940396\n",
      "22000/49000 loss: 0.37570668979273597\n",
      "24000/49000 loss: 0.4270666825181257\n",
      "26000/49000 loss: 0.38690639793265724\n",
      "28000/49000 loss: 0.40792297940566913\n",
      "30000/49000 loss: 0.533728079701973\n",
      "32000/49000 loss: 0.42041464414043933\n",
      "34000/49000 loss: 0.40938424538971974\n",
      "36000/49000 loss: 0.4096515021609142\n",
      "38000/49000 loss: 0.3765128123912934\n",
      "40000/49000 loss: 0.39163248361312736\n",
      "42000/49000 loss: 0.5012227516769666\n",
      "44000/49000 loss: 0.4025115490685609\n",
      "46000/49000 loss: 0.3427695569161288\n",
      "48000/49000 loss: 0.3420845563828011\n",
      "epoch 7: valid acc = 0.854, new learning rate = 0.00034916864804687486\n",
      "2000/49000 loss: 0.2877103766780674\n",
      "4000/49000 loss: 0.3496080624636379\n",
      "6000/49000 loss: 0.43574123562748013\n",
      "8000/49000 loss: 0.42157185239811623\n",
      "10000/49000 loss: 0.38461002159429747\n",
      "12000/49000 loss: 0.3301154325379285\n",
      "14000/49000 loss: 0.4142846431204641\n",
      "16000/49000 loss: 0.32865913561336135\n",
      "18000/49000 loss: 0.33623640950564293\n",
      "20000/49000 loss: 0.2688729978165813\n",
      "22000/49000 loss: 0.3895363326976368\n",
      "24000/49000 loss: 0.3708734393367155\n",
      "26000/49000 loss: 0.5154287096590036\n",
      "28000/49000 loss: 0.37004966722646293\n",
      "30000/49000 loss: 0.37792083990458486\n",
      "32000/49000 loss: 0.3638328864858169\n",
      "34000/49000 loss: 0.3128114274868541\n",
      "36000/49000 loss: 0.35881922811150363\n",
      "38000/49000 loss: 0.37453814040703154\n",
      "40000/49000 loss: 0.35818352304788154\n",
      "42000/49000 loss: 0.3926682849958389\n",
      "44000/49000 loss: 0.39494982227491804\n",
      "46000/49000 loss: 0.33075180774131535\n",
      "48000/49000 loss: 0.47937345489542293\n",
      "epoch 8: valid acc = 0.866, new learning rate = 0.0003317102156445311\n",
      "2000/49000 loss: 0.33991201759583844\n",
      "4000/49000 loss: 0.3569320287270854\n",
      "6000/49000 loss: 0.3148180606597608\n",
      "8000/49000 loss: 0.3908725192395309\n",
      "10000/49000 loss: 0.4254360696475404\n",
      "12000/49000 loss: 0.2876133894764915\n",
      "14000/49000 loss: 0.43515530889835013\n",
      "16000/49000 loss: 0.33968287918260887\n",
      "18000/49000 loss: 0.2905855728760481\n",
      "20000/49000 loss: 0.4072407847433778\n",
      "22000/49000 loss: 0.37355778267404954\n",
      "24000/49000 loss: 0.3546909992890676\n",
      "26000/49000 loss: 0.4502453669990216\n",
      "28000/49000 loss: 0.45061619180066265\n",
      "30000/49000 loss: 0.3773438224139935\n",
      "32000/49000 loss: 0.34596604639982786\n",
      "34000/49000 loss: 0.39497956038586224\n",
      "36000/49000 loss: 0.32411750665401956\n",
      "38000/49000 loss: 0.30873942943975585\n",
      "40000/49000 loss: 0.355480885784154\n",
      "42000/49000 loss: 0.3161310365899365\n",
      "44000/49000 loss: 0.33765485603564455\n",
      "46000/49000 loss: 0.37297063374476813\n",
      "48000/49000 loss: 0.31853151021237625\n",
      "epoch 9: valid acc = 0.869, new learning rate = 0.0003151247048623045\n",
      "2000/49000 loss: 0.3580693036400116\n",
      "4000/49000 loss: 0.4017566444102981\n",
      "6000/49000 loss: 0.3659530878130612\n",
      "8000/49000 loss: 0.4041184195009574\n",
      "10000/49000 loss: 0.47835009184545735\n",
      "12000/49000 loss: 0.2711580864554249\n",
      "14000/49000 loss: 0.4100236417057713\n",
      "16000/49000 loss: 0.40587425380888\n",
      "18000/49000 loss: 0.39698681726401347\n",
      "20000/49000 loss: 0.2974556921258204\n",
      "22000/49000 loss: 0.34319804824187694\n",
      "24000/49000 loss: 0.3635827021813399\n",
      "26000/49000 loss: 0.36656153890890014\n",
      "28000/49000 loss: 0.31122193181116703\n",
      "30000/49000 loss: 0.33972956779827923\n",
      "32000/49000 loss: 0.3335729525464304\n",
      "34000/49000 loss: 0.3377856862584146\n",
      "36000/49000 loss: 0.32203735744719414\n",
      "38000/49000 loss: 0.22625877057487886\n",
      "40000/49000 loss: 0.45592020301181113\n",
      "42000/49000 loss: 0.49853925210707184\n",
      "44000/49000 loss: 0.4477222996697163\n",
      "46000/49000 loss: 0.28250133114885834\n",
      "48000/49000 loss: 0.38689081731744\n",
      "epoch 10: valid acc = 0.865, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.8729795918367347\n",
      "test acc: 0.865\n",
      "test acc: 0.8531\n",
      "number of batches for training: 245\n",
      "epoch 1: valid acc = 0.743, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.813, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.823, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.847, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.855, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.859, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.863, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.871, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.873, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.874, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.873734693877551\n",
      "test acc: 0.874\n",
      "test acc: 0.8519\n",
      "number of batches for training: 81\n",
      "6000/49000 loss: 2.6114901865053475\n",
      "12000/49000 loss: 2.604324191667568\n",
      "18000/49000 loss: 2.5036370773794303\n",
      "24000/49000 loss: 2.4421050306446457\n",
      "30000/49000 loss: 2.2528451632982587\n",
      "36000/49000 loss: 2.0769040781782757\n",
      "42000/49000 loss: 1.9605812078439413\n",
      "48000/49000 loss: 1.83642444203213\n",
      "epoch 1: valid acc = 0.429, new learning rate = 0.000475\n",
      "6000/49000 loss: 1.4434042438783619\n",
      "12000/49000 loss: 1.3052967423876152\n",
      "18000/49000 loss: 1.1940714960904963\n",
      "24000/49000 loss: 1.1537145308251595\n",
      "30000/49000 loss: 1.1477656234026543\n",
      "36000/49000 loss: 1.076983716841536\n",
      "42000/49000 loss: 1.0436365286779765\n",
      "48000/49000 loss: 0.9610201299564111\n",
      "epoch 2: valid acc = 0.672, new learning rate = 0.00045125\n",
      "6000/49000 loss: 0.9645918849033059\n",
      "12000/49000 loss: 0.9449851986803354\n",
      "18000/49000 loss: 0.7643394120855216\n",
      "24000/49000 loss: 0.8202704104025388\n",
      "30000/49000 loss: 0.825900533922225\n",
      "36000/49000 loss: 0.7800219730536857\n",
      "42000/49000 loss: 0.7573679505536413\n",
      "48000/49000 loss: 0.7504345891599606\n",
      "epoch 3: valid acc = 0.741, new learning rate = 0.0004286875\n",
      "6000/49000 loss: 0.7054120131017501\n",
      "12000/49000 loss: 0.6445684842339531\n",
      "18000/49000 loss: 0.7125915171882549\n",
      "24000/49000 loss: 0.7092833441940971\n",
      "30000/49000 loss: 0.6445490568687492\n",
      "36000/49000 loss: 0.6187991484910612\n",
      "42000/49000 loss: 0.6027442639910221\n",
      "48000/49000 loss: 0.5901493624189003\n",
      "epoch 4: valid acc = 0.767, new learning rate = 0.00040725312499999993\n",
      "6000/49000 loss: 0.6665847348297086\n",
      "12000/49000 loss: 0.5331500778343917\n",
      "18000/49000 loss: 0.6376908617467604\n",
      "24000/49000 loss: 0.6087685176107627\n",
      "30000/49000 loss: 0.5947982081047691\n",
      "36000/49000 loss: 0.4975455853830387\n",
      "42000/49000 loss: 0.560197053826845\n",
      "48000/49000 loss: 0.49656749848467957\n",
      "epoch 5: valid acc = 0.785, new learning rate = 0.0003868904687499999\n",
      "6000/49000 loss: 0.5709317915383559\n",
      "12000/49000 loss: 0.5553168810099325\n",
      "18000/49000 loss: 0.557398342140624\n",
      "24000/49000 loss: 0.5382696639862196\n",
      "30000/49000 loss: 0.6021392515773317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36000/49000 loss: 0.5639236535647646\n",
      "42000/49000 loss: 0.5270697862574034\n",
      "48000/49000 loss: 0.5436846951829868\n",
      "epoch 6: valid acc = 0.806, new learning rate = 0.0003675459453124999\n",
      "6000/49000 loss: 0.5168832807326\n",
      "12000/49000 loss: 0.5350164279824539\n",
      "18000/49000 loss: 0.5316973368919796\n",
      "24000/49000 loss: 0.5398169942074243\n",
      "30000/49000 loss: 0.5399951526653197\n",
      "36000/49000 loss: 0.5310554715144759\n",
      "42000/49000 loss: 0.49996771538809165\n",
      "48000/49000 loss: 0.5092839030729363\n",
      "epoch 7: valid acc = 0.817, new learning rate = 0.00034916864804687486\n",
      "6000/49000 loss: 0.5058446340184042\n",
      "12000/49000 loss: 0.527418961315107\n",
      "18000/49000 loss: 0.45355069982109825\n",
      "24000/49000 loss: 0.4953460689867282\n",
      "30000/49000 loss: 0.44911249807864767\n",
      "36000/49000 loss: 0.4885909203599581\n",
      "42000/49000 loss: 0.46542467962597384\n",
      "48000/49000 loss: 0.47244696502743233\n",
      "epoch 8: valid acc = 0.829, new learning rate = 0.0003317102156445311\n",
      "6000/49000 loss: 0.49732106820371536\n",
      "12000/49000 loss: 0.4894974767609034\n",
      "18000/49000 loss: 0.5138856683564934\n",
      "24000/49000 loss: 0.44060090245234484\n",
      "30000/49000 loss: 0.4617515163017605\n",
      "36000/49000 loss: 0.49688369867968724\n",
      "42000/49000 loss: 0.48668023207153116\n",
      "48000/49000 loss: 0.4712236152779621\n",
      "epoch 9: valid acc = 0.828, new learning rate = 0.0003151247048623045\n",
      "6000/49000 loss: 0.5314513403429962\n",
      "12000/49000 loss: 0.44135336400382646\n",
      "18000/49000 loss: 0.46544926086546\n",
      "24000/49000 loss: 0.4707618415283725\n",
      "30000/49000 loss: 0.45331074917233827\n",
      "36000/49000 loss: 0.4760734831564196\n",
      "42000/49000 loss: 0.4955860155691369\n",
      "48000/49000 loss: 0.4354166358109572\n",
      "epoch 10: valid acc = 0.839, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.837\n",
      "test acc: 0.839\n",
      "test acc: 0.822\n",
      "number of batches for training: 81\n",
      "epoch 1: valid acc = 0.387, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.67, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.746, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.75, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.776, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.798, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.813, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.819, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.83, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.835, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.8368367346938775\n",
      "test acc: 0.835\n",
      "test acc: 0.8197\n",
      "number of batches for training: 81\n",
      "6000/49000 loss: 2.662502941726539\n",
      "12000/49000 loss: 2.623131440551003\n",
      "18000/49000 loss: 2.5552024205438597\n",
      "24000/49000 loss: 2.4093270026073825\n",
      "30000/49000 loss: 2.301116875978971\n",
      "36000/49000 loss: 2.1559086955202424\n",
      "42000/49000 loss: 2.043391507186059\n",
      "48000/49000 loss: 1.7654445476012746\n",
      "epoch 1: valid acc = 0.431, new learning rate = 0.000475\n",
      "6000/49000 loss: 1.4520835486938066\n",
      "12000/49000 loss: 1.265973280126325\n",
      "18000/49000 loss: 1.2373542446917047\n",
      "24000/49000 loss: 1.1412467698400188\n",
      "30000/49000 loss: 1.0824635224905663\n",
      "36000/49000 loss: 1.1310624948593726\n",
      "42000/49000 loss: 1.0580620199425368\n",
      "48000/49000 loss: 0.9719913817222017\n",
      "epoch 2: valid acc = 0.671, new learning rate = 0.00045125\n",
      "6000/49000 loss: 0.9167978452685972\n",
      "12000/49000 loss: 0.882313208742931\n",
      "18000/49000 loss: 0.8718187933518152\n",
      "24000/49000 loss: 0.8753754527069825\n",
      "30000/49000 loss: 0.7795210949153851\n",
      "36000/49000 loss: 0.7431114847321878\n",
      "42000/49000 loss: 0.7467923613443362\n",
      "48000/49000 loss: 0.7633292670016\n",
      "epoch 3: valid acc = 0.736, new learning rate = 0.0004286875\n",
      "6000/49000 loss: 0.7109411827384782\n",
      "12000/49000 loss: 0.7262219231777618\n",
      "18000/49000 loss: 0.7296936650274104\n",
      "24000/49000 loss: 0.6964837612573415\n",
      "30000/49000 loss: 0.659130796635387\n",
      "36000/49000 loss: 0.6177871737935804\n",
      "42000/49000 loss: 0.6100376051900942\n",
      "48000/49000 loss: 0.6479988040230735\n",
      "epoch 4: valid acc = 0.771, new learning rate = 0.00040725312499999993\n",
      "6000/49000 loss: 0.5707102478984735\n",
      "12000/49000 loss: 0.5597099882031364\n",
      "18000/49000 loss: 0.6188117331746334\n",
      "24000/49000 loss: 0.6573536776459701\n",
      "30000/49000 loss: 0.6053872456919878\n",
      "36000/49000 loss: 0.5996831787525241\n",
      "42000/49000 loss: 0.5329789517851673\n",
      "48000/49000 loss: 0.6002665670247707\n",
      "epoch 5: valid acc = 0.786, new learning rate = 0.0003868904687499999\n",
      "6000/49000 loss: 0.5681574564024651\n",
      "12000/49000 loss: 0.5702649452506908\n",
      "18000/49000 loss: 0.5860384703461496\n",
      "24000/49000 loss: 0.5053171248121835\n",
      "30000/49000 loss: 0.5452237329237618\n",
      "36000/49000 loss: 0.6208856059988078\n",
      "42000/49000 loss: 0.5156375408643499\n",
      "48000/49000 loss: 0.5717674581123849\n",
      "epoch 6: valid acc = 0.811, new learning rate = 0.0003675459453124999\n",
      "6000/49000 loss: 0.5357954327203801\n",
      "12000/49000 loss: 0.4946809691896536\n",
      "18000/49000 loss: 0.5553369537723645\n",
      "24000/49000 loss: 0.5650410417126581\n",
      "30000/49000 loss: 0.5143900755819609\n",
      "36000/49000 loss: 0.4949914729778954\n",
      "42000/49000 loss: 0.5458318939191901\n",
      "48000/49000 loss: 0.4375510912796603\n",
      "epoch 7: valid acc = 0.818, new learning rate = 0.00034916864804687486\n",
      "6000/49000 loss: 0.5376957339855285\n",
      "12000/49000 loss: 0.5159594874449056\n",
      "18000/49000 loss: 0.5001286829358412\n",
      "24000/49000 loss: 0.48508893068346054\n",
      "30000/49000 loss: 0.5341750516348659\n",
      "36000/49000 loss: 0.4748286254644276\n",
      "42000/49000 loss: 0.5014535534150187\n",
      "48000/49000 loss: 0.5213093711769672\n",
      "epoch 8: valid acc = 0.824, new learning rate = 0.0003317102156445311\n",
      "6000/49000 loss: 0.48954048042511283\n",
      "12000/49000 loss: 0.5293754770951606\n",
      "18000/49000 loss: 0.46626600655022854\n",
      "24000/49000 loss: 0.44619957086852363\n",
      "30000/49000 loss: 0.4746122082668788\n",
      "36000/49000 loss: 0.5405768468664487\n",
      "42000/49000 loss: 0.43898595005282126\n",
      "48000/49000 loss: 0.49285669309552127\n",
      "epoch 9: valid acc = 0.832, new learning rate = 0.0003151247048623045\n",
      "6000/49000 loss: 0.46940625804265645\n",
      "12000/49000 loss: 0.4491134562471665\n",
      "18000/49000 loss: 0.45490695548703247\n",
      "24000/49000 loss: 0.42553637936205024\n",
      "30000/49000 loss: 0.47133914254787157\n",
      "36000/49000 loss: 0.5109055196705181\n",
      "42000/49000 loss: 0.47554538787682155\n",
      "48000/49000 loss: 0.40449638128267873\n",
      "epoch 10: valid acc = 0.833, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.838265306122449\n",
      "test acc: 0.833\n",
      "test acc: 0.8245\n",
      "number of batches for training: 81\n",
      "epoch 1: valid acc = 0.513, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.666, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.739, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.768, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.783, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.795, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.814, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.817, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.826, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.831, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.8380204081632653\n",
      "test acc: 0.831\n",
      "test acc: 0.8229\n",
      "number of batches for training: 81\n",
      "6000/49000 loss: 2.6721505380785904\n",
      "12000/49000 loss: 2.6102503399381662\n",
      "18000/49000 loss: 2.5282008881695086\n",
      "24000/49000 loss: 2.4217550720602903\n",
      "30000/49000 loss: 2.1701647722285258\n",
      "36000/49000 loss: 2.0981038398555714\n",
      "42000/49000 loss: 1.8754537804066924\n",
      "48000/49000 loss: 1.6400211286649227\n",
      "epoch 1: valid acc = 0.472, new learning rate = 0.000475\n",
      "6000/49000 loss: 1.4073226452156873\n",
      "12000/49000 loss: 1.2519648076317917\n",
      "18000/49000 loss: 1.115441427850012\n",
      "24000/49000 loss: 1.1630382472426262\n",
      "30000/49000 loss: 1.1877317223679984\n",
      "36000/49000 loss: 1.1834999352360835\n",
      "42000/49000 loss: 1.0012167432192214\n",
      "48000/49000 loss: 1.0041950112392108\n",
      "epoch 2: valid acc = 0.644, new learning rate = 0.00045125\n",
      "6000/49000 loss: 0.9013429490042356\n",
      "12000/49000 loss: 0.9643502556586384\n",
      "18000/49000 loss: 0.8646567257585944\n",
      "24000/49000 loss: 0.848282359918269\n",
      "30000/49000 loss: 0.8785279649510745\n",
      "36000/49000 loss: 0.8178051029913682\n",
      "42000/49000 loss: 0.8017376866840455\n",
      "48000/49000 loss: 0.7457240836965227\n",
      "epoch 3: valid acc = 0.736, new learning rate = 0.0004286875\n",
      "6000/49000 loss: 0.6218318336846432\n",
      "12000/49000 loss: 0.7256138623568571\n",
      "18000/49000 loss: 0.66492660412804\n",
      "24000/49000 loss: 0.6551931938222418\n",
      "30000/49000 loss: 0.5889460597919844\n",
      "36000/49000 loss: 0.6100721316676224\n",
      "42000/49000 loss: 0.6002267707683642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/49000 loss: 0.6276192728531582\n",
      "epoch 4: valid acc = 0.766, new learning rate = 0.00040725312499999993\n",
      "6000/49000 loss: 0.5854883308878285\n",
      "12000/49000 loss: 0.5922033345146891\n",
      "18000/49000 loss: 0.6183662455912416\n",
      "24000/49000 loss: 0.6488516176964705\n",
      "30000/49000 loss: 0.5699012906156685\n",
      "36000/49000 loss: 0.5994137836370541\n",
      "42000/49000 loss: 0.5460570116649726\n",
      "48000/49000 loss: 0.616282124632062\n",
      "epoch 5: valid acc = 0.785, new learning rate = 0.0003868904687499999\n",
      "6000/49000 loss: 0.5587797421775503\n",
      "12000/49000 loss: 0.5598004984276477\n",
      "18000/49000 loss: 0.550974492341463\n",
      "24000/49000 loss: 0.638132417906512\n",
      "30000/49000 loss: 0.5729644009603996\n",
      "36000/49000 loss: 0.5818409543053995\n",
      "42000/49000 loss: 0.5498762954174213\n",
      "48000/49000 loss: 0.535958958886622\n",
      "epoch 6: valid acc = 0.801, new learning rate = 0.0003675459453124999\n",
      "6000/49000 loss: 0.5590036522702374\n",
      "12000/49000 loss: 0.49465720352524245\n",
      "18000/49000 loss: 0.544046921789734\n",
      "24000/49000 loss: 0.46103064499618723\n",
      "30000/49000 loss: 0.507666369959277\n",
      "36000/49000 loss: 0.48863698956568563\n",
      "42000/49000 loss: 0.49358764891938667\n",
      "48000/49000 loss: 0.539899135344038\n",
      "epoch 7: valid acc = 0.816, new learning rate = 0.00034916864804687486\n",
      "6000/49000 loss: 0.5472299146982119\n",
      "12000/49000 loss: 0.5502615387065151\n",
      "18000/49000 loss: 0.596646760869546\n",
      "24000/49000 loss: 0.601354914142418\n",
      "30000/49000 loss: 0.4924590394395842\n",
      "36000/49000 loss: 0.4551038295948354\n",
      "42000/49000 loss: 0.5047675185611339\n",
      "48000/49000 loss: 0.5026732298983377\n",
      "epoch 8: valid acc = 0.818, new learning rate = 0.0003317102156445311\n",
      "6000/49000 loss: 0.5186999643821691\n",
      "12000/49000 loss: 0.47776189378592626\n",
      "18000/49000 loss: 0.43913616679680567\n",
      "24000/49000 loss: 0.44197668816195396\n",
      "30000/49000 loss: 0.5372667827987899\n",
      "36000/49000 loss: 0.49064476668250073\n",
      "42000/49000 loss: 0.3966794723800953\n",
      "48000/49000 loss: 0.5263755027382458\n",
      "epoch 9: valid acc = 0.829, new learning rate = 0.0003151247048623045\n",
      "6000/49000 loss: 0.5162138389687344\n",
      "12000/49000 loss: 0.45070221307972613\n",
      "18000/49000 loss: 0.5140487864000849\n",
      "24000/49000 loss: 0.4655150157671808\n",
      "30000/49000 loss: 0.5168131084197198\n",
      "36000/49000 loss: 0.4869516588517787\n",
      "42000/49000 loss: 0.44587146778122705\n",
      "48000/49000 loss: 0.4595534400226372\n",
      "epoch 10: valid acc = 0.836, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.8369591836734694\n",
      "test acc: 0.836\n",
      "test acc: 0.823\n",
      "number of batches for training: 81\n",
      "epoch 1: valid acc = 0.483, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.667, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.748, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.763, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.777, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.794, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.808, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.817, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.825, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.835, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.8367551020408164\n",
      "test acc: 0.835\n",
      "test acc: 0.8226\n",
      "number of batches for training: 49\n",
      "10000/49000 loss: 2.6394113497890554\n",
      "20000/49000 loss: 2.598090464533096\n",
      "30000/49000 loss: 2.562284967054509\n",
      "40000/49000 loss: 2.4287725426929963\n",
      "epoch 1: valid acc = 0.36, new learning rate = 0.000475\n",
      "10000/49000 loss: 2.112082670258411\n",
      "20000/49000 loss: 2.0471125447093574\n",
      "30000/49000 loss: 1.8519674056163644\n",
      "40000/49000 loss: 1.6089751798869731\n",
      "epoch 2: valid acc = 0.521, new learning rate = 0.00045125\n",
      "10000/49000 loss: 1.2841322191283824\n",
      "20000/49000 loss: 1.2311662778586958\n",
      "30000/49000 loss: 1.1289749068150126\n",
      "40000/49000 loss: 1.095487297224542\n",
      "epoch 3: valid acc = 0.632, new learning rate = 0.0004286875\n",
      "10000/49000 loss: 0.9738638041679426\n",
      "20000/49000 loss: 0.9814812861515297\n",
      "30000/49000 loss: 0.9468439271996681\n",
      "40000/49000 loss: 0.914758056466488\n",
      "epoch 4: valid acc = 0.705, new learning rate = 0.00040725312499999993\n",
      "10000/49000 loss: 0.8764986679991972\n",
      "20000/49000 loss: 0.7782615672899565\n",
      "30000/49000 loss: 0.785626162526277\n",
      "40000/49000 loss: 0.737612304569054\n",
      "epoch 5: valid acc = 0.738, new learning rate = 0.0003868904687499999\n",
      "10000/49000 loss: 0.7555731452913491\n",
      "20000/49000 loss: 0.6831979750568338\n",
      "30000/49000 loss: 0.6787239307633698\n",
      "40000/49000 loss: 0.6954187974569968\n",
      "epoch 6: valid acc = 0.753, new learning rate = 0.0003675459453124999\n",
      "10000/49000 loss: 0.6620807877905394\n",
      "20000/49000 loss: 0.6411305969394171\n",
      "30000/49000 loss: 0.6270248975506503\n",
      "40000/49000 loss: 0.6415527270036209\n",
      "epoch 7: valid acc = 0.765, new learning rate = 0.00034916864804687486\n",
      "10000/49000 loss: 0.6392237841096802\n",
      "20000/49000 loss: 0.6279470825052715\n",
      "30000/49000 loss: 0.6090785464830961\n",
      "40000/49000 loss: 0.5632125827808129\n",
      "epoch 8: valid acc = 0.779, new learning rate = 0.0003317102156445311\n",
      "10000/49000 loss: 0.5825685073257639\n",
      "20000/49000 loss: 0.6223923603456496\n",
      "30000/49000 loss: 0.5849820962095031\n",
      "40000/49000 loss: 0.5468414773497303\n",
      "epoch 9: valid acc = 0.786, new learning rate = 0.0003151247048623045\n",
      "10000/49000 loss: 0.5446332256194121\n",
      "20000/49000 loss: 0.5425236104274495\n",
      "30000/49000 loss: 0.5365759978129532\n",
      "40000/49000 loss: 0.5720653851765551\n",
      "epoch 10: valid acc = 0.794, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.7975714285714286\n",
      "test acc: 0.794\n",
      "test acc: 0.7903\n",
      "number of batches for training: 49\n",
      "epoch 1: valid acc = 0.364, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.516, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.637, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.714, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.744, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.754, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.768, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.78, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.789, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.795, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.8005714285714286\n",
      "test acc: 0.795\n",
      "test acc: 0.7894\n",
      "number of batches for training: 49\n",
      "10000/49000 loss: 2.6381440198353476\n",
      "20000/49000 loss: 2.6252372232040666\n",
      "30000/49000 loss: 2.581342787947258\n",
      "40000/49000 loss: 2.448667567913014\n",
      "epoch 1: valid acc = 0.392, new learning rate = 0.000475\n",
      "10000/49000 loss: 2.149524678377988\n",
      "20000/49000 loss: 1.9824067156514569\n",
      "30000/49000 loss: 1.7846259564087132\n",
      "40000/49000 loss: 1.6083923513634903\n",
      "epoch 2: valid acc = 0.516, new learning rate = 0.00045125\n",
      "10000/49000 loss: 1.2444877213527192\n",
      "20000/49000 loss: 1.2133687797123707\n",
      "30000/49000 loss: 1.1486692665950227\n",
      "40000/49000 loss: 1.1119724636890989\n",
      "epoch 3: valid acc = 0.652, new learning rate = 0.0004286875\n",
      "10000/49000 loss: 1.007151058578719\n",
      "20000/49000 loss: 0.9998212324329749\n",
      "30000/49000 loss: 0.9591398753653173\n",
      "40000/49000 loss: 0.9459436420181827\n",
      "epoch 4: valid acc = 0.705, new learning rate = 0.00040725312499999993\n",
      "10000/49000 loss: 0.8027148203278962\n",
      "20000/49000 loss: 0.8585615727291459\n",
      "30000/49000 loss: 0.8040173729178541\n",
      "40000/49000 loss: 0.7738033659021557\n",
      "epoch 5: valid acc = 0.734, new learning rate = 0.0003868904687499999\n",
      "10000/49000 loss: 0.6658091066668973\n",
      "20000/49000 loss: 0.7103067184370911\n",
      "30000/49000 loss: 0.7383788674695088\n",
      "40000/49000 loss: 0.6532310305539599\n",
      "epoch 6: valid acc = 0.75, new learning rate = 0.0003675459453124999\n",
      "10000/49000 loss: 0.6665453611855022\n",
      "20000/49000 loss: 0.6690193847544681\n",
      "30000/49000 loss: 0.6352628489652931\n",
      "40000/49000 loss: 0.6848143221659867\n",
      "epoch 7: valid acc = 0.765, new learning rate = 0.00034916864804687486\n",
      "10000/49000 loss: 0.6420295665565405\n",
      "20000/49000 loss: 0.5913153783516238\n",
      "30000/49000 loss: 0.5826116718148678\n",
      "40000/49000 loss: 0.638616448268264\n",
      "epoch 8: valid acc = 0.773, new learning rate = 0.0003317102156445311\n",
      "10000/49000 loss: 0.5665067565450772\n",
      "20000/49000 loss: 0.5835417196970912\n",
      "30000/49000 loss: 0.5697794983548577\n",
      "40000/49000 loss: 0.5819524863592316\n",
      "epoch 9: valid acc = 0.794, new learning rate = 0.0003151247048623045\n",
      "10000/49000 loss: 0.586664796854101\n",
      "20000/49000 loss: 0.5656013564909043\n",
      "30000/49000 loss: 0.5537019040918545\n",
      "40000/49000 loss: 0.5232412140531351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10: valid acc = 0.796, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.8002448979591836\n",
      "test acc: 0.796\n",
      "test acc: 0.7905\n",
      "number of batches for training: 49\n",
      "epoch 1: valid acc = 0.39, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.512, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.585, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.703, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.729, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.744, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.758, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.778, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.785, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.792, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.7978571428571428\n",
      "test acc: 0.792\n",
      "test acc: 0.7887\n",
      "number of batches for training: 49\n",
      "10000/49000 loss: 2.656814330914043\n",
      "20000/49000 loss: 2.619846389924991\n",
      "30000/49000 loss: 2.546997946462856\n",
      "40000/49000 loss: 2.486294897694585\n",
      "epoch 1: valid acc = 0.378, new learning rate = 0.000475\n",
      "10000/49000 loss: 2.1139677191698256\n",
      "20000/49000 loss: 2.0355194271580963\n",
      "30000/49000 loss: 1.8181036857519037\n",
      "40000/49000 loss: 1.4440469434805996\n",
      "epoch 2: valid acc = 0.524, new learning rate = 0.00045125\n",
      "10000/49000 loss: 1.221223169929114\n",
      "20000/49000 loss: 1.1708860526524725\n",
      "30000/49000 loss: 1.176905616786182\n",
      "40000/49000 loss: 1.085232063128017\n",
      "epoch 3: valid acc = 0.613, new learning rate = 0.0004286875\n",
      "10000/49000 loss: 1.0426382945242372\n",
      "20000/49000 loss: 1.0226303019212641\n",
      "30000/49000 loss: 0.9309490472849004\n",
      "40000/49000 loss: 0.8882086526420823\n",
      "epoch 4: valid acc = 0.718, new learning rate = 0.00040725312499999993\n",
      "10000/49000 loss: 0.8619432690431417\n",
      "20000/49000 loss: 0.8114341123681226\n",
      "30000/49000 loss: 0.7741115569778034\n",
      "40000/49000 loss: 0.7859335620650051\n",
      "epoch 5: valid acc = 0.732, new learning rate = 0.0003868904687499999\n",
      "10000/49000 loss: 0.7520842545077849\n",
      "20000/49000 loss: 0.7062611300844055\n",
      "30000/49000 loss: 0.7154127071987041\n",
      "40000/49000 loss: 0.6675296500580111\n",
      "epoch 6: valid acc = 0.751, new learning rate = 0.0003675459453124999\n",
      "10000/49000 loss: 0.6913597138731753\n",
      "20000/49000 loss: 0.6994216777418002\n",
      "30000/49000 loss: 0.6946263535920841\n",
      "40000/49000 loss: 0.6412361176799761\n",
      "epoch 7: valid acc = 0.758, new learning rate = 0.00034916864804687486\n",
      "10000/49000 loss: 0.6017385806582303\n",
      "20000/49000 loss: 0.6461732951166641\n",
      "30000/49000 loss: 0.5957637758117603\n",
      "40000/49000 loss: 0.6215264397658945\n",
      "epoch 8: valid acc = 0.78, new learning rate = 0.0003317102156445311\n",
      "10000/49000 loss: 0.6120761731733235\n",
      "20000/49000 loss: 0.6174812062978992\n",
      "30000/49000 loss: 0.5858649564653448\n",
      "40000/49000 loss: 0.5880438556592789\n",
      "epoch 9: valid acc = 0.785, new learning rate = 0.0003151247048623045\n",
      "10000/49000 loss: 0.5351387822856087\n",
      "20000/49000 loss: 0.5344367670911837\n",
      "30000/49000 loss: 0.5739006196660378\n",
      "40000/49000 loss: 0.5895910485109699\n",
      "epoch 10: valid acc = 0.803, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.8008367346938775\n",
      "test acc: 0.803\n",
      "test acc: 0.7894\n",
      "number of batches for training: 49\n",
      "epoch 1: valid acc = 0.386, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.548, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.643, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.707, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.738, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.74, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.758, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.773, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.782, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.786, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.7980408163265306\n",
      "test acc: 0.786\n",
      "test acc: 0.7883\n",
      "number of batches for training: 245\n",
      "2000/49000 loss: 2.7112860415234703\n",
      "4000/49000 loss: 2.6109823331079736\n",
      "6000/49000 loss: 2.650472736060762\n",
      "8000/49000 loss: 2.5073914074387247\n",
      "10000/49000 loss: 2.315766679367069\n",
      "12000/49000 loss: 2.1525039634188228\n",
      "14000/49000 loss: 1.9816467404577194\n",
      "16000/49000 loss: 1.7916329670630329\n",
      "18000/49000 loss: 1.4499535592908603\n",
      "20000/49000 loss: 1.369004489609098\n",
      "22000/49000 loss: 1.2785044030392234\n",
      "24000/49000 loss: 1.191569835249121\n",
      "26000/49000 loss: 1.0989531355242415\n",
      "28000/49000 loss: 0.9869577699088239\n",
      "30000/49000 loss: 0.9721463501727432\n",
      "32000/49000 loss: 1.0052973320221656\n",
      "34000/49000 loss: 1.0225139106871493\n",
      "36000/49000 loss: 0.9688556892045849\n",
      "38000/49000 loss: 0.9256300673986276\n",
      "40000/49000 loss: 0.8836314356611887\n",
      "42000/49000 loss: 0.7335176487511839\n",
      "44000/49000 loss: 0.9017488376436877\n",
      "46000/49000 loss: 0.7062648645123449\n",
      "48000/49000 loss: 0.7902854762176932\n",
      "epoch 1: valid acc = 0.744, new learning rate = 0.000475\n",
      "2000/49000 loss: 0.6817989643048981\n",
      "4000/49000 loss: 0.7310479448072176\n",
      "6000/49000 loss: 0.6425734917231118\n",
      "8000/49000 loss: 0.6587955085853785\n",
      "10000/49000 loss: 0.6658001240037987\n",
      "12000/49000 loss: 0.6200742478023696\n",
      "14000/49000 loss: 0.6127753850818585\n",
      "16000/49000 loss: 0.6777395256273376\n",
      "18000/49000 loss: 0.5837416578215031\n",
      "20000/49000 loss: 0.5971973217496286\n",
      "22000/49000 loss: 0.6100039168111823\n",
      "24000/49000 loss: 0.5738757941705472\n",
      "26000/49000 loss: 0.630779088148384\n",
      "28000/49000 loss: 0.5750312416305662\n",
      "30000/49000 loss: 0.4626913282721937\n",
      "32000/49000 loss: 0.6721764061811342\n",
      "34000/49000 loss: 0.4960107551433939\n",
      "36000/49000 loss: 0.5662125065396304\n",
      "38000/49000 loss: 0.5609848684358569\n",
      "40000/49000 loss: 0.5760781399480188\n",
      "42000/49000 loss: 0.6139827766000441\n",
      "44000/49000 loss: 0.5172847703947424\n",
      "46000/49000 loss: 0.5179050995467919\n",
      "48000/49000 loss: 0.46429716935201887\n",
      "epoch 2: valid acc = 0.814, new learning rate = 0.00045125\n",
      "2000/49000 loss: 0.5186624653730643\n",
      "4000/49000 loss: 0.5321017716482218\n",
      "6000/49000 loss: 0.5696568155362597\n",
      "8000/49000 loss: 0.5123621653365436\n",
      "10000/49000 loss: 0.43760138121600034\n",
      "12000/49000 loss: 0.4769474005671336\n",
      "14000/49000 loss: 0.4688286979969549\n",
      "16000/49000 loss: 0.6390523494855305\n",
      "18000/49000 loss: 0.4258959248956239\n",
      "20000/49000 loss: 0.3884639082351917\n",
      "22000/49000 loss: 0.52821561275961\n",
      "24000/49000 loss: 0.5115242463105935\n",
      "26000/49000 loss: 0.3501449781855625\n",
      "28000/49000 loss: 0.5320958677777483\n",
      "30000/49000 loss: 0.4486285694504069\n",
      "32000/49000 loss: 0.4508365464643703\n",
      "34000/49000 loss: 0.5414729906369554\n",
      "36000/49000 loss: 0.5465591268064995\n",
      "38000/49000 loss: 0.48542411164040145\n",
      "40000/49000 loss: 0.4021749972548805\n",
      "42000/49000 loss: 0.46701196265980244\n",
      "44000/49000 loss: 0.5527059525544213\n",
      "46000/49000 loss: 0.5257782281349216\n",
      "48000/49000 loss: 0.3992239993263455\n",
      "epoch 3: valid acc = 0.829, new learning rate = 0.0004286875\n",
      "2000/49000 loss: 0.5406689115136344\n",
      "4000/49000 loss: 0.34981676524037786\n",
      "6000/49000 loss: 0.44784614953364377\n",
      "8000/49000 loss: 0.3694705464965363\n",
      "10000/49000 loss: 0.4009648936164595\n",
      "12000/49000 loss: 0.37682545667386863\n",
      "14000/49000 loss: 0.3892727328167879\n",
      "16000/49000 loss: 0.5089415290494358\n",
      "18000/49000 loss: 0.48617412103539315\n",
      "20000/49000 loss: 0.5371396701293311\n",
      "22000/49000 loss: 0.5303138717828006\n",
      "24000/49000 loss: 0.4890951381801133\n",
      "26000/49000 loss: 0.39042603754682487\n",
      "28000/49000 loss: 0.3641911062281452\n",
      "30000/49000 loss: 0.39910860711788987\n",
      "32000/49000 loss: 0.4677947122171165\n",
      "34000/49000 loss: 0.43296833426248454\n",
      "36000/49000 loss: 0.3388306578126658\n",
      "38000/49000 loss: 0.41508740694836516\n",
      "40000/49000 loss: 0.49262145469149055\n",
      "42000/49000 loss: 0.46425084766340846\n",
      "44000/49000 loss: 0.3928209664248962\n",
      "46000/49000 loss: 0.4483682469256795\n",
      "48000/49000 loss: 0.48407091962951865\n",
      "epoch 4: valid acc = 0.842, new learning rate = 0.00040725312499999993\n",
      "2000/49000 loss: 0.372502547373582\n",
      "4000/49000 loss: 0.4423626483007351\n",
      "6000/49000 loss: 0.3160478456854501\n",
      "8000/49000 loss: 0.36075822833405075\n",
      "10000/49000 loss: 0.4347595086903373\n",
      "12000/49000 loss: 0.48350868976875844\n",
      "14000/49000 loss: 0.43647882868322085\n",
      "16000/49000 loss: 0.4122023372594833\n",
      "18000/49000 loss: 0.42624502474189546\n",
      "20000/49000 loss: 0.4459747028027032\n",
      "22000/49000 loss: 0.3707907172797094\n",
      "24000/49000 loss: 0.39037277140055454\n",
      "26000/49000 loss: 0.4120873440083888\n",
      "28000/49000 loss: 0.3972185364372217\n",
      "30000/49000 loss: 0.3205086816210967\n",
      "32000/49000 loss: 0.3770147497855586\n",
      "34000/49000 loss: 0.46832481595891284\n",
      "36000/49000 loss: 0.42195240842262577\n",
      "38000/49000 loss: 0.4288121867102067\n",
      "40000/49000 loss: 0.5416172845087042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/49000 loss: 0.3244879822784691\n",
      "44000/49000 loss: 0.4076165431417475\n",
      "46000/49000 loss: 0.32590233437795124\n",
      "48000/49000 loss: 0.4045347040047097\n",
      "epoch 5: valid acc = 0.847, new learning rate = 0.0003868904687499999\n",
      "2000/49000 loss: 0.4311866979755552\n",
      "4000/49000 loss: 0.4122650774704047\n",
      "6000/49000 loss: 0.445056172521562\n",
      "8000/49000 loss: 0.35498211246016\n",
      "10000/49000 loss: 0.38760071815796393\n",
      "12000/49000 loss: 0.377690618860441\n",
      "14000/49000 loss: 0.4410024379377304\n",
      "16000/49000 loss: 0.414879886516763\n",
      "18000/49000 loss: 0.4187904718246063\n",
      "20000/49000 loss: 0.3242414053911856\n",
      "22000/49000 loss: 0.46305816076478096\n",
      "24000/49000 loss: 0.34175085284875756\n",
      "26000/49000 loss: 0.3815455888176675\n",
      "28000/49000 loss: 0.31124820732760433\n",
      "30000/49000 loss: 0.37185695693143556\n",
      "32000/49000 loss: 0.3686679046809978\n",
      "34000/49000 loss: 0.34842258311921387\n",
      "36000/49000 loss: 0.42272131524428896\n",
      "38000/49000 loss: 0.46441769224597856\n",
      "40000/49000 loss: 0.25629974333889494\n",
      "42000/49000 loss: 0.46974135677098666\n",
      "44000/49000 loss: 0.401827593251971\n",
      "46000/49000 loss: 0.4601442118605533\n",
      "48000/49000 loss: 0.3422970085065466\n",
      "epoch 6: valid acc = 0.855, new learning rate = 0.0003675459453124999\n",
      "2000/49000 loss: 0.33306620716366064\n",
      "4000/49000 loss: 0.3931850495749292\n",
      "6000/49000 loss: 0.3255545529770598\n",
      "8000/49000 loss: 0.3961577974038125\n",
      "10000/49000 loss: 0.30029809342274927\n",
      "12000/49000 loss: 0.37926895764960417\n",
      "14000/49000 loss: 0.45793839627541966\n",
      "16000/49000 loss: 0.3754162642016833\n",
      "18000/49000 loss: 0.3520487164497051\n",
      "20000/49000 loss: 0.42760715614508205\n",
      "22000/49000 loss: 0.36947827272083417\n",
      "24000/49000 loss: 0.38714667433498867\n",
      "26000/49000 loss: 0.31627105721639864\n",
      "28000/49000 loss: 0.3046119970391622\n",
      "30000/49000 loss: 0.34405127399752355\n",
      "32000/49000 loss: 0.4982086321313397\n",
      "34000/49000 loss: 0.36810264538935056\n",
      "36000/49000 loss: 0.39556878449253774\n",
      "38000/49000 loss: 0.40662984072196917\n",
      "40000/49000 loss: 0.3603755372486888\n",
      "42000/49000 loss: 0.4102948833062714\n",
      "44000/49000 loss: 0.36360707510611306\n",
      "46000/49000 loss: 0.42379897724878024\n",
      "48000/49000 loss: 0.4243797005418703\n",
      "epoch 7: valid acc = 0.862, new learning rate = 0.00034916864804687486\n",
      "2000/49000 loss: 0.39464542010201736\n",
      "4000/49000 loss: 0.4086794773126381\n",
      "6000/49000 loss: 0.2966886193820513\n",
      "8000/49000 loss: 0.3482261442423697\n",
      "10000/49000 loss: 0.4019930602249394\n",
      "12000/49000 loss: 0.3661518560050438\n",
      "14000/49000 loss: 0.2900863013240543\n",
      "16000/49000 loss: 0.3259204425962374\n",
      "18000/49000 loss: 0.39381695037335185\n",
      "20000/49000 loss: 0.32474265455962903\n",
      "22000/49000 loss: 0.3373555907004608\n",
      "24000/49000 loss: 0.25989483747712655\n",
      "26000/49000 loss: 0.23235525629964132\n",
      "28000/49000 loss: 0.5013591312757978\n",
      "30000/49000 loss: 0.3372489927478301\n",
      "32000/49000 loss: 0.34517671841135944\n",
      "34000/49000 loss: 0.2945756467922411\n",
      "36000/49000 loss: 0.3839047065159379\n",
      "38000/49000 loss: 0.321782437069589\n",
      "40000/49000 loss: 0.3354395731094591\n",
      "42000/49000 loss: 0.3491591809140028\n",
      "44000/49000 loss: 0.38552940329343205\n",
      "46000/49000 loss: 0.42935154674306497\n",
      "48000/49000 loss: 0.365760188228159\n",
      "epoch 8: valid acc = 0.866, new learning rate = 0.0003317102156445311\n",
      "2000/49000 loss: 0.4037757648021147\n",
      "4000/49000 loss: 0.3366319102528361\n",
      "6000/49000 loss: 0.3408145538459821\n",
      "8000/49000 loss: 0.4283274235835991\n",
      "10000/49000 loss: 0.3796885053055043\n",
      "12000/49000 loss: 0.3298304390407298\n",
      "14000/49000 loss: 0.35106480389169714\n",
      "16000/49000 loss: 0.35067731096480015\n",
      "18000/49000 loss: 0.3270593274689843\n",
      "20000/49000 loss: 0.3651161094811678\n",
      "22000/49000 loss: 0.46359199121416106\n",
      "24000/49000 loss: 0.4442452868540276\n",
      "26000/49000 loss: 0.3028955937861216\n",
      "28000/49000 loss: 0.4376719579696383\n",
      "30000/49000 loss: 0.43583009729322214\n",
      "32000/49000 loss: 0.37875114350899725\n",
      "34000/49000 loss: 0.31190878417642015\n",
      "36000/49000 loss: 0.4213929494821289\n",
      "38000/49000 loss: 0.4129736346028806\n",
      "40000/49000 loss: 0.3630936825800315\n",
      "42000/49000 loss: 0.4766137544795908\n",
      "44000/49000 loss: 0.3309928660397746\n",
      "46000/49000 loss: 0.38892088244507855\n",
      "48000/49000 loss: 0.4163583523874255\n",
      "epoch 9: valid acc = 0.874, new learning rate = 0.0003151247048623045\n",
      "2000/49000 loss: 0.32587967900908\n",
      "4000/49000 loss: 0.3703270710995069\n",
      "6000/49000 loss: 0.3372030649727259\n",
      "8000/49000 loss: 0.295350791294644\n",
      "10000/49000 loss: 0.39247257441786926\n",
      "12000/49000 loss: 0.35549203176968497\n",
      "14000/49000 loss: 0.35731375998343845\n",
      "16000/49000 loss: 0.36736404270922807\n",
      "18000/49000 loss: 0.5081996751903711\n",
      "20000/49000 loss: 0.31764596502878695\n",
      "22000/49000 loss: 0.3280277040347343\n",
      "24000/49000 loss: 0.32162627319573756\n",
      "26000/49000 loss: 0.34587792270092027\n",
      "28000/49000 loss: 0.29604322835527214\n",
      "30000/49000 loss: 0.3628213606125254\n",
      "32000/49000 loss: 0.3419216787777489\n",
      "34000/49000 loss: 0.3997872051511644\n",
      "36000/49000 loss: 0.3109215218156575\n",
      "38000/49000 loss: 0.3684002263763922\n",
      "40000/49000 loss: 0.3809591818327196\n",
      "42000/49000 loss: 0.43584905991393375\n",
      "44000/49000 loss: 0.4867241868232996\n",
      "46000/49000 loss: 0.3404695519638634\n",
      "48000/49000 loss: 0.27831242079104773\n",
      "epoch 10: valid acc = 0.882, new learning rate = 0.00029936846961918924\n",
      "2000/49000 loss: 0.395741474861941\n",
      "4000/49000 loss: 0.4052981536227772\n",
      "6000/49000 loss: 0.32455231372104154\n",
      "8000/49000 loss: 0.3489479423550864\n",
      "10000/49000 loss: 0.3752437461048692\n",
      "12000/49000 loss: 0.3166107697962454\n",
      "14000/49000 loss: 0.3581840391711322\n",
      "16000/49000 loss: 0.3636196432297822\n",
      "18000/49000 loss: 0.35234103282035595\n",
      "20000/49000 loss: 0.39706181002007507\n",
      "22000/49000 loss: 0.3695698565806933\n",
      "24000/49000 loss: 0.25235588091615296\n",
      "26000/49000 loss: 0.41017917788332575\n",
      "28000/49000 loss: 0.32968002736963387\n",
      "30000/49000 loss: 0.35679245972284696\n",
      "32000/49000 loss: 0.34795509851305845\n",
      "34000/49000 loss: 0.3840111183934824\n",
      "36000/49000 loss: 0.3526682428260265\n",
      "38000/49000 loss: 0.3431011815065856\n",
      "40000/49000 loss: 0.34128785638275444\n",
      "42000/49000 loss: 0.3606186711695152\n",
      "44000/49000 loss: 0.39599400652896205\n",
      "46000/49000 loss: 0.38066381949371153\n",
      "48000/49000 loss: 0.34162258028735903\n",
      "epoch 11: valid acc = 0.879, new learning rate = 0.00028440004613822977\n",
      "2000/49000 loss: 0.3156650172731104\n",
      "4000/49000 loss: 0.43325850056769477\n",
      "6000/49000 loss: 0.33573946836029156\n",
      "8000/49000 loss: 0.2811341069922712\n",
      "10000/49000 loss: 0.3505136470562913\n",
      "12000/49000 loss: 0.3175590797110196\n",
      "14000/49000 loss: 0.3631361057824064\n",
      "16000/49000 loss: 0.3024084317276168\n",
      "18000/49000 loss: 0.4716250264936757\n",
      "20000/49000 loss: 0.25337119157307486\n",
      "22000/49000 loss: 0.30162989274130714\n",
      "24000/49000 loss: 0.3937908932391701\n",
      "26000/49000 loss: 0.37175233282828435\n",
      "28000/49000 loss: 0.3156964189292264\n",
      "30000/49000 loss: 0.3281247404648568\n",
      "32000/49000 loss: 0.38213969903830247\n",
      "34000/49000 loss: 0.3924343157238193\n",
      "36000/49000 loss: 0.2887427607923892\n",
      "38000/49000 loss: 0.40612719361304617\n",
      "40000/49000 loss: 0.30824511998706244\n",
      "42000/49000 loss: 0.4285559696588337\n",
      "44000/49000 loss: 0.33034536487000177\n",
      "46000/49000 loss: 0.32157937738604947\n",
      "48000/49000 loss: 0.284118336871121\n",
      "epoch 12: valid acc = 0.881, new learning rate = 0.00027018004383131826\n",
      "2000/49000 loss: 0.2983329573259561\n",
      "4000/49000 loss: 0.3049645279593908\n",
      "6000/49000 loss: 0.45698113110599115\n",
      "8000/49000 loss: 0.35503362748583167\n",
      "10000/49000 loss: 0.38689900916367703\n",
      "12000/49000 loss: 0.36240630536443547\n",
      "14000/49000 loss: 0.34785708358982703\n",
      "16000/49000 loss: 0.26993918074722273\n",
      "18000/49000 loss: 0.4611930702730639\n",
      "20000/49000 loss: 0.3720476722563165\n",
      "22000/49000 loss: 0.27087290141802395\n",
      "24000/49000 loss: 0.4339621189829643\n",
      "26000/49000 loss: 0.431079419451453\n",
      "28000/49000 loss: 0.2826651475945626\n",
      "30000/49000 loss: 0.4219050082404593\n",
      "32000/49000 loss: 0.32074034767952075\n",
      "34000/49000 loss: 0.3828749661776321\n",
      "36000/49000 loss: 0.34832768099587197\n",
      "38000/49000 loss: 0.3888245714029035\n",
      "40000/49000 loss: 0.34847241942417134\n",
      "42000/49000 loss: 0.31433899810963367\n",
      "44000/49000 loss: 0.3083549681415435\n",
      "46000/49000 loss: 0.3075546415361602\n",
      "48000/49000 loss: 0.368293346263408\n",
      "epoch 13: valid acc = 0.877, new learning rate = 0.00025667104163975234\n",
      "2000/49000 loss: 0.36476060340347094\n",
      "4000/49000 loss: 0.45435659791636696\n",
      "6000/49000 loss: 0.3813738652955072\n",
      "8000/49000 loss: 0.3435451339368704\n",
      "10000/49000 loss: 0.32390604078675955\n",
      "12000/49000 loss: 0.296466050128124\n",
      "14000/49000 loss: 0.3076439435716848\n",
      "16000/49000 loss: 0.3472505100247231\n",
      "18000/49000 loss: 0.3338591850079231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/49000 loss: 0.28596935920222466\n",
      "22000/49000 loss: 0.4343446806983987\n",
      "24000/49000 loss: 0.26943896953120516\n",
      "26000/49000 loss: 0.36830121543584454\n",
      "28000/49000 loss: 0.34176717846879345\n",
      "30000/49000 loss: 0.4229291398971421\n",
      "32000/49000 loss: 0.314311942671184\n",
      "34000/49000 loss: 0.3364085732422809\n",
      "36000/49000 loss: 0.372290217123871\n",
      "38000/49000 loss: 0.4172646319582061\n",
      "40000/49000 loss: 0.34098372687797235\n",
      "42000/49000 loss: 0.26506972721231176\n",
      "44000/49000 loss: 0.33304458974074064\n",
      "46000/49000 loss: 0.36813983314497284\n",
      "48000/49000 loss: 0.33558120892116206\n",
      "epoch 14: valid acc = 0.881, new learning rate = 0.00024383748955776472\n",
      "2000/49000 loss: 0.30139593401412906\n",
      "4000/49000 loss: 0.3063082997421167\n",
      "6000/49000 loss: 0.335360154099388\n",
      "8000/49000 loss: 0.36827858509135875\n",
      "10000/49000 loss: 0.40589619209054323\n",
      "12000/49000 loss: 0.3960885552947185\n",
      "14000/49000 loss: 0.27646276374444734\n",
      "16000/49000 loss: 0.2956824323128027\n",
      "18000/49000 loss: 0.3019964652309083\n",
      "20000/49000 loss: 0.3362556724631313\n",
      "22000/49000 loss: 0.306074054828086\n",
      "24000/49000 loss: 0.4547239867215675\n",
      "26000/49000 loss: 0.31115035432993626\n",
      "28000/49000 loss: 0.24942949049074\n",
      "30000/49000 loss: 0.31970724401977807\n",
      "32000/49000 loss: 0.346119229316088\n",
      "34000/49000 loss: 0.34043824256371447\n",
      "36000/49000 loss: 0.30263958530021196\n",
      "38000/49000 loss: 0.33042783248210467\n",
      "40000/49000 loss: 0.3542586214363983\n",
      "42000/49000 loss: 0.3117723078019674\n",
      "44000/49000 loss: 0.330395118805468\n",
      "46000/49000 loss: 0.34420368602093254\n",
      "48000/49000 loss: 0.33293845343209816\n",
      "epoch 15: valid acc = 0.879, new learning rate = 0.00023164561507987649\n",
      "2000/49000 loss: 0.3151604561143469\n",
      "4000/49000 loss: 0.38787440061647227\n",
      "6000/49000 loss: 0.4282404237466846\n",
      "8000/49000 loss: 0.3776476422689361\n",
      "10000/49000 loss: 0.3431828649068155\n",
      "12000/49000 loss: 0.3790400586346211\n",
      "14000/49000 loss: 0.2697012737319104\n",
      "16000/49000 loss: 0.37098713243527803\n",
      "18000/49000 loss: 0.27714538875687444\n",
      "20000/49000 loss: 0.34483471363858637\n",
      "22000/49000 loss: 0.42632396125713745\n",
      "24000/49000 loss: 0.2675955674369673\n",
      "26000/49000 loss: 0.2524891443983565\n",
      "28000/49000 loss: 0.31571845647369196\n",
      "30000/49000 loss: 0.35136028580882\n",
      "32000/49000 loss: 0.46245785617124924\n",
      "34000/49000 loss: 0.26722184062653886\n",
      "36000/49000 loss: 0.31119948239266415\n",
      "38000/49000 loss: 0.29444094436835044\n",
      "40000/49000 loss: 0.29702667211962464\n",
      "42000/49000 loss: 0.3352385140771492\n",
      "44000/49000 loss: 0.41325431592187073\n",
      "46000/49000 loss: 0.3877473345954431\n",
      "48000/49000 loss: 0.36234471488601055\n",
      "epoch 16: valid acc = 0.881, new learning rate = 0.00022006333432588265\n",
      "2000/49000 loss: 0.38547398566296226\n",
      "4000/49000 loss: 0.27981269108048173\n",
      "6000/49000 loss: 0.33167544549775035\n",
      "8000/49000 loss: 0.31904503982594096\n",
      "10000/49000 loss: 0.23424674296905199\n",
      "12000/49000 loss: 0.3655068908164603\n",
      "14000/49000 loss: 0.3730411904619305\n",
      "16000/49000 loss: 0.44087144184928134\n",
      "18000/49000 loss: 0.3293265566941189\n",
      "20000/49000 loss: 0.35690568382875115\n",
      "22000/49000 loss: 0.28466023990897377\n",
      "24000/49000 loss: 0.302757345358631\n",
      "26000/49000 loss: 0.22341247499972927\n",
      "28000/49000 loss: 0.37441648636849756\n",
      "30000/49000 loss: 0.2807761195564168\n",
      "32000/49000 loss: 0.408566944796917\n",
      "34000/49000 loss: 0.3497092962998263\n",
      "36000/49000 loss: 0.3735635771824478\n",
      "38000/49000 loss: 0.3856513631325139\n",
      "40000/49000 loss: 0.35868811613892326\n",
      "42000/49000 loss: 0.30058226370917984\n",
      "44000/49000 loss: 0.3688284543016294\n",
      "46000/49000 loss: 0.3792218920597283\n",
      "48000/49000 loss: 0.28301102883440227\n",
      "epoch 17: valid acc = 0.885, new learning rate = 0.00020906016760958852\n",
      "2000/49000 loss: 0.3150067223914634\n",
      "4000/49000 loss: 0.2910729947007325\n",
      "6000/49000 loss: 0.3086403691467869\n",
      "8000/49000 loss: 0.3442755345134818\n",
      "10000/49000 loss: 0.34981598973646855\n",
      "12000/49000 loss: 0.34694222503579125\n",
      "14000/49000 loss: 0.3609567447846552\n",
      "16000/49000 loss: 0.2735811990070386\n",
      "18000/49000 loss: 0.3341399852958991\n",
      "20000/49000 loss: 0.29769541065457167\n",
      "22000/49000 loss: 0.30760103204097083\n",
      "24000/49000 loss: 0.28056099328073436\n",
      "26000/49000 loss: 0.29046669935081415\n",
      "28000/49000 loss: 0.3377873275187577\n",
      "30000/49000 loss: 0.34523204076749037\n",
      "32000/49000 loss: 0.3418658094376861\n",
      "34000/49000 loss: 0.2873612650279732\n",
      "36000/49000 loss: 0.30661793068355275\n",
      "38000/49000 loss: 0.333305951368104\n",
      "40000/49000 loss: 0.3419964874262964\n",
      "42000/49000 loss: 0.35163431474909745\n",
      "44000/49000 loss: 0.30545563565068845\n",
      "46000/49000 loss: 0.36253326067812564\n",
      "48000/49000 loss: 0.2939430213412547\n",
      "epoch 18: valid acc = 0.884, new learning rate = 0.00019860715922910907\n",
      "2000/49000 loss: 0.2586135819095691\n",
      "4000/49000 loss: 0.34096118760602134\n",
      "6000/49000 loss: 0.2784847822991629\n",
      "8000/49000 loss: 0.27163360301820677\n",
      "10000/49000 loss: 0.30308254672524587\n",
      "12000/49000 loss: 0.38882620156134207\n",
      "14000/49000 loss: 0.28669733431907973\n",
      "16000/49000 loss: 0.41496145512605237\n",
      "18000/49000 loss: 0.21877112239536597\n",
      "20000/49000 loss: 0.36316933707249277\n",
      "22000/49000 loss: 0.3453757672296769\n",
      "24000/49000 loss: 0.32154426531163094\n",
      "26000/49000 loss: 0.35994855263413816\n",
      "28000/49000 loss: 0.2855097723878629\n",
      "30000/49000 loss: 0.2574671283119348\n",
      "32000/49000 loss: 0.34998985884406064\n",
      "34000/49000 loss: 0.2819845043034132\n",
      "36000/49000 loss: 0.2352210347346808\n",
      "38000/49000 loss: 0.22178712195235703\n",
      "40000/49000 loss: 0.29995719205873994\n",
      "42000/49000 loss: 0.21824297015057345\n",
      "44000/49000 loss: 0.2982711305504229\n",
      "46000/49000 loss: 0.3813256760471717\n",
      "48000/49000 loss: 0.2991862314005284\n",
      "epoch 19: valid acc = 0.883, new learning rate = 0.0001886768012676536\n",
      "2000/49000 loss: 0.2802659007046039\n",
      "4000/49000 loss: 0.386903287896933\n",
      "6000/49000 loss: 0.34688347836349664\n",
      "8000/49000 loss: 0.3220801507860409\n",
      "10000/49000 loss: 0.3463359668201716\n",
      "12000/49000 loss: 0.3573686185459196\n",
      "14000/49000 loss: 0.3104714406373874\n",
      "16000/49000 loss: 0.3632269957489167\n",
      "18000/49000 loss: 0.3819363766742968\n",
      "20000/49000 loss: 0.27156051196117054\n",
      "22000/49000 loss: 0.31731875074498184\n",
      "24000/49000 loss: 0.30468203541008193\n",
      "26000/49000 loss: 0.3711622323822964\n",
      "28000/49000 loss: 0.33330551962246957\n",
      "30000/49000 loss: 0.2705095817858348\n",
      "32000/49000 loss: 0.3551234506445148\n",
      "34000/49000 loss: 0.2517524248176887\n",
      "36000/49000 loss: 0.2543435089272152\n",
      "38000/49000 loss: 0.30200802979273594\n",
      "40000/49000 loss: 0.3540965764948315\n",
      "42000/49000 loss: 0.44700275369442066\n",
      "44000/49000 loss: 0.2800932184634675\n",
      "46000/49000 loss: 0.31845157404832414\n",
      "48000/49000 loss: 0.3052096166079999\n",
      "epoch 20: valid acc = 0.89, new learning rate = 0.0001792429612042709\n",
      "2000/49000 loss: 0.35723776483091213\n",
      "4000/49000 loss: 0.36744153318053885\n",
      "6000/49000 loss: 0.34437069024566747\n",
      "8000/49000 loss: 0.3022847298708068\n",
      "10000/49000 loss: 0.33461694511463397\n",
      "12000/49000 loss: 0.31867037121216113\n",
      "14000/49000 loss: 0.34717816100456483\n",
      "16000/49000 loss: 0.29560631564959616\n",
      "18000/49000 loss: 0.3838476579834655\n",
      "20000/49000 loss: 0.27031644057862325\n",
      "22000/49000 loss: 0.2752452282025667\n",
      "24000/49000 loss: 0.4055867515669045\n",
      "26000/49000 loss: 0.43771888050518337\n",
      "28000/49000 loss: 0.26101042196825897\n",
      "30000/49000 loss: 0.24943430405666359\n",
      "32000/49000 loss: 0.28292811960158765\n",
      "34000/49000 loss: 0.34978029666352145\n",
      "36000/49000 loss: 0.29963402089364505\n",
      "38000/49000 loss: 0.3262179515786064\n",
      "40000/49000 loss: 0.34048394535930354\n",
      "42000/49000 loss: 0.33923474063683945\n",
      "44000/49000 loss: 0.2814084446727362\n",
      "46000/49000 loss: 0.24932089848996788\n",
      "48000/49000 loss: 0.29214035624106893\n",
      "epoch 21: valid acc = 0.884, new learning rate = 0.00017028081314405735\n",
      "2000/49000 loss: 0.32183402849992654\n",
      "4000/49000 loss: 0.23770849540544867\n",
      "6000/49000 loss: 0.28207001658817527\n",
      "8000/49000 loss: 0.3621061605870913\n",
      "10000/49000 loss: 0.2844142308511368\n",
      "12000/49000 loss: 0.30185392410031925\n",
      "14000/49000 loss: 0.21908335566310472\n",
      "16000/49000 loss: 0.30050047705261074\n",
      "18000/49000 loss: 0.4062138005954847\n",
      "20000/49000 loss: 0.2924895666376076\n",
      "22000/49000 loss: 0.3432513906004878\n",
      "24000/49000 loss: 0.23629602759711743\n",
      "26000/49000 loss: 0.2198286505922423\n",
      "28000/49000 loss: 0.2860651331880753\n",
      "30000/49000 loss: 0.27180701704789806\n",
      "32000/49000 loss: 0.2832087801229895\n",
      "34000/49000 loss: 0.34810788271890736\n",
      "36000/49000 loss: 0.27727000310010025\n",
      "38000/49000 loss: 0.3354104357935026\n",
      "40000/49000 loss: 0.32477381027406693\n",
      "42000/49000 loss: 0.2727847422932572\n",
      "44000/49000 loss: 0.36273554433372285\n",
      "46000/49000 loss: 0.3754629735257983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/49000 loss: 0.37030624388878786\n",
      "epoch 22: valid acc = 0.889, new learning rate = 0.00016176677248685447\n",
      "2000/49000 loss: 0.3427525690117752\n",
      "4000/49000 loss: 0.3128461941720084\n",
      "6000/49000 loss: 0.2985788991800865\n",
      "8000/49000 loss: 0.31281633404678016\n",
      "10000/49000 loss: 0.32359730096884426\n",
      "12000/49000 loss: 0.32154083717782955\n",
      "14000/49000 loss: 0.3996263323381049\n",
      "16000/49000 loss: 0.2668568121728337\n",
      "18000/49000 loss: 0.2210778307400988\n",
      "20000/49000 loss: 0.27763257078593456\n",
      "22000/49000 loss: 0.2726629001419789\n",
      "24000/49000 loss: 0.2913474333120322\n",
      "26000/49000 loss: 0.37021790363128454\n",
      "28000/49000 loss: 0.31557566264241943\n",
      "30000/49000 loss: 0.3359367431803671\n",
      "32000/49000 loss: 0.27994418850696123\n",
      "34000/49000 loss: 0.24641218247820915\n",
      "36000/49000 loss: 0.22273824563908218\n",
      "38000/49000 loss: 0.25362058047517144\n",
      "40000/49000 loss: 0.3813847554824267\n",
      "42000/49000 loss: 0.26767957970420636\n",
      "44000/49000 loss: 0.31930660545099604\n",
      "46000/49000 loss: 0.34951859775339794\n",
      "48000/49000 loss: 0.3139926816550826\n",
      "epoch 23: valid acc = 0.89, new learning rate = 0.00015367843386251173\n",
      "2000/49000 loss: 0.32409085601070864\n",
      "4000/49000 loss: 0.2871871202744524\n",
      "6000/49000 loss: 0.3795860555623633\n",
      "8000/49000 loss: 0.2949689367191517\n",
      "10000/49000 loss: 0.3454082418849998\n",
      "12000/49000 loss: 0.24686468483420895\n",
      "14000/49000 loss: 0.2724443279861368\n",
      "16000/49000 loss: 0.3092164019920795\n",
      "18000/49000 loss: 0.2751746027523736\n",
      "20000/49000 loss: 0.3450850335548138\n",
      "22000/49000 loss: 0.24573547326425652\n",
      "24000/49000 loss: 0.3157124957123143\n",
      "26000/49000 loss: 0.24805233255495768\n",
      "28000/49000 loss: 0.25430696546672427\n",
      "30000/49000 loss: 0.25029166347813675\n",
      "32000/49000 loss: 0.2950524192606892\n",
      "34000/49000 loss: 0.2833718976875595\n",
      "36000/49000 loss: 0.21662996251530864\n",
      "38000/49000 loss: 0.3359489236698475\n",
      "40000/49000 loss: 0.3123418909920469\n",
      "42000/49000 loss: 0.2853569305657121\n",
      "44000/49000 loss: 0.36762223766619445\n",
      "46000/49000 loss: 0.2429265157461881\n",
      "48000/49000 loss: 0.2972451729256035\n",
      "epoch 24: valid acc = 0.89, new learning rate = 0.00014599451216938612\n",
      "2000/49000 loss: 0.36024040407798885\n",
      "4000/49000 loss: 0.29629661642197924\n",
      "6000/49000 loss: 0.251448928240043\n",
      "8000/49000 loss: 0.3766263865365087\n",
      "10000/49000 loss: 0.3109372538750649\n",
      "12000/49000 loss: 0.2804196676360219\n",
      "14000/49000 loss: 0.3133164252469437\n",
      "16000/49000 loss: 0.20227311733725817\n",
      "18000/49000 loss: 0.3602519254297567\n",
      "20000/49000 loss: 0.289319652276891\n",
      "22000/49000 loss: 0.4439462078585595\n",
      "24000/49000 loss: 0.2828564520727301\n",
      "26000/49000 loss: 0.3196062804332733\n",
      "28000/49000 loss: 0.2787942688925575\n",
      "30000/49000 loss: 0.4235493589209041\n",
      "32000/49000 loss: 0.32459581127916315\n",
      "34000/49000 loss: 0.2693866731600296\n",
      "36000/49000 loss: 0.33087114388252903\n",
      "38000/49000 loss: 0.242547505397483\n",
      "40000/49000 loss: 0.3452385989253775\n",
      "42000/49000 loss: 0.3485081279005809\n",
      "44000/49000 loss: 0.34431804155293255\n",
      "46000/49000 loss: 0.37623070657669516\n",
      "48000/49000 loss: 0.2244130901189348\n",
      "epoch 25: valid acc = 0.886, new learning rate = 0.00013869478656091682\n",
      "2000/49000 loss: 0.2410759282418518\n",
      "4000/49000 loss: 0.3703952697423797\n",
      "6000/49000 loss: 0.3441008386359762\n",
      "8000/49000 loss: 0.3260464529406715\n",
      "10000/49000 loss: 0.28591401785020987\n",
      "12000/49000 loss: 0.29636996875817934\n",
      "14000/49000 loss: 0.2718904328868274\n",
      "16000/49000 loss: 0.3644637995535639\n",
      "18000/49000 loss: 0.2436937356074092\n",
      "20000/49000 loss: 0.21545336825439604\n",
      "22000/49000 loss: 0.3066571319686475\n",
      "24000/49000 loss: 0.33063516418798616\n",
      "26000/49000 loss: 0.31661391551714246\n",
      "28000/49000 loss: 0.36880974970168257\n",
      "30000/49000 loss: 0.3241932698219682\n",
      "32000/49000 loss: 0.25239731660348236\n",
      "34000/49000 loss: 0.409735779408873\n",
      "36000/49000 loss: 0.25644251362939685\n",
      "38000/49000 loss: 0.20237169554947526\n",
      "40000/49000 loss: 0.2991207481557225\n",
      "42000/49000 loss: 0.3114012406250847\n",
      "44000/49000 loss: 0.28889965883937524\n",
      "46000/49000 loss: 0.29621505543179266\n",
      "48000/49000 loss: 0.2813743318824589\n",
      "epoch 26: valid acc = 0.891, new learning rate = 0.00013176004723287096\n",
      "2000/49000 loss: 0.30738901240053307\n",
      "4000/49000 loss: 0.3111129437635612\n",
      "6000/49000 loss: 0.29937167352808386\n",
      "8000/49000 loss: 0.25521670652158085\n",
      "10000/49000 loss: 0.30621091625402697\n",
      "12000/49000 loss: 0.3070628111249618\n",
      "14000/49000 loss: 0.30601295630035813\n",
      "16000/49000 loss: 0.3680759937494175\n",
      "18000/49000 loss: 0.2167729261516176\n",
      "20000/49000 loss: 0.20968255704796962\n",
      "22000/49000 loss: 0.25595377163099053\n",
      "24000/49000 loss: 0.30656921363630596\n",
      "26000/49000 loss: 0.322301257542569\n",
      "28000/49000 loss: 0.35598732608830974\n",
      "30000/49000 loss: 0.2737853024072014\n",
      "32000/49000 loss: 0.35083568942687887\n",
      "34000/49000 loss: 0.3051414798995706\n",
      "36000/49000 loss: 0.23731015587457133\n",
      "38000/49000 loss: 0.2929898961908315\n",
      "40000/49000 loss: 0.2252765635917559\n",
      "42000/49000 loss: 0.26168885148263016\n",
      "44000/49000 loss: 0.2872082296925567\n",
      "46000/49000 loss: 0.31970483248985604\n",
      "48000/49000 loss: 0.2834013685340609\n",
      "epoch 27: valid acc = 0.893, new learning rate = 0.0001251720448712274\n",
      "2000/49000 loss: 0.22893865903506008\n",
      "4000/49000 loss: 0.276633911500105\n",
      "6000/49000 loss: 0.2948756364269416\n",
      "8000/49000 loss: 0.324816300997698\n",
      "10000/49000 loss: 0.27608561488835515\n",
      "12000/49000 loss: 0.34305028695217504\n",
      "14000/49000 loss: 0.24453386594033186\n",
      "16000/49000 loss: 0.293607805696882\n",
      "18000/49000 loss: 0.3051885951751275\n",
      "20000/49000 loss: 0.2611761731036744\n",
      "22000/49000 loss: 0.2910810182653448\n",
      "24000/49000 loss: 0.34378344925351156\n",
      "26000/49000 loss: 0.2880298444986483\n",
      "28000/49000 loss: 0.24264969468167982\n",
      "30000/49000 loss: 0.3132214459544782\n",
      "32000/49000 loss: 0.332070333260026\n",
      "34000/49000 loss: 0.29791798206684067\n",
      "36000/49000 loss: 0.28642591674951284\n",
      "38000/49000 loss: 0.22875606159251582\n",
      "40000/49000 loss: 0.30909741070566926\n",
      "42000/49000 loss: 0.3669827812617702\n",
      "44000/49000 loss: 0.3080034373841509\n",
      "46000/49000 loss: 0.4324653670006102\n",
      "48000/49000 loss: 0.2130241141234886\n",
      "epoch 28: valid acc = 0.889, new learning rate = 0.00011891344262766602\n",
      "2000/49000 loss: 0.21422405114177206\n",
      "4000/49000 loss: 0.2552350472812613\n",
      "6000/49000 loss: 0.27337677343020056\n",
      "8000/49000 loss: 0.23546977629858853\n",
      "10000/49000 loss: 0.2629085025331027\n",
      "12000/49000 loss: 0.34564133680474274\n",
      "14000/49000 loss: 0.3137516117211137\n",
      "16000/49000 loss: 0.2366763432748675\n",
      "18000/49000 loss: 0.22599044152584588\n",
      "20000/49000 loss: 0.2634079834707091\n",
      "22000/49000 loss: 0.3095908856550671\n",
      "24000/49000 loss: 0.3030513481284131\n",
      "26000/49000 loss: 0.2797019217540622\n",
      "28000/49000 loss: 0.39538709973647385\n",
      "30000/49000 loss: 0.3357232864250442\n",
      "32000/49000 loss: 0.34035344476970575\n",
      "34000/49000 loss: 0.36880312246481417\n",
      "36000/49000 loss: 0.3154108449661491\n",
      "38000/49000 loss: 0.36499236337589364\n",
      "40000/49000 loss: 0.31704758137661737\n",
      "42000/49000 loss: 0.3391173002826719\n",
      "44000/49000 loss: 0.24016261734284114\n",
      "46000/49000 loss: 0.21866042851511336\n",
      "48000/49000 loss: 0.37450847082241584\n",
      "epoch 29: valid acc = 0.888, new learning rate = 0.00011296777049628272\n",
      "2000/49000 loss: 0.42430267339229977\n",
      "4000/49000 loss: 0.23712474798783564\n",
      "6000/49000 loss: 0.25506916164348403\n",
      "8000/49000 loss: 0.3243848321065931\n",
      "10000/49000 loss: 0.26595986177155845\n",
      "12000/49000 loss: 0.3189778849375612\n",
      "14000/49000 loss: 0.2903474524063894\n",
      "16000/49000 loss: 0.27013324515628145\n",
      "18000/49000 loss: 0.35379487128060333\n",
      "20000/49000 loss: 0.2567684113384201\n",
      "22000/49000 loss: 0.25858308205983643\n",
      "24000/49000 loss: 0.42681805058579714\n",
      "26000/49000 loss: 0.2553185770259701\n",
      "28000/49000 loss: 0.2716784494891669\n",
      "30000/49000 loss: 0.27355878135122097\n",
      "32000/49000 loss: 0.2606644330156442\n",
      "34000/49000 loss: 0.2763492354291363\n",
      "36000/49000 loss: 0.2650233801154811\n",
      "38000/49000 loss: 0.3805752850737389\n",
      "40000/49000 loss: 0.3597897660548362\n",
      "42000/49000 loss: 0.2885594419804262\n",
      "44000/49000 loss: 0.352206333185335\n",
      "46000/49000 loss: 0.33385093159840373\n",
      "48000/49000 loss: 0.288616573407621\n",
      "epoch 30: valid acc = 0.895, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.895469387755102\n",
      "test acc: 0.895\n",
      "test acc: 0.8681\n",
      "number of batches for training: 245\n",
      "epoch 1: valid acc = 0.747, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.809, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.834, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.845, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.863, new learning rate = 0.0003868904687499999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6: valid acc = 0.854, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.863, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.866, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.877, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.878, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.873, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.881, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.884, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.886, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.883, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.885, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.888, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.891, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.883, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.888, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.888, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.886, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.883, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.888, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.89, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.89, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.891, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.89, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.89, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.887, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.894061224489796\n",
      "test acc: 0.887\n",
      "test acc: 0.8687\n",
      "number of batches for training: 245\n",
      "2000/49000 loss: 2.6587306866184495\n",
      "4000/49000 loss: 2.563369512504911\n",
      "6000/49000 loss: 2.497925882135702\n",
      "8000/49000 loss: 2.43684242650082\n",
      "10000/49000 loss: 2.28716395983095\n",
      "12000/49000 loss: 2.0865066023741874\n",
      "14000/49000 loss: 2.059018247388792\n",
      "16000/49000 loss: 1.7387627739320508\n",
      "18000/49000 loss: 1.422992378626495\n",
      "20000/49000 loss: 1.3804154262395463\n",
      "22000/49000 loss: 1.171869908103538\n",
      "24000/49000 loss: 1.037706555361405\n",
      "26000/49000 loss: 1.046397594812677\n",
      "28000/49000 loss: 1.0441986347996317\n",
      "30000/49000 loss: 1.0976967218683875\n",
      "32000/49000 loss: 0.9378714055195486\n",
      "34000/49000 loss: 0.9130632406211108\n",
      "36000/49000 loss: 0.8275859569447186\n",
      "38000/49000 loss: 0.7966235346286754\n",
      "40000/49000 loss: 0.8363507709185727\n",
      "42000/49000 loss: 0.81975592439085\n",
      "44000/49000 loss: 0.7800395510185039\n",
      "46000/49000 loss: 0.733166941198007\n",
      "48000/49000 loss: 0.6297372074655154\n",
      "epoch 1: valid acc = 0.746, new learning rate = 0.000475\n",
      "2000/49000 loss: 0.6098890647688079\n",
      "4000/49000 loss: 0.6929657443932601\n",
      "6000/49000 loss: 0.6518103677967558\n",
      "8000/49000 loss: 0.6904013450608176\n",
      "10000/49000 loss: 0.708046695458136\n",
      "12000/49000 loss: 0.6064225010017049\n",
      "14000/49000 loss: 0.5284071472863888\n",
      "16000/49000 loss: 0.6417452909738045\n",
      "18000/49000 loss: 0.6567836498173909\n",
      "20000/49000 loss: 0.6398112741157715\n",
      "22000/49000 loss: 0.5682375071421166\n",
      "24000/49000 loss: 0.5961935649306073\n",
      "26000/49000 loss: 0.6275117908660992\n",
      "28000/49000 loss: 0.5812035004488759\n",
      "30000/49000 loss: 0.5259834224540425\n",
      "32000/49000 loss: 0.528470647308834\n",
      "34000/49000 loss: 0.505478585163152\n",
      "36000/49000 loss: 0.6454807989867953\n",
      "38000/49000 loss: 0.49959011781396606\n",
      "40000/49000 loss: 0.5407458383360303\n",
      "42000/49000 loss: 0.49664989828512324\n",
      "44000/49000 loss: 0.5231432248782284\n",
      "46000/49000 loss: 0.5262775488481931\n",
      "48000/49000 loss: 0.5187044531979428\n",
      "epoch 2: valid acc = 0.809, new learning rate = 0.00045125\n",
      "2000/49000 loss: 0.5965042182083024\n",
      "4000/49000 loss: 0.58275080351447\n",
      "6000/49000 loss: 0.48711645614594307\n",
      "8000/49000 loss: 0.5707935133151171\n",
      "10000/49000 loss: 0.48904065573216476\n",
      "12000/49000 loss: 0.5267325282583311\n",
      "14000/49000 loss: 0.5250008787961625\n",
      "16000/49000 loss: 0.4553231468842002\n",
      "18000/49000 loss: 0.5627667748153872\n",
      "20000/49000 loss: 0.5110368309112333\n",
      "22000/49000 loss: 0.518568516212869\n",
      "24000/49000 loss: 0.5436173687777532\n",
      "26000/49000 loss: 0.4576711695096698\n",
      "28000/49000 loss: 0.5720437749241577\n",
      "30000/49000 loss: 0.46593152284527234\n",
      "32000/49000 loss: 0.4549446168329489\n",
      "34000/49000 loss: 0.48501277183414393\n",
      "36000/49000 loss: 0.465306502964897\n",
      "38000/49000 loss: 0.41170783665089894\n",
      "40000/49000 loss: 0.45597495436901575\n",
      "42000/49000 loss: 0.4303759993819955\n",
      "44000/49000 loss: 0.4814954083903808\n",
      "46000/49000 loss: 0.4116105399600302\n",
      "48000/49000 loss: 0.46587402390732224\n",
      "epoch 3: valid acc = 0.837, new learning rate = 0.0004286875\n",
      "2000/49000 loss: 0.42733001236737556\n",
      "4000/49000 loss: 0.42175599273147196\n",
      "6000/49000 loss: 0.45036262675481187\n",
      "8000/49000 loss: 0.34715833151176806\n",
      "10000/49000 loss: 0.48634090359076326\n",
      "12000/49000 loss: 0.3967039104991821\n",
      "14000/49000 loss: 0.4959855781302932\n",
      "16000/49000 loss: 0.4170565045211326\n",
      "18000/49000 loss: 0.5458680989699457\n",
      "20000/49000 loss: 0.5542932571701078\n",
      "22000/49000 loss: 0.5731504374652774\n",
      "24000/49000 loss: 0.376181790357839\n",
      "26000/49000 loss: 0.3990910958358389\n",
      "28000/49000 loss: 0.4794243224238445\n",
      "30000/49000 loss: 0.4484450308568134\n",
      "32000/49000 loss: 0.40386708305706226\n",
      "34000/49000 loss: 0.46905660303040825\n",
      "36000/49000 loss: 0.43918032553216807\n",
      "38000/49000 loss: 0.3689479666024746\n",
      "40000/49000 loss: 0.35375041170950716\n",
      "42000/49000 loss: 0.44192521142799257\n",
      "44000/49000 loss: 0.4991466015402816\n",
      "46000/49000 loss: 0.3524099815167428\n",
      "48000/49000 loss: 0.5195728751897506\n",
      "epoch 4: valid acc = 0.846, new learning rate = 0.00040725312499999993\n",
      "2000/49000 loss: 0.47649916156359884\n",
      "4000/49000 loss: 0.446485441517216\n",
      "6000/49000 loss: 0.4219815149369246\n",
      "8000/49000 loss: 0.3491769259062249\n",
      "10000/49000 loss: 0.39901753160292625\n",
      "12000/49000 loss: 0.388422661714799\n",
      "14000/49000 loss: 0.43683860754196235\n",
      "16000/49000 loss: 0.42073886804619054\n",
      "18000/49000 loss: 0.43599492477454366\n",
      "20000/49000 loss: 0.30685887440183524\n",
      "22000/49000 loss: 0.36772400653180176\n",
      "24000/49000 loss: 0.4132528868570755\n",
      "26000/49000 loss: 0.35040671641273535\n",
      "28000/49000 loss: 0.42802846313982285\n",
      "30000/49000 loss: 0.41944215486848874\n",
      "32000/49000 loss: 0.4305441485338069\n",
      "34000/49000 loss: 0.4976803589070644\n",
      "36000/49000 loss: 0.44574216879995415\n",
      "38000/49000 loss: 0.46349902738347654\n",
      "40000/49000 loss: 0.4402873224631889\n",
      "42000/49000 loss: 0.3950281033287734\n",
      "44000/49000 loss: 0.3689107137895975\n",
      "46000/49000 loss: 0.41674141760530287\n",
      "48000/49000 loss: 0.5188043201132423\n",
      "epoch 5: valid acc = 0.845, new learning rate = 0.0003868904687499999\n",
      "2000/49000 loss: 0.36763383862915955\n",
      "4000/49000 loss: 0.4096263556223763\n",
      "6000/49000 loss: 0.46420455811774247\n",
      "8000/49000 loss: 0.4261041181548981\n",
      "10000/49000 loss: 0.43657479991599524\n",
      "12000/49000 loss: 0.275076115951997\n",
      "14000/49000 loss: 0.3713984208547188\n",
      "16000/49000 loss: 0.4275463569018875\n",
      "18000/49000 loss: 0.44665235622962746\n",
      "20000/49000 loss: 0.32011953336473287\n",
      "22000/49000 loss: 0.4278537383459446\n",
      "24000/49000 loss: 0.4323072672442556\n",
      "26000/49000 loss: 0.35994896767709866\n",
      "28000/49000 loss: 0.39076936871343515\n",
      "30000/49000 loss: 0.43268969458731\n",
      "32000/49000 loss: 0.3563575802318054\n",
      "34000/49000 loss: 0.39877421475513863\n",
      "36000/49000 loss: 0.3266419238421587\n",
      "38000/49000 loss: 0.28726986820267697\n",
      "40000/49000 loss: 0.366934276640818\n",
      "42000/49000 loss: 0.4175480719384795\n",
      "44000/49000 loss: 0.3005150327751825\n",
      "46000/49000 loss: 0.36004850464943283\n",
      "48000/49000 loss: 0.3156504551479595\n",
      "epoch 6: valid acc = 0.864, new learning rate = 0.0003675459453124999\n",
      "2000/49000 loss: 0.4175973666586018\n",
      "4000/49000 loss: 0.42946972255813487\n",
      "6000/49000 loss: 0.32100219951758857\n",
      "8000/49000 loss: 0.4436663559931002\n",
      "10000/49000 loss: 0.35211858241588084\n",
      "12000/49000 loss: 0.3556053845337735\n",
      "14000/49000 loss: 0.44253304388753234\n",
      "16000/49000 loss: 0.3749867693409671\n",
      "18000/49000 loss: 0.37072229770833176\n",
      "20000/49000 loss: 0.4124359512609895\n",
      "22000/49000 loss: 0.4011528217156936\n",
      "24000/49000 loss: 0.47219414005762617\n",
      "26000/49000 loss: 0.29189619119162524\n",
      "28000/49000 loss: 0.36672975035910643\n",
      "30000/49000 loss: 0.31682709711688667\n",
      "32000/49000 loss: 0.36762211724608707\n",
      "34000/49000 loss: 0.36561703615288205\n",
      "36000/49000 loss: 0.37073524511113704\n",
      "38000/49000 loss: 0.3530663332377578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/49000 loss: 0.39876983965406365\n",
      "42000/49000 loss: 0.3603574344756087\n",
      "44000/49000 loss: 0.46606353667550565\n",
      "46000/49000 loss: 0.4573480186679175\n",
      "48000/49000 loss: 0.44955384058697356\n",
      "epoch 7: valid acc = 0.867, new learning rate = 0.00034916864804687486\n",
      "2000/49000 loss: 0.39394093545824754\n",
      "4000/49000 loss: 0.2858475551907608\n",
      "6000/49000 loss: 0.3828563119553212\n",
      "8000/49000 loss: 0.329440349732199\n",
      "10000/49000 loss: 0.3222192823262884\n",
      "12000/49000 loss: 0.39337753747252696\n",
      "14000/49000 loss: 0.33534336373016094\n",
      "16000/49000 loss: 0.4132567286458546\n",
      "18000/49000 loss: 0.3740526168880727\n",
      "20000/49000 loss: 0.3455849074267907\n",
      "22000/49000 loss: 0.4283408403821555\n",
      "24000/49000 loss: 0.31627469811326153\n",
      "26000/49000 loss: 0.31703004572854593\n",
      "28000/49000 loss: 0.3724940022035667\n",
      "30000/49000 loss: 0.4209254047453553\n",
      "32000/49000 loss: 0.24075847853030638\n",
      "34000/49000 loss: 0.31689209837034527\n",
      "36000/49000 loss: 0.25356518183000065\n",
      "38000/49000 loss: 0.4565655937126278\n",
      "40000/49000 loss: 0.4202050320840432\n",
      "42000/49000 loss: 0.3352614079287991\n",
      "44000/49000 loss: 0.3494420903425263\n",
      "46000/49000 loss: 0.36062786170156685\n",
      "48000/49000 loss: 0.40966905711210067\n",
      "epoch 8: valid acc = 0.872, new learning rate = 0.0003317102156445311\n",
      "2000/49000 loss: 0.35095750420286886\n",
      "4000/49000 loss: 0.3422639946075205\n",
      "6000/49000 loss: 0.3556261047999202\n",
      "8000/49000 loss: 0.34224238157418513\n",
      "10000/49000 loss: 0.30359506251651663\n",
      "12000/49000 loss: 0.3349771461813925\n",
      "14000/49000 loss: 0.4134220402584444\n",
      "16000/49000 loss: 0.3338524619041101\n",
      "18000/49000 loss: 0.34993224256771566\n",
      "20000/49000 loss: 0.36457534582289153\n",
      "22000/49000 loss: 0.4108407309581787\n",
      "24000/49000 loss: 0.3357843556161246\n",
      "26000/49000 loss: 0.3232943779181935\n",
      "28000/49000 loss: 0.4261415207800848\n",
      "30000/49000 loss: 0.33274884544537103\n",
      "32000/49000 loss: 0.3245069952944002\n",
      "34000/49000 loss: 0.3147200062049267\n",
      "36000/49000 loss: 0.3859569211410497\n",
      "38000/49000 loss: 0.2946701618947993\n",
      "40000/49000 loss: 0.36561095151280065\n",
      "42000/49000 loss: 0.3801882000067412\n",
      "44000/49000 loss: 0.5013771719911284\n",
      "46000/49000 loss: 0.273489990006637\n",
      "48000/49000 loss: 0.46745507083248583\n",
      "epoch 9: valid acc = 0.869, new learning rate = 0.0003151247048623045\n",
      "2000/49000 loss: 0.45708441276020123\n",
      "4000/49000 loss: 0.37502485056722495\n",
      "6000/49000 loss: 0.29804505589650937\n",
      "8000/49000 loss: 0.31394636559389366\n",
      "10000/49000 loss: 0.36797278152315044\n",
      "12000/49000 loss: 0.4343998829119321\n",
      "14000/49000 loss: 0.43591726942465986\n",
      "16000/49000 loss: 0.3353329777565838\n",
      "18000/49000 loss: 0.34091098239193696\n",
      "20000/49000 loss: 0.4050905227437099\n",
      "22000/49000 loss: 0.3880741186888059\n",
      "24000/49000 loss: 0.3848287705700394\n",
      "26000/49000 loss: 0.3495488091826265\n",
      "28000/49000 loss: 0.36992304012412763\n",
      "30000/49000 loss: 0.35198935727299546\n",
      "32000/49000 loss: 0.2898055307214276\n",
      "34000/49000 loss: 0.3342839666313124\n",
      "36000/49000 loss: 0.308116706874777\n",
      "38000/49000 loss: 0.3393589913584927\n",
      "40000/49000 loss: 0.33438662269029723\n",
      "42000/49000 loss: 0.3331253615200945\n",
      "44000/49000 loss: 0.33017425679779355\n",
      "46000/49000 loss: 0.3425674881270122\n",
      "48000/49000 loss: 0.3878634514910117\n",
      "epoch 10: valid acc = 0.872, new learning rate = 0.00029936846961918924\n",
      "2000/49000 loss: 0.3717186939390467\n",
      "4000/49000 loss: 0.37969811119783287\n",
      "6000/49000 loss: 0.2955214259343092\n",
      "8000/49000 loss: 0.3784155980584655\n",
      "10000/49000 loss: 0.467906928680625\n",
      "12000/49000 loss: 0.3225151749307007\n",
      "14000/49000 loss: 0.27880516886128753\n",
      "16000/49000 loss: 0.4825400387649345\n",
      "18000/49000 loss: 0.3488612016472561\n",
      "20000/49000 loss: 0.40603165994500373\n",
      "22000/49000 loss: 0.36055174718491206\n",
      "24000/49000 loss: 0.4190686555246609\n",
      "26000/49000 loss: 0.2568184887116458\n",
      "28000/49000 loss: 0.2706820250076584\n",
      "30000/49000 loss: 0.4046318253433155\n",
      "32000/49000 loss: 0.4114159924227783\n",
      "34000/49000 loss: 0.32472722190848674\n",
      "36000/49000 loss: 0.3726786223593404\n",
      "38000/49000 loss: 0.32057709918602645\n",
      "40000/49000 loss: 0.3245077064866639\n",
      "42000/49000 loss: 0.41718593027834294\n",
      "44000/49000 loss: 0.34109509683528044\n",
      "46000/49000 loss: 0.33454861281555526\n",
      "48000/49000 loss: 0.36130234273819806\n",
      "epoch 11: valid acc = 0.879, new learning rate = 0.00028440004613822977\n",
      "2000/49000 loss: 0.27201791626993443\n",
      "4000/49000 loss: 0.32040448158233237\n",
      "6000/49000 loss: 0.3906528781550248\n",
      "8000/49000 loss: 0.297940082460721\n",
      "10000/49000 loss: 0.34141552063227976\n",
      "12000/49000 loss: 0.347994611004365\n",
      "14000/49000 loss: 0.2963959734246348\n",
      "16000/49000 loss: 0.27164161538486814\n",
      "18000/49000 loss: 0.46473348142633797\n",
      "20000/49000 loss: 0.28573211055636455\n",
      "22000/49000 loss: 0.4032209606688144\n",
      "24000/49000 loss: 0.28174109382198864\n",
      "26000/49000 loss: 0.3192949758774584\n",
      "28000/49000 loss: 0.32546488378299543\n",
      "30000/49000 loss: 0.2634776966576053\n",
      "32000/49000 loss: 0.41010435515465826\n",
      "34000/49000 loss: 0.2699101585322901\n",
      "36000/49000 loss: 0.4094542643576321\n",
      "38000/49000 loss: 0.29931969457369106\n",
      "40000/49000 loss: 0.41227110462375616\n",
      "42000/49000 loss: 0.43270136596348213\n",
      "44000/49000 loss: 0.27222869720704546\n",
      "46000/49000 loss: 0.33313316241283547\n",
      "48000/49000 loss: 0.2940384400500716\n",
      "epoch 12: valid acc = 0.877, new learning rate = 0.00027018004383131826\n",
      "2000/49000 loss: 0.37030952413611506\n",
      "4000/49000 loss: 0.2640048019603999\n",
      "6000/49000 loss: 0.27104122049860147\n",
      "8000/49000 loss: 0.35603907234067206\n",
      "10000/49000 loss: 0.33091573642173494\n",
      "12000/49000 loss: 0.3481973975488064\n",
      "14000/49000 loss: 0.3991788948671569\n",
      "16000/49000 loss: 0.42849034142472175\n",
      "18000/49000 loss: 0.2487242956123573\n",
      "20000/49000 loss: 0.3721128373865472\n",
      "22000/49000 loss: 0.3093753969501368\n",
      "24000/49000 loss: 0.36704710199071633\n",
      "26000/49000 loss: 0.42271197189405335\n",
      "28000/49000 loss: 0.28661688692583986\n",
      "30000/49000 loss: 0.3539609772023014\n",
      "32000/49000 loss: 0.38117686548581153\n",
      "34000/49000 loss: 0.3202261983030336\n",
      "36000/49000 loss: 0.3928438409675652\n",
      "38000/49000 loss: 0.29604466850781497\n",
      "40000/49000 loss: 0.40297723045118655\n",
      "42000/49000 loss: 0.3318694678606897\n",
      "44000/49000 loss: 0.3522896686861337\n",
      "46000/49000 loss: 0.367129747968505\n",
      "48000/49000 loss: 0.2252704468705678\n",
      "epoch 13: valid acc = 0.873, new learning rate = 0.00025667104163975234\n",
      "2000/49000 loss: 0.36150694202525363\n",
      "4000/49000 loss: 0.48863376859256274\n",
      "6000/49000 loss: 0.29945782520549735\n",
      "8000/49000 loss: 0.3509653040329141\n",
      "10000/49000 loss: 0.3470594224467169\n",
      "12000/49000 loss: 0.41957762513425273\n",
      "14000/49000 loss: 0.3426340873768109\n",
      "16000/49000 loss: 0.25201006547977994\n",
      "18000/49000 loss: 0.36142686807234076\n",
      "20000/49000 loss: 0.4034052434542934\n",
      "22000/49000 loss: 0.2800544531033718\n",
      "24000/49000 loss: 0.2977800289682345\n",
      "26000/49000 loss: 0.460950527233727\n",
      "28000/49000 loss: 0.3469760612428433\n",
      "30000/49000 loss: 0.3991173279910844\n",
      "32000/49000 loss: 0.2907680998072576\n",
      "34000/49000 loss: 0.3086158513207507\n",
      "36000/49000 loss: 0.33853202707982166\n",
      "38000/49000 loss: 0.31346476742990126\n",
      "40000/49000 loss: 0.3752683539219251\n",
      "42000/49000 loss: 0.28002523670743146\n",
      "44000/49000 loss: 0.30443814980350115\n",
      "46000/49000 loss: 0.3387294681820225\n",
      "48000/49000 loss: 0.38407537771158384\n",
      "epoch 14: valid acc = 0.879, new learning rate = 0.00024383748955776472\n",
      "2000/49000 loss: 0.4354208415268699\n",
      "4000/49000 loss: 0.30290301616617427\n",
      "6000/49000 loss: 0.2754191725359998\n",
      "8000/49000 loss: 0.36247494102577654\n",
      "10000/49000 loss: 0.3744941311552101\n",
      "12000/49000 loss: 0.2576421682570507\n",
      "14000/49000 loss: 0.44772049517743073\n",
      "16000/49000 loss: 0.33167851872887916\n",
      "18000/49000 loss: 0.4304429488998927\n",
      "20000/49000 loss: 0.374592111248594\n",
      "22000/49000 loss: 0.3880881061312523\n",
      "24000/49000 loss: 0.29834865534036925\n",
      "26000/49000 loss: 0.25277909080143923\n",
      "28000/49000 loss: 0.2929862727080454\n",
      "30000/49000 loss: 0.2996050337321829\n",
      "32000/49000 loss: 0.29811376100503345\n",
      "34000/49000 loss: 0.3781439788785848\n",
      "36000/49000 loss: 0.2808121387443572\n",
      "38000/49000 loss: 0.33702958116035403\n",
      "40000/49000 loss: 0.31481471373582975\n",
      "42000/49000 loss: 0.37199217223701175\n",
      "44000/49000 loss: 0.37543901742181107\n",
      "46000/49000 loss: 0.3865692418548394\n",
      "48000/49000 loss: 0.3431061084048737\n",
      "epoch 15: valid acc = 0.884, new learning rate = 0.00023164561507987649\n",
      "2000/49000 loss: 0.30374656454200144\n",
      "4000/49000 loss: 0.3028764468732064\n",
      "6000/49000 loss: 0.33413675496730705\n",
      "8000/49000 loss: 0.35091117297052293\n",
      "10000/49000 loss: 0.3730052047795855\n",
      "12000/49000 loss: 0.3201392237267066\n",
      "14000/49000 loss: 0.3331208569458112\n",
      "16000/49000 loss: 0.3208135392341631\n",
      "18000/49000 loss: 0.28150557563955897\n",
      "20000/49000 loss: 0.2503237913901504\n",
      "22000/49000 loss: 0.3672197065676648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000/49000 loss: 0.4031375070279643\n",
      "26000/49000 loss: 0.3841850508796664\n",
      "28000/49000 loss: 0.35304701611578554\n",
      "30000/49000 loss: 0.2659531174154875\n",
      "32000/49000 loss: 0.41538413526440693\n",
      "34000/49000 loss: 0.2680712951162752\n",
      "36000/49000 loss: 0.3345212731649686\n",
      "38000/49000 loss: 0.3563048529697456\n",
      "40000/49000 loss: 0.2547730012677818\n",
      "42000/49000 loss: 0.3233166674648175\n",
      "44000/49000 loss: 0.240816777339\n",
      "46000/49000 loss: 0.3362571752698961\n",
      "48000/49000 loss: 0.3797407687712712\n",
      "epoch 16: valid acc = 0.88, new learning rate = 0.00022006333432588265\n",
      "2000/49000 loss: 0.2782333494926896\n",
      "4000/49000 loss: 0.3503995511874209\n",
      "6000/49000 loss: 0.31175600693624694\n",
      "8000/49000 loss: 0.4220217516010223\n",
      "10000/49000 loss: 0.3438777436059324\n",
      "12000/49000 loss: 0.3805431079832887\n",
      "14000/49000 loss: 0.30163605513901137\n",
      "16000/49000 loss: 0.2799233680483764\n",
      "18000/49000 loss: 0.3255005603483436\n",
      "20000/49000 loss: 0.2927973819415399\n",
      "22000/49000 loss: 0.32507907610858894\n",
      "24000/49000 loss: 0.36386527130265495\n",
      "26000/49000 loss: 0.24891458089305618\n",
      "28000/49000 loss: 0.3470350671824516\n",
      "30000/49000 loss: 0.30043798220543116\n",
      "32000/49000 loss: 0.29995241176782367\n",
      "34000/49000 loss: 0.3926176113894575\n",
      "36000/49000 loss: 0.3837518937758921\n",
      "38000/49000 loss: 0.32700965605193627\n",
      "40000/49000 loss: 0.3434456198887786\n",
      "42000/49000 loss: 0.31442716966746714\n",
      "44000/49000 loss: 0.3580638951282879\n",
      "46000/49000 loss: 0.28149046426221086\n",
      "48000/49000 loss: 0.33946486858381125\n",
      "epoch 17: valid acc = 0.88, new learning rate = 0.00020906016760958852\n",
      "2000/49000 loss: 0.3877263416009325\n",
      "4000/49000 loss: 0.24387364817729265\n",
      "6000/49000 loss: 0.3169770126356868\n",
      "8000/49000 loss: 0.2538941820243864\n",
      "10000/49000 loss: 0.37818077080927015\n",
      "12000/49000 loss: 0.35514403980112597\n",
      "14000/49000 loss: 0.345663543663383\n",
      "16000/49000 loss: 0.41014077286138084\n",
      "18000/49000 loss: 0.28720232090365955\n",
      "20000/49000 loss: 0.28736004651302705\n",
      "22000/49000 loss: 0.3738106983660498\n",
      "24000/49000 loss: 0.3655431807932952\n",
      "26000/49000 loss: 0.33244518768730275\n",
      "28000/49000 loss: 0.2533813251981543\n",
      "30000/49000 loss: 0.264380097589371\n",
      "32000/49000 loss: 0.3241126092667888\n",
      "34000/49000 loss: 0.27811292536054594\n",
      "36000/49000 loss: 0.46365449820383253\n",
      "38000/49000 loss: 0.3402437033240003\n",
      "40000/49000 loss: 0.3514277515068235\n",
      "42000/49000 loss: 0.4230920942848402\n",
      "44000/49000 loss: 0.24649873219706678\n",
      "46000/49000 loss: 0.3394623353347608\n",
      "48000/49000 loss: 0.42383786667639556\n",
      "epoch 18: valid acc = 0.883, new learning rate = 0.00019860715922910907\n",
      "2000/49000 loss: 0.3282562241145209\n",
      "4000/49000 loss: 0.32264163014975733\n",
      "6000/49000 loss: 0.24659696336962836\n",
      "8000/49000 loss: 0.22980765009988866\n",
      "10000/49000 loss: 0.3816083940900985\n",
      "12000/49000 loss: 0.3133669475033245\n",
      "14000/49000 loss: 0.28440700542778496\n",
      "16000/49000 loss: 0.2574593556110136\n",
      "18000/49000 loss: 0.33569681996558426\n",
      "20000/49000 loss: 0.39397538357388623\n",
      "22000/49000 loss: 0.3774669858392462\n",
      "24000/49000 loss: 0.3653579893261855\n",
      "26000/49000 loss: 0.33816932103550096\n",
      "28000/49000 loss: 0.34029772241076095\n",
      "30000/49000 loss: 0.2512902930420274\n",
      "32000/49000 loss: 0.23723602407495914\n",
      "34000/49000 loss: 0.310766039284227\n",
      "36000/49000 loss: 0.35064244211972934\n",
      "38000/49000 loss: 0.3136932447284614\n",
      "40000/49000 loss: 0.3254588778746427\n",
      "42000/49000 loss: 0.2907600533165444\n",
      "44000/49000 loss: 0.4088636639766659\n",
      "46000/49000 loss: 0.28345820338108374\n",
      "48000/49000 loss: 0.3330150622051899\n",
      "epoch 19: valid acc = 0.889, new learning rate = 0.0001886768012676536\n",
      "2000/49000 loss: 0.32716377097466404\n",
      "4000/49000 loss: 0.3954563001298894\n",
      "6000/49000 loss: 0.28732769160430066\n",
      "8000/49000 loss: 0.2882475861844855\n",
      "10000/49000 loss: 0.42469428847682134\n",
      "12000/49000 loss: 0.27789247882850265\n",
      "14000/49000 loss: 0.37260913317292116\n",
      "16000/49000 loss: 0.26357126741181003\n",
      "18000/49000 loss: 0.2919115040921179\n",
      "20000/49000 loss: 0.3630433615167957\n",
      "22000/49000 loss: 0.3604627804139983\n",
      "24000/49000 loss: 0.34744211633745314\n",
      "26000/49000 loss: 0.3252407231481121\n",
      "28000/49000 loss: 0.2541795108215564\n",
      "30000/49000 loss: 0.24424962850133333\n",
      "32000/49000 loss: 0.27007329091300447\n",
      "34000/49000 loss: 0.2775731534765389\n",
      "36000/49000 loss: 0.40463193095884054\n",
      "38000/49000 loss: 0.3129660286650554\n",
      "40000/49000 loss: 0.37112446008008143\n",
      "42000/49000 loss: 0.2762709825069017\n",
      "44000/49000 loss: 0.3326941604990797\n",
      "46000/49000 loss: 0.3040057577607683\n",
      "48000/49000 loss: 0.3493300156461186\n",
      "epoch 20: valid acc = 0.889, new learning rate = 0.0001792429612042709\n",
      "2000/49000 loss: 0.26571816429976874\n",
      "4000/49000 loss: 0.26987077133873116\n",
      "6000/49000 loss: 0.32157749067909536\n",
      "8000/49000 loss: 0.3431692551020686\n",
      "10000/49000 loss: 0.35021157401447683\n",
      "12000/49000 loss: 0.25466319328593695\n",
      "14000/49000 loss: 0.21208362970068836\n",
      "16000/49000 loss: 0.31213822249806444\n",
      "18000/49000 loss: 0.2251177101093793\n",
      "20000/49000 loss: 0.2173342967699597\n",
      "22000/49000 loss: 0.2889531384928151\n",
      "24000/49000 loss: 0.26740766782385306\n",
      "26000/49000 loss: 0.2620241347275031\n",
      "28000/49000 loss: 0.34497043077693795\n",
      "30000/49000 loss: 0.3215774724560199\n",
      "32000/49000 loss: 0.33053113843392806\n",
      "34000/49000 loss: 0.29574416005206267\n",
      "36000/49000 loss: 0.3363687505301413\n",
      "38000/49000 loss: 0.35620456981373655\n",
      "40000/49000 loss: 0.39183206799747605\n",
      "42000/49000 loss: 0.3144731432278855\n",
      "44000/49000 loss: 0.3217753531747225\n",
      "46000/49000 loss: 0.2727511323447188\n",
      "48000/49000 loss: 0.26719087228324084\n",
      "epoch 21: valid acc = 0.881, new learning rate = 0.00017028081314405735\n",
      "2000/49000 loss: 0.30723555311969947\n",
      "4000/49000 loss: 0.35062006540468543\n",
      "6000/49000 loss: 0.34573350138321934\n",
      "8000/49000 loss: 0.2800309872652859\n",
      "10000/49000 loss: 0.32614452969047114\n",
      "12000/49000 loss: 0.293122687184013\n",
      "14000/49000 loss: 0.4174621881296296\n",
      "16000/49000 loss: 0.28775737646491545\n",
      "18000/49000 loss: 0.2967088766390834\n",
      "20000/49000 loss: 0.3048069987392544\n",
      "22000/49000 loss: 0.40068306715079455\n",
      "24000/49000 loss: 0.23741760869585957\n",
      "26000/49000 loss: 0.3753191214496093\n",
      "28000/49000 loss: 0.35405570154006766\n",
      "30000/49000 loss: 0.3335627984758566\n",
      "32000/49000 loss: 0.2909860345770892\n",
      "34000/49000 loss: 0.29894195752140046\n",
      "36000/49000 loss: 0.25609840592806865\n",
      "38000/49000 loss: 0.3273294023176389\n",
      "40000/49000 loss: 0.37201833768080805\n",
      "42000/49000 loss: 0.35248257099894553\n",
      "44000/49000 loss: 0.3537575635646307\n",
      "46000/49000 loss: 0.35020507002099954\n",
      "48000/49000 loss: 0.37754771977378276\n",
      "epoch 22: valid acc = 0.88, new learning rate = 0.00016176677248685447\n",
      "2000/49000 loss: 0.3579790583980883\n",
      "4000/49000 loss: 0.3971136802992686\n",
      "6000/49000 loss: 0.3663783931661729\n",
      "8000/49000 loss: 0.25296518688448777\n",
      "10000/49000 loss: 0.32517970139419583\n",
      "12000/49000 loss: 0.4068887163678634\n",
      "14000/49000 loss: 0.2674838012133348\n",
      "16000/49000 loss: 0.33628333375398867\n",
      "18000/49000 loss: 0.31355459830520704\n",
      "20000/49000 loss: 0.341298660922052\n",
      "22000/49000 loss: 0.35758267973204977\n",
      "24000/49000 loss: 0.30493104043207053\n",
      "26000/49000 loss: 0.2603725671548501\n",
      "28000/49000 loss: 0.35148745114400753\n",
      "30000/49000 loss: 0.33376213373065106\n",
      "32000/49000 loss: 0.319807299735886\n",
      "34000/49000 loss: 0.2485695933026563\n",
      "36000/49000 loss: 0.35789183550162296\n",
      "38000/49000 loss: 0.31705703041761624\n",
      "40000/49000 loss: 0.2991393535296417\n",
      "42000/49000 loss: 0.3033467853717895\n",
      "44000/49000 loss: 0.3035565642161353\n",
      "46000/49000 loss: 0.28060956241580914\n",
      "48000/49000 loss: 0.3792819105224505\n",
      "epoch 23: valid acc = 0.886, new learning rate = 0.00015367843386251173\n",
      "2000/49000 loss: 0.2969723756795278\n",
      "4000/49000 loss: 0.23841493369010408\n",
      "6000/49000 loss: 0.34445937934276094\n",
      "8000/49000 loss: 0.3330626018373602\n",
      "10000/49000 loss: 0.3267013602384826\n",
      "12000/49000 loss: 0.32487351183150276\n",
      "14000/49000 loss: 0.25119000787096046\n",
      "16000/49000 loss: 0.24794181735710116\n",
      "18000/49000 loss: 0.38442392246589063\n",
      "20000/49000 loss: 0.28504423485247626\n",
      "22000/49000 loss: 0.30623911752844774\n",
      "24000/49000 loss: 0.25984896117665707\n",
      "26000/49000 loss: 0.3133040359487572\n",
      "28000/49000 loss: 0.3300229139714162\n",
      "30000/49000 loss: 0.36257475834218483\n",
      "32000/49000 loss: 0.3012056611933404\n",
      "34000/49000 loss: 0.3875230296073504\n",
      "36000/49000 loss: 0.28724983189790404\n",
      "38000/49000 loss: 0.30937006174510384\n",
      "40000/49000 loss: 0.36797688024911807\n",
      "42000/49000 loss: 0.3189715502986663\n",
      "44000/49000 loss: 0.34874151340559084\n",
      "46000/49000 loss: 0.31117615454934056\n",
      "48000/49000 loss: 0.35971097975694216\n",
      "epoch 24: valid acc = 0.887, new learning rate = 0.00014599451216938612\n",
      "2000/49000 loss: 0.4020309099554719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/49000 loss: 0.25690298125515415\n",
      "6000/49000 loss: 0.3395653656761251\n",
      "8000/49000 loss: 0.42398071979410057\n",
      "10000/49000 loss: 0.2667998886812492\n",
      "12000/49000 loss: 0.27414836566001705\n",
      "14000/49000 loss: 0.26364963164861316\n",
      "16000/49000 loss: 0.26935333990823546\n",
      "18000/49000 loss: 0.29472545555794444\n",
      "20000/49000 loss: 0.3362288224222973\n",
      "22000/49000 loss: 0.29020088648515074\n",
      "24000/49000 loss: 0.3025850538969278\n",
      "26000/49000 loss: 0.25203940617437576\n",
      "28000/49000 loss: 0.2908085601487033\n",
      "30000/49000 loss: 0.2900865464197007\n",
      "32000/49000 loss: 0.3654063323619556\n",
      "34000/49000 loss: 0.3975543035695977\n",
      "36000/49000 loss: 0.35114952914037895\n",
      "38000/49000 loss: 0.2546568798909582\n",
      "40000/49000 loss: 0.2473059267139767\n",
      "42000/49000 loss: 0.26447354449851196\n",
      "44000/49000 loss: 0.3912287641203826\n",
      "46000/49000 loss: 0.32906577946890936\n",
      "48000/49000 loss: 0.3394937700636825\n",
      "epoch 25: valid acc = 0.885, new learning rate = 0.00013869478656091682\n",
      "2000/49000 loss: 0.3121250102012543\n",
      "4000/49000 loss: 0.3003994845680401\n",
      "6000/49000 loss: 0.30158810823702326\n",
      "8000/49000 loss: 0.3130724823471152\n",
      "10000/49000 loss: 0.2588016777871572\n",
      "12000/49000 loss: 0.22654019927858302\n",
      "14000/49000 loss: 0.2703095624896679\n",
      "16000/49000 loss: 0.34923351855813833\n",
      "18000/49000 loss: 0.38342783064563374\n",
      "20000/49000 loss: 0.282347019328667\n",
      "22000/49000 loss: 0.30466757396699623\n",
      "24000/49000 loss: 0.24249899055041244\n",
      "26000/49000 loss: 0.3083026794027308\n",
      "28000/49000 loss: 0.23941511618234365\n",
      "30000/49000 loss: 0.269877303957608\n",
      "32000/49000 loss: 0.2612086844463356\n",
      "34000/49000 loss: 0.21742855002158556\n",
      "36000/49000 loss: 0.2691604068720424\n",
      "38000/49000 loss: 0.2633500403045017\n",
      "40000/49000 loss: 0.3326114320191747\n",
      "42000/49000 loss: 0.21013938419906206\n",
      "44000/49000 loss: 0.27260763900135787\n",
      "46000/49000 loss: 0.3060974329290091\n",
      "48000/49000 loss: 0.3381491134023481\n",
      "epoch 26: valid acc = 0.881, new learning rate = 0.00013176004723287096\n",
      "2000/49000 loss: 0.285773046802561\n",
      "4000/49000 loss: 0.3955196085674424\n",
      "6000/49000 loss: 0.31816901770481665\n",
      "8000/49000 loss: 0.28909191996178546\n",
      "10000/49000 loss: 0.37476810273533845\n",
      "12000/49000 loss: 0.38455535733042695\n",
      "14000/49000 loss: 0.36743256110352834\n",
      "16000/49000 loss: 0.31280027023915763\n",
      "18000/49000 loss: 0.20604657657195735\n",
      "20000/49000 loss: 0.2032713026445356\n",
      "22000/49000 loss: 0.2522972710514157\n",
      "24000/49000 loss: 0.2778551256144358\n",
      "26000/49000 loss: 0.29425727440942795\n",
      "28000/49000 loss: 0.2742565887430732\n",
      "30000/49000 loss: 0.26732821837257564\n",
      "32000/49000 loss: 0.27504415096789653\n",
      "34000/49000 loss: 0.38197787981639914\n",
      "36000/49000 loss: 0.433578418033231\n",
      "38000/49000 loss: 0.32533711400840004\n",
      "40000/49000 loss: 0.36900393722092234\n",
      "42000/49000 loss: 0.2826960369637169\n",
      "44000/49000 loss: 0.26303930067538245\n",
      "46000/49000 loss: 0.24463416478954833\n",
      "48000/49000 loss: 0.22914895580445127\n",
      "epoch 27: valid acc = 0.886, new learning rate = 0.0001251720448712274\n",
      "2000/49000 loss: 0.30535943948992117\n",
      "4000/49000 loss: 0.38479719650890215\n",
      "6000/49000 loss: 0.33784036472259205\n",
      "8000/49000 loss: 0.32016644661683547\n",
      "10000/49000 loss: 0.34207103002404643\n",
      "12000/49000 loss: 0.30365546665395304\n",
      "14000/49000 loss: 0.3436229304715279\n",
      "16000/49000 loss: 0.31168024648230835\n",
      "18000/49000 loss: 0.2744711491619905\n",
      "20000/49000 loss: 0.2801598507794785\n",
      "22000/49000 loss: 0.32741998195073974\n",
      "24000/49000 loss: 0.24949392213480587\n",
      "26000/49000 loss: 0.2935145887848152\n",
      "28000/49000 loss: 0.29092460668958675\n",
      "30000/49000 loss: 0.22192837254416387\n",
      "32000/49000 loss: 0.33754865255942423\n",
      "34000/49000 loss: 0.33763633885388633\n",
      "36000/49000 loss: 0.24616437547634187\n",
      "38000/49000 loss: 0.29371048889297857\n",
      "40000/49000 loss: 0.299653694113332\n",
      "42000/49000 loss: 0.22322551406766908\n",
      "44000/49000 loss: 0.28483776942454636\n",
      "46000/49000 loss: 0.34246202903484346\n",
      "48000/49000 loss: 0.31287363598751394\n",
      "epoch 28: valid acc = 0.887, new learning rate = 0.00011891344262766602\n",
      "2000/49000 loss: 0.2743368449861532\n",
      "4000/49000 loss: 0.3298481766893742\n",
      "6000/49000 loss: 0.3719243279342931\n",
      "8000/49000 loss: 0.24840402836872003\n",
      "10000/49000 loss: 0.3327009615998284\n",
      "12000/49000 loss: 0.24303368321689467\n",
      "14000/49000 loss: 0.23114051690380946\n",
      "16000/49000 loss: 0.3551200107828321\n",
      "18000/49000 loss: 0.2428366333268342\n",
      "20000/49000 loss: 0.26602412902627437\n",
      "22000/49000 loss: 0.20966414881499493\n",
      "24000/49000 loss: 0.2348139219626112\n",
      "26000/49000 loss: 0.468865962331398\n",
      "28000/49000 loss: 0.29043167541495296\n",
      "30000/49000 loss: 0.31345731424062656\n",
      "32000/49000 loss: 0.27373192791230005\n",
      "34000/49000 loss: 0.3476620091422673\n",
      "36000/49000 loss: 0.26807204326234396\n",
      "38000/49000 loss: 0.2536230530187813\n",
      "40000/49000 loss: 0.28602467520271435\n",
      "42000/49000 loss: 0.326025771363697\n",
      "44000/49000 loss: 0.24367100462841054\n",
      "46000/49000 loss: 0.2849332177343222\n",
      "48000/49000 loss: 0.27996228237021875\n",
      "epoch 29: valid acc = 0.89, new learning rate = 0.00011296777049628272\n",
      "2000/49000 loss: 0.27896966996094646\n",
      "4000/49000 loss: 0.2816619672459899\n",
      "6000/49000 loss: 0.4088578979970253\n",
      "8000/49000 loss: 0.34416699900770253\n",
      "10000/49000 loss: 0.23328509660030844\n",
      "12000/49000 loss: 0.3245976852060052\n",
      "14000/49000 loss: 0.2784436901936915\n",
      "16000/49000 loss: 0.28726673780955175\n",
      "18000/49000 loss: 0.278901692739942\n",
      "20000/49000 loss: 0.29905973050518425\n",
      "22000/49000 loss: 0.37793420318821863\n",
      "24000/49000 loss: 0.2807208648020783\n",
      "26000/49000 loss: 0.29725403401612327\n",
      "28000/49000 loss: 0.28200460174319886\n",
      "30000/49000 loss: 0.2568204644414368\n",
      "32000/49000 loss: 0.2904852839718067\n",
      "34000/49000 loss: 0.2681493463961174\n",
      "36000/49000 loss: 0.23145294578837555\n",
      "38000/49000 loss: 0.3204619418293505\n",
      "40000/49000 loss: 0.2736135829059528\n",
      "42000/49000 loss: 0.28220765139119985\n",
      "44000/49000 loss: 0.2994507274788057\n",
      "46000/49000 loss: 0.2650447433893364\n",
      "48000/49000 loss: 0.34125064288331397\n",
      "epoch 30: valid acc = 0.887, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.895061224489796\n",
      "test acc: 0.887\n",
      "test acc: 0.8692\n",
      "number of batches for training: 245\n",
      "epoch 1: valid acc = 0.748, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.815, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.826, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.841, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.849, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.863, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.861, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.869, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.868, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.875, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.874, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.874, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.878, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.878, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.879, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.887, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.881, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.886, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.886, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.884, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.892, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.89, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.888, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.892, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.892, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.889, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.894, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.891, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.891, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.892, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.895265306122449\n",
      "test acc: 0.892\n",
      "test acc: 0.8689\n",
      "number of batches for training: 245\n",
      "2000/49000 loss: 2.6226868793483957\n",
      "4000/49000 loss: 2.602390236752976\n",
      "6000/49000 loss: 2.6383007621004966\n",
      "8000/49000 loss: 2.525502917361862\n",
      "10000/49000 loss: 2.286597941143304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/49000 loss: 2.1424418198975044\n",
      "14000/49000 loss: 1.809349354348117\n",
      "16000/49000 loss: 1.839203579142886\n",
      "18000/49000 loss: 1.4246448817897293\n",
      "20000/49000 loss: 1.3121387367795396\n",
      "22000/49000 loss: 1.2881984259955508\n",
      "24000/49000 loss: 1.0819283115940268\n",
      "26000/49000 loss: 1.2714154174432488\n",
      "28000/49000 loss: 1.1459390146995456\n",
      "30000/49000 loss: 1.0081430394917996\n",
      "32000/49000 loss: 0.8675826940878442\n",
      "34000/49000 loss: 0.8751936447129541\n",
      "36000/49000 loss: 0.8917594737038596\n",
      "38000/49000 loss: 0.8760150826946285\n",
      "40000/49000 loss: 0.7870594310881551\n",
      "42000/49000 loss: 0.7212195503348842\n",
      "44000/49000 loss: 0.7257077152561058\n",
      "46000/49000 loss: 0.7464887010563054\n",
      "48000/49000 loss: 0.7686582848717839\n",
      "epoch 1: valid acc = 0.749, new learning rate = 0.000475\n",
      "2000/49000 loss: 0.7651601637535022\n",
      "4000/49000 loss: 0.5926321715905543\n",
      "6000/49000 loss: 0.5933379847000069\n",
      "8000/49000 loss: 0.6010477997585596\n",
      "10000/49000 loss: 0.6142716488850503\n",
      "12000/49000 loss: 0.669068939037906\n",
      "14000/49000 loss: 0.5940405124658602\n",
      "16000/49000 loss: 0.588329095384499\n",
      "18000/49000 loss: 0.5395103984179853\n",
      "20000/49000 loss: 0.581258227348628\n",
      "22000/49000 loss: 0.4941735376061815\n",
      "24000/49000 loss: 0.6064492295520669\n",
      "26000/49000 loss: 0.5046876791404632\n",
      "28000/49000 loss: 0.49914933338346745\n",
      "30000/49000 loss: 0.5011104054721645\n",
      "32000/49000 loss: 0.6579113650340387\n",
      "34000/49000 loss: 0.603086980408991\n",
      "36000/49000 loss: 0.4758639660925767\n",
      "38000/49000 loss: 0.4882660138107727\n",
      "40000/49000 loss: 0.4862546818819803\n",
      "42000/49000 loss: 0.5068254499957955\n",
      "44000/49000 loss: 0.4659664433758037\n",
      "46000/49000 loss: 0.4744882871969865\n",
      "48000/49000 loss: 0.5321201796079122\n",
      "epoch 2: valid acc = 0.809, new learning rate = 0.00045125\n",
      "2000/49000 loss: 0.43338810879804085\n",
      "4000/49000 loss: 0.5493008043997618\n",
      "6000/49000 loss: 0.46862124454065246\n",
      "8000/49000 loss: 0.500238181420169\n",
      "10000/49000 loss: 0.44244334090366916\n",
      "12000/49000 loss: 0.5614403423482878\n",
      "14000/49000 loss: 0.598950709123061\n",
      "16000/49000 loss: 0.43038610438780334\n",
      "18000/49000 loss: 0.4624437855423444\n",
      "20000/49000 loss: 0.5705664953347995\n",
      "22000/49000 loss: 0.545925644010726\n",
      "24000/49000 loss: 0.4525916591516014\n",
      "26000/49000 loss: 0.49464380754881015\n",
      "28000/49000 loss: 0.5384271199905747\n",
      "30000/49000 loss: 0.5391372449124092\n",
      "32000/49000 loss: 0.4400270469682157\n",
      "34000/49000 loss: 0.4927972335983113\n",
      "36000/49000 loss: 0.5402848383286297\n",
      "38000/49000 loss: 0.4953589310108991\n",
      "40000/49000 loss: 0.5130328772035517\n",
      "42000/49000 loss: 0.4283785234297594\n",
      "44000/49000 loss: 0.4487027953756235\n",
      "46000/49000 loss: 0.45856787444704405\n",
      "48000/49000 loss: 0.4206509474232324\n",
      "epoch 3: valid acc = 0.841, new learning rate = 0.0004286875\n",
      "2000/49000 loss: 0.40970809472490416\n",
      "4000/49000 loss: 0.3535699894211397\n",
      "6000/49000 loss: 0.387458125987382\n",
      "8000/49000 loss: 0.49048607437006064\n",
      "10000/49000 loss: 0.43844552091417044\n",
      "12000/49000 loss: 0.49193699997160845\n",
      "14000/49000 loss: 0.40066549199788254\n",
      "16000/49000 loss: 0.41385655844243924\n",
      "18000/49000 loss: 0.4109427616742355\n",
      "20000/49000 loss: 0.4573881551882463\n",
      "22000/49000 loss: 0.3601071706893943\n",
      "24000/49000 loss: 0.37761169309669157\n",
      "26000/49000 loss: 0.4619125942588528\n",
      "28000/49000 loss: 0.40769113544874147\n",
      "30000/49000 loss: 0.3578149625546738\n",
      "32000/49000 loss: 0.5657940939700942\n",
      "34000/49000 loss: 0.40904196930172504\n",
      "36000/49000 loss: 0.4917544022800956\n",
      "38000/49000 loss: 0.4874912873613756\n",
      "40000/49000 loss: 0.3302212928404442\n",
      "42000/49000 loss: 0.3811458119902891\n",
      "44000/49000 loss: 0.4135267223026083\n",
      "46000/49000 loss: 0.4534286777591028\n",
      "48000/49000 loss: 0.4180713014790205\n",
      "epoch 4: valid acc = 0.846, new learning rate = 0.00040725312499999993\n",
      "2000/49000 loss: 0.3896799483699053\n",
      "4000/49000 loss: 0.41499771996973067\n",
      "6000/49000 loss: 0.4056108927908312\n",
      "8000/49000 loss: 0.39580651829839897\n",
      "10000/49000 loss: 0.4564115586976409\n",
      "12000/49000 loss: 0.3687057967537431\n",
      "14000/49000 loss: 0.44296758474306824\n",
      "16000/49000 loss: 0.4218165901167954\n",
      "18000/49000 loss: 0.31468259533049303\n",
      "20000/49000 loss: 0.4030455395225552\n",
      "22000/49000 loss: 0.41792139711856374\n",
      "24000/49000 loss: 0.5093342175706409\n",
      "26000/49000 loss: 0.4012845139775733\n",
      "28000/49000 loss: 0.4705463612873319\n",
      "30000/49000 loss: 0.49632184158127923\n",
      "32000/49000 loss: 0.4071435826600965\n",
      "34000/49000 loss: 0.3578219698133755\n",
      "36000/49000 loss: 0.3415251099200103\n",
      "38000/49000 loss: 0.4071663134505344\n",
      "40000/49000 loss: 0.37905962407804855\n",
      "42000/49000 loss: 0.4662959776505705\n",
      "44000/49000 loss: 0.40444927795279234\n",
      "46000/49000 loss: 0.4071491127956138\n",
      "48000/49000 loss: 0.5451988042253176\n",
      "epoch 5: valid acc = 0.847, new learning rate = 0.0003868904687499999\n",
      "2000/49000 loss: 0.3295833576136038\n",
      "4000/49000 loss: 0.3835599061886419\n",
      "6000/49000 loss: 0.4291799603916252\n",
      "8000/49000 loss: 0.3361503628669089\n",
      "10000/49000 loss: 0.579315082701238\n",
      "12000/49000 loss: 0.3929409384854037\n",
      "14000/49000 loss: 0.415763273838994\n",
      "16000/49000 loss: 0.34695487314170026\n",
      "18000/49000 loss: 0.3201842032361346\n",
      "20000/49000 loss: 0.4507418483829073\n",
      "22000/49000 loss: 0.4013792202670115\n",
      "24000/49000 loss: 0.39697752692289273\n",
      "26000/49000 loss: 0.379255780701554\n",
      "28000/49000 loss: 0.4380861430268602\n",
      "30000/49000 loss: 0.4232137332485512\n",
      "32000/49000 loss: 0.4029375057902054\n",
      "34000/49000 loss: 0.45894548691671755\n",
      "36000/49000 loss: 0.41319875790325383\n",
      "38000/49000 loss: 0.4255777727125136\n",
      "40000/49000 loss: 0.3759448094359998\n",
      "42000/49000 loss: 0.4146018807153095\n",
      "44000/49000 loss: 0.4595178610576554\n",
      "46000/49000 loss: 0.3370698524455277\n",
      "48000/49000 loss: 0.37160929931865866\n",
      "epoch 6: valid acc = 0.855, new learning rate = 0.0003675459453124999\n",
      "2000/49000 loss: 0.49844267105776724\n",
      "4000/49000 loss: 0.4706918017314598\n",
      "6000/49000 loss: 0.35632419006767246\n",
      "8000/49000 loss: 0.3836177329754585\n",
      "10000/49000 loss: 0.4549836414346394\n",
      "12000/49000 loss: 0.3561893257012721\n",
      "14000/49000 loss: 0.389568315103735\n",
      "16000/49000 loss: 0.43631029240246694\n",
      "18000/49000 loss: 0.4051873939787573\n",
      "20000/49000 loss: 0.3597246215141379\n",
      "22000/49000 loss: 0.38381129448450807\n",
      "24000/49000 loss: 0.43264771950195957\n",
      "26000/49000 loss: 0.33944222845629984\n",
      "28000/49000 loss: 0.3694797141661616\n",
      "30000/49000 loss: 0.43123550240271125\n",
      "32000/49000 loss: 0.3754689995351655\n",
      "34000/49000 loss: 0.3195290714388905\n",
      "36000/49000 loss: 0.515685807769404\n",
      "38000/49000 loss: 0.41348612500728454\n",
      "40000/49000 loss: 0.3680631634017784\n",
      "42000/49000 loss: 0.3119896644596083\n",
      "44000/49000 loss: 0.36070046185448085\n",
      "46000/49000 loss: 0.47867702374952825\n",
      "48000/49000 loss: 0.4358727153176583\n",
      "epoch 7: valid acc = 0.865, new learning rate = 0.00034916864804687486\n",
      "2000/49000 loss: 0.48474840639603733\n",
      "4000/49000 loss: 0.346456151603896\n",
      "6000/49000 loss: 0.3891334482545561\n",
      "8000/49000 loss: 0.34729751166808315\n",
      "10000/49000 loss: 0.4400233456484529\n",
      "12000/49000 loss: 0.40166295160300264\n",
      "14000/49000 loss: 0.38207127468935576\n",
      "16000/49000 loss: 0.38647284920998065\n",
      "18000/49000 loss: 0.3373201154470711\n",
      "20000/49000 loss: 0.42231718382591615\n",
      "22000/49000 loss: 0.33709052639176773\n",
      "24000/49000 loss: 0.4400702407092542\n",
      "26000/49000 loss: 0.37550865993308713\n",
      "28000/49000 loss: 0.3875948978875067\n",
      "30000/49000 loss: 0.39008327823706995\n",
      "32000/49000 loss: 0.3502724945600441\n",
      "34000/49000 loss: 0.30387190922689344\n",
      "36000/49000 loss: 0.3775120252628643\n",
      "38000/49000 loss: 0.3079028727667334\n",
      "40000/49000 loss: 0.3712105193415841\n",
      "42000/49000 loss: 0.3723809829458046\n",
      "44000/49000 loss: 0.3209956346533718\n",
      "46000/49000 loss: 0.332443951922202\n",
      "48000/49000 loss: 0.34128735198125415\n",
      "epoch 8: valid acc = 0.87, new learning rate = 0.0003317102156445311\n",
      "2000/49000 loss: 0.38926327638517016\n",
      "4000/49000 loss: 0.3691810086858436\n",
      "6000/49000 loss: 0.35868880092318856\n",
      "8000/49000 loss: 0.3522217583936455\n",
      "10000/49000 loss: 0.48930151770372404\n",
      "12000/49000 loss: 0.318004467111572\n",
      "14000/49000 loss: 0.358111112126949\n",
      "16000/49000 loss: 0.300998600138384\n",
      "18000/49000 loss: 0.28702227077108877\n",
      "20000/49000 loss: 0.26938912177946717\n",
      "22000/49000 loss: 0.2725180827171543\n",
      "24000/49000 loss: 0.343776518209466\n",
      "26000/49000 loss: 0.2773447862721399\n",
      "28000/49000 loss: 0.4069857714047296\n",
      "30000/49000 loss: 0.4805020440812089\n",
      "32000/49000 loss: 0.39717023246956085\n",
      "34000/49000 loss: 0.34355901529513333\n",
      "36000/49000 loss: 0.32845820448481966\n",
      "38000/49000 loss: 0.38330037032690695\n",
      "40000/49000 loss: 0.31729760281904223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/49000 loss: 0.3915653336957061\n",
      "44000/49000 loss: 0.31274043623692144\n",
      "46000/49000 loss: 0.3984039021784954\n",
      "48000/49000 loss: 0.28478133733511346\n",
      "epoch 9: valid acc = 0.871, new learning rate = 0.0003151247048623045\n",
      "2000/49000 loss: 0.430151252691527\n",
      "4000/49000 loss: 0.3137702425179323\n",
      "6000/49000 loss: 0.41182680300689584\n",
      "8000/49000 loss: 0.3126257449952215\n",
      "10000/49000 loss: 0.2872190371239684\n",
      "12000/49000 loss: 0.4244019552382811\n",
      "14000/49000 loss: 0.3470743726240658\n",
      "16000/49000 loss: 0.327649362921143\n",
      "18000/49000 loss: 0.31100649169799227\n",
      "20000/49000 loss: 0.30970254512684714\n",
      "22000/49000 loss: 0.4156133623749514\n",
      "24000/49000 loss: 0.45170350979736906\n",
      "26000/49000 loss: 0.40230211752889933\n",
      "28000/49000 loss: 0.38128042759220804\n",
      "30000/49000 loss: 0.35462483411068746\n",
      "32000/49000 loss: 0.3393961805900125\n",
      "34000/49000 loss: 0.28572221158303035\n",
      "36000/49000 loss: 0.32586801105357877\n",
      "38000/49000 loss: 0.3409961899750617\n",
      "40000/49000 loss: 0.34843257961590834\n",
      "42000/49000 loss: 0.37010705480887995\n",
      "44000/49000 loss: 0.3280647716354016\n",
      "46000/49000 loss: 0.3121772026612103\n",
      "48000/49000 loss: 0.3377710732037469\n",
      "epoch 10: valid acc = 0.874, new learning rate = 0.00029936846961918924\n",
      "2000/49000 loss: 0.42266689742070485\n",
      "4000/49000 loss: 0.4141866751706342\n",
      "6000/49000 loss: 0.3013466993522049\n",
      "8000/49000 loss: 0.38659479098580385\n",
      "10000/49000 loss: 0.217109013801113\n",
      "12000/49000 loss: 0.3968668684340838\n",
      "14000/49000 loss: 0.34426462529398966\n",
      "16000/49000 loss: 0.32712400952879633\n",
      "18000/49000 loss: 0.39215561372291885\n",
      "20000/49000 loss: 0.35713078105993357\n",
      "22000/49000 loss: 0.4561496512816634\n",
      "24000/49000 loss: 0.3919186503968932\n",
      "26000/49000 loss: 0.3415966556386931\n",
      "28000/49000 loss: 0.3989127860626393\n",
      "30000/49000 loss: 0.22491722076326057\n",
      "32000/49000 loss: 0.3193115622605664\n",
      "34000/49000 loss: 0.42875647628072044\n",
      "36000/49000 loss: 0.4373598579461054\n",
      "38000/49000 loss: 0.3798032650942388\n",
      "40000/49000 loss: 0.4010173098456704\n",
      "42000/49000 loss: 0.3669356703653249\n",
      "44000/49000 loss: 0.3898131420973521\n",
      "46000/49000 loss: 0.3275254603550068\n",
      "48000/49000 loss: 0.4311444499468688\n",
      "epoch 11: valid acc = 0.866, new learning rate = 0.00028440004613822977\n",
      "2000/49000 loss: 0.4510327393494642\n",
      "4000/49000 loss: 0.32532475002296984\n",
      "6000/49000 loss: 0.4410062392302716\n",
      "8000/49000 loss: 0.312854442421218\n",
      "10000/49000 loss: 0.3638647901731182\n",
      "12000/49000 loss: 0.3567698719701444\n",
      "14000/49000 loss: 0.4184160765372798\n",
      "16000/49000 loss: 0.3180380539887955\n",
      "18000/49000 loss: 0.29680783383958553\n",
      "20000/49000 loss: 0.37469315238504564\n",
      "22000/49000 loss: 0.42845726908311715\n",
      "24000/49000 loss: 0.29002336557210845\n",
      "26000/49000 loss: 0.323761881586553\n",
      "28000/49000 loss: 0.34004371440012277\n",
      "30000/49000 loss: 0.3801376201680527\n",
      "32000/49000 loss: 0.3548807150604373\n",
      "34000/49000 loss: 0.35728928717121916\n",
      "36000/49000 loss: 0.3285005837861881\n",
      "38000/49000 loss: 0.32892985411575\n",
      "40000/49000 loss: 0.24497164494157414\n",
      "42000/49000 loss: 0.4074312062837726\n",
      "44000/49000 loss: 0.39742419503784304\n",
      "46000/49000 loss: 0.2213801206277194\n",
      "48000/49000 loss: 0.44885884023870876\n",
      "epoch 12: valid acc = 0.878, new learning rate = 0.00027018004383131826\n",
      "2000/49000 loss: 0.4812242710754546\n",
      "4000/49000 loss: 0.38781501452136885\n",
      "6000/49000 loss: 0.35156261772176706\n",
      "8000/49000 loss: 0.3163074430058752\n",
      "10000/49000 loss: 0.41448853545597586\n",
      "12000/49000 loss: 0.47893418278475486\n",
      "14000/49000 loss: 0.3262325544512406\n",
      "16000/49000 loss: 0.2738589652872187\n",
      "18000/49000 loss: 0.31828065951536716\n",
      "20000/49000 loss: 0.43164714719940345\n",
      "22000/49000 loss: 0.32556991174275723\n",
      "24000/49000 loss: 0.36973192572135144\n",
      "26000/49000 loss: 0.29631604883398815\n",
      "28000/49000 loss: 0.31815600688180873\n",
      "30000/49000 loss: 0.3019910062148508\n",
      "32000/49000 loss: 0.33046278485751274\n",
      "34000/49000 loss: 0.4785054242250903\n",
      "36000/49000 loss: 0.3063500498273435\n",
      "38000/49000 loss: 0.2830576467157372\n",
      "40000/49000 loss: 0.37701311218287725\n",
      "42000/49000 loss: 0.3587442044456345\n",
      "44000/49000 loss: 0.3283958047217483\n",
      "46000/49000 loss: 0.28375375069033676\n",
      "48000/49000 loss: 0.2655092016657156\n",
      "epoch 13: valid acc = 0.884, new learning rate = 0.00025667104163975234\n",
      "2000/49000 loss: 0.3479000415973219\n",
      "4000/49000 loss: 0.3520935692304497\n",
      "6000/49000 loss: 0.397156110362982\n",
      "8000/49000 loss: 0.26362991306368894\n",
      "10000/49000 loss: 0.34688895000204806\n",
      "12000/49000 loss: 0.3502049560026635\n",
      "14000/49000 loss: 0.3700596187659857\n",
      "16000/49000 loss: 0.41774348178171605\n",
      "18000/49000 loss: 0.3922798693677784\n",
      "20000/49000 loss: 0.32755521108916824\n",
      "22000/49000 loss: 0.3674998550683315\n",
      "24000/49000 loss: 0.3110043213090498\n",
      "26000/49000 loss: 0.3113143503552059\n",
      "28000/49000 loss: 0.360071068678943\n",
      "30000/49000 loss: 0.29633339706416023\n",
      "32000/49000 loss: 0.3689374949600733\n",
      "34000/49000 loss: 0.34175159837892827\n",
      "36000/49000 loss: 0.4180747168517891\n",
      "38000/49000 loss: 0.3008691575362664\n",
      "40000/49000 loss: 0.3536372031925106\n",
      "42000/49000 loss: 0.2764226224118275\n",
      "44000/49000 loss: 0.3121804604588986\n",
      "46000/49000 loss: 0.2495423443225361\n",
      "48000/49000 loss: 0.21596633086\n",
      "epoch 14: valid acc = 0.882, new learning rate = 0.00024383748955776472\n",
      "2000/49000 loss: 0.3209710042607322\n",
      "4000/49000 loss: 0.26418685193617625\n",
      "6000/49000 loss: 0.3585789107976453\n",
      "8000/49000 loss: 0.30371054487414895\n",
      "10000/49000 loss: 0.31727142733172015\n",
      "12000/49000 loss: 0.3919273730539898\n",
      "14000/49000 loss: 0.3942734905750501\n",
      "16000/49000 loss: 0.2904808479939531\n",
      "18000/49000 loss: 0.3435049483912023\n",
      "20000/49000 loss: 0.26046840080883177\n",
      "22000/49000 loss: 0.33727875625096193\n",
      "24000/49000 loss: 0.42833792773176094\n",
      "26000/49000 loss: 0.2976124241152022\n",
      "28000/49000 loss: 0.3318628354555599\n",
      "30000/49000 loss: 0.29909755224503337\n",
      "32000/49000 loss: 0.3338827259471047\n",
      "34000/49000 loss: 0.28417898809697334\n",
      "36000/49000 loss: 0.352492967612554\n",
      "38000/49000 loss: 0.4173941103537969\n",
      "40000/49000 loss: 0.3723968338810139\n",
      "42000/49000 loss: 0.3224631425115298\n",
      "44000/49000 loss: 0.37058511294591867\n",
      "46000/49000 loss: 0.3244668420228417\n",
      "48000/49000 loss: 0.31322793440401353\n",
      "epoch 15: valid acc = 0.884, new learning rate = 0.00023164561507987649\n",
      "2000/49000 loss: 0.38126307356948136\n",
      "4000/49000 loss: 0.2616115729698009\n",
      "6000/49000 loss: 0.3605737196038046\n",
      "8000/49000 loss: 0.2874890607500903\n",
      "10000/49000 loss: 0.3453767923916996\n",
      "12000/49000 loss: 0.30242930414875163\n",
      "14000/49000 loss: 0.2896906229378556\n",
      "16000/49000 loss: 0.3047776992322194\n",
      "18000/49000 loss: 0.4066102730516289\n",
      "20000/49000 loss: 0.34996248104369987\n",
      "22000/49000 loss: 0.33931140675259325\n",
      "24000/49000 loss: 0.30920987223838636\n",
      "26000/49000 loss: 0.3162511850956035\n",
      "28000/49000 loss: 0.29348246117640064\n",
      "30000/49000 loss: 0.3659491194163903\n",
      "32000/49000 loss: 0.29740533946114955\n",
      "34000/49000 loss: 0.4178094284606763\n",
      "36000/49000 loss: 0.3655762009626302\n",
      "38000/49000 loss: 0.3648481485369535\n",
      "40000/49000 loss: 0.38632993279814226\n",
      "42000/49000 loss: 0.29734924601132706\n",
      "44000/49000 loss: 0.3074603450535387\n",
      "46000/49000 loss: 0.3479444869573047\n",
      "48000/49000 loss: 0.3247314833845463\n",
      "epoch 16: valid acc = 0.886, new learning rate = 0.00022006333432588265\n",
      "2000/49000 loss: 0.40758219944431767\n",
      "4000/49000 loss: 0.31199595428829774\n",
      "6000/49000 loss: 0.4032534805972374\n",
      "8000/49000 loss: 0.2233603247533654\n",
      "10000/49000 loss: 0.2853899284541787\n",
      "12000/49000 loss: 0.2727821021963446\n",
      "14000/49000 loss: 0.2946167149928222\n",
      "16000/49000 loss: 0.3729787111703564\n",
      "18000/49000 loss: 0.3923607794850391\n",
      "20000/49000 loss: 0.27710405100235563\n",
      "22000/49000 loss: 0.3485555672590139\n",
      "24000/49000 loss: 0.2468199553255543\n",
      "26000/49000 loss: 0.3508423797591804\n",
      "28000/49000 loss: 0.24464016240960265\n",
      "30000/49000 loss: 0.3351611247111735\n",
      "32000/49000 loss: 0.2456748493439375\n",
      "34000/49000 loss: 0.32457567947283855\n",
      "36000/49000 loss: 0.4041656143280326\n",
      "38000/49000 loss: 0.35653488706400877\n",
      "40000/49000 loss: 0.22551799251002913\n",
      "42000/49000 loss: 0.25312253279194596\n",
      "44000/49000 loss: 0.30719293505356676\n",
      "46000/49000 loss: 0.3153965349501107\n",
      "48000/49000 loss: 0.35860733724574473\n",
      "epoch 17: valid acc = 0.89, new learning rate = 0.00020906016760958852\n",
      "2000/49000 loss: 0.39521780801507567\n",
      "4000/49000 loss: 0.34635296084991374\n",
      "6000/49000 loss: 0.37723295227572784\n",
      "8000/49000 loss: 0.28467238326995425\n",
      "10000/49000 loss: 0.3267545208336948\n",
      "12000/49000 loss: 0.3143190952280099\n",
      "14000/49000 loss: 0.509881444715418\n",
      "16000/49000 loss: 0.41425561079742207\n",
      "18000/49000 loss: 0.3607746064482697\n",
      "20000/49000 loss: 0.2686171343661137\n",
      "22000/49000 loss: 0.3230717711417119\n",
      "24000/49000 loss: 0.36847623596883416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26000/49000 loss: 0.27819166212618585\n",
      "28000/49000 loss: 0.45577870891213584\n",
      "30000/49000 loss: 0.35480996451146324\n",
      "32000/49000 loss: 0.3459043104201498\n",
      "34000/49000 loss: 0.3659091403262068\n",
      "36000/49000 loss: 0.3861945788297441\n",
      "38000/49000 loss: 0.25876758054744153\n",
      "40000/49000 loss: 0.3215023091268101\n",
      "42000/49000 loss: 0.3779275869373051\n",
      "44000/49000 loss: 0.26829364538466544\n",
      "46000/49000 loss: 0.23033358624937866\n",
      "48000/49000 loss: 0.31286682511449504\n",
      "epoch 18: valid acc = 0.879, new learning rate = 0.00019860715922910907\n",
      "2000/49000 loss: 0.4478235855612717\n",
      "4000/49000 loss: 0.30197780506795413\n",
      "6000/49000 loss: 0.2957626954239362\n",
      "8000/49000 loss: 0.3513750749756356\n",
      "10000/49000 loss: 0.2998116067371456\n",
      "12000/49000 loss: 0.2701444589720738\n",
      "14000/49000 loss: 0.339431121222916\n",
      "16000/49000 loss: 0.247707321608771\n",
      "18000/49000 loss: 0.3260362182277915\n",
      "20000/49000 loss: 0.26458767814123735\n",
      "22000/49000 loss: 0.26688538956697233\n",
      "24000/49000 loss: 0.29197968146981684\n",
      "26000/49000 loss: 0.3326457595061198\n",
      "28000/49000 loss: 0.24127668824924314\n",
      "30000/49000 loss: 0.3219109549571304\n",
      "32000/49000 loss: 0.3217792255221366\n",
      "34000/49000 loss: 0.3491175686041874\n",
      "36000/49000 loss: 0.37814587297426844\n",
      "38000/49000 loss: 0.3635453029911802\n",
      "40000/49000 loss: 0.2780513553394118\n",
      "42000/49000 loss: 0.3781869253340921\n",
      "44000/49000 loss: 0.27901397742640227\n",
      "46000/49000 loss: 0.34210589545077397\n",
      "48000/49000 loss: 0.2477853659440658\n",
      "epoch 19: valid acc = 0.884, new learning rate = 0.0001886768012676536\n",
      "2000/49000 loss: 0.285024816399034\n",
      "4000/49000 loss: 0.33894085336503527\n",
      "6000/49000 loss: 0.30703856755792297\n",
      "8000/49000 loss: 0.34299892406772176\n",
      "10000/49000 loss: 0.24178816714213108\n",
      "12000/49000 loss: 0.3489942561273432\n",
      "14000/49000 loss: 0.25311497128021193\n",
      "16000/49000 loss: 0.33311553773344665\n",
      "18000/49000 loss: 0.3471427662833983\n",
      "20000/49000 loss: 0.36769396278783945\n",
      "22000/49000 loss: 0.3278838762096291\n",
      "24000/49000 loss: 0.29340817530244817\n",
      "26000/49000 loss: 0.3003580775687081\n",
      "28000/49000 loss: 0.2879154310867217\n",
      "30000/49000 loss: 0.29671221382192065\n",
      "32000/49000 loss: 0.3209204701223316\n",
      "34000/49000 loss: 0.2905487991739008\n",
      "36000/49000 loss: 0.34216185301733804\n",
      "38000/49000 loss: 0.3218245351226181\n",
      "40000/49000 loss: 0.3169468989165099\n",
      "42000/49000 loss: 0.28348206804518633\n",
      "44000/49000 loss: 0.36559181285860626\n",
      "46000/49000 loss: 0.31640755573601215\n",
      "48000/49000 loss: 0.43232340331012137\n",
      "epoch 20: valid acc = 0.884, new learning rate = 0.0001792429612042709\n",
      "2000/49000 loss: 0.28382832009818404\n",
      "4000/49000 loss: 0.44573727003662944\n",
      "6000/49000 loss: 0.27258168416342227\n",
      "8000/49000 loss: 0.37071989568912694\n",
      "10000/49000 loss: 0.32492042485273154\n",
      "12000/49000 loss: 0.3323730815242755\n",
      "14000/49000 loss: 0.3236424065635216\n",
      "16000/49000 loss: 0.2548962956516059\n",
      "18000/49000 loss: 0.23016639218407126\n",
      "20000/49000 loss: 0.3423561744849685\n",
      "22000/49000 loss: 0.270285251950389\n",
      "24000/49000 loss: 0.3269532324417974\n",
      "26000/49000 loss: 0.32423724977416707\n",
      "28000/49000 loss: 0.3907250557266897\n",
      "30000/49000 loss: 0.2735431293346808\n",
      "32000/49000 loss: 0.4419552037081647\n",
      "34000/49000 loss: 0.2575167531014981\n",
      "36000/49000 loss: 0.2927621514817864\n",
      "38000/49000 loss: 0.33246170395241986\n",
      "40000/49000 loss: 0.4102019203675028\n",
      "42000/49000 loss: 0.22351798653088187\n",
      "44000/49000 loss: 0.26928165648688224\n",
      "46000/49000 loss: 0.31457968515317525\n",
      "48000/49000 loss: 0.2611166450204992\n",
      "epoch 21: valid acc = 0.89, new learning rate = 0.00017028081314405735\n",
      "2000/49000 loss: 0.28437622243360644\n",
      "4000/49000 loss: 0.4177937218372708\n",
      "6000/49000 loss: 0.3119452057820025\n",
      "8000/49000 loss: 0.35664195151249023\n",
      "10000/49000 loss: 0.28121829403563925\n",
      "12000/49000 loss: 0.2852164925698898\n",
      "14000/49000 loss: 0.26971980841136267\n",
      "16000/49000 loss: 0.29226826684492446\n",
      "18000/49000 loss: 0.22415282881157675\n",
      "20000/49000 loss: 0.20808248939341495\n",
      "22000/49000 loss: 0.3280124687610504\n",
      "24000/49000 loss: 0.256656548394097\n",
      "26000/49000 loss: 0.33874841292398755\n",
      "28000/49000 loss: 0.2717655800099495\n",
      "30000/49000 loss: 0.2927287721962523\n",
      "32000/49000 loss: 0.35475554768254147\n",
      "34000/49000 loss: 0.27011374542467237\n",
      "36000/49000 loss: 0.2845309115041282\n",
      "38000/49000 loss: 0.2745325121805397\n",
      "40000/49000 loss: 0.269697656976293\n",
      "42000/49000 loss: 0.25027708425268524\n",
      "44000/49000 loss: 0.2734657813892816\n",
      "46000/49000 loss: 0.2377769481651354\n",
      "48000/49000 loss: 0.34797002606461525\n",
      "epoch 22: valid acc = 0.886, new learning rate = 0.00016176677248685447\n",
      "2000/49000 loss: 0.35653949832439974\n",
      "4000/49000 loss: 0.28689433410304954\n",
      "6000/49000 loss: 0.26714477093368016\n",
      "8000/49000 loss: 0.317514998176016\n",
      "10000/49000 loss: 0.24339839345552744\n",
      "12000/49000 loss: 0.3017604198650771\n",
      "14000/49000 loss: 0.22481340075794817\n",
      "16000/49000 loss: 0.24366193903589162\n",
      "18000/49000 loss: 0.27337981212758383\n",
      "20000/49000 loss: 0.3553572897006833\n",
      "22000/49000 loss: 0.36871108263345004\n",
      "24000/49000 loss: 0.2789387569616468\n",
      "26000/49000 loss: 0.23376626167382034\n",
      "28000/49000 loss: 0.31699309876067017\n",
      "30000/49000 loss: 0.3100027108630331\n",
      "32000/49000 loss: 0.40967441072741156\n",
      "34000/49000 loss: 0.3536159739461926\n",
      "36000/49000 loss: 0.27365176458874974\n",
      "38000/49000 loss: 0.3320253832953264\n",
      "40000/49000 loss: 0.395311242592273\n",
      "42000/49000 loss: 0.3670648991991826\n",
      "44000/49000 loss: 0.3767265375091883\n",
      "46000/49000 loss: 0.36055270037743137\n",
      "48000/49000 loss: 0.25388512846420946\n",
      "epoch 23: valid acc = 0.886, new learning rate = 0.00015367843386251173\n",
      "2000/49000 loss: 0.26709391179162845\n",
      "4000/49000 loss: 0.4433571295798031\n",
      "6000/49000 loss: 0.36139767567693093\n",
      "8000/49000 loss: 0.2468336496090388\n",
      "10000/49000 loss: 0.26023087609384915\n",
      "12000/49000 loss: 0.23827595925044898\n",
      "14000/49000 loss: 0.2574928531244258\n",
      "16000/49000 loss: 0.25694557512584376\n",
      "18000/49000 loss: 0.3107497191254596\n",
      "20000/49000 loss: 0.3032908602690777\n",
      "22000/49000 loss: 0.31119921985588156\n",
      "24000/49000 loss: 0.2838951112664084\n",
      "26000/49000 loss: 0.3012761041913959\n",
      "28000/49000 loss: 0.2983648766244274\n",
      "30000/49000 loss: 0.37231977938607325\n",
      "32000/49000 loss: 0.39468377959473316\n",
      "34000/49000 loss: 0.3215204171521291\n",
      "36000/49000 loss: 0.31964661746693784\n",
      "38000/49000 loss: 0.3157305957935067\n",
      "40000/49000 loss: 0.3885253348255698\n",
      "42000/49000 loss: 0.32850839155160916\n",
      "44000/49000 loss: 0.25179313168090073\n",
      "46000/49000 loss: 0.3375735361079148\n",
      "48000/49000 loss: 0.29259887204249285\n",
      "epoch 24: valid acc = 0.885, new learning rate = 0.00014599451216938612\n",
      "2000/49000 loss: 0.3259556044259704\n",
      "4000/49000 loss: 0.32573029654505736\n",
      "6000/49000 loss: 0.29065967838330725\n",
      "8000/49000 loss: 0.24129263802198622\n",
      "10000/49000 loss: 0.3601026424852867\n",
      "12000/49000 loss: 0.2565464535720716\n",
      "14000/49000 loss: 0.2634069684169772\n",
      "16000/49000 loss: 0.3105899559294135\n",
      "18000/49000 loss: 0.3200546477557324\n",
      "20000/49000 loss: 0.24830961321226797\n",
      "22000/49000 loss: 0.25580479603799594\n",
      "24000/49000 loss: 0.3753287686384079\n",
      "26000/49000 loss: 0.3439125361269803\n",
      "28000/49000 loss: 0.3920658497636083\n",
      "30000/49000 loss: 0.3153654156541553\n",
      "32000/49000 loss: 0.2944086622465158\n",
      "34000/49000 loss: 0.24424450004375597\n",
      "36000/49000 loss: 0.3107118620034706\n",
      "38000/49000 loss: 0.2658959875658693\n",
      "40000/49000 loss: 0.2981790647346938\n",
      "42000/49000 loss: 0.2130958323572021\n",
      "44000/49000 loss: 0.3398810610367628\n",
      "46000/49000 loss: 0.27855683691427807\n",
      "48000/49000 loss: 0.32628045429136604\n",
      "epoch 25: valid acc = 0.89, new learning rate = 0.00013869478656091682\n",
      "2000/49000 loss: 0.2493080446673475\n",
      "4000/49000 loss: 0.2750217928211654\n",
      "6000/49000 loss: 0.38366881692985894\n",
      "8000/49000 loss: 0.35888390867345216\n",
      "10000/49000 loss: 0.3307982240925125\n",
      "12000/49000 loss: 0.3009338844995364\n",
      "14000/49000 loss: 0.23480584567395807\n",
      "16000/49000 loss: 0.30215352462002104\n",
      "18000/49000 loss: 0.3951977346747326\n",
      "20000/49000 loss: 0.26609856666939125\n",
      "22000/49000 loss: 0.33083188633794525\n",
      "24000/49000 loss: 0.34839632694297473\n",
      "26000/49000 loss: 0.2954974390038969\n",
      "28000/49000 loss: 0.29425110766785056\n",
      "30000/49000 loss: 0.2819994897266718\n",
      "32000/49000 loss: 0.3297951051611565\n",
      "34000/49000 loss: 0.2138868878763534\n",
      "36000/49000 loss: 0.2792124144447571\n",
      "38000/49000 loss: 0.35493151427939873\n",
      "40000/49000 loss: 0.3433462299113864\n",
      "42000/49000 loss: 0.3057737803219801\n",
      "44000/49000 loss: 0.3023054394751415\n",
      "46000/49000 loss: 0.2522233810134018\n",
      "48000/49000 loss: 0.30173619583583655\n",
      "epoch 26: valid acc = 0.882, new learning rate = 0.00013176004723287096\n",
      "2000/49000 loss: 0.301269276837382\n",
      "4000/49000 loss: 0.2613950015376984\n",
      "6000/49000 loss: 0.36141783547139034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/49000 loss: 0.2978617713171588\n",
      "10000/49000 loss: 0.31343337364217455\n",
      "12000/49000 loss: 0.33732801155528086\n",
      "14000/49000 loss: 0.2362594848869194\n",
      "16000/49000 loss: 0.3681829162561908\n",
      "18000/49000 loss: 0.339614976985754\n",
      "20000/49000 loss: 0.26552737279781324\n",
      "22000/49000 loss: 0.3992734499561779\n",
      "24000/49000 loss: 0.2552942190044309\n",
      "26000/49000 loss: 0.19603212652151292\n",
      "28000/49000 loss: 0.30940787341596204\n",
      "30000/49000 loss: 0.3265521268673905\n",
      "32000/49000 loss: 0.3450009333636102\n",
      "34000/49000 loss: 0.3752941471809567\n",
      "36000/49000 loss: 0.23661207942264814\n",
      "38000/49000 loss: 0.36070767690991085\n",
      "40000/49000 loss: 0.3611066573815651\n",
      "42000/49000 loss: 0.3634895671076711\n",
      "44000/49000 loss: 0.40693126691749626\n",
      "46000/49000 loss: 0.2532305712169399\n",
      "48000/49000 loss: 0.2743886375558305\n",
      "epoch 27: valid acc = 0.884, new learning rate = 0.0001251720448712274\n",
      "2000/49000 loss: 0.29327912056772926\n",
      "4000/49000 loss: 0.25815707568815477\n",
      "6000/49000 loss: 0.3021471591477747\n",
      "8000/49000 loss: 0.3796347004856199\n",
      "10000/49000 loss: 0.34677414734584766\n",
      "12000/49000 loss: 0.4013657620483492\n",
      "14000/49000 loss: 0.36745723542247727\n",
      "16000/49000 loss: 0.49654774236136\n",
      "18000/49000 loss: 0.38842568022059726\n",
      "20000/49000 loss: 0.21133513413894003\n",
      "22000/49000 loss: 0.32177827680276233\n",
      "24000/49000 loss: 0.28425663767279913\n",
      "26000/49000 loss: 0.3244388242546027\n",
      "28000/49000 loss: 0.3313389213811176\n",
      "30000/49000 loss: 0.2827232942766686\n",
      "32000/49000 loss: 0.2620505543941639\n",
      "34000/49000 loss: 0.3189961434633719\n",
      "36000/49000 loss: 0.1984584114650541\n",
      "38000/49000 loss: 0.24559079474486945\n",
      "40000/49000 loss: 0.3534910894839527\n",
      "42000/49000 loss: 0.30930324467866327\n",
      "44000/49000 loss: 0.29425006994731995\n",
      "46000/49000 loss: 0.24587138502611242\n",
      "48000/49000 loss: 0.286189432467945\n",
      "epoch 28: valid acc = 0.884, new learning rate = 0.00011891344262766602\n",
      "2000/49000 loss: 0.3008358620050688\n",
      "4000/49000 loss: 0.30461328018449024\n",
      "6000/49000 loss: 0.3629666058299059\n",
      "8000/49000 loss: 0.29690228468680013\n",
      "10000/49000 loss: 0.30230581392196926\n",
      "12000/49000 loss: 0.31514569252317826\n",
      "14000/49000 loss: 0.23664810387169244\n",
      "16000/49000 loss: 0.274821542325327\n",
      "18000/49000 loss: 0.3334794941794217\n",
      "20000/49000 loss: 0.3539389222017742\n",
      "22000/49000 loss: 0.3032633676891978\n",
      "24000/49000 loss: 0.26653661298886444\n",
      "26000/49000 loss: 0.3625990975875598\n",
      "28000/49000 loss: 0.34751433735636406\n",
      "30000/49000 loss: 0.33727235698063485\n",
      "32000/49000 loss: 0.28271341982966325\n",
      "34000/49000 loss: 0.29040328154720013\n",
      "36000/49000 loss: 0.3167229124959469\n",
      "38000/49000 loss: 0.3088986999756558\n",
      "40000/49000 loss: 0.3311998983389294\n",
      "42000/49000 loss: 0.28644039668412347\n",
      "44000/49000 loss: 0.2645557894211097\n",
      "46000/49000 loss: 0.24947399575214593\n",
      "48000/49000 loss: 0.3777507860154334\n",
      "epoch 29: valid acc = 0.886, new learning rate = 0.00011296777049628272\n",
      "2000/49000 loss: 0.2582775761638782\n",
      "4000/49000 loss: 0.29369216793059044\n",
      "6000/49000 loss: 0.3423233766928309\n",
      "8000/49000 loss: 0.20724895542401622\n",
      "10000/49000 loss: 0.2651201413059079\n",
      "12000/49000 loss: 0.3977757646615367\n",
      "14000/49000 loss: 0.2597372938017807\n",
      "16000/49000 loss: 0.2567045927411339\n",
      "18000/49000 loss: 0.272471354595991\n",
      "20000/49000 loss: 0.2638885583361071\n",
      "22000/49000 loss: 0.28728536691973805\n",
      "24000/49000 loss: 0.31938982075670846\n",
      "26000/49000 loss: 0.29485970799329625\n",
      "28000/49000 loss: 0.3428641487361465\n",
      "30000/49000 loss: 0.24624216911870372\n",
      "32000/49000 loss: 0.33064550717432095\n",
      "34000/49000 loss: 0.3814059993645216\n",
      "36000/49000 loss: 0.24805270969694235\n",
      "38000/49000 loss: 0.2383416906348585\n",
      "40000/49000 loss: 0.19740566351401512\n",
      "42000/49000 loss: 0.3522878828915446\n",
      "44000/49000 loss: 0.2507534130939725\n",
      "46000/49000 loss: 0.3046602868599379\n",
      "48000/49000 loss: 0.251404316101114\n",
      "epoch 30: valid acc = 0.885, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.8955510204081633\n",
      "test acc: 0.885\n",
      "test acc: 0.869\n",
      "number of batches for training: 245\n",
      "epoch 1: valid acc = 0.742, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.808, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.839, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.848, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.856, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.857, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.866, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.864, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.867, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.868, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.871, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.878, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.874, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.875, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.885, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.881, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.884, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.879, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.886, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.889, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.883, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.886, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.89, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.883, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.883, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.891, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.886, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.89, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.891, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.891, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.8959795918367347\n",
      "test acc: 0.891\n",
      "test acc: 0.8677\n",
      "number of batches for training: 81\n",
      "6000/49000 loss: 2.675972214350832\n",
      "12000/49000 loss: 2.662497633260363\n",
      "18000/49000 loss: 2.561266376344602\n",
      "24000/49000 loss: 2.469226696423989\n",
      "30000/49000 loss: 2.2206623763922324\n",
      "36000/49000 loss: 2.1419188269643095\n",
      "42000/49000 loss: 1.926659386114655\n",
      "48000/49000 loss: 1.8243832398424615\n",
      "epoch 1: valid acc = 0.439, new learning rate = 0.000475\n",
      "6000/49000 loss: 1.583566568133605\n",
      "12000/49000 loss: 1.3167117503552457\n",
      "18000/49000 loss: 1.2875269899828248\n",
      "24000/49000 loss: 1.1492954058320182\n",
      "30000/49000 loss: 1.1846694889151745\n",
      "36000/49000 loss: 1.0914056071839537\n",
      "42000/49000 loss: 1.0516488350943642\n",
      "48000/49000 loss: 0.9520698359758619\n",
      "epoch 2: valid acc = 0.656, new learning rate = 0.00045125\n",
      "6000/49000 loss: 0.8985192478071815\n",
      "12000/49000 loss: 0.9497822159385609\n",
      "18000/49000 loss: 0.8049676240732592\n",
      "24000/49000 loss: 0.8164595070862406\n",
      "30000/49000 loss: 0.8206958954415868\n",
      "36000/49000 loss: 0.7394132837202877\n",
      "42000/49000 loss: 0.672594935333915\n",
      "48000/49000 loss: 0.7539719809992633\n",
      "epoch 3: valid acc = 0.741, new learning rate = 0.0004286875\n",
      "6000/49000 loss: 0.7246201426972817\n",
      "12000/49000 loss: 0.7320560253579834\n",
      "18000/49000 loss: 0.6415743227439397\n",
      "24000/49000 loss: 0.7076019719531934\n",
      "30000/49000 loss: 0.6499744486687727\n",
      "36000/49000 loss: 0.5953570487760209\n",
      "42000/49000 loss: 0.6416363073364371\n",
      "48000/49000 loss: 0.6537744435643369\n",
      "epoch 4: valid acc = 0.763, new learning rate = 0.00040725312499999993\n",
      "6000/49000 loss: 0.6329001695431901\n",
      "12000/49000 loss: 0.5738220289810518\n",
      "18000/49000 loss: 0.5993732343490463\n",
      "24000/49000 loss: 0.6238285258133819\n",
      "30000/49000 loss: 0.6389353855630827\n",
      "36000/49000 loss: 0.6868958325976939\n",
      "42000/49000 loss: 0.593563849308107\n",
      "48000/49000 loss: 0.5867922094848678\n",
      "epoch 5: valid acc = 0.787, new learning rate = 0.0003868904687499999\n",
      "6000/49000 loss: 0.5757270762681816\n",
      "12000/49000 loss: 0.5706636032151835\n",
      "18000/49000 loss: 0.5326997231942195\n",
      "24000/49000 loss: 0.5722891579795315\n",
      "30000/49000 loss: 0.518181533316823\n",
      "36000/49000 loss: 0.5444138613841648\n",
      "42000/49000 loss: 0.5963586045186281\n",
      "48000/49000 loss: 0.5328908719668825\n",
      "epoch 6: valid acc = 0.803, new learning rate = 0.0003675459453124999\n",
      "6000/49000 loss: 0.6138086979852686\n",
      "12000/49000 loss: 0.5155067941001701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/49000 loss: 0.5780447329631319\n",
      "24000/49000 loss: 0.471605807789153\n",
      "30000/49000 loss: 0.5202012001300427\n",
      "36000/49000 loss: 0.5296732615960716\n",
      "42000/49000 loss: 0.5276232003728443\n",
      "48000/49000 loss: 0.46327305538710234\n",
      "epoch 7: valid acc = 0.818, new learning rate = 0.00034916864804687486\n",
      "6000/49000 loss: 0.4739309948176619\n",
      "12000/49000 loss: 0.5291384356997482\n",
      "18000/49000 loss: 0.5164877213639136\n",
      "24000/49000 loss: 0.47476017995689035\n",
      "30000/49000 loss: 0.44798709365824096\n",
      "36000/49000 loss: 0.5731467858519383\n",
      "42000/49000 loss: 0.45045098728240135\n",
      "48000/49000 loss: 0.5071155622421971\n",
      "epoch 8: valid acc = 0.82, new learning rate = 0.0003317102156445311\n",
      "6000/49000 loss: 0.4739823491760084\n",
      "12000/49000 loss: 0.4450841651369815\n",
      "18000/49000 loss: 0.45190763581706056\n",
      "24000/49000 loss: 0.5206481003918517\n",
      "30000/49000 loss: 0.4314487426318438\n",
      "36000/49000 loss: 0.4569721971373253\n",
      "42000/49000 loss: 0.43767915171076016\n",
      "48000/49000 loss: 0.4814352699629389\n",
      "epoch 9: valid acc = 0.824, new learning rate = 0.0003151247048623045\n",
      "6000/49000 loss: 0.4672151235299628\n",
      "12000/49000 loss: 0.47781881700603746\n",
      "18000/49000 loss: 0.4704775680373346\n",
      "24000/49000 loss: 0.47221549687804487\n",
      "30000/49000 loss: 0.4579659114640123\n",
      "36000/49000 loss: 0.4637211810577131\n",
      "42000/49000 loss: 0.49793044526996055\n",
      "48000/49000 loss: 0.4481404728409517\n",
      "epoch 10: valid acc = 0.835, new learning rate = 0.00029936846961918924\n",
      "6000/49000 loss: 0.4431988742867564\n",
      "12000/49000 loss: 0.46762708196708214\n",
      "18000/49000 loss: 0.41330420224271375\n",
      "24000/49000 loss: 0.44898651165398035\n",
      "30000/49000 loss: 0.46800613808703123\n",
      "36000/49000 loss: 0.4281878609721721\n",
      "42000/49000 loss: 0.5059085290119041\n",
      "48000/49000 loss: 0.4395086365276376\n",
      "epoch 11: valid acc = 0.835, new learning rate = 0.00028440004613822977\n",
      "6000/49000 loss: 0.46525036205789394\n",
      "12000/49000 loss: 0.40210982047912613\n",
      "18000/49000 loss: 0.44617248850634683\n",
      "24000/49000 loss: 0.46450843261113733\n",
      "30000/49000 loss: 0.41296499071440845\n",
      "36000/49000 loss: 0.4254376347494871\n",
      "42000/49000 loss: 0.45064735929443756\n",
      "48000/49000 loss: 0.398416355655392\n",
      "epoch 12: valid acc = 0.833, new learning rate = 0.00027018004383131826\n",
      "6000/49000 loss: 0.42318641419873265\n",
      "12000/49000 loss: 0.46153608476811875\n",
      "18000/49000 loss: 0.45324304198984927\n",
      "24000/49000 loss: 0.40183254258062046\n",
      "30000/49000 loss: 0.46094933504970537\n",
      "36000/49000 loss: 0.39824578814915546\n",
      "42000/49000 loss: 0.4835957826758269\n",
      "48000/49000 loss: 0.35898259803297167\n",
      "epoch 13: valid acc = 0.846, new learning rate = 0.00025667104163975234\n",
      "6000/49000 loss: 0.43037992292514565\n",
      "12000/49000 loss: 0.4368832419695368\n",
      "18000/49000 loss: 0.4313928602679683\n",
      "24000/49000 loss: 0.4786492758647365\n",
      "30000/49000 loss: 0.46591790144346357\n",
      "36000/49000 loss: 0.39300235014406415\n",
      "42000/49000 loss: 0.39796986118769834\n",
      "48000/49000 loss: 0.4277950307354391\n",
      "epoch 14: valid acc = 0.844, new learning rate = 0.00024383748955776472\n",
      "6000/49000 loss: 0.4970352672734092\n",
      "12000/49000 loss: 0.37220339527344753\n",
      "18000/49000 loss: 0.39064425253029944\n",
      "24000/49000 loss: 0.4641430494362641\n",
      "30000/49000 loss: 0.4470326996385319\n",
      "36000/49000 loss: 0.4804269335335305\n",
      "42000/49000 loss: 0.4522652140895361\n",
      "48000/49000 loss: 0.384986490869993\n",
      "epoch 15: valid acc = 0.845, new learning rate = 0.00023164561507987649\n",
      "6000/49000 loss: 0.4181235760401519\n",
      "12000/49000 loss: 0.4650938651384143\n",
      "18000/49000 loss: 0.42864217033032564\n",
      "24000/49000 loss: 0.4087939410526777\n",
      "30000/49000 loss: 0.40932963985387005\n",
      "36000/49000 loss: 0.4579767271974059\n",
      "42000/49000 loss: 0.4598678955021345\n",
      "48000/49000 loss: 0.3792307663252308\n",
      "epoch 16: valid acc = 0.85, new learning rate = 0.00022006333432588265\n",
      "6000/49000 loss: 0.43286346277013504\n",
      "12000/49000 loss: 0.4193346397516174\n",
      "18000/49000 loss: 0.48295428785340844\n",
      "24000/49000 loss: 0.3880390368461812\n",
      "30000/49000 loss: 0.4221776299464311\n",
      "36000/49000 loss: 0.417231237330044\n",
      "42000/49000 loss: 0.4690028489814619\n",
      "48000/49000 loss: 0.4984266427628299\n",
      "epoch 17: valid acc = 0.848, new learning rate = 0.00020906016760958852\n",
      "6000/49000 loss: 0.39755874637135086\n",
      "12000/49000 loss: 0.3786779507313886\n",
      "18000/49000 loss: 0.4134101543175981\n",
      "24000/49000 loss: 0.43209694567469026\n",
      "30000/49000 loss: 0.42486918195119494\n",
      "36000/49000 loss: 0.43979532814258904\n",
      "42000/49000 loss: 0.4317763839905835\n",
      "48000/49000 loss: 0.47370523206685605\n",
      "epoch 18: valid acc = 0.853, new learning rate = 0.00019860715922910907\n",
      "6000/49000 loss: 0.4630938673509896\n",
      "12000/49000 loss: 0.44756070754417415\n",
      "18000/49000 loss: 0.41288272262119424\n",
      "24000/49000 loss: 0.3890535395245341\n",
      "30000/49000 loss: 0.4548097749496719\n",
      "36000/49000 loss: 0.41942487517930044\n",
      "42000/49000 loss: 0.47939271826601876\n",
      "48000/49000 loss: 0.3901077450846475\n",
      "epoch 19: valid acc = 0.851, new learning rate = 0.0001886768012676536\n",
      "6000/49000 loss: 0.42437076125431766\n",
      "12000/49000 loss: 0.38868240397318565\n",
      "18000/49000 loss: 0.43687114173583225\n",
      "24000/49000 loss: 0.419638020987144\n",
      "30000/49000 loss: 0.39409337420530977\n",
      "36000/49000 loss: 0.470072277836533\n",
      "42000/49000 loss: 0.40584653528180037\n",
      "48000/49000 loss: 0.41235622773887826\n",
      "epoch 20: valid acc = 0.85, new learning rate = 0.0001792429612042709\n",
      "6000/49000 loss: 0.4374796470081346\n",
      "12000/49000 loss: 0.3710683665471523\n",
      "18000/49000 loss: 0.45263894604706983\n",
      "24000/49000 loss: 0.3736039551557916\n",
      "30000/49000 loss: 0.44830777635607727\n",
      "36000/49000 loss: 0.41559787496837514\n",
      "42000/49000 loss: 0.4017826453813345\n",
      "48000/49000 loss: 0.4075154864763421\n",
      "epoch 21: valid acc = 0.855, new learning rate = 0.00017028081314405735\n",
      "6000/49000 loss: 0.41485693454474154\n",
      "12000/49000 loss: 0.38831721872444164\n",
      "18000/49000 loss: 0.4125196200359677\n",
      "24000/49000 loss: 0.40637375789177643\n",
      "30000/49000 loss: 0.3490502658253735\n",
      "36000/49000 loss: 0.3910857075826002\n",
      "42000/49000 loss: 0.29752135465052154\n",
      "48000/49000 loss: 0.3784444104545608\n",
      "epoch 22: valid acc = 0.856, new learning rate = 0.00016176677248685447\n",
      "6000/49000 loss: 0.40235069776619425\n",
      "12000/49000 loss: 0.40413446765028316\n",
      "18000/49000 loss: 0.4023921086884747\n",
      "24000/49000 loss: 0.3988146419754811\n",
      "30000/49000 loss: 0.38394698354169965\n",
      "36000/49000 loss: 0.3397161380036454\n",
      "42000/49000 loss: 0.37678754570214745\n",
      "48000/49000 loss: 0.40061758689719856\n",
      "epoch 23: valid acc = 0.854, new learning rate = 0.00015367843386251173\n",
      "6000/49000 loss: 0.42692215241015713\n",
      "12000/49000 loss: 0.4359123968074909\n",
      "18000/49000 loss: 0.3968399358119588\n",
      "24000/49000 loss: 0.42178782928545566\n",
      "30000/49000 loss: 0.4070693434389573\n",
      "36000/49000 loss: 0.3634553644704944\n",
      "42000/49000 loss: 0.43794446790237695\n",
      "48000/49000 loss: 0.41508483760492126\n",
      "epoch 24: valid acc = 0.857, new learning rate = 0.00014599451216938612\n",
      "6000/49000 loss: 0.38976910901785056\n",
      "12000/49000 loss: 0.4398991468646159\n",
      "18000/49000 loss: 0.33307364981366755\n",
      "24000/49000 loss: 0.44140644156214054\n",
      "30000/49000 loss: 0.41738486737219255\n",
      "36000/49000 loss: 0.4342566632771582\n",
      "42000/49000 loss: 0.37860308880063565\n",
      "48000/49000 loss: 0.3637096735318134\n",
      "epoch 25: valid acc = 0.858, new learning rate = 0.00013869478656091682\n",
      "6000/49000 loss: 0.431384921892986\n",
      "12000/49000 loss: 0.3704782391432649\n",
      "18000/49000 loss: 0.4595214138126452\n",
      "24000/49000 loss: 0.3801860421723016\n",
      "30000/49000 loss: 0.3825595256821076\n",
      "36000/49000 loss: 0.35487329633966425\n",
      "42000/49000 loss: 0.4090563991198202\n",
      "48000/49000 loss: 0.4557878037727667\n",
      "epoch 26: valid acc = 0.859, new learning rate = 0.00013176004723287096\n",
      "6000/49000 loss: 0.40052735578236176\n",
      "12000/49000 loss: 0.44634446392143257\n",
      "18000/49000 loss: 0.363027177704607\n",
      "24000/49000 loss: 0.3933697269873788\n",
      "30000/49000 loss: 0.35327539383148143\n",
      "36000/49000 loss: 0.3778530673191132\n",
      "42000/49000 loss: 0.42349230883387806\n",
      "48000/49000 loss: 0.3545179887497239\n",
      "epoch 27: valid acc = 0.857, new learning rate = 0.0001251720448712274\n",
      "6000/49000 loss: 0.42177094665466147\n",
      "12000/49000 loss: 0.4040643588271017\n",
      "18000/49000 loss: 0.3973154615773354\n",
      "24000/49000 loss: 0.38748313476285723\n",
      "30000/49000 loss: 0.39818903797209454\n",
      "36000/49000 loss: 0.3945941713678696\n",
      "42000/49000 loss: 0.3642107370511565\n",
      "48000/49000 loss: 0.39250787990355657\n",
      "epoch 28: valid acc = 0.863, new learning rate = 0.00011891344262766602\n",
      "6000/49000 loss: 0.3283416989065023\n",
      "12000/49000 loss: 0.3800537948288709\n",
      "18000/49000 loss: 0.3498401180387216\n",
      "24000/49000 loss: 0.47000448188471505\n",
      "30000/49000 loss: 0.369748535689903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36000/49000 loss: 0.4159040002797149\n",
      "42000/49000 loss: 0.37845747948134423\n",
      "48000/49000 loss: 0.38728845057824124\n",
      "epoch 29: valid acc = 0.859, new learning rate = 0.00011296777049628272\n",
      "6000/49000 loss: 0.38522930931801336\n",
      "12000/49000 loss: 0.34570122858938096\n",
      "18000/49000 loss: 0.35227029427211654\n",
      "24000/49000 loss: 0.4160343761723185\n",
      "30000/49000 loss: 0.34646971571568785\n",
      "36000/49000 loss: 0.3365728513816427\n",
      "42000/49000 loss: 0.3986408247129463\n",
      "48000/49000 loss: 0.3959344548735041\n",
      "epoch 30: valid acc = 0.861, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.8646122448979592\n",
      "test acc: 0.861\n",
      "test acc: 0.8459\n",
      "number of batches for training: 81\n",
      "epoch 1: valid acc = 0.488, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.666, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.737, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.761, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.791, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.796, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.814, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.824, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.833, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.834, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.839, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.841, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.841, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.843, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.847, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.851, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.853, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.852, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.856, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.856, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.86, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.86, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.853, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.863, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.865, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.864, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.865, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.864, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.866, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.865, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.8623877551020408\n",
      "test acc: 0.865\n",
      "test acc: 0.8455\n",
      "number of batches for training: 81\n",
      "6000/49000 loss: 2.6989046862159145\n",
      "12000/49000 loss: 2.5810541240023155\n",
      "18000/49000 loss: 2.5400941161394135\n",
      "24000/49000 loss: 2.409071286946078\n",
      "30000/49000 loss: 2.2671741141081743\n",
      "36000/49000 loss: 2.1928082386582584\n",
      "42000/49000 loss: 2.0158367824574994\n",
      "48000/49000 loss: 1.707115888661014\n",
      "epoch 1: valid acc = 0.469, new learning rate = 0.000475\n",
      "6000/49000 loss: 1.406582496624094\n",
      "12000/49000 loss: 1.2660311549508971\n",
      "18000/49000 loss: 1.254954781578415\n",
      "24000/49000 loss: 1.1447096323699695\n",
      "30000/49000 loss: 1.1368616834211418\n",
      "36000/49000 loss: 1.1057314961243025\n",
      "42000/49000 loss: 1.0403234689817205\n",
      "48000/49000 loss: 1.057893566583258\n",
      "epoch 2: valid acc = 0.641, new learning rate = 0.00045125\n",
      "6000/49000 loss: 0.9592649456481452\n",
      "12000/49000 loss: 0.9546221003256885\n",
      "18000/49000 loss: 0.8469750924454315\n",
      "24000/49000 loss: 0.8110201240338873\n",
      "30000/49000 loss: 0.8438343865811635\n",
      "36000/49000 loss: 0.8559321930444813\n",
      "42000/49000 loss: 0.781014098494791\n",
      "48000/49000 loss: 0.6933117353659382\n",
      "epoch 3: valid acc = 0.739, new learning rate = 0.0004286875\n",
      "6000/49000 loss: 0.6926534152100822\n",
      "12000/49000 loss: 0.662572200755043\n",
      "18000/49000 loss: 0.7226124540548123\n",
      "24000/49000 loss: 0.6726698527444341\n",
      "30000/49000 loss: 0.6496373864561173\n",
      "36000/49000 loss: 0.7560394926017746\n",
      "42000/49000 loss: 0.6735626481147434\n",
      "48000/49000 loss: 0.6494087123860134\n",
      "epoch 4: valid acc = 0.775, new learning rate = 0.00040725312499999993\n",
      "6000/49000 loss: 0.5392020542371413\n",
      "12000/49000 loss: 0.6493079378800009\n",
      "18000/49000 loss: 0.6018930766074825\n",
      "24000/49000 loss: 0.6239528889754059\n",
      "30000/49000 loss: 0.6338666275688574\n",
      "36000/49000 loss: 0.590585617274284\n",
      "42000/49000 loss: 0.6069899036411935\n",
      "48000/49000 loss: 0.535637560273955\n",
      "epoch 5: valid acc = 0.794, new learning rate = 0.0003868904687499999\n",
      "6000/49000 loss: 0.5752972688537387\n",
      "12000/49000 loss: 0.5733616949404028\n",
      "18000/49000 loss: 0.5690017803153642\n",
      "24000/49000 loss: 0.5427932679632528\n",
      "30000/49000 loss: 0.4913696581664841\n",
      "36000/49000 loss: 0.5729819718096923\n",
      "42000/49000 loss: 0.5612342810144009\n",
      "48000/49000 loss: 0.4811768354887623\n",
      "epoch 6: valid acc = 0.807, new learning rate = 0.0003675459453124999\n",
      "6000/49000 loss: 0.5613376941287632\n",
      "12000/49000 loss: 0.5256437482405689\n",
      "18000/49000 loss: 0.5467535064695619\n",
      "24000/49000 loss: 0.5548936245930818\n",
      "30000/49000 loss: 0.5099737145569616\n",
      "36000/49000 loss: 0.5184544052004897\n",
      "42000/49000 loss: 0.49398062074476057\n",
      "48000/49000 loss: 0.4725889032674519\n",
      "epoch 7: valid acc = 0.812, new learning rate = 0.00034916864804687486\n",
      "6000/49000 loss: 0.5046305638607473\n",
      "12000/49000 loss: 0.4867324343743266\n",
      "18000/49000 loss: 0.480436401461171\n",
      "24000/49000 loss: 0.5035039110865301\n",
      "30000/49000 loss: 0.4748986371611588\n",
      "36000/49000 loss: 0.5350967399436682\n",
      "42000/49000 loss: 0.4436427719237715\n",
      "48000/49000 loss: 0.5031702707555306\n",
      "epoch 8: valid acc = 0.827, new learning rate = 0.0003317102156445311\n",
      "6000/49000 loss: 0.497410885024251\n",
      "12000/49000 loss: 0.46336213369260115\n",
      "18000/49000 loss: 0.5619666460430325\n",
      "24000/49000 loss: 0.456897575366106\n",
      "30000/49000 loss: 0.5373108979842005\n",
      "36000/49000 loss: 0.46929690821751857\n",
      "42000/49000 loss: 0.4158435749595748\n",
      "48000/49000 loss: 0.4710517444802327\n",
      "epoch 9: valid acc = 0.825, new learning rate = 0.0003151247048623045\n",
      "6000/49000 loss: 0.49180227687817185\n",
      "12000/49000 loss: 0.4667577284918747\n",
      "18000/49000 loss: 0.41287812971417254\n",
      "24000/49000 loss: 0.44886110156465814\n",
      "30000/49000 loss: 0.5099859782717866\n",
      "36000/49000 loss: 0.5014908338562187\n",
      "42000/49000 loss: 0.46072445123674216\n",
      "48000/49000 loss: 0.5267946611804926\n",
      "epoch 10: valid acc = 0.831, new learning rate = 0.00029936846961918924\n",
      "6000/49000 loss: 0.4025925335941526\n",
      "12000/49000 loss: 0.49128946250406097\n",
      "18000/49000 loss: 0.4830168280267951\n",
      "24000/49000 loss: 0.4728908873785366\n",
      "30000/49000 loss: 0.41995830017239194\n",
      "36000/49000 loss: 0.4893096348016014\n",
      "42000/49000 loss: 0.451532890750041\n",
      "48000/49000 loss: 0.4922435080247734\n",
      "epoch 11: valid acc = 0.83, new learning rate = 0.00028440004613822977\n",
      "6000/49000 loss: 0.4859713745709371\n",
      "12000/49000 loss: 0.44478995361380314\n",
      "18000/49000 loss: 0.4092817696562891\n",
      "24000/49000 loss: 0.40552659990587014\n",
      "30000/49000 loss: 0.4850172100230786\n",
      "36000/49000 loss: 0.5255058393027034\n",
      "42000/49000 loss: 0.4507976185249785\n",
      "48000/49000 loss: 0.3938912052588299\n",
      "epoch 12: valid acc = 0.836, new learning rate = 0.00027018004383131826\n",
      "6000/49000 loss: 0.43930353732147076\n",
      "12000/49000 loss: 0.45818893741058975\n",
      "18000/49000 loss: 0.45693244279853407\n",
      "24000/49000 loss: 0.41097134284040415\n",
      "30000/49000 loss: 0.40026853743457186\n",
      "36000/49000 loss: 0.42955705948808537\n",
      "42000/49000 loss: 0.4117611986781057\n",
      "48000/49000 loss: 0.4198923258944009\n",
      "epoch 13: valid acc = 0.845, new learning rate = 0.00025667104163975234\n",
      "6000/49000 loss: 0.4355047136975613\n",
      "12000/49000 loss: 0.4455641074538092\n",
      "18000/49000 loss: 0.46753817368004086\n",
      "24000/49000 loss: 0.47022228994571647\n",
      "30000/49000 loss: 0.4516386187063106\n",
      "36000/49000 loss: 0.45124401032764794\n",
      "42000/49000 loss: 0.41245313603176936\n",
      "48000/49000 loss: 0.4159369420800385\n",
      "epoch 14: valid acc = 0.835, new learning rate = 0.00024383748955776472\n",
      "6000/49000 loss: 0.40183443292306886\n",
      "12000/49000 loss: 0.4490657898625992\n",
      "18000/49000 loss: 0.44981220214999773\n",
      "24000/49000 loss: 0.44075144064069277\n",
      "30000/49000 loss: 0.46619824263371656\n",
      "36000/49000 loss: 0.4551682304394079\n",
      "42000/49000 loss: 0.4501235037645367\n",
      "48000/49000 loss: 0.4491974922891854\n",
      "epoch 15: valid acc = 0.843, new learning rate = 0.00023164561507987649\n",
      "6000/49000 loss: 0.3954165026481428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/49000 loss: 0.4370795283809634\n",
      "18000/49000 loss: 0.42371817302582726\n",
      "24000/49000 loss: 0.46633980527584074\n",
      "30000/49000 loss: 0.5117548417901382\n",
      "36000/49000 loss: 0.3726676360388941\n",
      "42000/49000 loss: 0.39889060583078767\n",
      "48000/49000 loss: 0.3749280576750305\n",
      "epoch 16: valid acc = 0.846, new learning rate = 0.00022006333432588265\n",
      "6000/49000 loss: 0.4111360908925049\n",
      "12000/49000 loss: 0.42363520673585014\n",
      "18000/49000 loss: 0.4437742498494896\n",
      "24000/49000 loss: 0.40509197656858986\n",
      "30000/49000 loss: 0.4233829106307822\n",
      "36000/49000 loss: 0.4700960720813208\n",
      "42000/49000 loss: 0.4308317828238027\n",
      "48000/49000 loss: 0.48635644516388504\n",
      "epoch 17: valid acc = 0.849, new learning rate = 0.00020906016760958852\n",
      "6000/49000 loss: 0.4333424732150472\n",
      "12000/49000 loss: 0.463938668129617\n",
      "18000/49000 loss: 0.417659713465102\n",
      "24000/49000 loss: 0.388319947478573\n",
      "30000/49000 loss: 0.42370478752237356\n",
      "36000/49000 loss: 0.465061335002695\n",
      "42000/49000 loss: 0.4283980037835736\n",
      "48000/49000 loss: 0.3950935484946949\n",
      "epoch 18: valid acc = 0.85, new learning rate = 0.00019860715922910907\n",
      "6000/49000 loss: 0.4329239491293961\n",
      "12000/49000 loss: 0.4539257241103654\n",
      "18000/49000 loss: 0.41473208108302706\n",
      "24000/49000 loss: 0.4126948316677291\n",
      "30000/49000 loss: 0.4179976309244521\n",
      "36000/49000 loss: 0.3906977775576294\n",
      "42000/49000 loss: 0.40621060156450983\n",
      "48000/49000 loss: 0.4292914836552971\n",
      "epoch 19: valid acc = 0.849, new learning rate = 0.0001886768012676536\n",
      "6000/49000 loss: 0.4128056478392342\n",
      "12000/49000 loss: 0.4174681954019024\n",
      "18000/49000 loss: 0.4621091085012703\n",
      "24000/49000 loss: 0.3987004045741119\n",
      "30000/49000 loss: 0.44007283101155453\n",
      "36000/49000 loss: 0.40053764030758354\n",
      "42000/49000 loss: 0.3937051843279327\n",
      "48000/49000 loss: 0.42424585985595814\n",
      "epoch 20: valid acc = 0.856, new learning rate = 0.0001792429612042709\n",
      "6000/49000 loss: 0.37713198048049335\n",
      "12000/49000 loss: 0.44912415281163504\n",
      "18000/49000 loss: 0.5068190014901077\n",
      "24000/49000 loss: 0.39995349882873327\n",
      "30000/49000 loss: 0.3939204643449387\n",
      "36000/49000 loss: 0.37728486684560164\n",
      "42000/49000 loss: 0.3796089268942517\n",
      "48000/49000 loss: 0.3927841481707155\n",
      "epoch 21: valid acc = 0.85, new learning rate = 0.00017028081314405735\n",
      "6000/49000 loss: 0.4349283003977779\n",
      "12000/49000 loss: 0.3840913378652088\n",
      "18000/49000 loss: 0.42326607094165014\n",
      "24000/49000 loss: 0.432160481819816\n",
      "30000/49000 loss: 0.39887507208177164\n",
      "36000/49000 loss: 0.43026855513717216\n",
      "42000/49000 loss: 0.3724705780854773\n",
      "48000/49000 loss: 0.36480190149835956\n",
      "epoch 22: valid acc = 0.851, new learning rate = 0.00016176677248685447\n",
      "6000/49000 loss: 0.40076660081127685\n",
      "12000/49000 loss: 0.44265546402276634\n",
      "18000/49000 loss: 0.39108747688880807\n",
      "24000/49000 loss: 0.44295809511997586\n",
      "30000/49000 loss: 0.4601520124365138\n",
      "36000/49000 loss: 0.3911413103375705\n",
      "42000/49000 loss: 0.45827279830632794\n",
      "48000/49000 loss: 0.3462568616563123\n",
      "epoch 23: valid acc = 0.854, new learning rate = 0.00015367843386251173\n",
      "6000/49000 loss: 0.38088791372299907\n",
      "12000/49000 loss: 0.4019556139916586\n",
      "18000/49000 loss: 0.34779629467009\n",
      "24000/49000 loss: 0.33452112858439864\n",
      "30000/49000 loss: 0.38485699516355226\n",
      "36000/49000 loss: 0.41221984023645414\n",
      "42000/49000 loss: 0.3844055796441219\n",
      "48000/49000 loss: 0.4223703561896198\n",
      "epoch 24: valid acc = 0.854, new learning rate = 0.00014599451216938612\n",
      "6000/49000 loss: 0.3460375845373884\n",
      "12000/49000 loss: 0.3655506627384016\n",
      "18000/49000 loss: 0.34351022931184566\n",
      "24000/49000 loss: 0.38832875820025964\n",
      "30000/49000 loss: 0.3972472210364717\n",
      "36000/49000 loss: 0.4378272881966312\n",
      "42000/49000 loss: 0.492521324314241\n",
      "48000/49000 loss: 0.38133326155980984\n",
      "epoch 25: valid acc = 0.855, new learning rate = 0.00013869478656091682\n",
      "6000/49000 loss: 0.37735868917723103\n",
      "12000/49000 loss: 0.35792034886471463\n",
      "18000/49000 loss: 0.3610060832524169\n",
      "24000/49000 loss: 0.3817286433481565\n",
      "30000/49000 loss: 0.37250703293026033\n",
      "36000/49000 loss: 0.3788581291329298\n",
      "42000/49000 loss: 0.41833215665344625\n",
      "48000/49000 loss: 0.4663996618563834\n",
      "epoch 26: valid acc = 0.855, new learning rate = 0.00013176004723287096\n",
      "6000/49000 loss: 0.3786599802384089\n",
      "12000/49000 loss: 0.408812193668541\n",
      "18000/49000 loss: 0.42936058617737277\n",
      "24000/49000 loss: 0.4146151479458247\n",
      "30000/49000 loss: 0.343246234607179\n",
      "36000/49000 loss: 0.40048936274803637\n",
      "42000/49000 loss: 0.4186416792784009\n",
      "48000/49000 loss: 0.4166165931661574\n",
      "epoch 27: valid acc = 0.857, new learning rate = 0.0001251720448712274\n",
      "6000/49000 loss: 0.35284084711957625\n",
      "12000/49000 loss: 0.41088196232320856\n",
      "18000/49000 loss: 0.3888251373554848\n",
      "24000/49000 loss: 0.3928916695406221\n",
      "30000/49000 loss: 0.4261170689764546\n",
      "36000/49000 loss: 0.3953971520025392\n",
      "42000/49000 loss: 0.3703238923222729\n",
      "48000/49000 loss: 0.37409855152997035\n",
      "epoch 28: valid acc = 0.854, new learning rate = 0.00011891344262766602\n",
      "6000/49000 loss: 0.3888883059456599\n",
      "12000/49000 loss: 0.3817693755224392\n",
      "18000/49000 loss: 0.4944738747749847\n",
      "24000/49000 loss: 0.40708547766751624\n",
      "30000/49000 loss: 0.3985583866505634\n",
      "36000/49000 loss: 0.3756425546926054\n",
      "42000/49000 loss: 0.3294994340291209\n",
      "48000/49000 loss: 0.3742916772340106\n",
      "epoch 29: valid acc = 0.856, new learning rate = 0.00011296777049628272\n",
      "6000/49000 loss: 0.41928013861081737\n",
      "12000/49000 loss: 0.409449629188363\n",
      "18000/49000 loss: 0.40149867206121737\n",
      "24000/49000 loss: 0.39478302400943716\n",
      "30000/49000 loss: 0.4247408127931752\n",
      "36000/49000 loss: 0.3478994169692973\n",
      "42000/49000 loss: 0.43007748867210066\n",
      "48000/49000 loss: 0.33714775678739356\n",
      "epoch 30: valid acc = 0.857, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.8626326530612245\n",
      "test acc: 0.857\n",
      "test acc: 0.8434\n",
      "number of batches for training: 81\n",
      "epoch 1: valid acc = 0.506, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.634, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.739, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.768, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.795, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.805, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.816, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.824, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.824, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.838, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.836, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.841, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.844, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.841, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.844, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.849, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.849, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.85, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.853, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.854, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.856, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.857, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.858, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.86, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.857, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.86, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.858, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.861, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.861, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.863, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.8644897959183674\n",
      "test acc: 0.863\n",
      "test acc: 0.8451\n",
      "number of batches for training: 81\n",
      "6000/49000 loss: 2.6506359604179752\n",
      "12000/49000 loss: 2.602274884081946\n",
      "18000/49000 loss: 2.534904589051642\n",
      "24000/49000 loss: 2.381660332123704\n",
      "30000/49000 loss: 2.3088512774595795\n",
      "36000/49000 loss: 2.163243299467369\n",
      "42000/49000 loss: 2.0183456956550283\n",
      "48000/49000 loss: 1.7161939521225298\n",
      "epoch 1: valid acc = 0.45, new learning rate = 0.000475\n",
      "6000/49000 loss: 1.3812474441877707\n",
      "12000/49000 loss: 1.2567394628690711\n",
      "18000/49000 loss: 1.2507417867405448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000/49000 loss: 1.227029863389501\n",
      "30000/49000 loss: 1.105084551654872\n",
      "36000/49000 loss: 1.0627581598228768\n",
      "42000/49000 loss: 1.0724891350806733\n",
      "48000/49000 loss: 1.0520914423423053\n",
      "epoch 2: valid acc = 0.66, new learning rate = 0.00045125\n",
      "6000/49000 loss: 0.9910285962479201\n",
      "12000/49000 loss: 0.9486935530071599\n",
      "18000/49000 loss: 0.8988925366931237\n",
      "24000/49000 loss: 0.7877905210776609\n",
      "30000/49000 loss: 0.8283179687039522\n",
      "36000/49000 loss: 0.7719184785416834\n",
      "42000/49000 loss: 0.7339492700270738\n",
      "48000/49000 loss: 0.7769400011942555\n",
      "epoch 3: valid acc = 0.736, new learning rate = 0.0004286875\n",
      "6000/49000 loss: 0.7183730960801156\n",
      "12000/49000 loss: 0.6455361623793279\n",
      "18000/49000 loss: 0.6530910942341872\n",
      "24000/49000 loss: 0.698774508182509\n",
      "30000/49000 loss: 0.6434145299112493\n",
      "36000/49000 loss: 0.5985325919951778\n",
      "42000/49000 loss: 0.62827563262683\n",
      "48000/49000 loss: 0.6429215142794561\n",
      "epoch 4: valid acc = 0.773, new learning rate = 0.00040725312499999993\n",
      "6000/49000 loss: 0.5795202871610197\n",
      "12000/49000 loss: 0.5841796725144952\n",
      "18000/49000 loss: 0.5710254658865033\n",
      "24000/49000 loss: 0.6325255231789292\n",
      "30000/49000 loss: 0.5571422910422759\n",
      "36000/49000 loss: 0.5642817301560675\n",
      "42000/49000 loss: 0.574067260528979\n",
      "48000/49000 loss: 0.5519206410555502\n",
      "epoch 5: valid acc = 0.79, new learning rate = 0.0003868904687499999\n",
      "6000/49000 loss: 0.52279768881054\n",
      "12000/49000 loss: 0.5823557670821125\n",
      "18000/49000 loss: 0.6008324020635183\n",
      "24000/49000 loss: 0.5271973876956061\n",
      "30000/49000 loss: 0.5888035446928794\n",
      "36000/49000 loss: 0.5332150813535882\n",
      "42000/49000 loss: 0.5147208856510616\n",
      "48000/49000 loss: 0.5014470887117739\n",
      "epoch 6: valid acc = 0.796, new learning rate = 0.0003675459453124999\n",
      "6000/49000 loss: 0.5077949947755851\n",
      "12000/49000 loss: 0.5019637825411192\n",
      "18000/49000 loss: 0.5243680293472259\n",
      "24000/49000 loss: 0.5165475812542045\n",
      "30000/49000 loss: 0.549078435072954\n",
      "36000/49000 loss: 0.5350521429356219\n",
      "42000/49000 loss: 0.4880153165171146\n",
      "48000/49000 loss: 0.5517734197542585\n",
      "epoch 7: valid acc = 0.816, new learning rate = 0.00034916864804687486\n",
      "6000/49000 loss: 0.5392092744993761\n",
      "12000/49000 loss: 0.4922486069184636\n",
      "18000/49000 loss: 0.48550289482245634\n",
      "24000/49000 loss: 0.4898996982203797\n",
      "30000/49000 loss: 0.5138352009593515\n",
      "36000/49000 loss: 0.4751625220073596\n",
      "42000/49000 loss: 0.563908504170521\n",
      "48000/49000 loss: 0.5009991318265059\n",
      "epoch 8: valid acc = 0.824, new learning rate = 0.0003317102156445311\n",
      "6000/49000 loss: 0.5453432312649048\n",
      "12000/49000 loss: 0.41241822459300703\n",
      "18000/49000 loss: 0.4834839271455911\n",
      "24000/49000 loss: 0.4727745198759336\n",
      "30000/49000 loss: 0.47807572053800257\n",
      "36000/49000 loss: 0.5327569143717266\n",
      "42000/49000 loss: 0.46066364453623393\n",
      "48000/49000 loss: 0.49613153641009616\n",
      "epoch 9: valid acc = 0.832, new learning rate = 0.0003151247048623045\n",
      "6000/49000 loss: 0.49710860958817454\n",
      "12000/49000 loss: 0.4340865137417591\n",
      "18000/49000 loss: 0.477954711416247\n",
      "24000/49000 loss: 0.4928459592871045\n",
      "30000/49000 loss: 0.4557263907520345\n",
      "36000/49000 loss: 0.5260525369650141\n",
      "42000/49000 loss: 0.45022000802540973\n",
      "48000/49000 loss: 0.44369297892183107\n",
      "epoch 10: valid acc = 0.839, new learning rate = 0.00029936846961918924\n",
      "6000/49000 loss: 0.5065805622221897\n",
      "12000/49000 loss: 0.48111537581499064\n",
      "18000/49000 loss: 0.4823535217366837\n",
      "24000/49000 loss: 0.4534620786401398\n",
      "30000/49000 loss: 0.4525868874714463\n",
      "36000/49000 loss: 0.41584341221888893\n",
      "42000/49000 loss: 0.5207209723411558\n",
      "48000/49000 loss: 0.4503884943399032\n",
      "epoch 11: valid acc = 0.842, new learning rate = 0.00028440004613822977\n",
      "6000/49000 loss: 0.4292090631813354\n",
      "12000/49000 loss: 0.405398184262389\n",
      "18000/49000 loss: 0.4565981104632298\n",
      "24000/49000 loss: 0.48810946785732595\n",
      "30000/49000 loss: 0.5418897587391349\n",
      "36000/49000 loss: 0.4536004382630272\n",
      "42000/49000 loss: 0.45843555925759866\n",
      "48000/49000 loss: 0.43933159887684253\n",
      "epoch 12: valid acc = 0.846, new learning rate = 0.00027018004383131826\n",
      "6000/49000 loss: 0.42184011916755476\n",
      "12000/49000 loss: 0.4743749014138198\n",
      "18000/49000 loss: 0.4423155997705029\n",
      "24000/49000 loss: 0.44016692584571426\n",
      "30000/49000 loss: 0.4395014408007239\n",
      "36000/49000 loss: 0.4826042299861437\n",
      "42000/49000 loss: 0.4482323012113132\n",
      "48000/49000 loss: 0.44662766773326706\n",
      "epoch 13: valid acc = 0.846, new learning rate = 0.00025667104163975234\n",
      "6000/49000 loss: 0.41451138884717537\n",
      "12000/49000 loss: 0.4333134485140778\n",
      "18000/49000 loss: 0.4379323346993769\n",
      "24000/49000 loss: 0.44305213449191194\n",
      "30000/49000 loss: 0.4612643497875844\n",
      "36000/49000 loss: 0.481192099436159\n",
      "42000/49000 loss: 0.4141731672010159\n",
      "48000/49000 loss: 0.4210212615837015\n",
      "epoch 14: valid acc = 0.842, new learning rate = 0.00024383748955776472\n",
      "6000/49000 loss: 0.4408649602900162\n",
      "12000/49000 loss: 0.4267914528467562\n",
      "18000/49000 loss: 0.4256757774894704\n",
      "24000/49000 loss: 0.4363345651811133\n",
      "30000/49000 loss: 0.44729464971297483\n",
      "36000/49000 loss: 0.39478332493747614\n",
      "42000/49000 loss: 0.4962819768825772\n",
      "48000/49000 loss: 0.42339317341006577\n",
      "epoch 15: valid acc = 0.847, new learning rate = 0.00023164561507987649\n",
      "6000/49000 loss: 0.4225580226845664\n",
      "12000/49000 loss: 0.43047104990656954\n",
      "18000/49000 loss: 0.4594525193415622\n",
      "24000/49000 loss: 0.40953495291317493\n",
      "30000/49000 loss: 0.37792762373523386\n",
      "36000/49000 loss: 0.4610747608429231\n",
      "42000/49000 loss: 0.3906439310035214\n",
      "48000/49000 loss: 0.3846690130353432\n",
      "epoch 16: valid acc = 0.853, new learning rate = 0.00022006333432588265\n",
      "6000/49000 loss: 0.41725794430149793\n",
      "12000/49000 loss: 0.43771708935010784\n",
      "18000/49000 loss: 0.40192033304818453\n",
      "24000/49000 loss: 0.5067508590986374\n",
      "30000/49000 loss: 0.4034371909158731\n",
      "36000/49000 loss: 0.4045729225369322\n",
      "42000/49000 loss: 0.44188290202144687\n",
      "48000/49000 loss: 0.3984668768921217\n",
      "epoch 17: valid acc = 0.847, new learning rate = 0.00020906016760958852\n",
      "6000/49000 loss: 0.4577056932851565\n",
      "12000/49000 loss: 0.42976723015373186\n",
      "18000/49000 loss: 0.42858332001359895\n",
      "24000/49000 loss: 0.42619559169451304\n",
      "30000/49000 loss: 0.41590694197119077\n",
      "36000/49000 loss: 0.42820099448602733\n",
      "42000/49000 loss: 0.37041415526491356\n",
      "48000/49000 loss: 0.41703115686261777\n",
      "epoch 18: valid acc = 0.855, new learning rate = 0.00019860715922910907\n",
      "6000/49000 loss: 0.4212495492303222\n",
      "12000/49000 loss: 0.45724564709542675\n",
      "18000/49000 loss: 0.39058933441691857\n",
      "24000/49000 loss: 0.38790556522617736\n",
      "30000/49000 loss: 0.40455457759165037\n",
      "36000/49000 loss: 0.3943058611987727\n",
      "42000/49000 loss: 0.4191271592975611\n",
      "48000/49000 loss: 0.4418392012865386\n",
      "epoch 19: valid acc = 0.851, new learning rate = 0.0001886768012676536\n",
      "6000/49000 loss: 0.3839765685013629\n",
      "12000/49000 loss: 0.46904936273260106\n",
      "18000/49000 loss: 0.3886604069148501\n",
      "24000/49000 loss: 0.3908221559315296\n",
      "30000/49000 loss: 0.40619931118403246\n",
      "36000/49000 loss: 0.39628264880769304\n",
      "42000/49000 loss: 0.4191800095912951\n",
      "48000/49000 loss: 0.4036617993103734\n",
      "epoch 20: valid acc = 0.855, new learning rate = 0.0001792429612042709\n",
      "6000/49000 loss: 0.440889352875473\n",
      "12000/49000 loss: 0.41955218361853275\n",
      "18000/49000 loss: 0.3866308201884168\n",
      "24000/49000 loss: 0.39310273095773324\n",
      "30000/49000 loss: 0.3882652569574986\n",
      "36000/49000 loss: 0.322287343941209\n",
      "42000/49000 loss: 0.4061393439817639\n",
      "48000/49000 loss: 0.45438018002718206\n",
      "epoch 21: valid acc = 0.859, new learning rate = 0.00017028081314405735\n",
      "6000/49000 loss: 0.3987313017150928\n",
      "12000/49000 loss: 0.4430123562302193\n",
      "18000/49000 loss: 0.38079928298719373\n",
      "24000/49000 loss: 0.4726337167563642\n",
      "30000/49000 loss: 0.3977733330738435\n",
      "36000/49000 loss: 0.3724243979248369\n",
      "42000/49000 loss: 0.41736870653312436\n",
      "48000/49000 loss: 0.37417299799988546\n",
      "epoch 22: valid acc = 0.861, new learning rate = 0.00016176677248685447\n",
      "6000/49000 loss: 0.4157765177076301\n",
      "12000/49000 loss: 0.3807602437159301\n",
      "18000/49000 loss: 0.379276068932812\n",
      "24000/49000 loss: 0.46139790770912603\n",
      "30000/49000 loss: 0.4296247275090512\n",
      "36000/49000 loss: 0.33739465807552194\n",
      "42000/49000 loss: 0.4362167886600777\n",
      "48000/49000 loss: 0.3610313674006765\n",
      "epoch 23: valid acc = 0.858, new learning rate = 0.00015367843386251173\n",
      "6000/49000 loss: 0.41743882154152834\n",
      "12000/49000 loss: 0.44245684835060306\n",
      "18000/49000 loss: 0.37929658524754384\n",
      "24000/49000 loss: 0.36851361078594336\n",
      "30000/49000 loss: 0.3898540459599632\n",
      "36000/49000 loss: 0.428424873536823\n",
      "42000/49000 loss: 0.39941746657882454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/49000 loss: 0.41066998601603905\n",
      "epoch 24: valid acc = 0.863, new learning rate = 0.00014599451216938612\n",
      "6000/49000 loss: 0.3950659634902408\n",
      "12000/49000 loss: 0.4252827038454177\n",
      "18000/49000 loss: 0.46988111507650415\n",
      "24000/49000 loss: 0.3801577187454263\n",
      "30000/49000 loss: 0.40968802496447454\n",
      "36000/49000 loss: 0.35193061973935724\n",
      "42000/49000 loss: 0.4068538580422123\n",
      "48000/49000 loss: 0.40911618445073106\n",
      "epoch 25: valid acc = 0.861, new learning rate = 0.00013869478656091682\n",
      "6000/49000 loss: 0.3858601901734893\n",
      "12000/49000 loss: 0.39142976308744015\n",
      "18000/49000 loss: 0.31440877511201\n",
      "24000/49000 loss: 0.37268880961900586\n",
      "30000/49000 loss: 0.37067947017014946\n",
      "36000/49000 loss: 0.4061268871368338\n",
      "42000/49000 loss: 0.4122818395774912\n",
      "48000/49000 loss: 0.39828609281753263\n",
      "epoch 26: valid acc = 0.864, new learning rate = 0.00013176004723287096\n",
      "6000/49000 loss: 0.41060220814391774\n",
      "12000/49000 loss: 0.38620060647954735\n",
      "18000/49000 loss: 0.36939011362723595\n",
      "24000/49000 loss: 0.39101203765712955\n",
      "30000/49000 loss: 0.4190839665021987\n",
      "36000/49000 loss: 0.3978428478144862\n",
      "42000/49000 loss: 0.33144602778350624\n",
      "48000/49000 loss: 0.3893125576168079\n",
      "epoch 27: valid acc = 0.864, new learning rate = 0.0001251720448712274\n",
      "6000/49000 loss: 0.3648732147639077\n",
      "12000/49000 loss: 0.3739612420838265\n",
      "18000/49000 loss: 0.44751918601980184\n",
      "24000/49000 loss: 0.42993780224584693\n",
      "30000/49000 loss: 0.32163648120702515\n",
      "36000/49000 loss: 0.36556724292752873\n",
      "42000/49000 loss: 0.3942121321546551\n",
      "48000/49000 loss: 0.37709025014099506\n",
      "epoch 28: valid acc = 0.864, new learning rate = 0.00011891344262766602\n",
      "6000/49000 loss: 0.4259905980595095\n",
      "12000/49000 loss: 0.4485363090169298\n",
      "18000/49000 loss: 0.40902043622323914\n",
      "24000/49000 loss: 0.41232561221355235\n",
      "30000/49000 loss: 0.3773638760145362\n",
      "36000/49000 loss: 0.3852210485416603\n",
      "42000/49000 loss: 0.3710318331208449\n",
      "48000/49000 loss: 0.37475516630831546\n",
      "epoch 29: valid acc = 0.862, new learning rate = 0.00011296777049628272\n",
      "6000/49000 loss: 0.42093325000641285\n",
      "12000/49000 loss: 0.35406742742416386\n",
      "18000/49000 loss: 0.41427779181778396\n",
      "24000/49000 loss: 0.3453300926788361\n",
      "30000/49000 loss: 0.35026761210047813\n",
      "36000/49000 loss: 0.33627537685337455\n",
      "42000/49000 loss: 0.3665686023292874\n",
      "48000/49000 loss: 0.38537916715988985\n",
      "epoch 30: valid acc = 0.861, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.862938775510204\n",
      "test acc: 0.861\n",
      "test acc: 0.8442\n",
      "number of batches for training: 81\n",
      "epoch 1: valid acc = 0.471, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.593, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.739, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.766, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.781, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.802, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.816, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.822, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.821, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.831, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.837, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.835, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.841, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.845, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.848, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.845, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.851, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.853, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.855, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.854, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.856, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.859, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.858, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.858, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.86, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.861, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.858, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.859, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.858, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.861, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.8632448979591837\n",
      "test acc: 0.861\n",
      "test acc: 0.8459\n",
      "number of batches for training: 49\n",
      "10000/49000 loss: 2.6506155701344785\n",
      "20000/49000 loss: 2.5964892848863528\n",
      "30000/49000 loss: 2.525685451072124\n",
      "40000/49000 loss: 2.466255937085856\n",
      "epoch 1: valid acc = 0.362, new learning rate = 0.000475\n",
      "10000/49000 loss: 2.168344235524268\n",
      "20000/49000 loss: 2.0438975059517976\n",
      "30000/49000 loss: 1.7872513928431706\n",
      "40000/49000 loss: 1.4862259977032959\n",
      "epoch 2: valid acc = 0.52, new learning rate = 0.00045125\n",
      "10000/49000 loss: 1.2637922436217934\n",
      "20000/49000 loss: 1.1735988610299721\n",
      "30000/49000 loss: 1.1138598805089803\n",
      "40000/49000 loss: 1.1304751781544649\n",
      "epoch 3: valid acc = 0.625, new learning rate = 0.0004286875\n",
      "10000/49000 loss: 1.015211922395576\n",
      "20000/49000 loss: 0.9719197125351969\n",
      "30000/49000 loss: 0.9895642418915525\n",
      "40000/49000 loss: 0.9285753700627092\n",
      "epoch 4: valid acc = 0.705, new learning rate = 0.00040725312499999993\n",
      "10000/49000 loss: 0.875387374733827\n",
      "20000/49000 loss: 0.7625079629281459\n",
      "30000/49000 loss: 0.7842892277855663\n",
      "40000/49000 loss: 0.7852005932638444\n",
      "epoch 5: valid acc = 0.731, new learning rate = 0.0003868904687499999\n",
      "10000/49000 loss: 0.731832779109282\n",
      "20000/49000 loss: 0.690601231841365\n",
      "30000/49000 loss: 0.7007356419631771\n",
      "40000/49000 loss: 0.6851298895256066\n",
      "epoch 6: valid acc = 0.747, new learning rate = 0.0003675459453124999\n",
      "10000/49000 loss: 0.6950671087402726\n",
      "20000/49000 loss: 0.6857316487848267\n",
      "30000/49000 loss: 0.6647456233291824\n",
      "40000/49000 loss: 0.6609996286014072\n",
      "epoch 7: valid acc = 0.758, new learning rate = 0.00034916864804687486\n",
      "10000/49000 loss: 0.6266919240283574\n",
      "20000/49000 loss: 0.6338693268341847\n",
      "30000/49000 loss: 0.574822476904203\n",
      "40000/49000 loss: 0.590636815645988\n",
      "epoch 8: valid acc = 0.775, new learning rate = 0.0003317102156445311\n",
      "10000/49000 loss: 0.5968334472660799\n",
      "20000/49000 loss: 0.6217622073839113\n",
      "30000/49000 loss: 0.565002714552402\n",
      "40000/49000 loss: 0.6283891781690298\n",
      "epoch 9: valid acc = 0.787, new learning rate = 0.0003151247048623045\n",
      "10000/49000 loss: 0.6072721729617623\n",
      "20000/49000 loss: 0.6286289122982839\n",
      "30000/49000 loss: 0.5355818691297473\n",
      "40000/49000 loss: 0.5420929199981144\n",
      "epoch 10: valid acc = 0.791, new learning rate = 0.00029936846961918924\n",
      "10000/49000 loss: 0.5289030679221398\n",
      "20000/49000 loss: 0.5605283606088646\n",
      "30000/49000 loss: 0.5552410688631724\n",
      "40000/49000 loss: 0.536491414102151\n",
      "epoch 11: valid acc = 0.796, new learning rate = 0.00028440004613822977\n",
      "10000/49000 loss: 0.536372044769815\n",
      "20000/49000 loss: 0.5342217930369331\n",
      "30000/49000 loss: 0.5263880280875128\n",
      "40000/49000 loss: 0.5639497595084386\n",
      "epoch 12: valid acc = 0.807, new learning rate = 0.00027018004383131826\n",
      "10000/49000 loss: 0.49053559146613607\n",
      "20000/49000 loss: 0.472374017961455\n",
      "30000/49000 loss: 0.4981244664636192\n",
      "40000/49000 loss: 0.5089935073223457\n",
      "epoch 13: valid acc = 0.809, new learning rate = 0.00025667104163975234\n",
      "10000/49000 loss: 0.4740754365572267\n",
      "20000/49000 loss: 0.5297292244025437\n",
      "30000/49000 loss: 0.5052827613567474\n",
      "40000/49000 loss: 0.5276080379708298\n",
      "epoch 14: valid acc = 0.812, new learning rate = 0.00024383748955776472\n",
      "10000/49000 loss: 0.5183477329157724\n",
      "20000/49000 loss: 0.49106345691105063\n",
      "30000/49000 loss: 0.5003486018395115\n",
      "40000/49000 loss: 0.4893593222664989\n",
      "epoch 15: valid acc = 0.82, new learning rate = 0.00023164561507987649\n",
      "10000/49000 loss: 0.5030063624432768\n",
      "20000/49000 loss: 0.5141939617060152\n",
      "30000/49000 loss: 0.48894120539256364\n",
      "40000/49000 loss: 0.4677873787939477\n",
      "epoch 16: valid acc = 0.827, new learning rate = 0.00022006333432588265\n",
      "10000/49000 loss: 0.419040405337941\n",
      "20000/49000 loss: 0.4835951433609769\n",
      "30000/49000 loss: 0.4737414812237885\n",
      "40000/49000 loss: 0.48118074713515135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17: valid acc = 0.828, new learning rate = 0.00020906016760958852\n",
      "10000/49000 loss: 0.48497349847099797\n",
      "20000/49000 loss: 0.470811647777827\n",
      "30000/49000 loss: 0.46825121554499965\n",
      "40000/49000 loss: 0.48940491485629206\n",
      "epoch 18: valid acc = 0.829, new learning rate = 0.00019860715922910907\n",
      "10000/49000 loss: 0.49855868649171486\n",
      "20000/49000 loss: 0.4685479161474814\n",
      "30000/49000 loss: 0.4299305723300587\n",
      "40000/49000 loss: 0.47040864609824923\n",
      "epoch 19: valid acc = 0.832, new learning rate = 0.0001886768012676536\n",
      "10000/49000 loss: 0.46388495144861325\n",
      "20000/49000 loss: 0.4792919771084327\n",
      "30000/49000 loss: 0.5013373810687702\n",
      "40000/49000 loss: 0.46988790325556157\n",
      "epoch 20: valid acc = 0.836, new learning rate = 0.0001792429612042709\n",
      "10000/49000 loss: 0.4781617324651958\n",
      "20000/49000 loss: 0.4730753504044052\n",
      "30000/49000 loss: 0.46570550287768553\n",
      "40000/49000 loss: 0.47068950352698835\n",
      "epoch 21: valid acc = 0.837, new learning rate = 0.00017028081314405735\n",
      "10000/49000 loss: 0.45390636282288505\n",
      "20000/49000 loss: 0.4922545367741528\n",
      "30000/49000 loss: 0.4614287622929942\n",
      "40000/49000 loss: 0.4604074536217194\n",
      "epoch 22: valid acc = 0.837, new learning rate = 0.00016176677248685447\n",
      "10000/49000 loss: 0.47244426752193125\n",
      "20000/49000 loss: 0.5014262291922365\n",
      "30000/49000 loss: 0.42537423834641763\n",
      "40000/49000 loss: 0.40384678486638287\n",
      "epoch 23: valid acc = 0.84, new learning rate = 0.00015367843386251173\n",
      "10000/49000 loss: 0.4797578706324647\n",
      "20000/49000 loss: 0.4801134057091324\n",
      "30000/49000 loss: 0.4665051462026496\n",
      "40000/49000 loss: 0.4431182841241966\n",
      "epoch 24: valid acc = 0.836, new learning rate = 0.00014599451216938612\n",
      "10000/49000 loss: 0.47783738415826804\n",
      "20000/49000 loss: 0.5111504927856182\n",
      "30000/49000 loss: 0.4573016400644026\n",
      "40000/49000 loss: 0.45766202067989104\n",
      "epoch 25: valid acc = 0.839, new learning rate = 0.00013869478656091682\n",
      "10000/49000 loss: 0.4335886857655662\n",
      "20000/49000 loss: 0.43550273519856664\n",
      "30000/49000 loss: 0.4903102663251492\n",
      "40000/49000 loss: 0.4609531010301338\n",
      "epoch 26: valid acc = 0.84, new learning rate = 0.00013176004723287096\n",
      "10000/49000 loss: 0.45098176822441066\n",
      "20000/49000 loss: 0.4307247875599227\n",
      "30000/49000 loss: 0.4518862184535604\n",
      "40000/49000 loss: 0.4487234314608305\n",
      "epoch 27: valid acc = 0.838, new learning rate = 0.0001251720448712274\n",
      "10000/49000 loss: 0.41995792196222287\n",
      "20000/49000 loss: 0.4416772030029627\n",
      "30000/49000 loss: 0.48716755366813197\n",
      "40000/49000 loss: 0.45400115928796614\n",
      "epoch 28: valid acc = 0.843, new learning rate = 0.00011891344262766602\n",
      "10000/49000 loss: 0.46220309242430896\n",
      "20000/49000 loss: 0.4738275739852827\n",
      "30000/49000 loss: 0.45087865422919593\n",
      "40000/49000 loss: 0.4203843406978188\n",
      "epoch 29: valid acc = 0.844, new learning rate = 0.00011296777049628272\n",
      "10000/49000 loss: 0.44412555326758824\n",
      "20000/49000 loss: 0.4470062912033667\n",
      "30000/49000 loss: 0.48031072703414396\n",
      "40000/49000 loss: 0.5041456272884901\n",
      "epoch 30: valid acc = 0.844, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.845469387755102\n",
      "test acc: 0.844\n",
      "test acc: 0.831\n",
      "number of batches for training: 49\n",
      "epoch 1: valid acc = 0.351, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.526, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.647, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.708, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.732, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.757, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.767, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.777, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.786, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.793, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.799, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.806, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.812, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.816, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.824, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.825, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.825, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.831, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.834, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.832, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.834, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.835, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.834, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.836, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.84, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.84, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.841, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.843, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.842, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.841, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.8451428571428572\n",
      "test acc: 0.841\n",
      "test acc: 0.831\n",
      "number of batches for training: 49\n",
      "10000/49000 loss: 2.6588615003499485\n",
      "20000/49000 loss: 2.611714164521678\n",
      "30000/49000 loss: 2.5605649557363424\n",
      "40000/49000 loss: 2.49117888249255\n",
      "epoch 1: valid acc = 0.39, new learning rate = 0.000475\n",
      "10000/49000 loss: 2.1975554833728004\n",
      "20000/49000 loss: 1.9570407911497776\n",
      "30000/49000 loss: 1.8348142772906253\n",
      "40000/49000 loss: 1.5863191318882723\n",
      "epoch 2: valid acc = 0.541, new learning rate = 0.00045125\n",
      "10000/49000 loss: 1.2267587918800797\n",
      "20000/49000 loss: 1.1711223819938228\n",
      "30000/49000 loss: 1.1459985770415204\n",
      "40000/49000 loss: 1.036464216676967\n",
      "epoch 3: valid acc = 0.656, new learning rate = 0.0004286875\n",
      "10000/49000 loss: 0.9895445593137918\n",
      "20000/49000 loss: 0.9795326570102463\n",
      "30000/49000 loss: 0.9306315237978873\n",
      "40000/49000 loss: 0.9026635612095011\n",
      "epoch 4: valid acc = 0.723, new learning rate = 0.00040725312499999993\n",
      "10000/49000 loss: 0.8792755630290703\n",
      "20000/49000 loss: 0.8198766241804808\n",
      "30000/49000 loss: 0.7696881075336323\n",
      "40000/49000 loss: 0.7406370070463467\n",
      "epoch 5: valid acc = 0.736, new learning rate = 0.0003868904687499999\n",
      "10000/49000 loss: 0.7328145509003008\n",
      "20000/49000 loss: 0.6788234719860199\n",
      "30000/49000 loss: 0.7003405768637675\n",
      "40000/49000 loss: 0.7133803216387515\n",
      "epoch 6: valid acc = 0.748, new learning rate = 0.0003675459453124999\n",
      "10000/49000 loss: 0.6616847122330054\n",
      "20000/49000 loss: 0.6533979455668637\n",
      "30000/49000 loss: 0.6406760595172742\n",
      "40000/49000 loss: 0.6203725284751145\n",
      "epoch 7: valid acc = 0.77, new learning rate = 0.00034916864804687486\n",
      "10000/49000 loss: 0.6213119232149363\n",
      "20000/49000 loss: 0.5592528952748714\n",
      "30000/49000 loss: 0.5798363901578512\n",
      "40000/49000 loss: 0.5829339221060599\n",
      "epoch 8: valid acc = 0.772, new learning rate = 0.0003317102156445311\n",
      "10000/49000 loss: 0.6097198901438613\n",
      "20000/49000 loss: 0.5576755815913375\n",
      "30000/49000 loss: 0.6166538928625213\n",
      "40000/49000 loss: 0.5663866960868941\n",
      "epoch 9: valid acc = 0.784, new learning rate = 0.0003151247048623045\n",
      "10000/49000 loss: 0.5734170798950466\n",
      "20000/49000 loss: 0.5509290232654674\n",
      "30000/49000 loss: 0.5570269549775231\n",
      "40000/49000 loss: 0.5292197998117715\n",
      "epoch 10: valid acc = 0.796, new learning rate = 0.00029936846961918924\n",
      "10000/49000 loss: 0.5540005795946699\n",
      "20000/49000 loss: 0.5086280639046306\n",
      "30000/49000 loss: 0.5572624056175193\n",
      "40000/49000 loss: 0.5028437775432337\n",
      "epoch 11: valid acc = 0.803, new learning rate = 0.00028440004613822977\n",
      "10000/49000 loss: 0.5106434863878577\n",
      "20000/49000 loss: 0.5062786030623712\n",
      "30000/49000 loss: 0.5129723053592483\n",
      "40000/49000 loss: 0.5138455411821906\n",
      "epoch 12: valid acc = 0.808, new learning rate = 0.00027018004383131826\n",
      "10000/49000 loss: 0.5026470198294904\n",
      "20000/49000 loss: 0.5139625195644639\n",
      "30000/49000 loss: 0.48942518790869394\n",
      "40000/49000 loss: 0.5211639020255553\n",
      "epoch 13: valid acc = 0.817, new learning rate = 0.00025667104163975234\n",
      "10000/49000 loss: 0.5095797255093464\n",
      "20000/49000 loss: 0.5433044915800791\n",
      "30000/49000 loss: 0.4845233770914802\n",
      "40000/49000 loss: 0.5184985612996217\n",
      "epoch 14: valid acc = 0.818, new learning rate = 0.00024383748955776472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/49000 loss: 0.5245155103879268\n",
      "20000/49000 loss: 0.4836074291022452\n",
      "30000/49000 loss: 0.49080938462114065\n",
      "40000/49000 loss: 0.4782634303182119\n",
      "epoch 15: valid acc = 0.823, new learning rate = 0.00023164561507987649\n",
      "10000/49000 loss: 0.4885528477908648\n",
      "20000/49000 loss: 0.46985798925293193\n",
      "30000/49000 loss: 0.4418546712428171\n",
      "40000/49000 loss: 0.5040823234379536\n",
      "epoch 16: valid acc = 0.828, new learning rate = 0.00022006333432588265\n",
      "10000/49000 loss: 0.5137170445598884\n",
      "20000/49000 loss: 0.4907089142817582\n",
      "30000/49000 loss: 0.47222540130336327\n",
      "40000/49000 loss: 0.44201151181543674\n",
      "epoch 17: valid acc = 0.827, new learning rate = 0.00020906016760958852\n",
      "10000/49000 loss: 0.4529689255200603\n",
      "20000/49000 loss: 0.4824631086605724\n",
      "30000/49000 loss: 0.4973034262009815\n",
      "40000/49000 loss: 0.4736556124326114\n",
      "epoch 18: valid acc = 0.831, new learning rate = 0.00019860715922910907\n",
      "10000/49000 loss: 0.4240501572083266\n",
      "20000/49000 loss: 0.4892889951070295\n",
      "30000/49000 loss: 0.4485049450089448\n",
      "40000/49000 loss: 0.48806790147455725\n",
      "epoch 19: valid acc = 0.83, new learning rate = 0.0001886768012676536\n",
      "10000/49000 loss: 0.43818649157576633\n",
      "20000/49000 loss: 0.4598742762798085\n",
      "30000/49000 loss: 0.4989477596363334\n",
      "40000/49000 loss: 0.43319739634051324\n",
      "epoch 20: valid acc = 0.83, new learning rate = 0.0001792429612042709\n",
      "10000/49000 loss: 0.39683457402363453\n",
      "20000/49000 loss: 0.4965078123479193\n",
      "30000/49000 loss: 0.46431598022442844\n",
      "40000/49000 loss: 0.43935263175890094\n",
      "epoch 21: valid acc = 0.834, new learning rate = 0.00017028081314405735\n",
      "10000/49000 loss: 0.4575925493544236\n",
      "20000/49000 loss: 0.46669919928756576\n",
      "30000/49000 loss: 0.46129231505239965\n",
      "40000/49000 loss: 0.48602794283571044\n",
      "epoch 22: valid acc = 0.833, new learning rate = 0.00016176677248685447\n",
      "10000/49000 loss: 0.4732507623453119\n",
      "20000/49000 loss: 0.4409300216534692\n",
      "30000/49000 loss: 0.48046946226707254\n",
      "40000/49000 loss: 0.47629767671928336\n",
      "epoch 23: valid acc = 0.834, new learning rate = 0.00015367843386251173\n",
      "10000/49000 loss: 0.451265747903503\n",
      "20000/49000 loss: 0.45741149418309385\n",
      "30000/49000 loss: 0.4441960040721527\n",
      "40000/49000 loss: 0.42131527541075775\n",
      "epoch 24: valid acc = 0.839, new learning rate = 0.00014599451216938612\n",
      "10000/49000 loss: 0.4623145697598306\n",
      "20000/49000 loss: 0.4602722988898838\n",
      "30000/49000 loss: 0.44323296751630203\n",
      "40000/49000 loss: 0.4280858941975371\n",
      "epoch 25: valid acc = 0.834, new learning rate = 0.00013869478656091682\n",
      "10000/49000 loss: 0.4135658614699523\n",
      "20000/49000 loss: 0.42631215984353216\n",
      "30000/49000 loss: 0.44696495384667184\n",
      "40000/49000 loss: 0.4289572950047766\n",
      "epoch 26: valid acc = 0.842, new learning rate = 0.00013176004723287096\n",
      "10000/49000 loss: 0.4874022312604874\n",
      "20000/49000 loss: 0.4621915105040845\n",
      "30000/49000 loss: 0.4842860656263749\n",
      "40000/49000 loss: 0.43312511216249694\n",
      "epoch 27: valid acc = 0.843, new learning rate = 0.0001251720448712274\n",
      "10000/49000 loss: 0.4067516570397995\n",
      "20000/49000 loss: 0.4383357081942463\n",
      "30000/49000 loss: 0.40657936286181257\n",
      "40000/49000 loss: 0.45357414217078845\n",
      "epoch 28: valid acc = 0.841, new learning rate = 0.00011891344262766602\n",
      "10000/49000 loss: 0.4540158113024074\n",
      "20000/49000 loss: 0.4573986018873725\n",
      "30000/49000 loss: 0.4334944099297104\n",
      "40000/49000 loss: 0.43929570759924685\n",
      "epoch 29: valid acc = 0.845, new learning rate = 0.00011296777049628272\n",
      "10000/49000 loss: 0.4300658402782425\n",
      "20000/49000 loss: 0.43569136405188585\n",
      "30000/49000 loss: 0.4137877481273724\n",
      "40000/49000 loss: 0.4710392846711846\n",
      "epoch 30: valid acc = 0.84, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.8457959183673469\n",
      "test acc: 0.84\n",
      "test acc: 0.831\n",
      "number of batches for training: 49\n",
      "epoch 1: valid acc = 0.362, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.534, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.586, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.689, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.733, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.745, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.759, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.779, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.781, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.79, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.797, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.8, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.805, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.814, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.815, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.822, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.825, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.826, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.833, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.83, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.832, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.833, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.834, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.831, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.837, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.839, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.838, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.841, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.838, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.844, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.8442244897959184\n",
      "test acc: 0.844\n",
      "test acc: 0.8311\n",
      "number of batches for training: 49\n",
      "10000/49000 loss: 2.6500390811386554\n",
      "20000/49000 loss: 2.609022810754\n",
      "30000/49000 loss: 2.5648670229712587\n",
      "40000/49000 loss: 2.4779510670725453\n",
      "epoch 1: valid acc = 0.344, new learning rate = 0.000475\n",
      "10000/49000 loss: 2.093142826515271\n",
      "20000/49000 loss: 1.9371839466755678\n",
      "30000/49000 loss: 1.608487520699623\n",
      "40000/49000 loss: 1.3380522988902677\n",
      "epoch 2: valid acc = 0.51, new learning rate = 0.00045125\n",
      "10000/49000 loss: 1.2073647824238576\n",
      "20000/49000 loss: 1.1901954866986966\n",
      "30000/49000 loss: 1.0914221445307957\n",
      "40000/49000 loss: 1.1524625512426385\n",
      "epoch 3: valid acc = 0.628, new learning rate = 0.0004286875\n",
      "10000/49000 loss: 0.9969362842872954\n",
      "20000/49000 loss: 1.0258480562016323\n",
      "30000/49000 loss: 0.9736534032549271\n",
      "40000/49000 loss: 0.9408677575982999\n",
      "epoch 4: valid acc = 0.703, new learning rate = 0.00040725312499999993\n",
      "10000/49000 loss: 0.8663917742137408\n",
      "20000/49000 loss: 0.8163419972182788\n",
      "30000/49000 loss: 0.7864593876697522\n",
      "40000/49000 loss: 0.7944405456163975\n",
      "epoch 5: valid acc = 0.737, new learning rate = 0.0003868904687499999\n",
      "10000/49000 loss: 0.7421273480975161\n",
      "20000/49000 loss: 0.6636006812761509\n",
      "30000/49000 loss: 0.7067837404461448\n",
      "40000/49000 loss: 0.7216193870449138\n",
      "epoch 6: valid acc = 0.749, new learning rate = 0.0003675459453124999\n",
      "10000/49000 loss: 0.6884797294595977\n",
      "20000/49000 loss: 0.6522271466275193\n",
      "30000/49000 loss: 0.6528972954166918\n",
      "40000/49000 loss: 0.6084138868989285\n",
      "epoch 7: valid acc = 0.769, new learning rate = 0.00034916864804687486\n",
      "10000/49000 loss: 0.6054623324787562\n",
      "20000/49000 loss: 0.5665597197387974\n",
      "30000/49000 loss: 0.6168589617723914\n",
      "40000/49000 loss: 0.6360936959377499\n",
      "epoch 8: valid acc = 0.775, new learning rate = 0.0003317102156445311\n",
      "10000/49000 loss: 0.6196704775530252\n",
      "20000/49000 loss: 0.5674329170672218\n",
      "30000/49000 loss: 0.5871233825849662\n",
      "40000/49000 loss: 0.57723036622622\n",
      "epoch 9: valid acc = 0.782, new learning rate = 0.0003151247048623045\n",
      "10000/49000 loss: 0.5935325490304042\n",
      "20000/49000 loss: 0.551194893568357\n",
      "30000/49000 loss: 0.5767867736544078\n",
      "40000/49000 loss: 0.515080194808023\n",
      "epoch 10: valid acc = 0.788, new learning rate = 0.00029936846961918924\n",
      "10000/49000 loss: 0.4861468185457147\n",
      "20000/49000 loss: 0.5547563530930053\n",
      "30000/49000 loss: 0.5699833744223439\n",
      "40000/49000 loss: 0.5515918031376066\n",
      "epoch 11: valid acc = 0.805, new learning rate = 0.00028440004613822977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/49000 loss: 0.535401123935773\n",
      "20000/49000 loss: 0.5274526132996005\n",
      "30000/49000 loss: 0.6040891947995306\n",
      "40000/49000 loss: 0.5374078842788623\n",
      "epoch 12: valid acc = 0.809, new learning rate = 0.00027018004383131826\n",
      "10000/49000 loss: 0.49843875030319623\n",
      "20000/49000 loss: 0.5175334728061037\n",
      "30000/49000 loss: 0.5325017964351951\n",
      "40000/49000 loss: 0.5120392551312847\n",
      "epoch 13: valid acc = 0.814, new learning rate = 0.00025667104163975234\n",
      "10000/49000 loss: 0.5347013570871314\n",
      "20000/49000 loss: 0.4847244115792134\n",
      "30000/49000 loss: 0.4653151048238559\n",
      "40000/49000 loss: 0.5123398274929832\n",
      "epoch 14: valid acc = 0.817, new learning rate = 0.00024383748955776472\n",
      "10000/49000 loss: 0.5222926967369212\n",
      "20000/49000 loss: 0.4955377181571094\n",
      "30000/49000 loss: 0.518937569830165\n",
      "40000/49000 loss: 0.4973249790568683\n",
      "epoch 15: valid acc = 0.818, new learning rate = 0.00023164561507987649\n",
      "10000/49000 loss: 0.5221231106296609\n",
      "20000/49000 loss: 0.46060626926857906\n",
      "30000/49000 loss: 0.5051082353410927\n",
      "40000/49000 loss: 0.5021832129632228\n",
      "epoch 16: valid acc = 0.818, new learning rate = 0.00022006333432588265\n",
      "10000/49000 loss: 0.5136653423266468\n",
      "20000/49000 loss: 0.46057769018194816\n",
      "30000/49000 loss: 0.5238357821245377\n",
      "40000/49000 loss: 0.4674345781662918\n",
      "epoch 17: valid acc = 0.822, new learning rate = 0.00020906016760958852\n",
      "10000/49000 loss: 0.5020205806294012\n",
      "20000/49000 loss: 0.49515980759763417\n",
      "30000/49000 loss: 0.4754058879579951\n",
      "40000/49000 loss: 0.485043650245226\n",
      "epoch 18: valid acc = 0.828, new learning rate = 0.00019860715922910907\n",
      "10000/49000 loss: 0.5537014101671982\n",
      "20000/49000 loss: 0.4371281181760507\n",
      "30000/49000 loss: 0.4717707595897034\n",
      "40000/49000 loss: 0.5240874820070532\n",
      "epoch 19: valid acc = 0.831, new learning rate = 0.0001886768012676536\n",
      "10000/49000 loss: 0.49191361103662345\n",
      "20000/49000 loss: 0.4762791543366428\n",
      "30000/49000 loss: 0.4776228260596769\n",
      "40000/49000 loss: 0.43495628820010107\n",
      "epoch 20: valid acc = 0.833, new learning rate = 0.0001792429612042709\n",
      "10000/49000 loss: 0.4287246281043654\n",
      "20000/49000 loss: 0.47286843309989596\n",
      "30000/49000 loss: 0.4852674847086253\n",
      "40000/49000 loss: 0.44111838991782776\n",
      "epoch 21: valid acc = 0.832, new learning rate = 0.00017028081314405735\n",
      "10000/49000 loss: 0.44593929399858107\n",
      "20000/49000 loss: 0.47366168451781016\n",
      "30000/49000 loss: 0.45150645487431074\n",
      "40000/49000 loss: 0.497304531566947\n",
      "epoch 22: valid acc = 0.832, new learning rate = 0.00016176677248685447\n",
      "10000/49000 loss: 0.436683364931009\n",
      "20000/49000 loss: 0.4245914991981635\n",
      "30000/49000 loss: 0.44780157991549446\n",
      "40000/49000 loss: 0.4426644584936165\n",
      "epoch 23: valid acc = 0.831, new learning rate = 0.00015367843386251173\n",
      "10000/49000 loss: 0.4727781690728516\n",
      "20000/49000 loss: 0.4708822425380489\n",
      "30000/49000 loss: 0.4514529845077633\n",
      "40000/49000 loss: 0.47019435851645974\n",
      "epoch 24: valid acc = 0.835, new learning rate = 0.00014599451216938612\n",
      "10000/49000 loss: 0.41246733781011247\n",
      "20000/49000 loss: 0.4116126755334923\n",
      "30000/49000 loss: 0.45631556578145627\n",
      "40000/49000 loss: 0.42682112111442666\n",
      "epoch 25: valid acc = 0.832, new learning rate = 0.00013869478656091682\n",
      "10000/49000 loss: 0.4637114025659287\n",
      "20000/49000 loss: 0.42883916130888894\n",
      "30000/49000 loss: 0.5074354497407023\n",
      "40000/49000 loss: 0.4096694629710588\n",
      "epoch 26: valid acc = 0.837, new learning rate = 0.00013176004723287096\n",
      "10000/49000 loss: 0.47783260871845906\n",
      "20000/49000 loss: 0.4861417456831535\n",
      "30000/49000 loss: 0.4092355646309324\n",
      "40000/49000 loss: 0.4367622144685793\n",
      "epoch 27: valid acc = 0.837, new learning rate = 0.0001251720448712274\n",
      "10000/49000 loss: 0.4357650955163494\n",
      "20000/49000 loss: 0.4348126594457172\n",
      "30000/49000 loss: 0.4730517510999804\n",
      "40000/49000 loss: 0.4347542303728125\n",
      "epoch 28: valid acc = 0.835, new learning rate = 0.00011891344262766602\n",
      "10000/49000 loss: 0.4918172158413255\n",
      "20000/49000 loss: 0.44158672815164524\n",
      "30000/49000 loss: 0.47428581554918403\n",
      "40000/49000 loss: 0.47228482810498995\n",
      "epoch 29: valid acc = 0.838, new learning rate = 0.00011296777049628272\n",
      "10000/49000 loss: 0.46499623545811763\n",
      "20000/49000 loss: 0.4443441512454627\n",
      "30000/49000 loss: 0.4889199331800382\n",
      "40000/49000 loss: 0.4491293152585425\n",
      "epoch 30: valid acc = 0.836, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.8460408163265306\n",
      "test acc: 0.836\n",
      "test acc: 0.8307\n",
      "number of batches for training: 49\n",
      "epoch 1: valid acc = 0.394, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.519, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.597, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.686, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.725, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.744, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.772, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.781, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.785, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.794, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.804, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.81, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.813, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.814, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.817, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.819, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.823, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.825, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.828, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.829, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.83, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.83, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.838, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.836, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.838, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.84, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.838, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.84, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.845, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.844, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.8456122448979592\n",
      "test acc: 0.844\n",
      "test acc: 0.8301\n",
      "number of batches for training: 245\n",
      "2000/49000 loss: 2.691017122322168\n",
      "4000/49000 loss: 2.607309575618245\n",
      "6000/49000 loss: 2.5093629572007523\n",
      "8000/49000 loss: 2.486417251544816\n",
      "10000/49000 loss: 2.208646483621503\n",
      "12000/49000 loss: 2.0847835283253993\n",
      "14000/49000 loss: 2.018815005189718\n",
      "16000/49000 loss: 1.6950873186053923\n",
      "18000/49000 loss: 1.4392741330228331\n",
      "20000/49000 loss: 1.2032875099060683\n",
      "22000/49000 loss: 1.1577832414669227\n",
      "24000/49000 loss: 0.928855786578191\n",
      "26000/49000 loss: 1.1846715034988589\n",
      "28000/49000 loss: 1.1485118905730733\n",
      "30000/49000 loss: 1.008145651648233\n",
      "32000/49000 loss: 0.8663759590252125\n",
      "34000/49000 loss: 0.8624169236365986\n",
      "36000/49000 loss: 0.9316942419555309\n",
      "38000/49000 loss: 0.9118879365259301\n",
      "40000/49000 loss: 0.9049418605422662\n",
      "42000/49000 loss: 0.8560139734387726\n",
      "44000/49000 loss: 0.6615508699325415\n",
      "46000/49000 loss: 0.7076763694231097\n",
      "48000/49000 loss: 0.6851174485411453\n",
      "epoch 1: valid acc = 0.735, new learning rate = 0.000475\n",
      "2000/49000 loss: 0.6546559287471153\n",
      "4000/49000 loss: 0.6434084297682262\n",
      "6000/49000 loss: 0.6440951402470436\n",
      "8000/49000 loss: 0.6305705161272261\n",
      "10000/49000 loss: 0.6444956823100383\n",
      "12000/49000 loss: 0.6649586661779473\n",
      "14000/49000 loss: 0.6224459858253784\n",
      "16000/49000 loss: 0.6073441302056232\n",
      "18000/49000 loss: 0.600706766641799\n",
      "20000/49000 loss: 0.589312094641211\n",
      "22000/49000 loss: 0.6839648556625565\n",
      "24000/49000 loss: 0.545002084132766\n",
      "26000/49000 loss: 0.48850781640255114\n",
      "28000/49000 loss: 0.6590379532951589\n",
      "30000/49000 loss: 0.5591787688325373\n",
      "32000/49000 loss: 0.5887489859090768\n",
      "34000/49000 loss: 0.5561956357289498\n",
      "36000/49000 loss: 0.5460788510469072\n",
      "38000/49000 loss: 0.5416224260409432\n",
      "40000/49000 loss: 0.5879011165132347\n",
      "42000/49000 loss: 0.4930182886732033\n",
      "44000/49000 loss: 0.4562876662241078\n",
      "46000/49000 loss: 0.4670275299722331\n",
      "48000/49000 loss: 0.4403622868910975\n",
      "epoch 2: valid acc = 0.808, new learning rate = 0.00045125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/49000 loss: 0.5433161399364494\n",
      "4000/49000 loss: 0.5572262624936435\n",
      "6000/49000 loss: 0.44368521346522977\n",
      "8000/49000 loss: 0.4576174536587442\n",
      "10000/49000 loss: 0.4716908171941566\n",
      "12000/49000 loss: 0.4833837176482422\n",
      "14000/49000 loss: 0.5038888804333752\n",
      "16000/49000 loss: 0.5214362059543755\n",
      "18000/49000 loss: 0.5520574086723313\n",
      "20000/49000 loss: 0.5141048951524189\n",
      "22000/49000 loss: 0.5214617622514197\n",
      "24000/49000 loss: 0.49106045437913665\n",
      "26000/49000 loss: 0.5035914775988072\n",
      "28000/49000 loss: 0.4926436419490277\n",
      "30000/49000 loss: 0.534497155106331\n",
      "32000/49000 loss: 0.45140925220553535\n",
      "34000/49000 loss: 0.4656346743511672\n",
      "36000/49000 loss: 0.3333903261621092\n",
      "38000/49000 loss: 0.5277933912080617\n",
      "40000/49000 loss: 0.4536721627988304\n",
      "42000/49000 loss: 0.42084980919993475\n",
      "44000/49000 loss: 0.523210820109057\n",
      "46000/49000 loss: 0.3771205202239601\n",
      "48000/49000 loss: 0.49498564310161924\n",
      "epoch 3: valid acc = 0.823, new learning rate = 0.0004286875\n",
      "2000/49000 loss: 0.34267495920317953\n",
      "4000/49000 loss: 0.4630429048642474\n",
      "6000/49000 loss: 0.45275654513123326\n",
      "8000/49000 loss: 0.4688184321131013\n",
      "10000/49000 loss: 0.47081882999428937\n",
      "12000/49000 loss: 0.39053106643405966\n",
      "14000/49000 loss: 0.4272274818058145\n",
      "16000/49000 loss: 0.4308316513895412\n",
      "18000/49000 loss: 0.4503433426384324\n",
      "20000/49000 loss: 0.3851394274482669\n",
      "22000/49000 loss: 0.4184381563929538\n",
      "24000/49000 loss: 0.4274086436693319\n",
      "26000/49000 loss: 0.41821491273913075\n",
      "28000/49000 loss: 0.37844320955500055\n",
      "30000/49000 loss: 0.46449459871868615\n",
      "32000/49000 loss: 0.4634205342023671\n",
      "34000/49000 loss: 0.3936953799832768\n",
      "36000/49000 loss: 0.4405462017197111\n",
      "38000/49000 loss: 0.35141501657125834\n",
      "40000/49000 loss: 0.4097338983858175\n",
      "42000/49000 loss: 0.43912391724950817\n",
      "44000/49000 loss: 0.4257610017334014\n",
      "46000/49000 loss: 0.4009446570004906\n",
      "48000/49000 loss: 0.4054853977190546\n",
      "epoch 4: valid acc = 0.842, new learning rate = 0.00040725312499999993\n",
      "2000/49000 loss: 0.37983306845035075\n",
      "4000/49000 loss: 0.3504982176813381\n",
      "6000/49000 loss: 0.43586638637586617\n",
      "8000/49000 loss: 0.4524812581402762\n",
      "10000/49000 loss: 0.46348790065840356\n",
      "12000/49000 loss: 0.3526717273644257\n",
      "14000/49000 loss: 0.5027662285705687\n",
      "16000/49000 loss: 0.4595496636199934\n",
      "18000/49000 loss: 0.29841006923192764\n",
      "20000/49000 loss: 0.4668470259831101\n",
      "22000/49000 loss: 0.3709767020137961\n",
      "24000/49000 loss: 0.459762492728112\n",
      "26000/49000 loss: 0.48651957339583835\n",
      "28000/49000 loss: 0.3899781104523144\n",
      "30000/49000 loss: 0.3477119282676498\n",
      "32000/49000 loss: 0.4402199986498\n",
      "34000/49000 loss: 0.35706013633815786\n",
      "36000/49000 loss: 0.38381261262310823\n",
      "38000/49000 loss: 0.47895043658512143\n",
      "40000/49000 loss: 0.3858996069742549\n",
      "42000/49000 loss: 0.4941222828458911\n",
      "44000/49000 loss: 0.522871337889728\n",
      "46000/49000 loss: 0.44600583498279334\n",
      "48000/49000 loss: 0.4132178506754976\n",
      "epoch 5: valid acc = 0.857, new learning rate = 0.0003868904687499999\n",
      "2000/49000 loss: 0.43107012486570123\n",
      "4000/49000 loss: 0.4458745658189078\n",
      "6000/49000 loss: 0.40177839533757\n",
      "8000/49000 loss: 0.39800346081748994\n",
      "10000/49000 loss: 0.37273009793200884\n",
      "12000/49000 loss: 0.3503176685898184\n",
      "14000/49000 loss: 0.3382088791815841\n",
      "16000/49000 loss: 0.3295299071443748\n",
      "18000/49000 loss: 0.37983378689440694\n",
      "20000/49000 loss: 0.37602559487686915\n",
      "22000/49000 loss: 0.43834767256307755\n",
      "24000/49000 loss: 0.35485290566808664\n",
      "26000/49000 loss: 0.5344629247143184\n",
      "28000/49000 loss: 0.3136816752018549\n",
      "30000/49000 loss: 0.4434653261667261\n",
      "32000/49000 loss: 0.33967838511027243\n",
      "34000/49000 loss: 0.4672174445730923\n",
      "36000/49000 loss: 0.4467212673400959\n",
      "38000/49000 loss: 0.42916904504608566\n",
      "40000/49000 loss: 0.3427242665806161\n",
      "42000/49000 loss: 0.3924387465990096\n",
      "44000/49000 loss: 0.3519634463921082\n",
      "46000/49000 loss: 0.36144345883492346\n",
      "48000/49000 loss: 0.3969008294328326\n",
      "epoch 6: valid acc = 0.863, new learning rate = 0.0003675459453124999\n",
      "2000/49000 loss: 0.4548349017474674\n",
      "4000/49000 loss: 0.4617513897303084\n",
      "6000/49000 loss: 0.3522056014790947\n",
      "8000/49000 loss: 0.44150696804778244\n",
      "10000/49000 loss: 0.3392240301508488\n",
      "12000/49000 loss: 0.4258471599472531\n",
      "14000/49000 loss: 0.4655540652447143\n",
      "16000/49000 loss: 0.41704919275052305\n",
      "18000/49000 loss: 0.36562434092002505\n",
      "20000/49000 loss: 0.4218885262985396\n",
      "22000/49000 loss: 0.30686600405556447\n",
      "24000/49000 loss: 0.4141100017915907\n",
      "26000/49000 loss: 0.41617150972072203\n",
      "28000/49000 loss: 0.3809037858613209\n",
      "30000/49000 loss: 0.36770890834610814\n",
      "32000/49000 loss: 0.3709246512761165\n",
      "34000/49000 loss: 0.3196568825045341\n",
      "36000/49000 loss: 0.44287124013450885\n",
      "38000/49000 loss: 0.3130019585463071\n",
      "40000/49000 loss: 0.5317542640415557\n",
      "42000/49000 loss: 0.3875667579262224\n",
      "44000/49000 loss: 0.3995669631784783\n",
      "46000/49000 loss: 0.392966685569714\n",
      "48000/49000 loss: 0.31826143181973304\n",
      "epoch 7: valid acc = 0.871, new learning rate = 0.00034916864804687486\n",
      "2000/49000 loss: 0.41030354407299463\n",
      "4000/49000 loss: 0.389600375160954\n",
      "6000/49000 loss: 0.4926490922719519\n",
      "8000/49000 loss: 0.48677087516451945\n",
      "10000/49000 loss: 0.4958136971964976\n",
      "12000/49000 loss: 0.4074532952327495\n",
      "14000/49000 loss: 0.36456583494370565\n",
      "16000/49000 loss: 0.4163816117087798\n",
      "18000/49000 loss: 0.4847348191513105\n",
      "20000/49000 loss: 0.34981569130779355\n",
      "22000/49000 loss: 0.4297161953625247\n",
      "24000/49000 loss: 0.30469309444679754\n",
      "26000/49000 loss: 0.32962028834984763\n",
      "28000/49000 loss: 0.38882755137630653\n",
      "30000/49000 loss: 0.3174747500592959\n",
      "32000/49000 loss: 0.33016315066489677\n",
      "34000/49000 loss: 0.3707333136677046\n",
      "36000/49000 loss: 0.3948317842352696\n",
      "38000/49000 loss: 0.43760906979939457\n",
      "40000/49000 loss: 0.4404494028404967\n",
      "42000/49000 loss: 0.4175711527851415\n",
      "44000/49000 loss: 0.3778209550712999\n",
      "46000/49000 loss: 0.33490198098143564\n",
      "48000/49000 loss: 0.39684540023979786\n",
      "epoch 8: valid acc = 0.871, new learning rate = 0.0003317102156445311\n",
      "2000/49000 loss: 0.3836874180027275\n",
      "4000/49000 loss: 0.48396291658407115\n",
      "6000/49000 loss: 0.37846248204266325\n",
      "8000/49000 loss: 0.37417457192722503\n",
      "10000/49000 loss: 0.40252521955867715\n",
      "12000/49000 loss: 0.43343520728362683\n",
      "14000/49000 loss: 0.4167557385391452\n",
      "16000/49000 loss: 0.38505816672293924\n",
      "18000/49000 loss: 0.5006344803148791\n",
      "20000/49000 loss: 0.41590520994292396\n",
      "22000/49000 loss: 0.38301248215065903\n",
      "24000/49000 loss: 0.3489402089019497\n",
      "26000/49000 loss: 0.32432166620688185\n",
      "28000/49000 loss: 0.3165317069052866\n",
      "30000/49000 loss: 0.3814023151629152\n",
      "32000/49000 loss: 0.4274722893136762\n",
      "34000/49000 loss: 0.374877595921512\n",
      "36000/49000 loss: 0.2762487137102338\n",
      "38000/49000 loss: 0.2998830498378101\n",
      "40000/49000 loss: 0.3328930375232255\n",
      "42000/49000 loss: 0.2805146130649252\n",
      "44000/49000 loss: 0.3143965392822427\n",
      "46000/49000 loss: 0.3381779874576211\n",
      "48000/49000 loss: 0.36755476889096345\n",
      "epoch 9: valid acc = 0.872, new learning rate = 0.0003151247048623045\n",
      "2000/49000 loss: 0.2938746535792169\n",
      "4000/49000 loss: 0.4290673347012672\n",
      "6000/49000 loss: 0.41199656031557896\n",
      "8000/49000 loss: 0.3852802109555527\n",
      "10000/49000 loss: 0.49223263599374745\n",
      "12000/49000 loss: 0.3400830698765778\n",
      "14000/49000 loss: 0.33635372390475976\n",
      "16000/49000 loss: 0.3806555167414321\n",
      "18000/49000 loss: 0.3558678484576885\n",
      "20000/49000 loss: 0.28513379785936915\n",
      "22000/49000 loss: 0.4111612327319601\n",
      "24000/49000 loss: 0.35629215452085133\n",
      "26000/49000 loss: 0.313518834036733\n",
      "28000/49000 loss: 0.45983085315360867\n",
      "30000/49000 loss: 0.31506864665768275\n",
      "32000/49000 loss: 0.3604688093195464\n",
      "34000/49000 loss: 0.3035809086859932\n",
      "36000/49000 loss: 0.360933460299765\n",
      "38000/49000 loss: 0.26825753444923356\n",
      "40000/49000 loss: 0.33165432617614665\n",
      "42000/49000 loss: 0.27733413891193115\n",
      "44000/49000 loss: 0.47958217412427134\n",
      "46000/49000 loss: 0.32253654382210234\n",
      "48000/49000 loss: 0.3229068326115148\n",
      "epoch 10: valid acc = 0.87, new learning rate = 0.00029936846961918924\n",
      "2000/49000 loss: 0.27764036790816793\n",
      "4000/49000 loss: 0.37326410663774945\n",
      "6000/49000 loss: 0.4839368739026594\n",
      "8000/49000 loss: 0.298640850796611\n",
      "10000/49000 loss: 0.34930439097447413\n",
      "12000/49000 loss: 0.32825673519680143\n",
      "14000/49000 loss: 0.2765998628352589\n",
      "16000/49000 loss: 0.3445096683008099\n",
      "18000/49000 loss: 0.26311359744269636\n",
      "20000/49000 loss: 0.32397323517016385\n",
      "22000/49000 loss: 0.3683684785971497\n",
      "24000/49000 loss: 0.3490111197406637\n",
      "26000/49000 loss: 0.3348145412607737\n",
      "28000/49000 loss: 0.33123366418717215\n",
      "30000/49000 loss: 0.4045719378033443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000/49000 loss: 0.3852007202436261\n",
      "34000/49000 loss: 0.45534090304497077\n",
      "36000/49000 loss: 0.3505785427159063\n",
      "38000/49000 loss: 0.2964458957393002\n",
      "40000/49000 loss: 0.3844169272276098\n",
      "42000/49000 loss: 0.43957270071371485\n",
      "44000/49000 loss: 0.32490706722743723\n",
      "46000/49000 loss: 0.3453424364671207\n",
      "48000/49000 loss: 0.3115201058694048\n",
      "epoch 11: valid acc = 0.875, new learning rate = 0.00028440004613822977\n",
      "2000/49000 loss: 0.3082995715494816\n",
      "4000/49000 loss: 0.3426263824496173\n",
      "6000/49000 loss: 0.4882640753331094\n",
      "8000/49000 loss: 0.387117620098262\n",
      "10000/49000 loss: 0.35687668332329175\n",
      "12000/49000 loss: 0.33556670351402035\n",
      "14000/49000 loss: 0.38239096336191064\n",
      "16000/49000 loss: 0.36964631408743154\n",
      "18000/49000 loss: 0.38701026246491277\n",
      "20000/49000 loss: 0.3358147562897056\n",
      "22000/49000 loss: 0.2720280946928194\n",
      "24000/49000 loss: 0.4002901736292629\n",
      "26000/49000 loss: 0.3525656488017849\n",
      "28000/49000 loss: 0.45215851281358305\n",
      "30000/49000 loss: 0.24667315916577987\n",
      "32000/49000 loss: 0.31074008759614213\n",
      "34000/49000 loss: 0.27879898569422557\n",
      "36000/49000 loss: 0.3765548304060624\n",
      "38000/49000 loss: 0.30541768876538455\n",
      "40000/49000 loss: 0.3023477148181545\n",
      "42000/49000 loss: 0.27483130086415375\n",
      "44000/49000 loss: 0.32179560683503666\n",
      "46000/49000 loss: 0.4860494971307409\n",
      "48000/49000 loss: 0.3666723348802567\n",
      "epoch 12: valid acc = 0.874, new learning rate = 0.00027018004383131826\n",
      "2000/49000 loss: 0.33665365022008886\n",
      "4000/49000 loss: 0.33359046457332747\n",
      "6000/49000 loss: 0.37534297545232276\n",
      "8000/49000 loss: 0.40759686620482666\n",
      "10000/49000 loss: 0.3244582564268483\n",
      "12000/49000 loss: 0.3157031948463501\n",
      "14000/49000 loss: 0.37695851681866704\n",
      "16000/49000 loss: 0.36342058852015263\n",
      "18000/49000 loss: 0.43628358701962744\n",
      "20000/49000 loss: 0.39312521987220034\n",
      "22000/49000 loss: 0.3119840698192087\n",
      "24000/49000 loss: 0.33181439594327633\n",
      "26000/49000 loss: 0.38832662062193085\n",
      "28000/49000 loss: 0.31790170223863917\n",
      "30000/49000 loss: 0.3940037770044362\n",
      "32000/49000 loss: 0.4086635786302295\n",
      "34000/49000 loss: 0.33741452509824676\n",
      "36000/49000 loss: 0.24999250863365427\n",
      "38000/49000 loss: 0.28284405359826625\n",
      "40000/49000 loss: 0.31455344150172426\n",
      "42000/49000 loss: 0.3270216929575647\n",
      "44000/49000 loss: 0.3668854368000176\n",
      "46000/49000 loss: 0.4393592573045711\n",
      "48000/49000 loss: 0.3927031906001765\n",
      "epoch 13: valid acc = 0.876, new learning rate = 0.00025667104163975234\n",
      "2000/49000 loss: 0.2970766005230345\n",
      "4000/49000 loss: 0.2958621185748363\n",
      "6000/49000 loss: 0.28338033329049234\n",
      "8000/49000 loss: 0.3460366862928956\n",
      "10000/49000 loss: 0.2890631292398778\n",
      "12000/49000 loss: 0.28847290418201427\n",
      "14000/49000 loss: 0.342743878453695\n",
      "16000/49000 loss: 0.2968626304189578\n",
      "18000/49000 loss: 0.3078937819553126\n",
      "20000/49000 loss: 0.2829730000906289\n",
      "22000/49000 loss: 0.40057349100335476\n",
      "24000/49000 loss: 0.36836862452186087\n",
      "26000/49000 loss: 0.38096294898281996\n",
      "28000/49000 loss: 0.2825016972766108\n",
      "30000/49000 loss: 0.37619219402208154\n",
      "32000/49000 loss: 0.35202774514692364\n",
      "34000/49000 loss: 0.3337975292924532\n",
      "36000/49000 loss: 0.4533010781410299\n",
      "38000/49000 loss: 0.26538257913942004\n",
      "40000/49000 loss: 0.32155930591335236\n",
      "42000/49000 loss: 0.28868253862735826\n",
      "44000/49000 loss: 0.32917500530614424\n",
      "46000/49000 loss: 0.37233882583971367\n",
      "48000/49000 loss: 0.3355181111673057\n",
      "epoch 14: valid acc = 0.879, new learning rate = 0.00024383748955776472\n",
      "2000/49000 loss: 0.4121282081264209\n",
      "4000/49000 loss: 0.2883698313150164\n",
      "6000/49000 loss: 0.33445493662422715\n",
      "8000/49000 loss: 0.33094810832170024\n",
      "10000/49000 loss: 0.34587780647925054\n",
      "12000/49000 loss: 0.3867520467583574\n",
      "14000/49000 loss: 0.3640453216064424\n",
      "16000/49000 loss: 0.31528754792071395\n",
      "18000/49000 loss: 0.40498700451003944\n",
      "20000/49000 loss: 0.28527295878076697\n",
      "22000/49000 loss: 0.26907728335041753\n",
      "24000/49000 loss: 0.33453522507571426\n",
      "26000/49000 loss: 0.2917378676202528\n",
      "28000/49000 loss: 0.31483401945075673\n",
      "30000/49000 loss: 0.3505424757234396\n",
      "32000/49000 loss: 0.29852472043184297\n",
      "34000/49000 loss: 0.3113269188474594\n",
      "36000/49000 loss: 0.4074085138067669\n",
      "38000/49000 loss: 0.28538285197794505\n",
      "40000/49000 loss: 0.36082757562987083\n",
      "42000/49000 loss: 0.3697133796316304\n",
      "44000/49000 loss: 0.3719854434125562\n",
      "46000/49000 loss: 0.3534033965845666\n",
      "48000/49000 loss: 0.3991744922402646\n",
      "epoch 15: valid acc = 0.878, new learning rate = 0.00023164561507987649\n",
      "2000/49000 loss: 0.39332631008153013\n",
      "4000/49000 loss: 0.3728765082453824\n",
      "6000/49000 loss: 0.317366668216818\n",
      "8000/49000 loss: 0.2659738338491721\n",
      "10000/49000 loss: 0.40011191593139955\n",
      "12000/49000 loss: 0.3435238046540253\n",
      "14000/49000 loss: 0.28074331868422486\n",
      "16000/49000 loss: 0.35699426709936993\n",
      "18000/49000 loss: 0.3243595695250309\n",
      "20000/49000 loss: 0.35845053720656767\n",
      "22000/49000 loss: 0.2342000081169586\n",
      "24000/49000 loss: 0.3214855852516647\n",
      "26000/49000 loss: 0.30430315428126287\n",
      "28000/49000 loss: 0.3358120944997261\n",
      "30000/49000 loss: 0.38423020308518635\n",
      "32000/49000 loss: 0.33487178350119917\n",
      "34000/49000 loss: 0.2982915495826233\n",
      "36000/49000 loss: 0.3135477994172065\n",
      "38000/49000 loss: 0.32546275129913566\n",
      "40000/49000 loss: 0.26748577901591164\n",
      "42000/49000 loss: 0.35532705359147315\n",
      "44000/49000 loss: 0.427908335472628\n",
      "46000/49000 loss: 0.2979140382188874\n",
      "48000/49000 loss: 0.3743247884537839\n",
      "epoch 16: valid acc = 0.88, new learning rate = 0.00022006333432588265\n",
      "2000/49000 loss: 0.2493196717467891\n",
      "4000/49000 loss: 0.36424438173877\n",
      "6000/49000 loss: 0.3634305405295419\n",
      "8000/49000 loss: 0.31213985759480817\n",
      "10000/49000 loss: 0.35386979393570994\n",
      "12000/49000 loss: 0.25852503112755515\n",
      "14000/49000 loss: 0.34207944871049223\n",
      "16000/49000 loss: 0.3780790139634417\n",
      "18000/49000 loss: 0.3266106867056369\n",
      "20000/49000 loss: 0.2574713250081838\n",
      "22000/49000 loss: 0.32562836971841375\n",
      "24000/49000 loss: 0.32633036514904346\n",
      "26000/49000 loss: 0.25288832081036927\n",
      "28000/49000 loss: 0.3117205375931025\n",
      "30000/49000 loss: 0.316969261676166\n",
      "32000/49000 loss: 0.24074006549152016\n",
      "34000/49000 loss: 0.3236615355777106\n",
      "36000/49000 loss: 0.3539948389651328\n",
      "38000/49000 loss: 0.22306787447208468\n",
      "40000/49000 loss: 0.33998504192975626\n",
      "42000/49000 loss: 0.39031947310841847\n",
      "44000/49000 loss: 0.30929503034491107\n",
      "46000/49000 loss: 0.28320116220884806\n",
      "48000/49000 loss: 0.3573760222184241\n",
      "epoch 17: valid acc = 0.885, new learning rate = 0.00020906016760958852\n",
      "2000/49000 loss: 0.3117555827823143\n",
      "4000/49000 loss: 0.3083745496079941\n",
      "6000/49000 loss: 0.25140332470836796\n",
      "8000/49000 loss: 0.35036351267256605\n",
      "10000/49000 loss: 0.3249063149403233\n",
      "12000/49000 loss: 0.3111435721947521\n",
      "14000/49000 loss: 0.38285500551566615\n",
      "16000/49000 loss: 0.2887976571754348\n",
      "18000/49000 loss: 0.3576014822246461\n",
      "20000/49000 loss: 0.34156401896583133\n",
      "22000/49000 loss: 0.343245965403467\n",
      "24000/49000 loss: 0.33050736876094167\n",
      "26000/49000 loss: 0.40517925601773863\n",
      "28000/49000 loss: 0.3304207211445551\n",
      "30000/49000 loss: 0.26763310358717907\n",
      "32000/49000 loss: 0.36890635171422453\n",
      "34000/49000 loss: 0.3757539366657754\n",
      "36000/49000 loss: 0.3133404941202283\n",
      "38000/49000 loss: 0.31843820392739597\n",
      "40000/49000 loss: 0.32619568224619105\n",
      "42000/49000 loss: 0.26815552256501857\n",
      "44000/49000 loss: 0.2941162591539055\n",
      "46000/49000 loss: 0.303540434285275\n",
      "48000/49000 loss: 0.2590847597784835\n",
      "epoch 18: valid acc = 0.88, new learning rate = 0.00019860715922910907\n",
      "2000/49000 loss: 0.358300829358299\n",
      "4000/49000 loss: 0.322822247804732\n",
      "6000/49000 loss: 0.29122950215660626\n",
      "8000/49000 loss: 0.3235217917899594\n",
      "10000/49000 loss: 0.2613556249435198\n",
      "12000/49000 loss: 0.3406454856863621\n",
      "14000/49000 loss: 0.2718636428623996\n",
      "16000/49000 loss: 0.37397925828323225\n",
      "18000/49000 loss: 0.3454027680343491\n",
      "20000/49000 loss: 0.30883669690883514\n",
      "22000/49000 loss: 0.31424053816834124\n",
      "24000/49000 loss: 0.223673329012335\n",
      "26000/49000 loss: 0.27901414462830487\n",
      "28000/49000 loss: 0.2299888189337928\n",
      "30000/49000 loss: 0.3060272081874851\n",
      "32000/49000 loss: 0.19516687644309086\n",
      "34000/49000 loss: 0.34110005741683547\n",
      "36000/49000 loss: 0.39247426874159425\n",
      "38000/49000 loss: 0.2412072900317105\n",
      "40000/49000 loss: 0.2635685858716924\n",
      "42000/49000 loss: 0.25202035332789857\n",
      "44000/49000 loss: 0.3104398603798564\n",
      "46000/49000 loss: 0.3747234336520044\n",
      "48000/49000 loss: 0.31116553585117096\n",
      "epoch 19: valid acc = 0.88, new learning rate = 0.0001886768012676536\n",
      "2000/49000 loss: 0.2544777826121413\n",
      "4000/49000 loss: 0.2656833493885024\n",
      "6000/49000 loss: 0.29342949908841204\n",
      "8000/49000 loss: 0.3573134404293476\n",
      "10000/49000 loss: 0.32059605820005116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/49000 loss: 0.2719838534306402\n",
      "14000/49000 loss: 0.32640718747893954\n",
      "16000/49000 loss: 0.32379271706236895\n",
      "18000/49000 loss: 0.27779902363168374\n",
      "20000/49000 loss: 0.27227287444939763\n",
      "22000/49000 loss: 0.2527866954413748\n",
      "24000/49000 loss: 0.261147705135109\n",
      "26000/49000 loss: 0.2809440359466602\n",
      "28000/49000 loss: 0.2696311940272602\n",
      "30000/49000 loss: 0.2875736656269135\n",
      "32000/49000 loss: 0.3473059956951737\n",
      "34000/49000 loss: 0.36943178058179826\n",
      "36000/49000 loss: 0.32908920298158073\n",
      "38000/49000 loss: 0.3225986161297678\n",
      "40000/49000 loss: 0.35170749169104376\n",
      "42000/49000 loss: 0.3423553667040273\n",
      "44000/49000 loss: 0.27854212500205305\n",
      "46000/49000 loss: 0.2615425613276013\n",
      "48000/49000 loss: 0.33732214991984366\n",
      "epoch 20: valid acc = 0.883, new learning rate = 0.0001792429612042709\n",
      "2000/49000 loss: 0.39823771734611524\n",
      "4000/49000 loss: 0.4905669371179481\n",
      "6000/49000 loss: 0.27932817377466734\n",
      "8000/49000 loss: 0.2866557759466109\n",
      "10000/49000 loss: 0.31725489254866013\n",
      "12000/49000 loss: 0.27540077080288994\n",
      "14000/49000 loss: 0.31572525918985084\n",
      "16000/49000 loss: 0.3264898262613848\n",
      "18000/49000 loss: 0.36806010055113453\n",
      "20000/49000 loss: 0.3775054764657237\n",
      "22000/49000 loss: 0.32948824254004966\n",
      "24000/49000 loss: 0.3666420366255114\n",
      "26000/49000 loss: 0.3595249324741168\n",
      "28000/49000 loss: 0.3993781023512651\n",
      "30000/49000 loss: 0.3257611381058607\n",
      "32000/49000 loss: 0.42065526671946457\n",
      "34000/49000 loss: 0.337907686573547\n",
      "36000/49000 loss: 0.34461730087264536\n",
      "38000/49000 loss: 0.288770849681584\n",
      "40000/49000 loss: 0.33293634124420113\n",
      "42000/49000 loss: 0.33136599974467185\n",
      "44000/49000 loss: 0.18545919673494302\n",
      "46000/49000 loss: 0.27907647749621395\n",
      "48000/49000 loss: 0.4368146105722093\n",
      "epoch 21: valid acc = 0.886, new learning rate = 0.00017028081314405735\n",
      "2000/49000 loss: 0.3894900685918061\n",
      "4000/49000 loss: 0.2870718792600642\n",
      "6000/49000 loss: 0.24252861682849977\n",
      "8000/49000 loss: 0.27781808505824873\n",
      "10000/49000 loss: 0.3169262825303974\n",
      "12000/49000 loss: 0.39858208226788183\n",
      "14000/49000 loss: 0.2959582520279164\n",
      "16000/49000 loss: 0.2647382197691955\n",
      "18000/49000 loss: 0.3112055233668005\n",
      "20000/49000 loss: 0.2956318173959679\n",
      "22000/49000 loss: 0.29562042294304575\n",
      "24000/49000 loss: 0.3309738384089649\n",
      "26000/49000 loss: 0.37168293881405434\n",
      "28000/49000 loss: 0.3034618944350532\n",
      "30000/49000 loss: 0.3070477874676376\n",
      "32000/49000 loss: 0.39047769265981536\n",
      "34000/49000 loss: 0.3453480578016755\n",
      "36000/49000 loss: 0.4267801250275526\n",
      "38000/49000 loss: 0.32448298361473393\n",
      "40000/49000 loss: 0.2997310393946614\n",
      "42000/49000 loss: 0.24393543726610079\n",
      "44000/49000 loss: 0.31434590142318386\n",
      "46000/49000 loss: 0.2757627962886026\n",
      "48000/49000 loss: 0.3338980030334921\n",
      "epoch 22: valid acc = 0.886, new learning rate = 0.00016176677248685447\n",
      "2000/49000 loss: 0.3319358379555418\n",
      "4000/49000 loss: 0.2171724892790444\n",
      "6000/49000 loss: 0.29584168109911\n",
      "8000/49000 loss: 0.32504809451445976\n",
      "10000/49000 loss: 0.40738810542986603\n",
      "12000/49000 loss: 0.317960164358894\n",
      "14000/49000 loss: 0.2106311940399662\n",
      "16000/49000 loss: 0.2893668657611496\n",
      "18000/49000 loss: 0.26907567720188624\n",
      "20000/49000 loss: 0.3271817597832215\n",
      "22000/49000 loss: 0.2835160853168766\n",
      "24000/49000 loss: 0.3156738975523842\n",
      "26000/49000 loss: 0.3036890565813185\n",
      "28000/49000 loss: 0.352912603282706\n",
      "30000/49000 loss: 0.20288426849853805\n",
      "32000/49000 loss: 0.42428669066759667\n",
      "34000/49000 loss: 0.30138311498310455\n",
      "36000/49000 loss: 0.29032794116661276\n",
      "38000/49000 loss: 0.38159487556726296\n",
      "40000/49000 loss: 0.40933834409927905\n",
      "42000/49000 loss: 0.2325726030512251\n",
      "44000/49000 loss: 0.3074435582836637\n",
      "46000/49000 loss: 0.3433012516726519\n",
      "48000/49000 loss: 0.28023476760356364\n",
      "epoch 23: valid acc = 0.886, new learning rate = 0.00015367843386251173\n",
      "2000/49000 loss: 0.3787823849255515\n",
      "4000/49000 loss: 0.3543190267125446\n",
      "6000/49000 loss: 0.2834716829864174\n",
      "8000/49000 loss: 0.2507611610015053\n",
      "10000/49000 loss: 0.3581792813720208\n",
      "12000/49000 loss: 0.29369740700426855\n",
      "14000/49000 loss: 0.3527736034902603\n",
      "16000/49000 loss: 0.3943979866438449\n",
      "18000/49000 loss: 0.4407204686086849\n",
      "20000/49000 loss: 0.3247023182455086\n",
      "22000/49000 loss: 0.3521101070970867\n",
      "24000/49000 loss: 0.31422458803962156\n",
      "26000/49000 loss: 0.2522643315236543\n",
      "28000/49000 loss: 0.2864470754085464\n",
      "30000/49000 loss: 0.24701275234292247\n",
      "32000/49000 loss: 0.27537730513362585\n",
      "34000/49000 loss: 0.2723853731212482\n",
      "36000/49000 loss: 0.2538340969107913\n",
      "38000/49000 loss: 0.234015123370973\n",
      "40000/49000 loss: 0.24053772387838618\n",
      "42000/49000 loss: 0.3688576121933811\n",
      "44000/49000 loss: 0.2533293534734648\n",
      "46000/49000 loss: 0.23066657426776022\n",
      "48000/49000 loss: 0.3018911409700966\n",
      "epoch 24: valid acc = 0.884, new learning rate = 0.00014599451216938612\n",
      "2000/49000 loss: 0.3231964884641272\n",
      "4000/49000 loss: 0.3788228643276901\n",
      "6000/49000 loss: 0.2745124409426432\n",
      "8000/49000 loss: 0.34369085814457334\n",
      "10000/49000 loss: 0.23451861125817183\n",
      "12000/49000 loss: 0.27895558923951014\n",
      "14000/49000 loss: 0.38497471762749663\n",
      "16000/49000 loss: 0.21813283203741668\n",
      "18000/49000 loss: 0.2819172426991039\n",
      "20000/49000 loss: 0.2993807612650185\n",
      "22000/49000 loss: 0.27908284926607607\n",
      "24000/49000 loss: 0.21712950493192296\n",
      "26000/49000 loss: 0.29665875814638837\n",
      "28000/49000 loss: 0.2931524077504179\n",
      "30000/49000 loss: 0.44678482171056455\n",
      "32000/49000 loss: 0.28977048661809124\n",
      "34000/49000 loss: 0.3430759646200975\n",
      "36000/49000 loss: 0.2774782088964751\n",
      "38000/49000 loss: 0.2856239660450282\n",
      "40000/49000 loss: 0.3675389562938474\n",
      "42000/49000 loss: 0.23851968480371974\n",
      "44000/49000 loss: 0.23419977834757552\n",
      "46000/49000 loss: 0.3175444229196565\n",
      "48000/49000 loss: 0.3364106533969817\n",
      "epoch 25: valid acc = 0.888, new learning rate = 0.00013869478656091682\n",
      "2000/49000 loss: 0.2758819436204799\n",
      "4000/49000 loss: 0.24078247718439658\n",
      "6000/49000 loss: 0.29899410919519565\n",
      "8000/49000 loss: 0.2734680420774732\n",
      "10000/49000 loss: 0.30510818433258097\n",
      "12000/49000 loss: 0.2427673959575861\n",
      "14000/49000 loss: 0.2829469071324665\n",
      "16000/49000 loss: 0.2614133307832645\n",
      "18000/49000 loss: 0.4002642823154133\n",
      "20000/49000 loss: 0.31923199573686334\n",
      "22000/49000 loss: 0.3612741577858467\n",
      "24000/49000 loss: 0.2848928376571234\n",
      "26000/49000 loss: 0.2798948110554745\n",
      "28000/49000 loss: 0.3326101040736276\n",
      "30000/49000 loss: 0.3530297586855759\n",
      "32000/49000 loss: 0.3142839323764379\n",
      "34000/49000 loss: 0.2922841173543624\n",
      "36000/49000 loss: 0.3526793931620074\n",
      "38000/49000 loss: 0.27667024109652133\n",
      "40000/49000 loss: 0.3117207805966243\n",
      "42000/49000 loss: 0.3085314953634729\n",
      "44000/49000 loss: 0.30056343950000064\n",
      "46000/49000 loss: 0.34051373209780816\n",
      "48000/49000 loss: 0.42429099025951\n",
      "epoch 26: valid acc = 0.886, new learning rate = 0.00013176004723287096\n",
      "2000/49000 loss: 0.3256549398786814\n",
      "4000/49000 loss: 0.2754707554911258\n",
      "6000/49000 loss: 0.31819133040026365\n",
      "8000/49000 loss: 0.2924474077679337\n",
      "10000/49000 loss: 0.35034227207326063\n",
      "12000/49000 loss: 0.28988511012150336\n",
      "14000/49000 loss: 0.2637246768786519\n",
      "16000/49000 loss: 0.24579097766030397\n",
      "18000/49000 loss: 0.38388113137222146\n",
      "20000/49000 loss: 0.34407216642498506\n",
      "22000/49000 loss: 0.4264104918431261\n",
      "24000/49000 loss: 0.2306388884787016\n",
      "26000/49000 loss: 0.21866297620345568\n",
      "28000/49000 loss: 0.3333793378444111\n",
      "30000/49000 loss: 0.3253155403116953\n",
      "32000/49000 loss: 0.32732073467968753\n",
      "34000/49000 loss: 0.3656427622569923\n",
      "36000/49000 loss: 0.28769231107310966\n",
      "38000/49000 loss: 0.2673762209278407\n",
      "40000/49000 loss: 0.30369240266153646\n",
      "42000/49000 loss: 0.27877725235855383\n",
      "44000/49000 loss: 0.2365057285309912\n",
      "46000/49000 loss: 0.3090054980086813\n",
      "48000/49000 loss: 0.377188726324039\n",
      "epoch 27: valid acc = 0.882, new learning rate = 0.0001251720448712274\n",
      "2000/49000 loss: 0.25261632852285065\n",
      "4000/49000 loss: 0.28755847205384427\n",
      "6000/49000 loss: 0.2536799852216282\n",
      "8000/49000 loss: 0.3720274222311982\n",
      "10000/49000 loss: 0.2947742420039752\n",
      "12000/49000 loss: 0.30183339173707857\n",
      "14000/49000 loss: 0.3737533700900427\n",
      "16000/49000 loss: 0.33670481660411455\n",
      "18000/49000 loss: 0.3457184290075054\n",
      "20000/49000 loss: 0.26282689184236774\n",
      "22000/49000 loss: 0.37761630814916197\n",
      "24000/49000 loss: 0.3219515963149003\n",
      "26000/49000 loss: 0.3041958568947245\n",
      "28000/49000 loss: 0.3537006570974725\n",
      "30000/49000 loss: 0.30405275571608686\n",
      "32000/49000 loss: 0.2930736774066618\n",
      "34000/49000 loss: 0.24855424598261347\n",
      "36000/49000 loss: 0.289395297076116\n",
      "38000/49000 loss: 0.3762405800773046\n",
      "40000/49000 loss: 0.24148712630718588\n",
      "42000/49000 loss: 0.35479595176631334\n",
      "44000/49000 loss: 0.2797450148512379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46000/49000 loss: 0.2603323528612026\n",
      "48000/49000 loss: 0.24705265382432498\n",
      "epoch 28: valid acc = 0.886, new learning rate = 0.00011891344262766602\n",
      "2000/49000 loss: 0.33280813796495856\n",
      "4000/49000 loss: 0.3168194405351066\n",
      "6000/49000 loss: 0.2661771167131158\n",
      "8000/49000 loss: 0.31796630555533734\n",
      "10000/49000 loss: 0.29824738488243135\n",
      "12000/49000 loss: 0.3769491679292496\n",
      "14000/49000 loss: 0.28300274361460764\n",
      "16000/49000 loss: 0.25670456123535157\n",
      "18000/49000 loss: 0.27386958128217576\n",
      "20000/49000 loss: 0.20249583640477978\n",
      "22000/49000 loss: 0.2761068217614365\n",
      "24000/49000 loss: 0.3136365961457286\n",
      "26000/49000 loss: 0.4172555694403527\n",
      "28000/49000 loss: 0.280424888018525\n",
      "30000/49000 loss: 0.2847606883922643\n",
      "32000/49000 loss: 0.3044096664707882\n",
      "34000/49000 loss: 0.2596616955628238\n",
      "36000/49000 loss: 0.30292350599618845\n",
      "38000/49000 loss: 0.2991133152068069\n",
      "40000/49000 loss: 0.3155845130279448\n",
      "42000/49000 loss: 0.3279024707175584\n",
      "44000/49000 loss: 0.2787046459201617\n",
      "46000/49000 loss: 0.32369126626864564\n",
      "48000/49000 loss: 0.2569604224779293\n",
      "epoch 29: valid acc = 0.893, new learning rate = 0.00011296777049628272\n",
      "2000/49000 loss: 0.3271967120842221\n",
      "4000/49000 loss: 0.2702506920158025\n",
      "6000/49000 loss: 0.27389519559268555\n",
      "8000/49000 loss: 0.3050375952891276\n",
      "10000/49000 loss: 0.22302159996462306\n",
      "12000/49000 loss: 0.4124934893718374\n",
      "14000/49000 loss: 0.29146717950261425\n",
      "16000/49000 loss: 0.20129109009626645\n",
      "18000/49000 loss: 0.33177526003265395\n",
      "20000/49000 loss: 0.29952022659697725\n",
      "22000/49000 loss: 0.3219549757828488\n",
      "24000/49000 loss: 0.3057696492178238\n",
      "26000/49000 loss: 0.26893311049682983\n",
      "28000/49000 loss: 0.20712480598670394\n",
      "30000/49000 loss: 0.3051201542980387\n",
      "32000/49000 loss: 0.28379635494115496\n",
      "34000/49000 loss: 0.2971469082672993\n",
      "36000/49000 loss: 0.19808951642214248\n",
      "38000/49000 loss: 0.36095288606139375\n",
      "40000/49000 loss: 0.2792929836971036\n",
      "42000/49000 loss: 0.22321906494141558\n",
      "44000/49000 loss: 0.2888669501507152\n",
      "46000/49000 loss: 0.2853070950017676\n",
      "48000/49000 loss: 0.380988588058807\n",
      "epoch 30: valid acc = 0.887, new learning rate = 0.00010731938197146858\n",
      "2000/49000 loss: 0.3164458836716874\n",
      "4000/49000 loss: 0.23229118532124518\n",
      "6000/49000 loss: 0.3913837380812174\n",
      "8000/49000 loss: 0.3330363492599586\n",
      "10000/49000 loss: 0.22084025108400882\n",
      "12000/49000 loss: 0.2856665696606314\n",
      "14000/49000 loss: 0.3782091585919569\n",
      "16000/49000 loss: 0.33525757203930207\n",
      "18000/49000 loss: 0.27548532445827206\n",
      "20000/49000 loss: 0.24339854185942827\n",
      "22000/49000 loss: 0.3843572897522551\n",
      "24000/49000 loss: 0.2682238644186324\n",
      "26000/49000 loss: 0.23618436237248805\n",
      "28000/49000 loss: 0.4061346414542335\n",
      "30000/49000 loss: 0.27928979646215557\n",
      "32000/49000 loss: 0.3092127065536565\n",
      "34000/49000 loss: 0.34818858961537635\n",
      "36000/49000 loss: 0.29968460525627255\n",
      "38000/49000 loss: 0.3332593594932095\n",
      "40000/49000 loss: 0.2689594665826824\n",
      "42000/49000 loss: 0.2611734999478276\n",
      "44000/49000 loss: 0.2640346255569028\n",
      "46000/49000 loss: 0.3246773633463445\n",
      "48000/49000 loss: 0.21652877688944838\n",
      "epoch 31: valid acc = 0.888, new learning rate = 0.00010195341287289515\n",
      "2000/49000 loss: 0.2753513525940262\n",
      "4000/49000 loss: 0.22481582593713692\n",
      "6000/49000 loss: 0.30908355020167283\n",
      "8000/49000 loss: 0.2999239408693341\n",
      "10000/49000 loss: 0.28566905646074414\n",
      "12000/49000 loss: 0.2813816193914986\n",
      "14000/49000 loss: 0.23099671302265595\n",
      "16000/49000 loss: 0.2537140563800496\n",
      "18000/49000 loss: 0.41129629146751356\n",
      "20000/49000 loss: 0.31669923802455135\n",
      "22000/49000 loss: 0.38146689097875514\n",
      "24000/49000 loss: 0.31117331207179066\n",
      "26000/49000 loss: 0.2700837483446456\n",
      "28000/49000 loss: 0.36396022308095577\n",
      "30000/49000 loss: 0.28612535537905526\n",
      "32000/49000 loss: 0.23030865746927054\n",
      "34000/49000 loss: 0.3217348440712552\n",
      "36000/49000 loss: 0.30915554079352753\n",
      "38000/49000 loss: 0.3184676397770502\n",
      "40000/49000 loss: 0.33220244661374465\n",
      "42000/49000 loss: 0.244329334508062\n",
      "44000/49000 loss: 0.2395163843972284\n",
      "46000/49000 loss: 0.2898423060416632\n",
      "48000/49000 loss: 0.3212524418977502\n",
      "epoch 32: valid acc = 0.884, new learning rate = 9.685574222925039e-05\n",
      "2000/49000 loss: 0.3532888963715538\n",
      "4000/49000 loss: 0.2513987080318697\n",
      "6000/49000 loss: 0.2953790742308143\n",
      "8000/49000 loss: 0.2359327676273454\n",
      "10000/49000 loss: 0.24179859831789133\n",
      "12000/49000 loss: 0.2657211391574544\n",
      "14000/49000 loss: 0.32211024495778123\n",
      "16000/49000 loss: 0.3031228346449995\n",
      "18000/49000 loss: 0.3295585707406734\n",
      "20000/49000 loss: 0.3341053446304475\n",
      "22000/49000 loss: 0.29169163102937123\n",
      "24000/49000 loss: 0.28549659997167126\n",
      "26000/49000 loss: 0.2947394520269848\n",
      "28000/49000 loss: 0.26001720221490093\n",
      "30000/49000 loss: 0.2788577081970984\n",
      "32000/49000 loss: 0.33533503566700834\n",
      "34000/49000 loss: 0.34309445003732475\n",
      "36000/49000 loss: 0.3386912895012447\n",
      "38000/49000 loss: 0.24183678315788096\n",
      "40000/49000 loss: 0.27788230289062704\n",
      "42000/49000 loss: 0.2819642138531186\n",
      "44000/49000 loss: 0.38947644733121345\n",
      "46000/49000 loss: 0.2899972743756866\n",
      "48000/49000 loss: 0.2936201544230667\n",
      "epoch 33: valid acc = 0.891, new learning rate = 9.201295511778786e-05\n",
      "2000/49000 loss: 0.2741379769950219\n",
      "4000/49000 loss: 0.25826859343997344\n",
      "6000/49000 loss: 0.28730474026361347\n",
      "8000/49000 loss: 0.24752923120919101\n",
      "10000/49000 loss: 0.28368261787104476\n",
      "12000/49000 loss: 0.23226938691658422\n",
      "14000/49000 loss: 0.31267047261246533\n",
      "16000/49000 loss: 0.30042435144329654\n",
      "18000/49000 loss: 0.36827635114676466\n",
      "20000/49000 loss: 0.3040062140170551\n",
      "22000/49000 loss: 0.19681504858411863\n",
      "24000/49000 loss: 0.29768254332229344\n",
      "26000/49000 loss: 0.32930993114215795\n",
      "28000/49000 loss: 0.31335131668146277\n",
      "30000/49000 loss: 0.2720251072432564\n",
      "32000/49000 loss: 0.34596147232388386\n",
      "34000/49000 loss: 0.3247025167000293\n",
      "36000/49000 loss: 0.3091486638945205\n",
      "38000/49000 loss: 0.24070009419640234\n",
      "40000/49000 loss: 0.2787688516356079\n",
      "42000/49000 loss: 0.26434277273059165\n",
      "44000/49000 loss: 0.3858831480944279\n",
      "46000/49000 loss: 0.24082515562901255\n",
      "48000/49000 loss: 0.266925182304952\n",
      "epoch 34: valid acc = 0.89, new learning rate = 8.741230736189846e-05\n",
      "2000/49000 loss: 0.26752969109326985\n",
      "4000/49000 loss: 0.23782776413393164\n",
      "6000/49000 loss: 0.27661966591041537\n",
      "8000/49000 loss: 0.42444329216420756\n",
      "10000/49000 loss: 0.27373039408386507\n",
      "12000/49000 loss: 0.33389467924217603\n",
      "14000/49000 loss: 0.2715740721012347\n",
      "16000/49000 loss: 0.39702576663348405\n",
      "18000/49000 loss: 0.3342582689852112\n",
      "20000/49000 loss: 0.33490643122018765\n",
      "22000/49000 loss: 0.31565040782679066\n",
      "24000/49000 loss: 0.29435659392929425\n",
      "26000/49000 loss: 0.41280696864210986\n",
      "28000/49000 loss: 0.2523717331462982\n",
      "30000/49000 loss: 0.3273318925202604\n",
      "32000/49000 loss: 0.3078170526667418\n",
      "34000/49000 loss: 0.27972340387947936\n",
      "36000/49000 loss: 0.21691731861855412\n",
      "38000/49000 loss: 0.3046573069572479\n",
      "40000/49000 loss: 0.4143388419816459\n",
      "42000/49000 loss: 0.3410319625040237\n",
      "44000/49000 loss: 0.28660210264803726\n",
      "46000/49000 loss: 0.3076316746184524\n",
      "48000/49000 loss: 0.2931491626797595\n",
      "epoch 35: valid acc = 0.889, new learning rate = 8.304169199380353e-05\n",
      "2000/49000 loss: 0.4167442771174737\n",
      "4000/49000 loss: 0.3189614345107727\n",
      "6000/49000 loss: 0.2423558240111636\n",
      "8000/49000 loss: 0.3106580968380832\n",
      "10000/49000 loss: 0.28633245261526313\n",
      "12000/49000 loss: 0.2551046729119401\n",
      "14000/49000 loss: 0.27674988955700647\n",
      "16000/49000 loss: 0.44443579112556914\n",
      "18000/49000 loss: 0.274812665636262\n",
      "20000/49000 loss: 0.2988989693260132\n",
      "22000/49000 loss: 0.28960544916287334\n",
      "24000/49000 loss: 0.3065567187936662\n",
      "26000/49000 loss: 0.3091192265712106\n",
      "28000/49000 loss: 0.2702976245169142\n",
      "30000/49000 loss: 0.330063289114915\n",
      "32000/49000 loss: 0.3363518905985577\n",
      "34000/49000 loss: 0.3276343173847489\n",
      "36000/49000 loss: 0.3179563435308299\n",
      "38000/49000 loss: 0.3072843629785967\n",
      "40000/49000 loss: 0.3234715773833271\n",
      "42000/49000 loss: 0.2517931619756972\n",
      "44000/49000 loss: 0.25181008629637625\n",
      "46000/49000 loss: 0.35945133967478804\n",
      "48000/49000 loss: 0.27067039191218284\n",
      "epoch 36: valid acc = 0.89, new learning rate = 7.888960739411335e-05\n",
      "2000/49000 loss: 0.233142296885311\n",
      "4000/49000 loss: 0.36331428189672765\n",
      "6000/49000 loss: 0.20462239216640188\n",
      "8000/49000 loss: 0.2942185770635534\n",
      "10000/49000 loss: 0.3064202624200233\n",
      "12000/49000 loss: 0.3109621759576346\n",
      "14000/49000 loss: 0.2710923446168593\n",
      "16000/49000 loss: 0.2563151689482845\n",
      "18000/49000 loss: 0.3654827729629291\n",
      "20000/49000 loss: 0.3504078759647379\n",
      "22000/49000 loss: 0.36319168441924593\n",
      "24000/49000 loss: 0.27555841654632957\n",
      "26000/49000 loss: 0.3206774729000078\n",
      "28000/49000 loss: 0.31899359287188456\n",
      "30000/49000 loss: 0.25919289358331954\n",
      "32000/49000 loss: 0.26465871959770704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34000/49000 loss: 0.32693481046924205\n",
      "36000/49000 loss: 0.24500482956402\n",
      "38000/49000 loss: 0.30933165438504034\n",
      "40000/49000 loss: 0.2824199589522133\n",
      "42000/49000 loss: 0.3015906247494745\n",
      "44000/49000 loss: 0.24078477365301462\n",
      "46000/49000 loss: 0.3514339244923952\n",
      "48000/49000 loss: 0.2581316900425808\n",
      "epoch 37: valid acc = 0.884, new learning rate = 7.494512702440768e-05\n",
      "2000/49000 loss: 0.2931287807201279\n",
      "4000/49000 loss: 0.24947782251832365\n",
      "6000/49000 loss: 0.22061206146979342\n",
      "8000/49000 loss: 0.3728654096884058\n",
      "10000/49000 loss: 0.1783538296047266\n",
      "12000/49000 loss: 0.3571156015755442\n",
      "14000/49000 loss: 0.2767389028340636\n",
      "16000/49000 loss: 0.30193906012742117\n",
      "18000/49000 loss: 0.31054388064694133\n",
      "20000/49000 loss: 0.28435029959176694\n",
      "22000/49000 loss: 0.3053169340916006\n",
      "24000/49000 loss: 0.3238713363027844\n",
      "26000/49000 loss: 0.30320054836709504\n",
      "28000/49000 loss: 0.300753965279117\n",
      "30000/49000 loss: 0.2690413419429264\n",
      "32000/49000 loss: 0.3223700520195141\n",
      "34000/49000 loss: 0.29514585995102643\n",
      "36000/49000 loss: 0.2779911937742273\n",
      "38000/49000 loss: 0.24924922231969693\n",
      "40000/49000 loss: 0.319230238353509\n",
      "42000/49000 loss: 0.2775790056700324\n",
      "44000/49000 loss: 0.3036767582133777\n",
      "46000/49000 loss: 0.3833387393463528\n",
      "48000/49000 loss: 0.23010699334971244\n",
      "epoch 38: valid acc = 0.884, new learning rate = 7.119787067318729e-05\n",
      "2000/49000 loss: 0.2451549376293886\n",
      "4000/49000 loss: 0.30331861656914527\n",
      "6000/49000 loss: 0.3730325093083905\n",
      "8000/49000 loss: 0.26537775316695916\n",
      "10000/49000 loss: 0.3423213054138767\n",
      "12000/49000 loss: 0.38842415839953315\n",
      "14000/49000 loss: 0.34968675839854385\n",
      "16000/49000 loss: 0.36267119108487766\n",
      "18000/49000 loss: 0.25159364986833826\n",
      "20000/49000 loss: 0.33271663020238645\n",
      "22000/49000 loss: 0.3036768680885532\n",
      "24000/49000 loss: 0.34907907942014005\n",
      "26000/49000 loss: 0.26209951100507844\n",
      "28000/49000 loss: 0.30910892846810273\n",
      "30000/49000 loss: 0.32045080720036\n",
      "32000/49000 loss: 0.2617567548449667\n",
      "34000/49000 loss: 0.3360109738196785\n",
      "36000/49000 loss: 0.2854172597818361\n",
      "38000/49000 loss: 0.3289055772562541\n",
      "40000/49000 loss: 0.31277012239923757\n",
      "42000/49000 loss: 0.28631217168255185\n",
      "44000/49000 loss: 0.1989086115509639\n",
      "46000/49000 loss: 0.3147739495843561\n",
      "48000/49000 loss: 0.3701218429344051\n",
      "epoch 39: valid acc = 0.891, new learning rate = 6.763797713952792e-05\n",
      "2000/49000 loss: 0.32797738834387835\n",
      "4000/49000 loss: 0.27398796872091763\n",
      "6000/49000 loss: 0.3793076643882548\n",
      "8000/49000 loss: 0.302742232342476\n",
      "10000/49000 loss: 0.4192931603736656\n",
      "12000/49000 loss: 0.27899353413638317\n",
      "14000/49000 loss: 0.3003449627308103\n",
      "16000/49000 loss: 0.2592023204365175\n",
      "18000/49000 loss: 0.3030952233861186\n",
      "20000/49000 loss: 0.23668848964178152\n",
      "22000/49000 loss: 0.28655431209227167\n",
      "24000/49000 loss: 0.25430718843295935\n",
      "26000/49000 loss: 0.3600136372035141\n",
      "28000/49000 loss: 0.3378754213037242\n",
      "30000/49000 loss: 0.23937364511129128\n",
      "32000/49000 loss: 0.23140938260891472\n",
      "34000/49000 loss: 0.2782395014047519\n",
      "36000/49000 loss: 0.26456944167331387\n",
      "38000/49000 loss: 0.24422201800645282\n",
      "40000/49000 loss: 0.21925242323930164\n",
      "42000/49000 loss: 0.32771624539027366\n",
      "44000/49000 loss: 0.2682102277977088\n",
      "46000/49000 loss: 0.30493230891009426\n",
      "48000/49000 loss: 0.2748288757456315\n",
      "epoch 40: valid acc = 0.888, new learning rate = 6.425607828255152e-05\n",
      "2000/49000 loss: 0.297919425374117\n",
      "4000/49000 loss: 0.26321825441552205\n",
      "6000/49000 loss: 0.21265170042019224\n",
      "8000/49000 loss: 0.34026556687810283\n",
      "10000/49000 loss: 0.2889076486606316\n",
      "12000/49000 loss: 0.33269512580759986\n",
      "14000/49000 loss: 0.27942909656745174\n",
      "16000/49000 loss: 0.28427235502840603\n",
      "18000/49000 loss: 0.32290615964547636\n",
      "20000/49000 loss: 0.2502755564077024\n",
      "22000/49000 loss: 0.3329144948569458\n",
      "24000/49000 loss: 0.19476390863365373\n",
      "26000/49000 loss: 0.31955310166347306\n",
      "28000/49000 loss: 0.2851072971711403\n",
      "30000/49000 loss: 0.2840521484899828\n",
      "32000/49000 loss: 0.35073467998175983\n",
      "34000/49000 loss: 0.22496594531201158\n",
      "36000/49000 loss: 0.26691670625047104\n",
      "38000/49000 loss: 0.24115154366672342\n",
      "40000/49000 loss: 0.26027678285124406\n",
      "42000/49000 loss: 0.30311194856888685\n",
      "44000/49000 loss: 0.2634343971822425\n",
      "46000/49000 loss: 0.27088033997781175\n",
      "48000/49000 loss: 0.34585519805957937\n",
      "epoch 41: valid acc = 0.893, new learning rate = 6.104327436842394e-05\n",
      "2000/49000 loss: 0.20423591800816723\n",
      "4000/49000 loss: 0.3285675063887652\n",
      "6000/49000 loss: 0.32155085481617934\n",
      "8000/49000 loss: 0.25625692397386357\n",
      "10000/49000 loss: 0.27455465600818946\n",
      "12000/49000 loss: 0.218354415330461\n",
      "14000/49000 loss: 0.29597476150127444\n",
      "16000/49000 loss: 0.307147793560126\n",
      "18000/49000 loss: 0.24660297668668704\n",
      "20000/49000 loss: 0.29330839170953404\n",
      "22000/49000 loss: 0.25097169814269643\n",
      "24000/49000 loss: 0.2633628831682932\n",
      "26000/49000 loss: 0.31768547066931085\n",
      "28000/49000 loss: 0.2951053275672578\n",
      "30000/49000 loss: 0.3046330234586143\n",
      "32000/49000 loss: 0.2825973612902817\n",
      "34000/49000 loss: 0.3869724363478187\n",
      "36000/49000 loss: 0.26432948130995965\n",
      "38000/49000 loss: 0.35322851059511745\n",
      "40000/49000 loss: 0.2920721442022426\n",
      "42000/49000 loss: 0.2801234389039403\n",
      "44000/49000 loss: 0.330611775579401\n",
      "46000/49000 loss: 0.19227190966997929\n",
      "48000/49000 loss: 0.2497586802058583\n",
      "epoch 42: valid acc = 0.888, new learning rate = 5.799111065000274e-05\n",
      "2000/49000 loss: 0.29338712197692557\n",
      "4000/49000 loss: 0.3162823614749148\n",
      "6000/49000 loss: 0.2182635640053289\n",
      "8000/49000 loss: 0.2304999348998746\n",
      "10000/49000 loss: 0.3379955313262531\n",
      "12000/49000 loss: 0.3351431112783193\n",
      "14000/49000 loss: 0.2574011652199019\n",
      "16000/49000 loss: 0.2388787541486246\n",
      "18000/49000 loss: 0.2640673748735833\n",
      "20000/49000 loss: 0.43193983719443624\n",
      "22000/49000 loss: 0.2971851607242416\n",
      "24000/49000 loss: 0.27944974381638144\n",
      "26000/49000 loss: 0.2459105144293992\n",
      "28000/49000 loss: 0.26349219033956056\n",
      "30000/49000 loss: 0.31640864793186396\n",
      "32000/49000 loss: 0.28325326982234006\n",
      "34000/49000 loss: 0.2831370433387457\n",
      "36000/49000 loss: 0.3116310020612959\n",
      "38000/49000 loss: 0.245954376870376\n",
      "40000/49000 loss: 0.25342890828452846\n",
      "42000/49000 loss: 0.32970631817657164\n",
      "44000/49000 loss: 0.2694347518865485\n",
      "46000/49000 loss: 0.2562429942919054\n",
      "48000/49000 loss: 0.27877621295226196\n",
      "epoch 43: valid acc = 0.885, new learning rate = 5.5091555117502596e-05\n",
      "2000/49000 loss: 0.32490959059965174\n",
      "4000/49000 loss: 0.2972293935096637\n",
      "6000/49000 loss: 0.20855238133463147\n",
      "8000/49000 loss: 0.35125284835283394\n",
      "10000/49000 loss: 0.23914173835400218\n",
      "12000/49000 loss: 0.2955753392747884\n",
      "14000/49000 loss: 0.29691115773511906\n",
      "16000/49000 loss: 0.2394415004686712\n",
      "18000/49000 loss: 0.24829537965255838\n",
      "20000/49000 loss: 0.33612176055266146\n",
      "22000/49000 loss: 0.261289554543155\n",
      "24000/49000 loss: 0.2729647617404931\n",
      "26000/49000 loss: 0.42508685580809236\n",
      "28000/49000 loss: 0.276449774132621\n",
      "30000/49000 loss: 0.22056775521597327\n",
      "32000/49000 loss: 0.3869376624159071\n",
      "34000/49000 loss: 0.30455619113676696\n",
      "36000/49000 loss: 0.3573002357864657\n",
      "38000/49000 loss: 0.22409876909715903\n",
      "40000/49000 loss: 0.38084793047557347\n",
      "42000/49000 loss: 0.29932714929239906\n",
      "44000/49000 loss: 0.23808354161117104\n",
      "46000/49000 loss: 0.2558093194627373\n",
      "48000/49000 loss: 0.30619135691460364\n",
      "epoch 44: valid acc = 0.886, new learning rate = 5.2336977361627463e-05\n",
      "2000/49000 loss: 0.23776124177076974\n",
      "4000/49000 loss: 0.2611069791215823\n",
      "6000/49000 loss: 0.3035251447390186\n",
      "8000/49000 loss: 0.25211465726470256\n",
      "10000/49000 loss: 0.35437259699580864\n",
      "12000/49000 loss: 0.21672062178728604\n",
      "14000/49000 loss: 0.31793017654043193\n",
      "16000/49000 loss: 0.31732288318307367\n",
      "18000/49000 loss: 0.27616684116819507\n",
      "20000/49000 loss: 0.30669235981339493\n",
      "22000/49000 loss: 0.4045121851393384\n",
      "24000/49000 loss: 0.27536399937774053\n",
      "26000/49000 loss: 0.3120445912513949\n",
      "28000/49000 loss: 0.32779175624054685\n",
      "30000/49000 loss: 0.30760456727587354\n",
      "32000/49000 loss: 0.32303758951684153\n",
      "34000/49000 loss: 0.2686045610913585\n",
      "36000/49000 loss: 0.2721526954823685\n",
      "38000/49000 loss: 0.25329757525805024\n",
      "40000/49000 loss: 0.3486659351597828\n",
      "42000/49000 loss: 0.31816708448343906\n",
      "44000/49000 loss: 0.2875526197011736\n",
      "46000/49000 loss: 0.32969935064653827\n",
      "48000/49000 loss: 0.25695923177272545\n",
      "epoch 45: valid acc = 0.891, new learning rate = 4.972012849354609e-05\n",
      "2000/49000 loss: 0.24483952022412542\n",
      "4000/49000 loss: 0.3191834464622769\n",
      "6000/49000 loss: 0.3002423500593379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/49000 loss: 0.21036502820697944\n",
      "10000/49000 loss: 0.23986771460047862\n",
      "12000/49000 loss: 0.2129972009734333\n",
      "14000/49000 loss: 0.25654426936450836\n",
      "16000/49000 loss: 0.441327983682291\n",
      "18000/49000 loss: 0.23862993790359274\n",
      "20000/49000 loss: 0.3887543402272001\n",
      "22000/49000 loss: 0.2520316338606149\n",
      "24000/49000 loss: 0.2325498191669245\n",
      "26000/49000 loss: 0.27015949509082826\n",
      "28000/49000 loss: 0.3327752681136617\n",
      "30000/49000 loss: 0.2107952164878429\n",
      "32000/49000 loss: 0.2601023743462871\n",
      "34000/49000 loss: 0.2941741280876503\n",
      "36000/49000 loss: 0.2536479030487205\n",
      "38000/49000 loss: 0.23106417181970854\n",
      "40000/49000 loss: 0.2513358284100603\n",
      "42000/49000 loss: 0.26859374035539846\n",
      "44000/49000 loss: 0.24755700821339133\n",
      "46000/49000 loss: 0.2796861480997446\n",
      "48000/49000 loss: 0.34130673508268283\n",
      "epoch 46: valid acc = 0.888, new learning rate = 4.723412206886878e-05\n",
      "2000/49000 loss: 0.24713761655986918\n",
      "4000/49000 loss: 0.30678038804289054\n",
      "6000/49000 loss: 0.2863963388691177\n",
      "8000/49000 loss: 0.3074948748592936\n",
      "10000/49000 loss: 0.28369690583140084\n",
      "12000/49000 loss: 0.28655179202588144\n",
      "14000/49000 loss: 0.2786164984546865\n",
      "16000/49000 loss: 0.3015911099942834\n",
      "18000/49000 loss: 0.29373246713958845\n",
      "20000/49000 loss: 0.27622157458296803\n",
      "22000/49000 loss: 0.2856897410674487\n",
      "24000/49000 loss: 0.4459694420237989\n",
      "26000/49000 loss: 0.3117383220908928\n",
      "28000/49000 loss: 0.3161261967799847\n",
      "30000/49000 loss: 0.3566145005832726\n",
      "32000/49000 loss: 0.244892331648516\n",
      "34000/49000 loss: 0.3172123083183199\n",
      "36000/49000 loss: 0.2268625577436794\n",
      "38000/49000 loss: 0.34569686710619274\n",
      "40000/49000 loss: 0.18551428356787553\n",
      "42000/49000 loss: 0.2366473319755291\n",
      "44000/49000 loss: 0.20069414895269522\n",
      "46000/49000 loss: 0.26212304509946516\n",
      "48000/49000 loss: 0.317069119977407\n",
      "epoch 47: valid acc = 0.89, new learning rate = 4.487241596542534e-05\n",
      "2000/49000 loss: 0.18830135471993803\n",
      "4000/49000 loss: 0.3463880582615953\n",
      "6000/49000 loss: 0.27057101003357054\n",
      "8000/49000 loss: 0.2806786507400295\n",
      "10000/49000 loss: 0.32971455307977915\n",
      "12000/49000 loss: 0.26599331970203693\n",
      "14000/49000 loss: 0.32013731577217114\n",
      "16000/49000 loss: 0.2930896970136869\n",
      "18000/49000 loss: 0.30313390932150974\n",
      "20000/49000 loss: 0.26801439681882205\n",
      "22000/49000 loss: 0.4082256456599877\n",
      "24000/49000 loss: 0.31811998056880386\n",
      "26000/49000 loss: 0.24791333440059574\n",
      "28000/49000 loss: 0.33428522369335484\n",
      "30000/49000 loss: 0.27670722410272675\n",
      "32000/49000 loss: 0.2183385653116785\n",
      "34000/49000 loss: 0.3152361242227401\n",
      "36000/49000 loss: 0.34324210686577195\n",
      "38000/49000 loss: 0.27936213732634335\n",
      "40000/49000 loss: 0.27156820566136264\n",
      "42000/49000 loss: 0.27360654758104375\n",
      "44000/49000 loss: 0.20632026810650922\n",
      "46000/49000 loss: 0.30097886597751794\n",
      "48000/49000 loss: 0.25120248230413206\n",
      "epoch 48: valid acc = 0.888, new learning rate = 4.262879516715407e-05\n",
      "2000/49000 loss: 0.24351055349490208\n",
      "4000/49000 loss: 0.2370442147693015\n",
      "6000/49000 loss: 0.2835732309051435\n",
      "8000/49000 loss: 0.3774865487756492\n",
      "10000/49000 loss: 0.3086561773855707\n",
      "12000/49000 loss: 0.29687704155994354\n",
      "14000/49000 loss: 0.3223506320553682\n",
      "16000/49000 loss: 0.30144071602821615\n",
      "18000/49000 loss: 0.2653431373873584\n",
      "20000/49000 loss: 0.2546125373322644\n",
      "22000/49000 loss: 0.328691327172361\n",
      "24000/49000 loss: 0.2577883451569125\n",
      "26000/49000 loss: 0.28604250384949975\n",
      "28000/49000 loss: 0.24988690881940703\n",
      "30000/49000 loss: 0.17707124313113484\n",
      "32000/49000 loss: 0.3338033728907109\n",
      "34000/49000 loss: 0.32419049277960965\n",
      "36000/49000 loss: 0.2589633416375397\n",
      "38000/49000 loss: 0.34814335218845954\n",
      "40000/49000 loss: 0.28096016664768475\n",
      "42000/49000 loss: 0.27772011168546257\n",
      "44000/49000 loss: 0.278040111746524\n",
      "46000/49000 loss: 0.2720472853708965\n",
      "48000/49000 loss: 0.27079391103313233\n",
      "epoch 49: valid acc = 0.886, new learning rate = 4.049735540879637e-05\n",
      "2000/49000 loss: 0.24392937551580374\n",
      "4000/49000 loss: 0.22754711522312496\n",
      "6000/49000 loss: 0.31138906037073066\n",
      "8000/49000 loss: 0.3069130740425271\n",
      "10000/49000 loss: 0.22475282954385747\n",
      "12000/49000 loss: 0.2753576080097371\n",
      "14000/49000 loss: 0.25471597660536266\n",
      "16000/49000 loss: 0.30611461338687956\n",
      "18000/49000 loss: 0.21807551081468843\n",
      "20000/49000 loss: 0.251873896393719\n",
      "22000/49000 loss: 0.23164131133349983\n",
      "24000/49000 loss: 0.3378591059792527\n",
      "26000/49000 loss: 0.3199267280543519\n",
      "28000/49000 loss: 0.19931703662171893\n",
      "30000/49000 loss: 0.4176013629950658\n",
      "32000/49000 loss: 0.2601685430167786\n",
      "34000/49000 loss: 0.20135228704411146\n",
      "36000/49000 loss: 0.31624357072299225\n",
      "38000/49000 loss: 0.27839914951105804\n",
      "40000/49000 loss: 0.3455855408380909\n",
      "42000/49000 loss: 0.30848496962669114\n",
      "44000/49000 loss: 0.22923288375356313\n",
      "46000/49000 loss: 0.2734218278156553\n",
      "48000/49000 loss: 0.34647314758161046\n",
      "epoch 50: valid acc = 0.892, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.8999387755102041\n",
      "test acc: 0.892\n",
      "test acc: 0.8709\n",
      "number of batches for training: 245\n",
      "epoch 1: valid acc = 0.745, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.817, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.833, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.842, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.845, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.859, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.856, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.86, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.873, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.87, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.872, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.873, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.88, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.879, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.88, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.878, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.878, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.884, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.884, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.889, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.884, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.891, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.89, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.887, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.891, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.889, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.888, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.894, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.888, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.894, new learning rate = 0.00010731938197146858\n",
      "epoch 31: valid acc = 0.891, new learning rate = 0.00010195341287289515\n",
      "epoch 32: valid acc = 0.891, new learning rate = 9.685574222925039e-05\n",
      "epoch 33: valid acc = 0.893, new learning rate = 9.201295511778786e-05\n",
      "epoch 34: valid acc = 0.891, new learning rate = 8.741230736189846e-05\n",
      "epoch 35: valid acc = 0.886, new learning rate = 8.304169199380353e-05\n",
      "epoch 36: valid acc = 0.889, new learning rate = 7.888960739411335e-05\n",
      "epoch 37: valid acc = 0.889, new learning rate = 7.494512702440768e-05\n",
      "epoch 38: valid acc = 0.894, new learning rate = 7.119787067318729e-05\n",
      "epoch 39: valid acc = 0.891, new learning rate = 6.763797713952792e-05\n",
      "epoch 40: valid acc = 0.887, new learning rate = 6.425607828255152e-05\n",
      "epoch 41: valid acc = 0.89, new learning rate = 6.104327436842394e-05\n",
      "epoch 42: valid acc = 0.89, new learning rate = 5.799111065000274e-05\n",
      "epoch 43: valid acc = 0.89, new learning rate = 5.5091555117502596e-05\n",
      "epoch 44: valid acc = 0.895, new learning rate = 5.2336977361627463e-05\n",
      "epoch 45: valid acc = 0.888, new learning rate = 4.972012849354609e-05\n",
      "epoch 46: valid acc = 0.895, new learning rate = 4.723412206886878e-05\n",
      "epoch 47: valid acc = 0.891, new learning rate = 4.487241596542534e-05\n",
      "epoch 48: valid acc = 0.893, new learning rate = 4.262879516715407e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 49: valid acc = 0.886, new learning rate = 4.049735540879637e-05\n",
      "epoch 50: valid acc = 0.89, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.9001428571428571\n",
      "test acc: 0.89\n",
      "test acc: 0.8711\n",
      "number of batches for training: 245\n",
      "2000/49000 loss: 2.6953984287113757\n",
      "4000/49000 loss: 2.5773140653104596\n",
      "6000/49000 loss: 2.587784932586316\n",
      "8000/49000 loss: 2.437755662595141\n",
      "10000/49000 loss: 2.250638895980077\n",
      "12000/49000 loss: 2.168353966300763\n",
      "14000/49000 loss: 1.8860466333629597\n",
      "16000/49000 loss: 1.667783681314126\n",
      "18000/49000 loss: 1.4759731145511352\n",
      "20000/49000 loss: 1.3006869848643197\n",
      "22000/49000 loss: 1.1374660369207286\n",
      "24000/49000 loss: 1.0432756523779052\n",
      "26000/49000 loss: 1.1973449191256338\n",
      "28000/49000 loss: 0.9997457531851969\n",
      "30000/49000 loss: 0.8760878754473579\n",
      "32000/49000 loss: 0.8895756329169958\n",
      "34000/49000 loss: 0.980502224049587\n",
      "36000/49000 loss: 0.929201283093445\n",
      "38000/49000 loss: 0.8021865007605402\n",
      "40000/49000 loss: 0.9152402607804021\n",
      "42000/49000 loss: 0.8035956447592497\n",
      "44000/49000 loss: 0.7229043587843985\n",
      "46000/49000 loss: 0.6695707087701687\n",
      "48000/49000 loss: 0.6902894667368906\n",
      "epoch 1: valid acc = 0.743, new learning rate = 0.000475\n",
      "2000/49000 loss: 0.6694315865132514\n",
      "4000/49000 loss: 0.7195350203428379\n",
      "6000/49000 loss: 0.6388916888670585\n",
      "8000/49000 loss: 0.6364962969447989\n",
      "10000/49000 loss: 0.7187740698147794\n",
      "12000/49000 loss: 0.6260778062921168\n",
      "14000/49000 loss: 0.7253000170063237\n",
      "16000/49000 loss: 0.684667305161828\n",
      "18000/49000 loss: 0.6167381177158259\n",
      "20000/49000 loss: 0.6364288699382873\n",
      "22000/49000 loss: 0.6416258756195038\n",
      "24000/49000 loss: 0.45807254522164675\n",
      "26000/49000 loss: 0.6505285015835808\n",
      "28000/49000 loss: 0.6638623124735746\n",
      "30000/49000 loss: 0.5324820253901763\n",
      "32000/49000 loss: 0.5531035513787833\n",
      "34000/49000 loss: 0.6052674198356135\n",
      "36000/49000 loss: 0.5084023693485928\n",
      "38000/49000 loss: 0.536120312412296\n",
      "40000/49000 loss: 0.5222415201232185\n",
      "42000/49000 loss: 0.5105801484965268\n",
      "44000/49000 loss: 0.4801429719368331\n",
      "46000/49000 loss: 0.5342290399544953\n",
      "48000/49000 loss: 0.503649648176719\n",
      "epoch 2: valid acc = 0.816, new learning rate = 0.00045125\n",
      "2000/49000 loss: 0.5055282456064459\n",
      "4000/49000 loss: 0.4706559131035355\n",
      "6000/49000 loss: 0.5820108872110512\n",
      "8000/49000 loss: 0.5662452797533696\n",
      "10000/49000 loss: 0.39099181072713035\n",
      "12000/49000 loss: 0.4842404225268331\n",
      "14000/49000 loss: 0.4942584414074021\n",
      "16000/49000 loss: 0.461595273621163\n",
      "18000/49000 loss: 0.5147152459276022\n",
      "20000/49000 loss: 0.4647374843615018\n",
      "22000/49000 loss: 0.6265546904094529\n",
      "24000/49000 loss: 0.5471013383369152\n",
      "26000/49000 loss: 0.48959625453683964\n",
      "28000/49000 loss: 0.5095948572982241\n",
      "30000/49000 loss: 0.46937218799850466\n",
      "32000/49000 loss: 0.5484502385720914\n",
      "34000/49000 loss: 0.4546269925089929\n",
      "36000/49000 loss: 0.45171123025069176\n",
      "38000/49000 loss: 0.4207857103413598\n",
      "40000/49000 loss: 0.4224278941522734\n",
      "42000/49000 loss: 0.41117190212844185\n",
      "44000/49000 loss: 0.43165533461060684\n",
      "46000/49000 loss: 0.5718864332973081\n",
      "48000/49000 loss: 0.41814664186145395\n",
      "epoch 3: valid acc = 0.838, new learning rate = 0.0004286875\n",
      "2000/49000 loss: 0.37124167427318905\n",
      "4000/49000 loss: 0.5626888038012682\n",
      "6000/49000 loss: 0.44779164381781866\n",
      "8000/49000 loss: 0.40833654224735694\n",
      "10000/49000 loss: 0.48652669443698765\n",
      "12000/49000 loss: 0.5311533102906131\n",
      "14000/49000 loss: 0.3959447825434294\n",
      "16000/49000 loss: 0.41032616407238387\n",
      "18000/49000 loss: 0.4616141491334913\n",
      "20000/49000 loss: 0.33527610289871584\n",
      "22000/49000 loss: 0.44396928557081333\n",
      "24000/49000 loss: 0.3619721463116478\n",
      "26000/49000 loss: 0.4249792429949648\n",
      "28000/49000 loss: 0.3798554433689091\n",
      "30000/49000 loss: 0.5405926597364452\n",
      "32000/49000 loss: 0.4236994992236002\n",
      "34000/49000 loss: 0.44624418495243945\n",
      "36000/49000 loss: 0.4318683675399073\n",
      "38000/49000 loss: 0.42264126108955236\n",
      "40000/49000 loss: 0.4231742181159423\n",
      "42000/49000 loss: 0.40274087043990275\n",
      "44000/49000 loss: 0.4681909375521563\n",
      "46000/49000 loss: 0.453291259952554\n",
      "48000/49000 loss: 0.39245392029021714\n",
      "epoch 4: valid acc = 0.842, new learning rate = 0.00040725312499999993\n",
      "2000/49000 loss: 0.34245668217436087\n",
      "4000/49000 loss: 0.4407141365505613\n",
      "6000/49000 loss: 0.4439977595525389\n",
      "8000/49000 loss: 0.3262256968662249\n",
      "10000/49000 loss: 0.397927146017957\n",
      "12000/49000 loss: 0.41626348093326015\n",
      "14000/49000 loss: 0.39066748782902666\n",
      "16000/49000 loss: 0.34209982104465925\n",
      "18000/49000 loss: 0.39807464964182143\n",
      "20000/49000 loss: 0.3937208570291671\n",
      "22000/49000 loss: 0.47544512925086674\n",
      "24000/49000 loss: 0.49101212600240474\n",
      "26000/49000 loss: 0.4399922482684084\n",
      "28000/49000 loss: 0.2974837968991508\n",
      "30000/49000 loss: 0.3950260907491618\n",
      "32000/49000 loss: 0.38748611932160887\n",
      "34000/49000 loss: 0.3845189570119856\n",
      "36000/49000 loss: 0.5462456271891546\n",
      "38000/49000 loss: 0.3831805637893964\n",
      "40000/49000 loss: 0.5366193672036162\n",
      "42000/49000 loss: 0.43005780582891523\n",
      "44000/49000 loss: 0.44650713186739893\n",
      "46000/49000 loss: 0.42716625221544186\n",
      "48000/49000 loss: 0.3942664545999152\n",
      "epoch 5: valid acc = 0.856, new learning rate = 0.0003868904687499999\n",
      "2000/49000 loss: 0.5036271991127729\n",
      "4000/49000 loss: 0.42095943937974\n",
      "6000/49000 loss: 0.42721942133485563\n",
      "8000/49000 loss: 0.4588964427209274\n",
      "10000/49000 loss: 0.41997991604271995\n",
      "12000/49000 loss: 0.41808197527734187\n",
      "14000/49000 loss: 0.46106597847703334\n",
      "16000/49000 loss: 0.4363053958393925\n",
      "18000/49000 loss: 0.4562537530557315\n",
      "20000/49000 loss: 0.3131335694058698\n",
      "22000/49000 loss: 0.42609679126836875\n",
      "24000/49000 loss: 0.41800002690410404\n",
      "26000/49000 loss: 0.3703299094389549\n",
      "28000/49000 loss: 0.3742759031829764\n",
      "30000/49000 loss: 0.4675793572928083\n",
      "32000/49000 loss: 0.38094233736120947\n",
      "34000/49000 loss: 0.3933783637132134\n",
      "36000/49000 loss: 0.3697535032505442\n",
      "38000/49000 loss: 0.3636925535569934\n",
      "40000/49000 loss: 0.37620624988035767\n",
      "42000/49000 loss: 0.4177864508451483\n",
      "44000/49000 loss: 0.3500051553534004\n",
      "46000/49000 loss: 0.4409455491376756\n",
      "48000/49000 loss: 0.42533768722476667\n",
      "epoch 6: valid acc = 0.858, new learning rate = 0.0003675459453124999\n",
      "2000/49000 loss: 0.5270154558594508\n",
      "4000/49000 loss: 0.4357771625817924\n",
      "6000/49000 loss: 0.4739960646348836\n",
      "8000/49000 loss: 0.4006567425181634\n",
      "10000/49000 loss: 0.4150983109695938\n",
      "12000/49000 loss: 0.4457621240064843\n",
      "14000/49000 loss: 0.425083334571569\n",
      "16000/49000 loss: 0.47555352927834244\n",
      "18000/49000 loss: 0.3685802739534357\n",
      "20000/49000 loss: 0.4552377441883379\n",
      "22000/49000 loss: 0.3480517198366185\n",
      "24000/49000 loss: 0.35983680654857675\n",
      "26000/49000 loss: 0.3137583427165642\n",
      "28000/49000 loss: 0.4762685136503641\n",
      "30000/49000 loss: 0.3199855364226138\n",
      "32000/49000 loss: 0.42475375270745525\n",
      "34000/49000 loss: 0.34547458042248763\n",
      "36000/49000 loss: 0.43394776099329124\n",
      "38000/49000 loss: 0.5268092568156532\n",
      "40000/49000 loss: 0.35393010512771444\n",
      "42000/49000 loss: 0.48702680866049836\n",
      "44000/49000 loss: 0.4200082677811304\n",
      "46000/49000 loss: 0.32375970791618996\n",
      "48000/49000 loss: 0.38350654372864457\n",
      "epoch 7: valid acc = 0.858, new learning rate = 0.00034916864804687486\n",
      "2000/49000 loss: 0.4414814290578673\n",
      "4000/49000 loss: 0.3124913015691976\n",
      "6000/49000 loss: 0.41487110699982405\n",
      "8000/49000 loss: 0.3464812622639756\n",
      "10000/49000 loss: 0.35514601813475927\n",
      "12000/49000 loss: 0.34163597247587724\n",
      "14000/49000 loss: 0.4300203519038981\n",
      "16000/49000 loss: 0.35533227259020167\n",
      "18000/49000 loss: 0.35594340105307504\n",
      "20000/49000 loss: 0.35040859189850837\n",
      "22000/49000 loss: 0.36195751427486794\n",
      "24000/49000 loss: 0.4254008485667541\n",
      "26000/49000 loss: 0.3896393626170997\n",
      "28000/49000 loss: 0.3622323500009757\n",
      "30000/49000 loss: 0.43221820643255376\n",
      "32000/49000 loss: 0.3968760973022197\n",
      "34000/49000 loss: 0.3197585702588447\n",
      "36000/49000 loss: 0.38534202662151257\n",
      "38000/49000 loss: 0.3870583264767488\n",
      "40000/49000 loss: 0.37361850777974953\n",
      "42000/49000 loss: 0.3876727190181396\n",
      "44000/49000 loss: 0.4625508067762068\n",
      "46000/49000 loss: 0.3650183766051755\n",
      "48000/49000 loss: 0.3722371827506968\n",
      "epoch 8: valid acc = 0.863, new learning rate = 0.0003317102156445311\n",
      "2000/49000 loss: 0.33000562161871877\n",
      "4000/49000 loss: 0.4194002007122356\n",
      "6000/49000 loss: 0.440304015254091\n",
      "8000/49000 loss: 0.48305632242985164\n",
      "10000/49000 loss: 0.4178938130167874\n",
      "12000/49000 loss: 0.3998347060401409\n",
      "14000/49000 loss: 0.3908461156147264\n",
      "16000/49000 loss: 0.43410347535245997\n",
      "18000/49000 loss: 0.35503442403186675\n",
      "20000/49000 loss: 0.49472260211565355\n",
      "22000/49000 loss: 0.32139310112660385\n",
      "24000/49000 loss: 0.34874920008081667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26000/49000 loss: 0.35463146431631876\n",
      "28000/49000 loss: 0.3243233951313786\n",
      "30000/49000 loss: 0.4270726698107615\n",
      "32000/49000 loss: 0.37166676307393187\n",
      "34000/49000 loss: 0.48382876027503813\n",
      "36000/49000 loss: 0.4082514897156289\n",
      "38000/49000 loss: 0.3458101769502842\n",
      "40000/49000 loss: 0.2861613264358404\n",
      "42000/49000 loss: 0.37951219717300994\n",
      "44000/49000 loss: 0.5668687328488871\n",
      "46000/49000 loss: 0.32573500291717683\n",
      "48000/49000 loss: 0.2650200497841752\n",
      "epoch 9: valid acc = 0.874, new learning rate = 0.0003151247048623045\n",
      "2000/49000 loss: 0.42734495242623505\n",
      "4000/49000 loss: 0.38843879757981087\n",
      "6000/49000 loss: 0.41712819091721154\n",
      "8000/49000 loss: 0.3494216181731233\n",
      "10000/49000 loss: 0.3734353357626386\n",
      "12000/49000 loss: 0.3771043973192602\n",
      "14000/49000 loss: 0.3853348810229738\n",
      "16000/49000 loss: 0.3897982605623088\n",
      "18000/49000 loss: 0.3206868346639307\n",
      "20000/49000 loss: 0.3446289164928661\n",
      "22000/49000 loss: 0.3885949477538896\n",
      "24000/49000 loss: 0.2753369247691916\n",
      "26000/49000 loss: 0.3756874515629459\n",
      "28000/49000 loss: 0.4437711556933585\n",
      "30000/49000 loss: 0.38169617906615405\n",
      "32000/49000 loss: 0.4662280276976518\n",
      "34000/49000 loss: 0.3626912802227876\n",
      "36000/49000 loss: 0.34659512577819196\n",
      "38000/49000 loss: 0.38178489721644665\n",
      "40000/49000 loss: 0.3174348095028769\n",
      "42000/49000 loss: 0.3450063097158091\n",
      "44000/49000 loss: 0.28115354015119476\n",
      "46000/49000 loss: 0.38275704760056456\n",
      "48000/49000 loss: 0.3873671874230329\n",
      "epoch 10: valid acc = 0.874, new learning rate = 0.00029936846961918924\n",
      "2000/49000 loss: 0.3154970380098587\n",
      "4000/49000 loss: 0.2769726439275911\n",
      "6000/49000 loss: 0.39571937302697235\n",
      "8000/49000 loss: 0.38303078706833515\n",
      "10000/49000 loss: 0.39502825353262255\n",
      "12000/49000 loss: 0.3070815364608707\n",
      "14000/49000 loss: 0.42025565969663037\n",
      "16000/49000 loss: 0.3556812634912681\n",
      "18000/49000 loss: 0.3244290288829626\n",
      "20000/49000 loss: 0.350648263617233\n",
      "22000/49000 loss: 0.3367683207272929\n",
      "24000/49000 loss: 0.36748011526837543\n",
      "26000/49000 loss: 0.2930708614791003\n",
      "28000/49000 loss: 0.3289332340911745\n",
      "30000/49000 loss: 0.38552742054919664\n",
      "32000/49000 loss: 0.35216017008266676\n",
      "34000/49000 loss: 0.4170893802168312\n",
      "36000/49000 loss: 0.39827784201894656\n",
      "38000/49000 loss: 0.3326583183808162\n",
      "40000/49000 loss: 0.30256478213146465\n",
      "42000/49000 loss: 0.36671854185555675\n",
      "44000/49000 loss: 0.3358760731398723\n",
      "46000/49000 loss: 0.37301661063348146\n",
      "48000/49000 loss: 0.29378371895761834\n",
      "epoch 11: valid acc = 0.878, new learning rate = 0.00028440004613822977\n",
      "2000/49000 loss: 0.3004395268858474\n",
      "4000/49000 loss: 0.317464755857529\n",
      "6000/49000 loss: 0.2626844028226882\n",
      "8000/49000 loss: 0.4003714080666584\n",
      "10000/49000 loss: 0.31756340359823854\n",
      "12000/49000 loss: 0.4119312595350265\n",
      "14000/49000 loss: 0.4033458558798141\n",
      "16000/49000 loss: 0.37795809237363653\n",
      "18000/49000 loss: 0.3140528250618436\n",
      "20000/49000 loss: 0.3530727276629205\n",
      "22000/49000 loss: 0.36148419289713163\n",
      "24000/49000 loss: 0.34195679659774314\n",
      "26000/49000 loss: 0.3483539676674448\n",
      "28000/49000 loss: 0.41743437906131464\n",
      "30000/49000 loss: 0.3720880579926794\n",
      "32000/49000 loss: 0.2708233068841796\n",
      "34000/49000 loss: 0.2445820685185631\n",
      "36000/49000 loss: 0.30536574341817657\n",
      "38000/49000 loss: 0.268313620394728\n",
      "40000/49000 loss: 0.3418275169920251\n",
      "42000/49000 loss: 0.3716584618861239\n",
      "44000/49000 loss: 0.3496898538994128\n",
      "46000/49000 loss: 0.3529361378695361\n",
      "48000/49000 loss: 0.31532853017525736\n",
      "epoch 12: valid acc = 0.885, new learning rate = 0.00027018004383131826\n",
      "2000/49000 loss: 0.34882021413404773\n",
      "4000/49000 loss: 0.28750146061265613\n",
      "6000/49000 loss: 0.32395204836006886\n",
      "8000/49000 loss: 0.2981762887837779\n",
      "10000/49000 loss: 0.4354397287926245\n",
      "12000/49000 loss: 0.3227563309449724\n",
      "14000/49000 loss: 0.3200331069982732\n",
      "16000/49000 loss: 0.35304272017494465\n",
      "18000/49000 loss: 0.37134197629159293\n",
      "20000/49000 loss: 0.4229607778834957\n",
      "22000/49000 loss: 0.3425909343341584\n",
      "24000/49000 loss: 0.32979521891289226\n",
      "26000/49000 loss: 0.3705753555919442\n",
      "28000/49000 loss: 0.30972431631403435\n",
      "30000/49000 loss: 0.3556426605971569\n",
      "32000/49000 loss: 0.3520391265074811\n",
      "34000/49000 loss: 0.2985867258282586\n",
      "36000/49000 loss: 0.40443461276618486\n",
      "38000/49000 loss: 0.33773352886799574\n",
      "40000/49000 loss: 0.3792031652943907\n",
      "42000/49000 loss: 0.3315111205511455\n",
      "44000/49000 loss: 0.3732674548466758\n",
      "46000/49000 loss: 0.3813742809364839\n",
      "48000/49000 loss: 0.3918246461670154\n",
      "epoch 13: valid acc = 0.879, new learning rate = 0.00025667104163975234\n",
      "2000/49000 loss: 0.32657729934659946\n",
      "4000/49000 loss: 0.3191796769637461\n",
      "6000/49000 loss: 0.2941751355087132\n",
      "8000/49000 loss: 0.27744999057439723\n",
      "10000/49000 loss: 0.32535015529701017\n",
      "12000/49000 loss: 0.3264635099158947\n",
      "14000/49000 loss: 0.28825996266552023\n",
      "16000/49000 loss: 0.3589259059684001\n",
      "18000/49000 loss: 0.3165635414862307\n",
      "20000/49000 loss: 0.3275995832742185\n",
      "22000/49000 loss: 0.3701483719580581\n",
      "24000/49000 loss: 0.3241572703646563\n",
      "26000/49000 loss: 0.2792201614646996\n",
      "28000/49000 loss: 0.34996135128171174\n",
      "30000/49000 loss: 0.364358986713838\n",
      "32000/49000 loss: 0.32049414134524074\n",
      "34000/49000 loss: 0.32068714727326136\n",
      "36000/49000 loss: 0.3366760312005067\n",
      "38000/49000 loss: 0.3515113756918668\n",
      "40000/49000 loss: 0.37470282589947906\n",
      "42000/49000 loss: 0.27917201644705925\n",
      "44000/49000 loss: 0.4024118651707105\n",
      "46000/49000 loss: 0.303771189338953\n",
      "48000/49000 loss: 0.3120601385311436\n",
      "epoch 14: valid acc = 0.88, new learning rate = 0.00024383748955776472\n",
      "2000/49000 loss: 0.45807642470020515\n",
      "4000/49000 loss: 0.32659825225680356\n",
      "6000/49000 loss: 0.3183332165784282\n",
      "8000/49000 loss: 0.3982975483124107\n",
      "10000/49000 loss: 0.33601585222728\n",
      "12000/49000 loss: 0.381801415749519\n",
      "14000/49000 loss: 0.38350639792559144\n",
      "16000/49000 loss: 0.3708325187821279\n",
      "18000/49000 loss: 0.322345783483727\n",
      "20000/49000 loss: 0.2652339742440929\n",
      "22000/49000 loss: 0.3455640724401973\n",
      "24000/49000 loss: 0.36000846442742496\n",
      "26000/49000 loss: 0.3756373536267492\n",
      "28000/49000 loss: 0.22511747338425572\n",
      "30000/49000 loss: 0.3481322220538384\n",
      "32000/49000 loss: 0.37275582801702906\n",
      "34000/49000 loss: 0.3033943569869435\n",
      "36000/49000 loss: 0.4041295257440856\n",
      "38000/49000 loss: 0.38384291760913736\n",
      "40000/49000 loss: 0.35633853974701024\n",
      "42000/49000 loss: 0.3037198672798494\n",
      "44000/49000 loss: 0.28090000782366753\n",
      "46000/49000 loss: 0.30750472588990974\n",
      "48000/49000 loss: 0.36969845453000555\n",
      "epoch 15: valid acc = 0.88, new learning rate = 0.00023164561507987649\n",
      "2000/49000 loss: 0.393670987313027\n",
      "4000/49000 loss: 0.3094969607440501\n",
      "6000/49000 loss: 0.3199240348283258\n",
      "8000/49000 loss: 0.350229077842674\n",
      "10000/49000 loss: 0.3719289251215774\n",
      "12000/49000 loss: 0.32736766596802896\n",
      "14000/49000 loss: 0.3269079418422951\n",
      "16000/49000 loss: 0.269504390435978\n",
      "18000/49000 loss: 0.2722075516097789\n",
      "20000/49000 loss: 0.26819784175440614\n",
      "22000/49000 loss: 0.237032784286292\n",
      "24000/49000 loss: 0.3624049701052986\n",
      "26000/49000 loss: 0.25991857990285433\n",
      "28000/49000 loss: 0.2775479266778094\n",
      "30000/49000 loss: 0.34026291962327726\n",
      "32000/49000 loss: 0.31016898892126965\n",
      "34000/49000 loss: 0.31770195247797084\n",
      "36000/49000 loss: 0.33412561911106625\n",
      "38000/49000 loss: 0.282400924380723\n",
      "40000/49000 loss: 0.22747052058690762\n",
      "42000/49000 loss: 0.29449278962923087\n",
      "44000/49000 loss: 0.301721475537631\n",
      "46000/49000 loss: 0.3345042005399351\n",
      "48000/49000 loss: 0.31552989250718416\n",
      "epoch 16: valid acc = 0.891, new learning rate = 0.00022006333432588265\n",
      "2000/49000 loss: 0.30278385477271114\n",
      "4000/49000 loss: 0.2914746460788463\n",
      "6000/49000 loss: 0.3226881177234523\n",
      "8000/49000 loss: 0.3377349506744557\n",
      "10000/49000 loss: 0.36658129774370823\n",
      "12000/49000 loss: 0.36347343203376953\n",
      "14000/49000 loss: 0.30948746504491653\n",
      "16000/49000 loss: 0.32257356040206725\n",
      "18000/49000 loss: 0.3469683490322493\n",
      "20000/49000 loss: 0.341764837271494\n",
      "22000/49000 loss: 0.35510001450730644\n",
      "24000/49000 loss: 0.4015554162089137\n",
      "26000/49000 loss: 0.3074897810593881\n",
      "28000/49000 loss: 0.3528466432453642\n",
      "30000/49000 loss: 0.30172743490792536\n",
      "32000/49000 loss: 0.3270702255341786\n",
      "34000/49000 loss: 0.349800417993322\n",
      "36000/49000 loss: 0.38082389137825245\n",
      "38000/49000 loss: 0.2829039154107175\n",
      "40000/49000 loss: 0.390042448616927\n",
      "42000/49000 loss: 0.31708790543758686\n",
      "44000/49000 loss: 0.3036938641348707\n",
      "46000/49000 loss: 0.28478974337935137\n",
      "48000/49000 loss: 0.31346218313682744\n",
      "epoch 17: valid acc = 0.876, new learning rate = 0.00020906016760958852\n",
      "2000/49000 loss: 0.3165020258001552\n",
      "4000/49000 loss: 0.3345291834243353\n",
      "6000/49000 loss: 0.31099015219394455\n",
      "8000/49000 loss: 0.38145585845124436\n",
      "10000/49000 loss: 0.31914066029873356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/49000 loss: 0.343498232403045\n",
      "14000/49000 loss: 0.2882661383007068\n",
      "16000/49000 loss: 0.2964621600926927\n",
      "18000/49000 loss: 0.3116245117144527\n",
      "20000/49000 loss: 0.28931072498955474\n",
      "22000/49000 loss: 0.37503422092059224\n",
      "24000/49000 loss: 0.3849030567894616\n",
      "26000/49000 loss: 0.36153481115771446\n",
      "28000/49000 loss: 0.3336757578304839\n",
      "30000/49000 loss: 0.3643855282775885\n",
      "32000/49000 loss: 0.28732256163920544\n",
      "34000/49000 loss: 0.29544076354361615\n",
      "36000/49000 loss: 0.34526833504206933\n",
      "38000/49000 loss: 0.2985541290967559\n",
      "40000/49000 loss: 0.39987027892036847\n",
      "42000/49000 loss: 0.2838240159588866\n",
      "44000/49000 loss: 0.3152601802820667\n",
      "46000/49000 loss: 0.28005087803007933\n",
      "48000/49000 loss: 0.24915626057280615\n",
      "epoch 18: valid acc = 0.887, new learning rate = 0.00019860715922910907\n",
      "2000/49000 loss: 0.3087714608661004\n",
      "4000/49000 loss: 0.28080716570258085\n",
      "6000/49000 loss: 0.29319551769937613\n",
      "8000/49000 loss: 0.25396286097240345\n",
      "10000/49000 loss: 0.3832883278749464\n",
      "12000/49000 loss: 0.2940690964860076\n",
      "14000/49000 loss: 0.2413512695202036\n",
      "16000/49000 loss: 0.2991544309048755\n",
      "18000/49000 loss: 0.27165576972850997\n",
      "20000/49000 loss: 0.40782779541510567\n",
      "22000/49000 loss: 0.3631584657174169\n",
      "24000/49000 loss: 0.35325612827070535\n",
      "26000/49000 loss: 0.3492888175279699\n",
      "28000/49000 loss: 0.3203829597759654\n",
      "30000/49000 loss: 0.35521585892838836\n",
      "32000/49000 loss: 0.25798219426088403\n",
      "34000/49000 loss: 0.3329998418773622\n",
      "36000/49000 loss: 0.39133336968089244\n",
      "38000/49000 loss: 0.3425233346085414\n",
      "40000/49000 loss: 0.2809476284730026\n",
      "42000/49000 loss: 0.40999849032012725\n",
      "44000/49000 loss: 0.2767592559517633\n",
      "46000/49000 loss: 0.32114750459743846\n",
      "48000/49000 loss: 0.481693043559839\n",
      "epoch 19: valid acc = 0.889, new learning rate = 0.0001886768012676536\n",
      "2000/49000 loss: 0.30287421556441724\n",
      "4000/49000 loss: 0.34771467411417606\n",
      "6000/49000 loss: 0.3010257554664153\n",
      "8000/49000 loss: 0.35356853461245513\n",
      "10000/49000 loss: 0.21801464254372993\n",
      "12000/49000 loss: 0.3747045801971899\n",
      "14000/49000 loss: 0.33664926085242114\n",
      "16000/49000 loss: 0.3139086996677243\n",
      "18000/49000 loss: 0.3617238808954847\n",
      "20000/49000 loss: 0.3993740250180362\n",
      "22000/49000 loss: 0.3138640358081436\n",
      "24000/49000 loss: 0.2593011599516691\n",
      "26000/49000 loss: 0.31108446035537896\n",
      "28000/49000 loss: 0.32412980625701404\n",
      "30000/49000 loss: 0.2427031559739837\n",
      "32000/49000 loss: 0.35121326754012583\n",
      "34000/49000 loss: 0.25946133557461765\n",
      "36000/49000 loss: 0.2929443820411489\n",
      "38000/49000 loss: 0.33957447350792175\n",
      "40000/49000 loss: 0.3180935173876274\n",
      "42000/49000 loss: 0.24289014125371283\n",
      "44000/49000 loss: 0.4582460661025152\n",
      "46000/49000 loss: 0.3047622534571956\n",
      "48000/49000 loss: 0.3080965195369325\n",
      "epoch 20: valid acc = 0.884, new learning rate = 0.0001792429612042709\n",
      "2000/49000 loss: 0.2622873948830354\n",
      "4000/49000 loss: 0.3004238880842675\n",
      "6000/49000 loss: 0.3134861714158716\n",
      "8000/49000 loss: 0.3780228418912017\n",
      "10000/49000 loss: 0.3277807726232362\n",
      "12000/49000 loss: 0.3280148452062848\n",
      "14000/49000 loss: 0.3934718467140304\n",
      "16000/49000 loss: 0.25591556960999323\n",
      "18000/49000 loss: 0.2599671623944605\n",
      "20000/49000 loss: 0.3507755957966883\n",
      "22000/49000 loss: 0.354341740361796\n",
      "24000/49000 loss: 0.3648842370092599\n",
      "26000/49000 loss: 0.33437583210028465\n",
      "28000/49000 loss: 0.3573882486881492\n",
      "30000/49000 loss: 0.34616334583668706\n",
      "32000/49000 loss: 0.2869986250826471\n",
      "34000/49000 loss: 0.2993207551516946\n",
      "36000/49000 loss: 0.28950652253475034\n",
      "38000/49000 loss: 0.30695015399526593\n",
      "40000/49000 loss: 0.31880123926512094\n",
      "42000/49000 loss: 0.25162548121735206\n",
      "44000/49000 loss: 0.2434954836983002\n",
      "46000/49000 loss: 0.31528323877501174\n",
      "48000/49000 loss: 0.2823248990429134\n",
      "epoch 21: valid acc = 0.886, new learning rate = 0.00017028081314405735\n",
      "2000/49000 loss: 0.31079191983082666\n",
      "4000/49000 loss: 0.3002467609178895\n",
      "6000/49000 loss: 0.32252336464109344\n",
      "8000/49000 loss: 0.29070755166544543\n",
      "10000/49000 loss: 0.2903398083222079\n",
      "12000/49000 loss: 0.3148994020697881\n",
      "14000/49000 loss: 0.3007727165076012\n",
      "16000/49000 loss: 0.2946273826609628\n",
      "18000/49000 loss: 0.3296284007537104\n",
      "20000/49000 loss: 0.2603979025410186\n",
      "22000/49000 loss: 0.2621601466824761\n",
      "24000/49000 loss: 0.3739436187955337\n",
      "26000/49000 loss: 0.29637167520831653\n",
      "28000/49000 loss: 0.30955445547383476\n",
      "30000/49000 loss: 0.23504694032049334\n",
      "32000/49000 loss: 0.2947708570457176\n",
      "34000/49000 loss: 0.2809690816397401\n",
      "36000/49000 loss: 0.2703368160266631\n",
      "38000/49000 loss: 0.27471506349938435\n",
      "40000/49000 loss: 0.2745742095716011\n",
      "42000/49000 loss: 0.3510553704739672\n",
      "44000/49000 loss: 0.31883515377334465\n",
      "46000/49000 loss: 0.3448268159667953\n",
      "48000/49000 loss: 0.28396617418771425\n",
      "epoch 22: valid acc = 0.891, new learning rate = 0.00016176677248685447\n",
      "2000/49000 loss: 0.33834903934203137\n",
      "4000/49000 loss: 0.31103875722729707\n",
      "6000/49000 loss: 0.29369522978379853\n",
      "8000/49000 loss: 0.30187361617182595\n",
      "10000/49000 loss: 0.37281524464420435\n",
      "12000/49000 loss: 0.4276385451093112\n",
      "14000/49000 loss: 0.28607130113486423\n",
      "16000/49000 loss: 0.21109934967084332\n",
      "18000/49000 loss: 0.38616760463033695\n",
      "20000/49000 loss: 0.32352483235343893\n",
      "22000/49000 loss: 0.27431715745351193\n",
      "24000/49000 loss: 0.2744900554174136\n",
      "26000/49000 loss: 0.25759998547925117\n",
      "28000/49000 loss: 0.23754989514168598\n",
      "30000/49000 loss: 0.24389942529972522\n",
      "32000/49000 loss: 0.2852560818236614\n",
      "34000/49000 loss: 0.280181804868425\n",
      "36000/49000 loss: 0.33150252529703117\n",
      "38000/49000 loss: 0.3536901654870383\n",
      "40000/49000 loss: 0.34074976416911656\n",
      "42000/49000 loss: 0.3195789393208708\n",
      "44000/49000 loss: 0.273823256539593\n",
      "46000/49000 loss: 0.3443024101029497\n",
      "48000/49000 loss: 0.26220196565405535\n",
      "epoch 23: valid acc = 0.885, new learning rate = 0.00015367843386251173\n",
      "2000/49000 loss: 0.28421015817825485\n",
      "4000/49000 loss: 0.3351723898680175\n",
      "6000/49000 loss: 0.33134867800686\n",
      "8000/49000 loss: 0.3118185117829353\n",
      "10000/49000 loss: 0.24276398759271642\n",
      "12000/49000 loss: 0.31509416564455506\n",
      "14000/49000 loss: 0.3115676927369344\n",
      "16000/49000 loss: 0.3276879571285118\n",
      "18000/49000 loss: 0.3591957851288532\n",
      "20000/49000 loss: 0.279282126021501\n",
      "22000/49000 loss: 0.3513115247293472\n",
      "24000/49000 loss: 0.31584697169708587\n",
      "26000/49000 loss: 0.2988949038158219\n",
      "28000/49000 loss: 0.31285897738342\n",
      "30000/49000 loss: 0.410949685429268\n",
      "32000/49000 loss: 0.23509666620759034\n",
      "34000/49000 loss: 0.39595709406048263\n",
      "36000/49000 loss: 0.360213859795563\n",
      "38000/49000 loss: 0.24906134109170822\n",
      "40000/49000 loss: 0.3013165332705249\n",
      "42000/49000 loss: 0.402746462109372\n",
      "44000/49000 loss: 0.2301141323006862\n",
      "46000/49000 loss: 0.32961317706250215\n",
      "48000/49000 loss: 0.33485819298242897\n",
      "epoch 24: valid acc = 0.88, new learning rate = 0.00014599451216938612\n",
      "2000/49000 loss: 0.3350291339926384\n",
      "4000/49000 loss: 0.266455153154461\n",
      "6000/49000 loss: 0.3599649300793619\n",
      "8000/49000 loss: 0.3580474995265087\n",
      "10000/49000 loss: 0.28288414536203743\n",
      "12000/49000 loss: 0.3641725600947007\n",
      "14000/49000 loss: 0.29446728955750356\n",
      "16000/49000 loss: 0.30456194111491847\n",
      "18000/49000 loss: 0.2738037060742881\n",
      "20000/49000 loss: 0.2732703242512041\n",
      "22000/49000 loss: 0.24319179288398166\n",
      "24000/49000 loss: 0.27599796036828955\n",
      "26000/49000 loss: 0.27698752240039154\n",
      "28000/49000 loss: 0.27844074300339056\n",
      "30000/49000 loss: 0.21645531360145043\n",
      "32000/49000 loss: 0.28919491020025434\n",
      "34000/49000 loss: 0.28480518425610296\n",
      "36000/49000 loss: 0.35773281048912586\n",
      "38000/49000 loss: 0.29388882851694464\n",
      "40000/49000 loss: 0.2842340830037292\n",
      "42000/49000 loss: 0.29106871242915533\n",
      "44000/49000 loss: 0.252783649013534\n",
      "46000/49000 loss: 0.324584781875219\n",
      "48000/49000 loss: 0.34859112402926673\n",
      "epoch 25: valid acc = 0.885, new learning rate = 0.00013869478656091682\n",
      "2000/49000 loss: 0.37988164810183733\n",
      "4000/49000 loss: 0.24233684545804832\n",
      "6000/49000 loss: 0.29582356063570275\n",
      "8000/49000 loss: 0.2994396662363665\n",
      "10000/49000 loss: 0.31970480970746706\n",
      "12000/49000 loss: 0.2193504207159412\n",
      "14000/49000 loss: 0.3879605067410657\n",
      "16000/49000 loss: 0.26704983910011365\n",
      "18000/49000 loss: 0.3104584029775874\n",
      "20000/49000 loss: 0.278946099971702\n",
      "22000/49000 loss: 0.26115016489126625\n",
      "24000/49000 loss: 0.34471908120298705\n",
      "26000/49000 loss: 0.31194553111791784\n",
      "28000/49000 loss: 0.2704537361753435\n",
      "30000/49000 loss: 0.2711639272960915\n",
      "32000/49000 loss: 0.2829261258838846\n",
      "34000/49000 loss: 0.29580047560142114\n",
      "36000/49000 loss: 0.2829814193898252\n",
      "38000/49000 loss: 0.39730319957890986\n",
      "40000/49000 loss: 0.27845828631009506\n",
      "42000/49000 loss: 0.2820520877782119\n",
      "44000/49000 loss: 0.2810779219854649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46000/49000 loss: 0.30665754765016556\n",
      "48000/49000 loss: 0.33553287128208986\n",
      "epoch 26: valid acc = 0.886, new learning rate = 0.00013176004723287096\n",
      "2000/49000 loss: 0.2521478417251247\n",
      "4000/49000 loss: 0.3040706850339904\n",
      "6000/49000 loss: 0.27810623801017254\n",
      "8000/49000 loss: 0.2806341418732923\n",
      "10000/49000 loss: 0.30380324910903544\n",
      "12000/49000 loss: 0.302318798376293\n",
      "14000/49000 loss: 0.26533402982240195\n",
      "16000/49000 loss: 0.3115218361586827\n",
      "18000/49000 loss: 0.2977883765054586\n",
      "20000/49000 loss: 0.2722054897544241\n",
      "22000/49000 loss: 0.264567021325973\n",
      "24000/49000 loss: 0.32762684898225863\n",
      "26000/49000 loss: 0.27323447237561266\n",
      "28000/49000 loss: 0.3252446525139019\n",
      "30000/49000 loss: 0.27741255944951154\n",
      "32000/49000 loss: 0.25618974900114094\n",
      "34000/49000 loss: 0.2001979475534949\n",
      "36000/49000 loss: 0.2756909675149666\n",
      "38000/49000 loss: 0.3329983206695872\n",
      "40000/49000 loss: 0.34236893749493613\n",
      "42000/49000 loss: 0.33724665671312076\n",
      "44000/49000 loss: 0.33891002741798465\n",
      "46000/49000 loss: 0.2976077916719023\n",
      "48000/49000 loss: 0.23593964071224044\n",
      "epoch 27: valid acc = 0.89, new learning rate = 0.0001251720448712274\n",
      "2000/49000 loss: 0.3032562989076499\n",
      "4000/49000 loss: 0.3595165075757369\n",
      "6000/49000 loss: 0.269452229883652\n",
      "8000/49000 loss: 0.2539906263684593\n",
      "10000/49000 loss: 0.3200814251502532\n",
      "12000/49000 loss: 0.27668118623285204\n",
      "14000/49000 loss: 0.34305895837814987\n",
      "16000/49000 loss: 0.28664776084596194\n",
      "18000/49000 loss: 0.3079889948818818\n",
      "20000/49000 loss: 0.3937801188780047\n",
      "22000/49000 loss: 0.3430005018641562\n",
      "24000/49000 loss: 0.27987087193895344\n",
      "26000/49000 loss: 0.3684298187310356\n",
      "28000/49000 loss: 0.2690478544830984\n",
      "30000/49000 loss: 0.2763598310996516\n",
      "32000/49000 loss: 0.2822684447174839\n",
      "34000/49000 loss: 0.3982392299574735\n",
      "36000/49000 loss: 0.3417494680212954\n",
      "38000/49000 loss: 0.28266890771305553\n",
      "40000/49000 loss: 0.3301672907786202\n",
      "42000/49000 loss: 0.34700504435095186\n",
      "44000/49000 loss: 0.2798629891704031\n",
      "46000/49000 loss: 0.2518615954287779\n",
      "48000/49000 loss: 0.2551950708286295\n",
      "epoch 28: valid acc = 0.888, new learning rate = 0.00011891344262766602\n",
      "2000/49000 loss: 0.29060513100054536\n",
      "4000/49000 loss: 0.419277417130931\n",
      "6000/49000 loss: 0.3823649369070446\n",
      "8000/49000 loss: 0.2791020173924889\n",
      "10000/49000 loss: 0.2972482983136174\n",
      "12000/49000 loss: 0.3535287929006413\n",
      "14000/49000 loss: 0.25263650199300214\n",
      "16000/49000 loss: 0.35799097829897253\n",
      "18000/49000 loss: 0.24009937565677617\n",
      "20000/49000 loss: 0.2414147397526281\n",
      "22000/49000 loss: 0.3504715225925472\n",
      "24000/49000 loss: 0.2532770120990846\n",
      "26000/49000 loss: 0.34149553557361556\n",
      "28000/49000 loss: 0.2766137026188285\n",
      "30000/49000 loss: 0.35597362973319596\n",
      "32000/49000 loss: 0.35331114892420784\n",
      "34000/49000 loss: 0.3146693695582525\n",
      "36000/49000 loss: 0.2852248628417754\n",
      "38000/49000 loss: 0.303125440853405\n",
      "40000/49000 loss: 0.2230322799700838\n",
      "42000/49000 loss: 0.28037049028708333\n",
      "44000/49000 loss: 0.329066157918144\n",
      "46000/49000 loss: 0.3038176616382068\n",
      "48000/49000 loss: 0.23914458262943922\n",
      "epoch 29: valid acc = 0.883, new learning rate = 0.00011296777049628272\n",
      "2000/49000 loss: 0.281798104018645\n",
      "4000/49000 loss: 0.27230296854817015\n",
      "6000/49000 loss: 0.3086085807984642\n",
      "8000/49000 loss: 0.35153236883515154\n",
      "10000/49000 loss: 0.2993329535520574\n",
      "12000/49000 loss: 0.3114951367178099\n",
      "14000/49000 loss: 0.27168038073024664\n",
      "16000/49000 loss: 0.31387367324001125\n",
      "18000/49000 loss: 0.21617391604520494\n",
      "20000/49000 loss: 0.20342040817117246\n",
      "22000/49000 loss: 0.28767968560072144\n",
      "24000/49000 loss: 0.28912108308580625\n",
      "26000/49000 loss: 0.24591616511845518\n",
      "28000/49000 loss: 0.35434484460189825\n",
      "30000/49000 loss: 0.2924394225197321\n",
      "32000/49000 loss: 0.2332546229028575\n",
      "34000/49000 loss: 0.29422619566809527\n",
      "36000/49000 loss: 0.31100383425511113\n",
      "38000/49000 loss: 0.26765905512822896\n",
      "40000/49000 loss: 0.3206079030653799\n",
      "42000/49000 loss: 0.29206195321948836\n",
      "44000/49000 loss: 0.2463683696786591\n",
      "46000/49000 loss: 0.1974974743233108\n",
      "48000/49000 loss: 0.28069737410394674\n",
      "epoch 30: valid acc = 0.882, new learning rate = 0.00010731938197146858\n",
      "2000/49000 loss: 0.26118820111638086\n",
      "4000/49000 loss: 0.2520879530331174\n",
      "6000/49000 loss: 0.3086883151711199\n",
      "8000/49000 loss: 0.28244288843531973\n",
      "10000/49000 loss: 0.3330263343827401\n",
      "12000/49000 loss: 0.2212008553230814\n",
      "14000/49000 loss: 0.2985451654129664\n",
      "16000/49000 loss: 0.32716320080294914\n",
      "18000/49000 loss: 0.2314579900693082\n",
      "20000/49000 loss: 0.30327663000131927\n",
      "22000/49000 loss: 0.31083071919161187\n",
      "24000/49000 loss: 0.33480823533943577\n",
      "26000/49000 loss: 0.3259645606160831\n",
      "28000/49000 loss: 0.3246359418146808\n",
      "30000/49000 loss: 0.23809709199712792\n",
      "32000/49000 loss: 0.40220126163136144\n",
      "34000/49000 loss: 0.2296987210730094\n",
      "36000/49000 loss: 0.30150327796484555\n",
      "38000/49000 loss: 0.23980598406552533\n",
      "40000/49000 loss: 0.2202379328530511\n",
      "42000/49000 loss: 0.2894250958618761\n",
      "44000/49000 loss: 0.3791711738896131\n",
      "46000/49000 loss: 0.25868794937933376\n",
      "48000/49000 loss: 0.25487965923897776\n",
      "epoch 31: valid acc = 0.885, new learning rate = 0.00010195341287289515\n",
      "2000/49000 loss: 0.2406806246189622\n",
      "4000/49000 loss: 0.22476876906780544\n",
      "6000/49000 loss: 0.244434918296292\n",
      "8000/49000 loss: 0.2856393847200552\n",
      "10000/49000 loss: 0.2940758789230924\n",
      "12000/49000 loss: 0.25467906897863674\n",
      "14000/49000 loss: 0.2352152743566731\n",
      "16000/49000 loss: 0.3325950111839039\n",
      "18000/49000 loss: 0.3240783220028972\n",
      "20000/49000 loss: 0.3039719556333902\n",
      "22000/49000 loss: 0.2489522657219221\n",
      "24000/49000 loss: 0.36464223209574537\n",
      "26000/49000 loss: 0.35205269700985825\n",
      "28000/49000 loss: 0.2663228029043394\n",
      "30000/49000 loss: 0.2930275932869308\n",
      "32000/49000 loss: 0.2551539265068258\n",
      "34000/49000 loss: 0.3851349211039755\n",
      "36000/49000 loss: 0.30312778710381455\n",
      "38000/49000 loss: 0.2463674030605742\n",
      "40000/49000 loss: 0.3140706046841993\n",
      "42000/49000 loss: 0.32883488193699056\n",
      "44000/49000 loss: 0.25095511741895005\n",
      "46000/49000 loss: 0.28058960784587156\n",
      "48000/49000 loss: 0.2585357886779402\n",
      "epoch 32: valid acc = 0.887, new learning rate = 9.685574222925039e-05\n",
      "2000/49000 loss: 0.3816549234352239\n",
      "4000/49000 loss: 0.26738813976380343\n",
      "6000/49000 loss: 0.2860786062292539\n",
      "8000/49000 loss: 0.28336865261209937\n",
      "10000/49000 loss: 0.3327632328964913\n",
      "12000/49000 loss: 0.28425520237991614\n",
      "14000/49000 loss: 0.38664665041958074\n",
      "16000/49000 loss: 0.4057064776578516\n",
      "18000/49000 loss: 0.23372582274836637\n",
      "20000/49000 loss: 0.33569109193879915\n",
      "22000/49000 loss: 0.2751326579551636\n",
      "24000/49000 loss: 0.2821056003804495\n",
      "26000/49000 loss: 0.2625519085560918\n",
      "28000/49000 loss: 0.3010338013301399\n",
      "30000/49000 loss: 0.22812270370907975\n",
      "32000/49000 loss: 0.305375635474665\n",
      "34000/49000 loss: 0.2858499232067694\n",
      "36000/49000 loss: 0.3108248665818881\n",
      "38000/49000 loss: 0.33855363001271177\n",
      "40000/49000 loss: 0.24099986317906064\n",
      "42000/49000 loss: 0.286108307392015\n",
      "44000/49000 loss: 0.22757063715263737\n",
      "46000/49000 loss: 0.27743171145285284\n",
      "48000/49000 loss: 0.2915888970432922\n",
      "epoch 33: valid acc = 0.884, new learning rate = 9.201295511778786e-05\n",
      "2000/49000 loss: 0.21249156681314407\n",
      "4000/49000 loss: 0.29735470892713256\n",
      "6000/49000 loss: 0.35181184648088876\n",
      "8000/49000 loss: 0.3070011531051675\n",
      "10000/49000 loss: 0.32963439319661514\n",
      "12000/49000 loss: 0.4478526493202145\n",
      "14000/49000 loss: 0.343588517257051\n",
      "16000/49000 loss: 0.3119094990472308\n",
      "18000/49000 loss: 0.23748132992405552\n",
      "20000/49000 loss: 0.3185747557005637\n",
      "22000/49000 loss: 0.33149968539387137\n",
      "24000/49000 loss: 0.2275476204193775\n",
      "26000/49000 loss: 0.2559016868497585\n",
      "28000/49000 loss: 0.33351204525922507\n",
      "30000/49000 loss: 0.2827316906790937\n",
      "32000/49000 loss: 0.3536671465856705\n",
      "34000/49000 loss: 0.3228084660486367\n",
      "36000/49000 loss: 0.26357320096426357\n",
      "38000/49000 loss: 0.24594352048661736\n",
      "40000/49000 loss: 0.18471768253767162\n",
      "42000/49000 loss: 0.2953141320462121\n",
      "44000/49000 loss: 0.3379979900544406\n",
      "46000/49000 loss: 0.2601171057726123\n",
      "48000/49000 loss: 0.28343688278512325\n",
      "epoch 34: valid acc = 0.885, new learning rate = 8.741230736189846e-05\n",
      "2000/49000 loss: 0.3579216076278545\n",
      "4000/49000 loss: 0.3166246255363304\n",
      "6000/49000 loss: 0.2887324127557806\n",
      "8000/49000 loss: 0.2519215671137758\n",
      "10000/49000 loss: 0.3354896096249008\n",
      "12000/49000 loss: 0.359773888739059\n",
      "14000/49000 loss: 0.24267363632755864\n",
      "16000/49000 loss: 0.2385941222651191\n",
      "18000/49000 loss: 0.36066155462164207\n",
      "20000/49000 loss: 0.32628808009437377\n",
      "22000/49000 loss: 0.3029933051465418\n",
      "24000/49000 loss: 0.26630763891591963\n",
      "26000/49000 loss: 0.2386024617620071\n",
      "28000/49000 loss: 0.3244209367330759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000/49000 loss: 0.32562605979208853\n",
      "32000/49000 loss: 0.2788823666922595\n",
      "34000/49000 loss: 0.30719862201204773\n",
      "36000/49000 loss: 0.2996371026529486\n",
      "38000/49000 loss: 0.20489020028511903\n",
      "40000/49000 loss: 0.3921304441502213\n",
      "42000/49000 loss: 0.22342269555051913\n",
      "44000/49000 loss: 0.30413285530713635\n",
      "46000/49000 loss: 0.28183181022336534\n",
      "48000/49000 loss: 0.26078620531562324\n",
      "epoch 35: valid acc = 0.886, new learning rate = 8.304169199380353e-05\n",
      "2000/49000 loss: 0.34356735572428265\n",
      "4000/49000 loss: 0.30994251374078957\n",
      "6000/49000 loss: 0.30921590652673026\n",
      "8000/49000 loss: 0.31638355959989295\n",
      "10000/49000 loss: 0.3295147544416065\n",
      "12000/49000 loss: 0.3337617157560572\n",
      "14000/49000 loss: 0.3013763720057378\n",
      "16000/49000 loss: 0.2649557240184876\n",
      "18000/49000 loss: 0.22752141406780046\n",
      "20000/49000 loss: 0.35162868581218143\n",
      "22000/49000 loss: 0.23802907830309084\n",
      "24000/49000 loss: 0.32269246944975616\n",
      "26000/49000 loss: 0.2673860436641922\n",
      "28000/49000 loss: 0.3555017787635739\n",
      "30000/49000 loss: 0.19727560614210296\n",
      "32000/49000 loss: 0.3764098690087084\n",
      "34000/49000 loss: 0.21260635483633716\n",
      "36000/49000 loss: 0.2751694641582874\n",
      "38000/49000 loss: 0.2742468393953185\n",
      "40000/49000 loss: 0.2845245856730416\n",
      "42000/49000 loss: 0.30777609398993006\n",
      "44000/49000 loss: 0.3076287060523623\n",
      "46000/49000 loss: 0.2428370273657503\n",
      "48000/49000 loss: 0.4133897777858881\n",
      "epoch 36: valid acc = 0.886, new learning rate = 7.888960739411335e-05\n",
      "2000/49000 loss: 0.2571647002749911\n",
      "4000/49000 loss: 0.29807859918903884\n",
      "6000/49000 loss: 0.24788653341263683\n",
      "8000/49000 loss: 0.361038963357839\n",
      "10000/49000 loss: 0.2840737729080313\n",
      "12000/49000 loss: 0.3504026694322539\n",
      "14000/49000 loss: 0.29675677856525795\n",
      "16000/49000 loss: 0.2904797430676404\n",
      "18000/49000 loss: 0.22847608641989622\n",
      "20000/49000 loss: 0.2733674458943099\n",
      "22000/49000 loss: 0.24319025610086695\n",
      "24000/49000 loss: 0.2263746770122843\n",
      "26000/49000 loss: 0.28144016493036206\n",
      "28000/49000 loss: 0.27043679144673616\n",
      "30000/49000 loss: 0.27570894968267434\n",
      "32000/49000 loss: 0.31662641942540154\n",
      "34000/49000 loss: 0.3025512938425695\n",
      "36000/49000 loss: 0.24035746771586072\n",
      "38000/49000 loss: 0.27833736013823185\n",
      "40000/49000 loss: 0.2471648672539575\n",
      "42000/49000 loss: 0.3720894266922125\n",
      "44000/49000 loss: 0.26683463518108747\n",
      "46000/49000 loss: 0.2813922217034184\n",
      "48000/49000 loss: 0.260020042445824\n",
      "epoch 37: valid acc = 0.884, new learning rate = 7.494512702440768e-05\n",
      "2000/49000 loss: 0.19203301687825172\n",
      "4000/49000 loss: 0.18519086150142025\n",
      "6000/49000 loss: 0.2610825098141057\n",
      "8000/49000 loss: 0.2645450412192478\n",
      "10000/49000 loss: 0.19625478687991105\n",
      "12000/49000 loss: 0.40457299992741486\n",
      "14000/49000 loss: 0.22891399742463434\n",
      "16000/49000 loss: 0.2228510771981553\n",
      "18000/49000 loss: 0.2938943569726564\n",
      "20000/49000 loss: 0.26906396793821546\n",
      "22000/49000 loss: 0.31029612892727687\n",
      "24000/49000 loss: 0.38506943140867234\n",
      "26000/49000 loss: 0.2983297738212355\n",
      "28000/49000 loss: 0.27265258636711803\n",
      "30000/49000 loss: 0.2593243903003212\n",
      "32000/49000 loss: 0.3709918216874008\n",
      "34000/49000 loss: 0.2573313630807138\n",
      "36000/49000 loss: 0.3412658455001953\n",
      "38000/49000 loss: 0.27645305532888165\n",
      "40000/49000 loss: 0.24419535958318117\n",
      "42000/49000 loss: 0.26661044241746085\n",
      "44000/49000 loss: 0.26018421070199155\n",
      "46000/49000 loss: 0.21236342641950642\n",
      "48000/49000 loss: 0.28534162016465947\n",
      "epoch 38: valid acc = 0.885, new learning rate = 7.119787067318729e-05\n",
      "2000/49000 loss: 0.31352315297264893\n",
      "4000/49000 loss: 0.25631765628001013\n",
      "6000/49000 loss: 0.2314502855576307\n",
      "8000/49000 loss: 0.374238051009527\n",
      "10000/49000 loss: 0.2644649360390482\n",
      "12000/49000 loss: 0.20113838387449376\n",
      "14000/49000 loss: 0.33250783946418944\n",
      "16000/49000 loss: 0.24207643253780164\n",
      "18000/49000 loss: 0.2584808398784009\n",
      "20000/49000 loss: 0.20348761331950124\n",
      "22000/49000 loss: 0.311303931572737\n",
      "24000/49000 loss: 0.3474743582912968\n",
      "26000/49000 loss: 0.23564547118889353\n",
      "28000/49000 loss: 0.29947884527711716\n",
      "30000/49000 loss: 0.2221121444008815\n",
      "32000/49000 loss: 0.2064550928595297\n",
      "34000/49000 loss: 0.2032253789029348\n",
      "36000/49000 loss: 0.31633016162246097\n",
      "38000/49000 loss: 0.29678071666639483\n",
      "40000/49000 loss: 0.2925748024441314\n",
      "42000/49000 loss: 0.2788469694178573\n",
      "44000/49000 loss: 0.28874507570214925\n",
      "46000/49000 loss: 0.24971049494974984\n",
      "48000/49000 loss: 0.2041199432109991\n",
      "epoch 39: valid acc = 0.881, new learning rate = 6.763797713952792e-05\n",
      "2000/49000 loss: 0.37061106725111354\n",
      "4000/49000 loss: 0.26105901797720804\n",
      "6000/49000 loss: 0.2961274050311576\n",
      "8000/49000 loss: 0.32530112498096597\n",
      "10000/49000 loss: 0.22742258947189015\n",
      "12000/49000 loss: 0.33997733636350125\n",
      "14000/49000 loss: 0.2541162031037413\n",
      "16000/49000 loss: 0.32661898894763103\n",
      "18000/49000 loss: 0.2096399074693578\n",
      "20000/49000 loss: 0.288373187743691\n",
      "22000/49000 loss: 0.3170033675829159\n",
      "24000/49000 loss: 0.2626902100339999\n",
      "26000/49000 loss: 0.3246658073492294\n",
      "28000/49000 loss: 0.2785374659953701\n",
      "30000/49000 loss: 0.24022861733753997\n",
      "32000/49000 loss: 0.21814115350761465\n",
      "34000/49000 loss: 0.2701401107292106\n",
      "36000/49000 loss: 0.26532172490782485\n",
      "38000/49000 loss: 0.29240535535537526\n",
      "40000/49000 loss: 0.26716952903670793\n",
      "42000/49000 loss: 0.25933416068122184\n",
      "44000/49000 loss: 0.3087391508719631\n",
      "46000/49000 loss: 0.2530754934490863\n",
      "48000/49000 loss: 0.2744372291042308\n",
      "epoch 40: valid acc = 0.882, new learning rate = 6.425607828255152e-05\n",
      "2000/49000 loss: 0.2973341044558475\n",
      "4000/49000 loss: 0.41089161081567527\n",
      "6000/49000 loss: 0.33016138767833453\n",
      "8000/49000 loss: 0.33099645692611795\n",
      "10000/49000 loss: 0.21214695881396983\n",
      "12000/49000 loss: 0.2710279075108161\n",
      "14000/49000 loss: 0.3127879299395692\n",
      "16000/49000 loss: 0.31707889501995523\n",
      "18000/49000 loss: 0.2907646121706857\n",
      "20000/49000 loss: 0.2173700955049591\n",
      "22000/49000 loss: 0.32533922940457366\n",
      "24000/49000 loss: 0.29611548685820077\n",
      "26000/49000 loss: 0.323499129737314\n",
      "28000/49000 loss: 0.28599556937461734\n",
      "30000/49000 loss: 0.322698788828784\n",
      "32000/49000 loss: 0.24639407307063488\n",
      "34000/49000 loss: 0.29239745957825936\n",
      "36000/49000 loss: 0.2607506626804337\n",
      "38000/49000 loss: 0.1876654442724083\n",
      "40000/49000 loss: 0.18131030910129847\n",
      "42000/49000 loss: 0.2931161825880144\n",
      "44000/49000 loss: 0.24691215336659428\n",
      "46000/49000 loss: 0.3033430469797074\n",
      "48000/49000 loss: 0.21936008774781063\n",
      "epoch 41: valid acc = 0.887, new learning rate = 6.104327436842394e-05\n",
      "2000/49000 loss: 0.28147825102756396\n",
      "4000/49000 loss: 0.33788207846248036\n",
      "6000/49000 loss: 0.3172013207200953\n",
      "8000/49000 loss: 0.24196385367576903\n",
      "10000/49000 loss: 0.3135234318752904\n",
      "12000/49000 loss: 0.20736865287190626\n",
      "14000/49000 loss: 0.2594723949430213\n",
      "16000/49000 loss: 0.3194719935894549\n",
      "18000/49000 loss: 0.2947834524856179\n",
      "20000/49000 loss: 0.30446272539154934\n",
      "22000/49000 loss: 0.25709556408305\n",
      "24000/49000 loss: 0.28348813590172767\n",
      "26000/49000 loss: 0.32778370987055966\n",
      "28000/49000 loss: 0.15932469697677365\n",
      "30000/49000 loss: 0.22842217389745886\n",
      "32000/49000 loss: 0.3446993940871653\n",
      "34000/49000 loss: 0.3556749625305727\n",
      "36000/49000 loss: 0.3313161137450769\n",
      "38000/49000 loss: 0.2855954926689022\n",
      "40000/49000 loss: 0.30809322317023596\n",
      "42000/49000 loss: 0.2996085246552931\n",
      "44000/49000 loss: 0.19714251319487372\n",
      "46000/49000 loss: 0.35407828071147696\n",
      "48000/49000 loss: 0.18888201487471362\n",
      "epoch 42: valid acc = 0.888, new learning rate = 5.799111065000274e-05\n",
      "2000/49000 loss: 0.26820873248550003\n",
      "4000/49000 loss: 0.29680965974261814\n",
      "6000/49000 loss: 0.27260884695349613\n",
      "8000/49000 loss: 0.2821682178047911\n",
      "10000/49000 loss: 0.16900139623860133\n",
      "12000/49000 loss: 0.2272473993281163\n",
      "14000/49000 loss: 0.27429925453115667\n",
      "16000/49000 loss: 0.2898402323086045\n",
      "18000/49000 loss: 0.3355724129710971\n",
      "20000/49000 loss: 0.22444856303682265\n",
      "22000/49000 loss: 0.32153183008286845\n",
      "24000/49000 loss: 0.46464397552655684\n",
      "26000/49000 loss: 0.23777253432143142\n",
      "28000/49000 loss: 0.3695582656440297\n",
      "30000/49000 loss: 0.253197255813342\n",
      "32000/49000 loss: 0.263303667543536\n",
      "34000/49000 loss: 0.22793208226111353\n",
      "36000/49000 loss: 0.2729096040684161\n",
      "38000/49000 loss: 0.2512061747491858\n",
      "40000/49000 loss: 0.3306966548315364\n",
      "42000/49000 loss: 0.29206441892328977\n",
      "44000/49000 loss: 0.2723572775553526\n",
      "46000/49000 loss: 0.2823583285981683\n",
      "48000/49000 loss: 0.2808683187675634\n",
      "epoch 43: valid acc = 0.886, new learning rate = 5.5091555117502596e-05\n",
      "2000/49000 loss: 0.2274443084853977\n",
      "4000/49000 loss: 0.19441942104013693\n",
      "6000/49000 loss: 0.29409354272001637\n",
      "8000/49000 loss: 0.2193133884381771\n",
      "10000/49000 loss: 0.2588816904961407\n",
      "12000/49000 loss: 0.33312769043021334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14000/49000 loss: 0.2969847123432204\n",
      "16000/49000 loss: 0.2744070610581456\n",
      "18000/49000 loss: 0.33536857024392147\n",
      "20000/49000 loss: 0.25250388258964335\n",
      "22000/49000 loss: 0.3209384036626424\n",
      "24000/49000 loss: 0.31584787170186746\n",
      "26000/49000 loss: 0.22711452270764593\n",
      "28000/49000 loss: 0.3258206578234835\n",
      "30000/49000 loss: 0.2976815865598384\n",
      "32000/49000 loss: 0.2594983045692561\n",
      "34000/49000 loss: 0.3470159888540015\n",
      "36000/49000 loss: 0.2373092170748869\n",
      "38000/49000 loss: 0.3251618945877671\n",
      "40000/49000 loss: 0.27556079328882077\n",
      "42000/49000 loss: 0.20654031405897855\n",
      "44000/49000 loss: 0.2772919115202707\n",
      "46000/49000 loss: 0.295456530931271\n",
      "48000/49000 loss: 0.2775292086111195\n",
      "epoch 44: valid acc = 0.883, new learning rate = 5.2336977361627463e-05\n",
      "2000/49000 loss: 0.31112545570602546\n",
      "4000/49000 loss: 0.2819469660309677\n",
      "6000/49000 loss: 0.3579243353328558\n",
      "8000/49000 loss: 0.28574377941730994\n",
      "10000/49000 loss: 0.27370953984633406\n",
      "12000/49000 loss: 0.36154384328499345\n",
      "14000/49000 loss: 0.37058491957607664\n",
      "16000/49000 loss: 0.33899600458103135\n",
      "18000/49000 loss: 0.24574193335228647\n",
      "20000/49000 loss: 0.28457144915448884\n",
      "22000/49000 loss: 0.22344851974328425\n",
      "24000/49000 loss: 0.23089963286785478\n",
      "26000/49000 loss: 0.2516259390881318\n",
      "28000/49000 loss: 0.3063340388204382\n",
      "30000/49000 loss: 0.22889425647450742\n",
      "32000/49000 loss: 0.30442912281420176\n",
      "34000/49000 loss: 0.3208091611638968\n",
      "36000/49000 loss: 0.2196364495556034\n",
      "38000/49000 loss: 0.2782626593391883\n",
      "40000/49000 loss: 0.26123154069861854\n",
      "42000/49000 loss: 0.2869481917418798\n",
      "44000/49000 loss: 0.2736047817932413\n",
      "46000/49000 loss: 0.25079256709128545\n",
      "48000/49000 loss: 0.26817729507668425\n",
      "epoch 45: valid acc = 0.883, new learning rate = 4.972012849354609e-05\n",
      "2000/49000 loss: 0.24022907065880256\n",
      "4000/49000 loss: 0.30838957570129755\n",
      "6000/49000 loss: 0.21187906046342558\n",
      "8000/49000 loss: 0.3062528341715485\n",
      "10000/49000 loss: 0.2783935770202928\n",
      "12000/49000 loss: 0.3220562693164196\n",
      "14000/49000 loss: 0.2936070227555004\n",
      "16000/49000 loss: 0.22406329474038553\n",
      "18000/49000 loss: 0.2569238249442878\n",
      "20000/49000 loss: 0.2760464514635034\n",
      "22000/49000 loss: 0.3147549193428642\n",
      "24000/49000 loss: 0.3611405876504459\n",
      "26000/49000 loss: 0.2634667073452065\n",
      "28000/49000 loss: 0.4092047671584155\n",
      "30000/49000 loss: 0.2753706752586347\n",
      "32000/49000 loss: 0.3490342177441012\n",
      "34000/49000 loss: 0.33320772146891187\n",
      "36000/49000 loss: 0.299935409186972\n",
      "38000/49000 loss: 0.30615869202418505\n",
      "40000/49000 loss: 0.2805611199807004\n",
      "42000/49000 loss: 0.26763396170347986\n",
      "44000/49000 loss: 0.2891450818473538\n",
      "46000/49000 loss: 0.30401832940670226\n",
      "48000/49000 loss: 0.2501116020457948\n",
      "epoch 46: valid acc = 0.887, new learning rate = 4.723412206886878e-05\n",
      "2000/49000 loss: 0.3117531731860702\n",
      "4000/49000 loss: 0.3324352070292825\n",
      "6000/49000 loss: 0.28053721902932044\n",
      "8000/49000 loss: 0.23284436632220318\n",
      "10000/49000 loss: 0.3186399054878962\n",
      "12000/49000 loss: 0.2762091804125734\n",
      "14000/49000 loss: 0.24299016538278026\n",
      "16000/49000 loss: 0.34478584831592884\n",
      "18000/49000 loss: 0.32406372234788744\n",
      "20000/49000 loss: 0.30125299618366846\n",
      "22000/49000 loss: 0.3365881163530012\n",
      "24000/49000 loss: 0.3426289103989166\n",
      "26000/49000 loss: 0.33143879643340884\n",
      "28000/49000 loss: 0.3001263490825489\n",
      "30000/49000 loss: 0.25251322803652354\n",
      "32000/49000 loss: 0.21416215157081644\n",
      "34000/49000 loss: 0.2393223927528186\n",
      "36000/49000 loss: 0.23372347829983336\n",
      "38000/49000 loss: 0.2408050976241133\n",
      "40000/49000 loss: 0.3073130641786676\n",
      "42000/49000 loss: 0.24035547701088347\n",
      "44000/49000 loss: 0.32749472732213475\n",
      "46000/49000 loss: 0.3033818980440404\n",
      "48000/49000 loss: 0.2916046201610756\n",
      "epoch 47: valid acc = 0.887, new learning rate = 4.487241596542534e-05\n",
      "2000/49000 loss: 0.2584903404439373\n",
      "4000/49000 loss: 0.2576639995587424\n",
      "6000/49000 loss: 0.3318457010273268\n",
      "8000/49000 loss: 0.25445282180431744\n",
      "10000/49000 loss: 0.24932421622538237\n",
      "12000/49000 loss: 0.3592498705377519\n",
      "14000/49000 loss: 0.2677907732650566\n",
      "16000/49000 loss: 0.26534854208019604\n",
      "18000/49000 loss: 0.31669265058116397\n",
      "20000/49000 loss: 0.25382617672839863\n",
      "22000/49000 loss: 0.2325252770815174\n",
      "24000/49000 loss: 0.31835402553685194\n",
      "26000/49000 loss: 0.3569835443175362\n",
      "28000/49000 loss: 0.2898341812724576\n",
      "30000/49000 loss: 0.33404803413698125\n",
      "32000/49000 loss: 0.29088142386380944\n",
      "34000/49000 loss: 0.2735394218630613\n",
      "36000/49000 loss: 0.34971045957785085\n",
      "38000/49000 loss: 0.20411736632955546\n",
      "40000/49000 loss: 0.3377661743288029\n",
      "42000/49000 loss: 0.2751489849268344\n",
      "44000/49000 loss: 0.26636351269035546\n",
      "46000/49000 loss: 0.24630932285175183\n",
      "48000/49000 loss: 0.3637029221405654\n",
      "epoch 48: valid acc = 0.887, new learning rate = 4.262879516715407e-05\n",
      "2000/49000 loss: 0.341169771712804\n",
      "4000/49000 loss: 0.34582961851022564\n",
      "6000/49000 loss: 0.25454814296294137\n",
      "8000/49000 loss: 0.32227916422913366\n",
      "10000/49000 loss: 0.26764389516064135\n",
      "12000/49000 loss: 0.286456302548673\n",
      "14000/49000 loss: 0.31758494997575865\n",
      "16000/49000 loss: 0.2697748222281743\n",
      "18000/49000 loss: 0.22701659030651186\n",
      "20000/49000 loss: 0.3030423798463346\n",
      "22000/49000 loss: 0.30066173802862584\n",
      "24000/49000 loss: 0.3240936838521561\n",
      "26000/49000 loss: 0.38562439434338047\n",
      "28000/49000 loss: 0.35317286628017874\n",
      "30000/49000 loss: 0.30729241273556457\n",
      "32000/49000 loss: 0.2532171745997917\n",
      "34000/49000 loss: 0.2560662115098911\n",
      "36000/49000 loss: 0.2522481367835779\n",
      "38000/49000 loss: 0.27623769743532295\n",
      "40000/49000 loss: 0.2684270729828783\n",
      "42000/49000 loss: 0.28110502078916283\n",
      "44000/49000 loss: 0.1928352385964237\n",
      "46000/49000 loss: 0.2563911306689494\n",
      "48000/49000 loss: 0.2805970828603081\n",
      "epoch 49: valid acc = 0.883, new learning rate = 4.049735540879637e-05\n",
      "2000/49000 loss: 0.3274765863056111\n",
      "4000/49000 loss: 0.3194916053933032\n",
      "6000/49000 loss: 0.23557683248549025\n",
      "8000/49000 loss: 0.2377731310489691\n",
      "10000/49000 loss: 0.27614817530763347\n",
      "12000/49000 loss: 0.3091672332780609\n",
      "14000/49000 loss: 0.22928608860826005\n",
      "16000/49000 loss: 0.35718102691934284\n",
      "18000/49000 loss: 0.2391328432690394\n",
      "20000/49000 loss: 0.274177331431939\n",
      "22000/49000 loss: 0.3352887891963519\n",
      "24000/49000 loss: 0.2020229821782465\n",
      "26000/49000 loss: 0.38039199299248216\n",
      "28000/49000 loss: 0.3278923267220276\n",
      "30000/49000 loss: 0.27375022162001983\n",
      "32000/49000 loss: 0.21434840290193866\n",
      "34000/49000 loss: 0.347549490012975\n",
      "36000/49000 loss: 0.28995993538025183\n",
      "38000/49000 loss: 0.29591987059725877\n",
      "40000/49000 loss: 0.2288316225941148\n",
      "42000/49000 loss: 0.33512300419229407\n",
      "44000/49000 loss: 0.36694384191000373\n",
      "46000/49000 loss: 0.3397168438036084\n",
      "48000/49000 loss: 0.35661382727083\n",
      "epoch 50: valid acc = 0.885, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.9012040816326531\n",
      "test acc: 0.885\n",
      "test acc: 0.8731\n",
      "number of batches for training: 245\n",
      "epoch 1: valid acc = 0.736, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.801, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.84, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.848, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.85, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.86, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.864, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.869, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.874, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.873, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.874, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.872, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.88, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.882, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.876, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.879, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.88, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.879, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.877, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.887, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.879, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.881, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.884, new learning rate = 0.00015367843386251173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 24: valid acc = 0.881, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.889, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.883, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.887, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.886, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.884, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.888, new learning rate = 0.00010731938197146858\n",
      "epoch 31: valid acc = 0.883, new learning rate = 0.00010195341287289515\n",
      "epoch 32: valid acc = 0.889, new learning rate = 9.685574222925039e-05\n",
      "epoch 33: valid acc = 0.885, new learning rate = 9.201295511778786e-05\n",
      "epoch 34: valid acc = 0.887, new learning rate = 8.741230736189846e-05\n",
      "epoch 35: valid acc = 0.884, new learning rate = 8.304169199380353e-05\n",
      "epoch 36: valid acc = 0.883, new learning rate = 7.888960739411335e-05\n",
      "epoch 37: valid acc = 0.884, new learning rate = 7.494512702440768e-05\n",
      "epoch 38: valid acc = 0.886, new learning rate = 7.119787067318729e-05\n",
      "epoch 39: valid acc = 0.889, new learning rate = 6.763797713952792e-05\n",
      "epoch 40: valid acc = 0.888, new learning rate = 6.425607828255152e-05\n",
      "epoch 41: valid acc = 0.885, new learning rate = 6.104327436842394e-05\n",
      "epoch 42: valid acc = 0.886, new learning rate = 5.799111065000274e-05\n",
      "epoch 43: valid acc = 0.882, new learning rate = 5.5091555117502596e-05\n",
      "epoch 44: valid acc = 0.884, new learning rate = 5.2336977361627463e-05\n",
      "epoch 45: valid acc = 0.884, new learning rate = 4.972012849354609e-05\n",
      "epoch 46: valid acc = 0.885, new learning rate = 4.723412206886878e-05\n",
      "epoch 47: valid acc = 0.886, new learning rate = 4.487241596542534e-05\n",
      "epoch 48: valid acc = 0.884, new learning rate = 4.262879516715407e-05\n",
      "epoch 49: valid acc = 0.886, new learning rate = 4.049735540879637e-05\n",
      "epoch 50: valid acc = 0.886, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.9015306122448979\n",
      "test acc: 0.886\n",
      "test acc: 0.8693\n",
      "number of batches for training: 245\n",
      "2000/49000 loss: 2.6814404794587996\n",
      "4000/49000 loss: 2.678915049700126\n",
      "6000/49000 loss: 2.548896696968952\n",
      "8000/49000 loss: 2.369107452397291\n",
      "10000/49000 loss: 2.221066300506253\n",
      "12000/49000 loss: 2.118665630488222\n",
      "14000/49000 loss: 1.8636185857793586\n",
      "16000/49000 loss: 1.8026245102087686\n",
      "18000/49000 loss: 1.6061929903762395\n",
      "20000/49000 loss: 1.243294053987226\n",
      "22000/49000 loss: 1.2094615590119158\n",
      "24000/49000 loss: 1.0840574209143674\n",
      "26000/49000 loss: 1.1750589398990128\n",
      "28000/49000 loss: 1.0467470203677103\n",
      "30000/49000 loss: 1.1862332602432635\n",
      "32000/49000 loss: 1.071312640046395\n",
      "34000/49000 loss: 0.9103805086985446\n",
      "36000/49000 loss: 0.9678990857433272\n",
      "38000/49000 loss: 0.8492300933560506\n",
      "40000/49000 loss: 0.8216039196424477\n",
      "42000/49000 loss: 0.8582923946693202\n",
      "44000/49000 loss: 0.72070919364858\n",
      "46000/49000 loss: 0.7076570985603559\n",
      "48000/49000 loss: 0.714475520918335\n",
      "epoch 1: valid acc = 0.738, new learning rate = 0.000475\n",
      "2000/49000 loss: 0.7919097032910187\n",
      "4000/49000 loss: 0.6773611574426596\n",
      "6000/49000 loss: 0.6102508713550872\n",
      "8000/49000 loss: 0.6204572691473051\n",
      "10000/49000 loss: 0.5776359184640837\n",
      "12000/49000 loss: 0.5896130737232369\n",
      "14000/49000 loss: 0.6099077841226612\n",
      "16000/49000 loss: 0.5994236271829997\n",
      "18000/49000 loss: 0.5747448033123729\n",
      "20000/49000 loss: 0.5970795909622978\n",
      "22000/49000 loss: 0.5860911608132643\n",
      "24000/49000 loss: 0.5771758937954272\n",
      "26000/49000 loss: 0.5174127446760274\n",
      "28000/49000 loss: 0.5884496553414293\n",
      "30000/49000 loss: 0.6319499260803487\n",
      "32000/49000 loss: 0.5253225645878608\n",
      "34000/49000 loss: 0.5065930662663563\n",
      "36000/49000 loss: 0.6534061556534598\n",
      "38000/49000 loss: 0.5124837536336025\n",
      "40000/49000 loss: 0.5490263552596031\n",
      "42000/49000 loss: 0.485531615205157\n",
      "44000/49000 loss: 0.5313271044338634\n",
      "46000/49000 loss: 0.47396531996339414\n",
      "48000/49000 loss: 0.5280814663662008\n",
      "epoch 2: valid acc = 0.804, new learning rate = 0.00045125\n",
      "2000/49000 loss: 0.5659368465686591\n",
      "4000/49000 loss: 0.4754654629094904\n",
      "6000/49000 loss: 0.4287325635628578\n",
      "8000/49000 loss: 0.4570603452175349\n",
      "10000/49000 loss: 0.48373343145863484\n",
      "12000/49000 loss: 0.5744223269934754\n",
      "14000/49000 loss: 0.4695152703805538\n",
      "16000/49000 loss: 0.4926096866540045\n",
      "18000/49000 loss: 0.45206621563459864\n",
      "20000/49000 loss: 0.4676132180559761\n",
      "22000/49000 loss: 0.4658954962334142\n",
      "24000/49000 loss: 0.49037146690078487\n",
      "26000/49000 loss: 0.43708874540408427\n",
      "28000/49000 loss: 0.43547477893931874\n",
      "30000/49000 loss: 0.48743032863513397\n",
      "32000/49000 loss: 0.6399417160694784\n",
      "34000/49000 loss: 0.4997715228091022\n",
      "36000/49000 loss: 0.4647069293123396\n",
      "38000/49000 loss: 0.4013453821457496\n",
      "40000/49000 loss: 0.487200176754412\n",
      "42000/49000 loss: 0.3233102750529727\n",
      "44000/49000 loss: 0.5087466145645875\n",
      "46000/49000 loss: 0.4442772430029449\n",
      "48000/49000 loss: 0.4798534124605678\n",
      "epoch 3: valid acc = 0.843, new learning rate = 0.0004286875\n",
      "2000/49000 loss: 0.4535688529550882\n",
      "4000/49000 loss: 0.48748639279467043\n",
      "6000/49000 loss: 0.3686167623890087\n",
      "8000/49000 loss: 0.48673017131826707\n",
      "10000/49000 loss: 0.4540094964673027\n",
      "12000/49000 loss: 0.5470599521850618\n",
      "14000/49000 loss: 0.42352926665499263\n",
      "16000/49000 loss: 0.3753491252056187\n",
      "18000/49000 loss: 0.39466934266022263\n",
      "20000/49000 loss: 0.6182827255975034\n",
      "22000/49000 loss: 0.5038016518216427\n",
      "24000/49000 loss: 0.34529049819040186\n",
      "26000/49000 loss: 0.36714461664615755\n",
      "28000/49000 loss: 0.3590177116543053\n",
      "30000/49000 loss: 0.3312584156003077\n",
      "32000/49000 loss: 0.39423808705582764\n",
      "34000/49000 loss: 0.5317804993450911\n",
      "36000/49000 loss: 0.36182632502361606\n",
      "38000/49000 loss: 0.48874981424114217\n",
      "40000/49000 loss: 0.49750819202178975\n",
      "42000/49000 loss: 0.4605395535101718\n",
      "44000/49000 loss: 0.4781658673265006\n",
      "46000/49000 loss: 0.3741924798657543\n",
      "48000/49000 loss: 0.41535026708748085\n",
      "epoch 4: valid acc = 0.856, new learning rate = 0.00040725312499999993\n",
      "2000/49000 loss: 0.4751072045254604\n",
      "4000/49000 loss: 0.4237902154223542\n",
      "6000/49000 loss: 0.3817823695296225\n",
      "8000/49000 loss: 0.460741900001238\n",
      "10000/49000 loss: 0.39675878346440235\n",
      "12000/49000 loss: 0.4876450333480736\n",
      "14000/49000 loss: 0.4387490842709704\n",
      "16000/49000 loss: 0.44941900376839794\n",
      "18000/49000 loss: 0.44187675033037055\n",
      "20000/49000 loss: 0.4367969465214111\n",
      "22000/49000 loss: 0.32251069189815035\n",
      "24000/49000 loss: 0.45746887873067804\n",
      "26000/49000 loss: 0.4348050937317284\n",
      "28000/49000 loss: 0.4012427237809097\n",
      "30000/49000 loss: 0.373261915585002\n",
      "32000/49000 loss: 0.4539712886554424\n",
      "34000/49000 loss: 0.3956254067845873\n",
      "36000/49000 loss: 0.3249608510402466\n",
      "38000/49000 loss: 0.514704759228847\n",
      "40000/49000 loss: 0.44733920963513774\n",
      "42000/49000 loss: 0.4646307749680444\n",
      "44000/49000 loss: 0.3661032887066928\n",
      "46000/49000 loss: 0.35214056513410014\n",
      "48000/49000 loss: 0.3577404010090039\n",
      "epoch 5: valid acc = 0.85, new learning rate = 0.0003868904687499999\n",
      "2000/49000 loss: 0.44733577778136513\n",
      "4000/49000 loss: 0.41563353588909524\n",
      "6000/49000 loss: 0.5169438607525259\n",
      "8000/49000 loss: 0.4171523870710446\n",
      "10000/49000 loss: 0.34583299470949186\n",
      "12000/49000 loss: 0.5099841388622174\n",
      "14000/49000 loss: 0.5071737524446681\n",
      "16000/49000 loss: 0.457299001940069\n",
      "18000/49000 loss: 0.485234154192449\n",
      "20000/49000 loss: 0.3820555432789266\n",
      "22000/49000 loss: 0.37316720543454174\n",
      "24000/49000 loss: 0.37710008244531146\n",
      "26000/49000 loss: 0.43792734217993745\n",
      "28000/49000 loss: 0.3946028562264253\n",
      "30000/49000 loss: 0.3958522557180608\n",
      "32000/49000 loss: 0.3845425657893325\n",
      "34000/49000 loss: 0.36033460576201826\n",
      "36000/49000 loss: 0.41630606671846726\n",
      "38000/49000 loss: 0.391874667915196\n",
      "40000/49000 loss: 0.3761724352507956\n",
      "42000/49000 loss: 0.4923304916697936\n",
      "44000/49000 loss: 0.4212070228546346\n",
      "46000/49000 loss: 0.37856668504875424\n",
      "48000/49000 loss: 0.3894488740919235\n",
      "epoch 6: valid acc = 0.857, new learning rate = 0.0003675459453124999\n",
      "2000/49000 loss: 0.3374861944085225\n",
      "4000/49000 loss: 0.5003305404014697\n",
      "6000/49000 loss: 0.4675635322317083\n",
      "8000/49000 loss: 0.4335083682015032\n",
      "10000/49000 loss: 0.38590676958206344\n",
      "12000/49000 loss: 0.3765215240738873\n",
      "14000/49000 loss: 0.44241369783640627\n",
      "16000/49000 loss: 0.36603853003072256\n",
      "18000/49000 loss: 0.3785654100097445\n",
      "20000/49000 loss: 0.3688882148221231\n",
      "22000/49000 loss: 0.3928615789197588\n",
      "24000/49000 loss: 0.3672547533836251\n",
      "26000/49000 loss: 0.40088149090135244\n",
      "28000/49000 loss: 0.4634936746125769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000/49000 loss: 0.32546558275711923\n",
      "32000/49000 loss: 0.37848770384816965\n",
      "34000/49000 loss: 0.37171405870077034\n",
      "36000/49000 loss: 0.4403171815742272\n",
      "38000/49000 loss: 0.406521835134631\n",
      "40000/49000 loss: 0.43643973564524663\n",
      "42000/49000 loss: 0.4407616046896526\n",
      "44000/49000 loss: 0.37311981739855343\n",
      "46000/49000 loss: 0.4600748304250871\n",
      "48000/49000 loss: 0.39315217925558577\n",
      "epoch 7: valid acc = 0.864, new learning rate = 0.00034916864804687486\n",
      "2000/49000 loss: 0.41096092180277477\n",
      "4000/49000 loss: 0.3843404794771129\n",
      "6000/49000 loss: 0.36026300496737\n",
      "8000/49000 loss: 0.4031428436065025\n",
      "10000/49000 loss: 0.28124067002555314\n",
      "12000/49000 loss: 0.3018073228280739\n",
      "14000/49000 loss: 0.3259368549418775\n",
      "16000/49000 loss: 0.24952955589801473\n",
      "18000/49000 loss: 0.3281128953745238\n",
      "20000/49000 loss: 0.3224812853394635\n",
      "22000/49000 loss: 0.34616970612474496\n",
      "24000/49000 loss: 0.38179549436467763\n",
      "26000/49000 loss: 0.33710525514159634\n",
      "28000/49000 loss: 0.40227307654798883\n",
      "30000/49000 loss: 0.38780393984013395\n",
      "32000/49000 loss: 0.3947962326796553\n",
      "34000/49000 loss: 0.31413141175680726\n",
      "36000/49000 loss: 0.3626080109839901\n",
      "38000/49000 loss: 0.4758646106739247\n",
      "40000/49000 loss: 0.34104251070836733\n",
      "42000/49000 loss: 0.37140344780688045\n",
      "44000/49000 loss: 0.195043694738969\n",
      "46000/49000 loss: 0.29208957262055174\n",
      "48000/49000 loss: 0.35493957505879326\n",
      "epoch 8: valid acc = 0.862, new learning rate = 0.0003317102156445311\n",
      "2000/49000 loss: 0.3526585022200055\n",
      "4000/49000 loss: 0.3030817309743124\n",
      "6000/49000 loss: 0.3555156714578979\n",
      "8000/49000 loss: 0.3057947755215956\n",
      "10000/49000 loss: 0.340650341679451\n",
      "12000/49000 loss: 0.3855977712282927\n",
      "14000/49000 loss: 0.35265161569523734\n",
      "16000/49000 loss: 0.3670427348793995\n",
      "18000/49000 loss: 0.30295903592731194\n",
      "20000/49000 loss: 0.4725366575999464\n",
      "22000/49000 loss: 0.2824802367997313\n",
      "24000/49000 loss: 0.3675513397155239\n",
      "26000/49000 loss: 0.380508461917707\n",
      "28000/49000 loss: 0.3089450265654772\n",
      "30000/49000 loss: 0.4129879346810608\n",
      "32000/49000 loss: 0.4675797404808505\n",
      "34000/49000 loss: 0.31144061361051195\n",
      "36000/49000 loss: 0.3249773668541045\n",
      "38000/49000 loss: 0.3603392697484423\n",
      "40000/49000 loss: 0.3372965923171454\n",
      "42000/49000 loss: 0.2579052761758683\n",
      "44000/49000 loss: 0.31494617440342904\n",
      "46000/49000 loss: 0.37357185193109427\n",
      "48000/49000 loss: 0.30445833561276725\n",
      "epoch 9: valid acc = 0.869, new learning rate = 0.0003151247048623045\n",
      "2000/49000 loss: 0.4619294545999201\n",
      "4000/49000 loss: 0.36577145605028966\n",
      "6000/49000 loss: 0.3671765113408762\n",
      "8000/49000 loss: 0.48118223580756214\n",
      "10000/49000 loss: 0.4252020108871623\n",
      "12000/49000 loss: 0.31665422006608906\n",
      "14000/49000 loss: 0.4324099161743076\n",
      "16000/49000 loss: 0.41619480777098283\n",
      "18000/49000 loss: 0.33077902535744014\n",
      "20000/49000 loss: 0.33840517383087093\n",
      "22000/49000 loss: 0.4051257435671161\n",
      "24000/49000 loss: 0.4136856211511033\n",
      "26000/49000 loss: 0.34119997751396103\n",
      "28000/49000 loss: 0.4528727704306555\n",
      "30000/49000 loss: 0.3240243406962677\n",
      "32000/49000 loss: 0.27610234249859794\n",
      "34000/49000 loss: 0.32485568957253\n",
      "36000/49000 loss: 0.31916849520426305\n",
      "38000/49000 loss: 0.262001747059037\n",
      "40000/49000 loss: 0.36730176533585285\n",
      "42000/49000 loss: 0.4119972971803589\n",
      "44000/49000 loss: 0.3774667901559974\n",
      "46000/49000 loss: 0.282484647586213\n",
      "48000/49000 loss: 0.3754817246886733\n",
      "epoch 10: valid acc = 0.861, new learning rate = 0.00029936846961918924\n",
      "2000/49000 loss: 0.458948700964453\n",
      "4000/49000 loss: 0.3317983467641798\n",
      "6000/49000 loss: 0.38319615111612165\n",
      "8000/49000 loss: 0.371213659747187\n",
      "10000/49000 loss: 0.32758005680993224\n",
      "12000/49000 loss: 0.2566527164404245\n",
      "14000/49000 loss: 0.3520386477468719\n",
      "16000/49000 loss: 0.3465549715931741\n",
      "18000/49000 loss: 0.22049598255030015\n",
      "20000/49000 loss: 0.3475365031342829\n",
      "22000/49000 loss: 0.3724669807361949\n",
      "24000/49000 loss: 0.32737319752093363\n",
      "26000/49000 loss: 0.3345748041581616\n",
      "28000/49000 loss: 0.2952155008852943\n",
      "30000/49000 loss: 0.3591902135158318\n",
      "32000/49000 loss: 0.2855454125860229\n",
      "34000/49000 loss: 0.3718661106175329\n",
      "36000/49000 loss: 0.30573480682922455\n",
      "38000/49000 loss: 0.3558266729542741\n",
      "40000/49000 loss: 0.277267877872999\n",
      "42000/49000 loss: 0.37791767050467034\n",
      "44000/49000 loss: 0.2947445292641805\n",
      "46000/49000 loss: 0.3272467158043349\n",
      "48000/49000 loss: 0.33165145910543486\n",
      "epoch 11: valid acc = 0.875, new learning rate = 0.00028440004613822977\n",
      "2000/49000 loss: 0.4032377127757875\n",
      "4000/49000 loss: 0.3528042900590446\n",
      "6000/49000 loss: 0.41078798685377504\n",
      "8000/49000 loss: 0.4531565210169173\n",
      "10000/49000 loss: 0.3443132120286165\n",
      "12000/49000 loss: 0.37536459885849066\n",
      "14000/49000 loss: 0.3579761217716661\n",
      "16000/49000 loss: 0.3231989501631042\n",
      "18000/49000 loss: 0.3384594231252115\n",
      "20000/49000 loss: 0.3559682788848104\n",
      "22000/49000 loss: 0.4380010174137135\n",
      "24000/49000 loss: 0.3212827891794416\n",
      "26000/49000 loss: 0.32615342274064113\n",
      "28000/49000 loss: 0.30202185242704366\n",
      "30000/49000 loss: 0.3590947524940711\n",
      "32000/49000 loss: 0.3277854576241903\n",
      "34000/49000 loss: 0.3883631438483795\n",
      "36000/49000 loss: 0.4595005800789261\n",
      "38000/49000 loss: 0.3517386525908753\n",
      "40000/49000 loss: 0.32016989311783217\n",
      "42000/49000 loss: 0.35473217717245\n",
      "44000/49000 loss: 0.3051591808734923\n",
      "46000/49000 loss: 0.26552230907297775\n",
      "48000/49000 loss: 0.411427276891175\n",
      "epoch 12: valid acc = 0.873, new learning rate = 0.00027018004383131826\n",
      "2000/49000 loss: 0.3921576145144905\n",
      "4000/49000 loss: 0.24449554350082514\n",
      "6000/49000 loss: 0.3176613721021656\n",
      "8000/49000 loss: 0.273893493169201\n",
      "10000/49000 loss: 0.3476706547526013\n",
      "12000/49000 loss: 0.3897931041739165\n",
      "14000/49000 loss: 0.3787657578437398\n",
      "16000/49000 loss: 0.3852024781416348\n",
      "18000/49000 loss: 0.2376390338256346\n",
      "20000/49000 loss: 0.33027306245784643\n",
      "22000/49000 loss: 0.2737421580427244\n",
      "24000/49000 loss: 0.3451754026159828\n",
      "26000/49000 loss: 0.3456052464495467\n",
      "28000/49000 loss: 0.27540053928759023\n",
      "30000/49000 loss: 0.36632569751499805\n",
      "32000/49000 loss: 0.30848624303049244\n",
      "34000/49000 loss: 0.31640553569322105\n",
      "36000/49000 loss: 0.3903611753362479\n",
      "38000/49000 loss: 0.2811656616255708\n",
      "40000/49000 loss: 0.36543982513092055\n",
      "42000/49000 loss: 0.3022594018887113\n",
      "44000/49000 loss: 0.29719188761958504\n",
      "46000/49000 loss: 0.3297952714773066\n",
      "48000/49000 loss: 0.31065905344602696\n",
      "epoch 13: valid acc = 0.879, new learning rate = 0.00025667104163975234\n",
      "2000/49000 loss: 0.3044270979964591\n",
      "4000/49000 loss: 0.466197602817412\n",
      "6000/49000 loss: 0.361893998946073\n",
      "8000/49000 loss: 0.3323706262485679\n",
      "10000/49000 loss: 0.41565967856485697\n",
      "12000/49000 loss: 0.3316282111602486\n",
      "14000/49000 loss: 0.2541777291184618\n",
      "16000/49000 loss: 0.358379800702191\n",
      "18000/49000 loss: 0.4035891689347358\n",
      "20000/49000 loss: 0.3364345185387469\n",
      "22000/49000 loss: 0.3629795100305183\n",
      "24000/49000 loss: 0.2858766195247739\n",
      "26000/49000 loss: 0.30706388109255195\n",
      "28000/49000 loss: 0.5210252318166472\n",
      "30000/49000 loss: 0.3477529336818731\n",
      "32000/49000 loss: 0.3331231822804982\n",
      "34000/49000 loss: 0.327719505032564\n",
      "36000/49000 loss: 0.2989635119426485\n",
      "38000/49000 loss: 0.232193105370177\n",
      "40000/49000 loss: 0.4102105452737857\n",
      "42000/49000 loss: 0.2681584073959784\n",
      "44000/49000 loss: 0.2957199476842633\n",
      "46000/49000 loss: 0.32454455746706123\n",
      "48000/49000 loss: 0.3780629745261592\n",
      "epoch 14: valid acc = 0.875, new learning rate = 0.00024383748955776472\n",
      "2000/49000 loss: 0.30828908059348337\n",
      "4000/49000 loss: 0.3597504174665479\n",
      "6000/49000 loss: 0.30987353084345565\n",
      "8000/49000 loss: 0.3444022853641554\n",
      "10000/49000 loss: 0.3491570157828895\n",
      "12000/49000 loss: 0.36927497012410426\n",
      "14000/49000 loss: 0.28853737717233424\n",
      "16000/49000 loss: 0.35043684333626307\n",
      "18000/49000 loss: 0.3186146794186848\n",
      "20000/49000 loss: 0.3073290291077319\n",
      "22000/49000 loss: 0.31129508935848943\n",
      "24000/49000 loss: 0.3342485179706739\n",
      "26000/49000 loss: 0.3301539162618504\n",
      "28000/49000 loss: 0.39584452303780154\n",
      "30000/49000 loss: 0.31613307777118027\n",
      "32000/49000 loss: 0.26714488921269214\n",
      "34000/49000 loss: 0.38638152472923487\n",
      "36000/49000 loss: 0.2335983204789131\n",
      "38000/49000 loss: 0.34978060455876525\n",
      "40000/49000 loss: 0.32288460534999874\n",
      "42000/49000 loss: 0.3263901990982299\n",
      "44000/49000 loss: 0.4147451850215689\n",
      "46000/49000 loss: 0.46876625709767517\n",
      "48000/49000 loss: 0.28801632726257154\n",
      "epoch 15: valid acc = 0.883, new learning rate = 0.00023164561507987649\n",
      "2000/49000 loss: 0.28420761107923304\n",
      "4000/49000 loss: 0.2918178053226679\n",
      "6000/49000 loss: 0.3019913467694606\n",
      "8000/49000 loss: 0.38806228163973266\n",
      "10000/49000 loss: 0.323869008928138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/49000 loss: 0.29614095027381704\n",
      "14000/49000 loss: 0.32480450361562224\n",
      "16000/49000 loss: 0.38321421549523443\n",
      "18000/49000 loss: 0.3840345109796222\n",
      "20000/49000 loss: 0.2720717580473465\n",
      "22000/49000 loss: 0.3396483160205331\n",
      "24000/49000 loss: 0.43108785583033193\n",
      "26000/49000 loss: 0.3236113215033794\n",
      "28000/49000 loss: 0.3080238444541801\n",
      "30000/49000 loss: 0.30550544993651224\n",
      "32000/49000 loss: 0.385038529275407\n",
      "34000/49000 loss: 0.3154599122557456\n",
      "36000/49000 loss: 0.3173442545476987\n",
      "38000/49000 loss: 0.35247893718511475\n",
      "40000/49000 loss: 0.39936106384034076\n",
      "42000/49000 loss: 0.2706779722809188\n",
      "44000/49000 loss: 0.34623896327674464\n",
      "46000/49000 loss: 0.3518403211503407\n",
      "48000/49000 loss: 0.35892477722120536\n",
      "epoch 16: valid acc = 0.887, new learning rate = 0.00022006333432588265\n",
      "2000/49000 loss: 0.36233040774545033\n",
      "4000/49000 loss: 0.3129978895485466\n",
      "6000/49000 loss: 0.36969705089855864\n",
      "8000/49000 loss: 0.30697793496306836\n",
      "10000/49000 loss: 0.2822850315292571\n",
      "12000/49000 loss: 0.32840942212692187\n",
      "14000/49000 loss: 0.37105671302257487\n",
      "16000/49000 loss: 0.3208759542717608\n",
      "18000/49000 loss: 0.2892052028678013\n",
      "20000/49000 loss: 0.2858442072249675\n",
      "22000/49000 loss: 0.26604014502428\n",
      "24000/49000 loss: 0.3796356286317787\n",
      "26000/49000 loss: 0.21318531182735975\n",
      "28000/49000 loss: 0.3653639528893818\n",
      "30000/49000 loss: 0.33326161324774833\n",
      "32000/49000 loss: 0.3040200072536789\n",
      "34000/49000 loss: 0.33620676472471833\n",
      "36000/49000 loss: 0.4628417957256876\n",
      "38000/49000 loss: 0.35499049879705263\n",
      "40000/49000 loss: 0.3609987009293122\n",
      "42000/49000 loss: 0.35732825250378847\n",
      "44000/49000 loss: 0.39778389830551625\n",
      "46000/49000 loss: 0.3352467218174596\n",
      "48000/49000 loss: 0.29165769931788604\n",
      "epoch 17: valid acc = 0.883, new learning rate = 0.00020906016760958852\n",
      "2000/49000 loss: 0.5281224221755896\n",
      "4000/49000 loss: 0.4114106350974983\n",
      "6000/49000 loss: 0.39229251024437795\n",
      "8000/49000 loss: 0.34883228871891525\n",
      "10000/49000 loss: 0.3623670628194407\n",
      "12000/49000 loss: 0.3495316454231514\n",
      "14000/49000 loss: 0.3969797524798308\n",
      "16000/49000 loss: 0.3049628038651239\n",
      "18000/49000 loss: 0.36458092257594\n",
      "20000/49000 loss: 0.33353319860736297\n",
      "22000/49000 loss: 0.2232987449615457\n",
      "24000/49000 loss: 0.1942407832643034\n",
      "26000/49000 loss: 0.27275695151452817\n",
      "28000/49000 loss: 0.2998849633712793\n",
      "30000/49000 loss: 0.30359790032875317\n",
      "32000/49000 loss: 0.2680899708545114\n",
      "34000/49000 loss: 0.2463927411507535\n",
      "36000/49000 loss: 0.32679390445976375\n",
      "38000/49000 loss: 0.34274323793617245\n",
      "40000/49000 loss: 0.29429475264552696\n",
      "42000/49000 loss: 0.2769856412603372\n",
      "44000/49000 loss: 0.31513866704217003\n",
      "46000/49000 loss: 0.34607098495783517\n",
      "48000/49000 loss: 0.3565537765616939\n",
      "epoch 18: valid acc = 0.884, new learning rate = 0.00019860715922910907\n",
      "2000/49000 loss: 0.3406061176812884\n",
      "4000/49000 loss: 0.30576995420012937\n",
      "6000/49000 loss: 0.32983155198752123\n",
      "8000/49000 loss: 0.23503218762180483\n",
      "10000/49000 loss: 0.3627316181582082\n",
      "12000/49000 loss: 0.30714001513820743\n",
      "14000/49000 loss: 0.2726004229303393\n",
      "16000/49000 loss: 0.3435010210661568\n",
      "18000/49000 loss: 0.31679266516622684\n",
      "20000/49000 loss: 0.3628842528169667\n",
      "22000/49000 loss: 0.3370703282026267\n",
      "24000/49000 loss: 0.36897025635346264\n",
      "26000/49000 loss: 0.36199004890616393\n",
      "28000/49000 loss: 0.3545193020161038\n",
      "30000/49000 loss: 0.31248396340525647\n",
      "32000/49000 loss: 0.30463913464958936\n",
      "34000/49000 loss: 0.4088945748874641\n",
      "36000/49000 loss: 0.3135612272362282\n",
      "38000/49000 loss: 0.339168865755202\n",
      "40000/49000 loss: 0.34226320406950567\n",
      "42000/49000 loss: 0.2880798909491026\n",
      "44000/49000 loss: 0.3549916645409404\n",
      "46000/49000 loss: 0.2733538698733709\n",
      "48000/49000 loss: 0.3389813204179955\n",
      "epoch 19: valid acc = 0.887, new learning rate = 0.0001886768012676536\n",
      "2000/49000 loss: 0.3543157519036264\n",
      "4000/49000 loss: 0.2934208486416319\n",
      "6000/49000 loss: 0.3723424433866578\n",
      "8000/49000 loss: 0.3297001227074453\n",
      "10000/49000 loss: 0.2156968121482818\n",
      "12000/49000 loss: 0.40001092593339865\n",
      "14000/49000 loss: 0.30227625266077507\n",
      "16000/49000 loss: 0.26884715637934836\n",
      "18000/49000 loss: 0.27524590842560054\n",
      "20000/49000 loss: 0.3480819187532969\n",
      "22000/49000 loss: 0.2782175273114455\n",
      "24000/49000 loss: 0.293239929927157\n",
      "26000/49000 loss: 0.3375561021444521\n",
      "28000/49000 loss: 0.3149897533587777\n",
      "30000/49000 loss: 0.30751464181233984\n",
      "32000/49000 loss: 0.32756732283249035\n",
      "34000/49000 loss: 0.3523625119212844\n",
      "36000/49000 loss: 0.3137151461068001\n",
      "38000/49000 loss: 0.31172726093789266\n",
      "40000/49000 loss: 0.37900775114561597\n",
      "42000/49000 loss: 0.34028901404972633\n",
      "44000/49000 loss: 0.30656832179155385\n",
      "46000/49000 loss: 0.3340540126833176\n",
      "48000/49000 loss: 0.31092281219118106\n",
      "epoch 20: valid acc = 0.888, new learning rate = 0.0001792429612042709\n",
      "2000/49000 loss: 0.3296613937487427\n",
      "4000/49000 loss: 0.27667673712121427\n",
      "6000/49000 loss: 0.32219302617946183\n",
      "8000/49000 loss: 0.4025709520303766\n",
      "10000/49000 loss: 0.3191935347733434\n",
      "12000/49000 loss: 0.30513345456312546\n",
      "14000/49000 loss: 0.3386824805704011\n",
      "16000/49000 loss: 0.3463287548489429\n",
      "18000/49000 loss: 0.2901236257952843\n",
      "20000/49000 loss: 0.2923098892656361\n",
      "22000/49000 loss: 0.29980823829790454\n",
      "24000/49000 loss: 0.267240376968707\n",
      "26000/49000 loss: 0.40109231063501655\n",
      "28000/49000 loss: 0.31302350546602054\n",
      "30000/49000 loss: 0.264304891063188\n",
      "32000/49000 loss: 0.31067118314650954\n",
      "34000/49000 loss: 0.3570328774703218\n",
      "36000/49000 loss: 0.35083479206805257\n",
      "38000/49000 loss: 0.30653866462537505\n",
      "40000/49000 loss: 0.31945826441414427\n",
      "42000/49000 loss: 0.3629555254033148\n",
      "44000/49000 loss: 0.31580090785540904\n",
      "46000/49000 loss: 0.3201719017620226\n",
      "48000/49000 loss: 0.31293683751310475\n",
      "epoch 21: valid acc = 0.886, new learning rate = 0.00017028081314405735\n",
      "2000/49000 loss: 0.3730304364664569\n",
      "4000/49000 loss: 0.30511995598443525\n",
      "6000/49000 loss: 0.3414923696756447\n",
      "8000/49000 loss: 0.29720081820560956\n",
      "10000/49000 loss: 0.352949020903663\n",
      "12000/49000 loss: 0.38191417732292277\n",
      "14000/49000 loss: 0.3195479602537831\n",
      "16000/49000 loss: 0.34503166597861384\n",
      "18000/49000 loss: 0.299761420266186\n",
      "20000/49000 loss: 0.2329425174544826\n",
      "22000/49000 loss: 0.2722399988307823\n",
      "24000/49000 loss: 0.3292612396822323\n",
      "26000/49000 loss: 0.2840565038002655\n",
      "28000/49000 loss: 0.3465597533774996\n",
      "30000/49000 loss: 0.2636222274580772\n",
      "32000/49000 loss: 0.37825240865830706\n",
      "34000/49000 loss: 0.3143923081952696\n",
      "36000/49000 loss: 0.3215130441113614\n",
      "38000/49000 loss: 0.2968911543962786\n",
      "40000/49000 loss: 0.2702086791118478\n",
      "42000/49000 loss: 0.2991117130154389\n",
      "44000/49000 loss: 0.34304663865900836\n",
      "46000/49000 loss: 0.42377166162385793\n",
      "48000/49000 loss: 0.2832031309671193\n",
      "epoch 22: valid acc = 0.889, new learning rate = 0.00016176677248685447\n",
      "2000/49000 loss: 0.3293831463295269\n",
      "4000/49000 loss: 0.2895253592403328\n",
      "6000/49000 loss: 0.2955837308808234\n",
      "8000/49000 loss: 0.34119616662226376\n",
      "10000/49000 loss: 0.28780606354207877\n",
      "12000/49000 loss: 0.29141457622417866\n",
      "14000/49000 loss: 0.2568988594133712\n",
      "16000/49000 loss: 0.26306898904629433\n",
      "18000/49000 loss: 0.3114899079621906\n",
      "20000/49000 loss: 0.26400710488216866\n",
      "22000/49000 loss: 0.36535206692609384\n",
      "24000/49000 loss: 0.2643843130784589\n",
      "26000/49000 loss: 0.2833669567723852\n",
      "28000/49000 loss: 0.29763075328439237\n",
      "30000/49000 loss: 0.2728148608067612\n",
      "32000/49000 loss: 0.3129469322606533\n",
      "34000/49000 loss: 0.32760282399240687\n",
      "36000/49000 loss: 0.27760185914202773\n",
      "38000/49000 loss: 0.2990583933920984\n",
      "40000/49000 loss: 0.3261220784258085\n",
      "42000/49000 loss: 0.250337561919512\n",
      "44000/49000 loss: 0.3293980804780148\n",
      "46000/49000 loss: 0.3459907494084033\n",
      "48000/49000 loss: 0.27244639040022534\n",
      "epoch 23: valid acc = 0.884, new learning rate = 0.00015367843386251173\n",
      "2000/49000 loss: 0.3809201877601805\n",
      "4000/49000 loss: 0.2825263368950946\n",
      "6000/49000 loss: 0.37737624405314873\n",
      "8000/49000 loss: 0.33646684522773257\n",
      "10000/49000 loss: 0.2887155550181409\n",
      "12000/49000 loss: 0.322300278838729\n",
      "14000/49000 loss: 0.33523920212356295\n",
      "16000/49000 loss: 0.43146892885202554\n",
      "18000/49000 loss: 0.34140601894385064\n",
      "20000/49000 loss: 0.22231777214484924\n",
      "22000/49000 loss: 0.2597890896249122\n",
      "24000/49000 loss: 0.27173284400323855\n",
      "26000/49000 loss: 0.303155977042946\n",
      "28000/49000 loss: 0.2567706960885014\n",
      "30000/49000 loss: 0.2689566307848341\n",
      "32000/49000 loss: 0.2915943036482243\n",
      "34000/49000 loss: 0.3987263551634618\n",
      "36000/49000 loss: 0.27559660285340853\n",
      "38000/49000 loss: 0.2902748536379876\n",
      "40000/49000 loss: 0.3441056839309439\n",
      "42000/49000 loss: 0.2537886971726986\n",
      "44000/49000 loss: 0.26707377726966874\n",
      "46000/49000 loss: 0.4995807805342771\n",
      "48000/49000 loss: 0.3113702450442054\n",
      "epoch 24: valid acc = 0.887, new learning rate = 0.00014599451216938612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/49000 loss: 0.33147788193150934\n",
      "4000/49000 loss: 0.3923389363232805\n",
      "6000/49000 loss: 0.2911249329259675\n",
      "8000/49000 loss: 0.32623329468728435\n",
      "10000/49000 loss: 0.3106169559192078\n",
      "12000/49000 loss: 0.33682403954400963\n",
      "14000/49000 loss: 0.2249511625159729\n",
      "16000/49000 loss: 0.2983723854654224\n",
      "18000/49000 loss: 0.37685259550298417\n",
      "20000/49000 loss: 0.3721351634228278\n",
      "22000/49000 loss: 0.36564700642252507\n",
      "24000/49000 loss: 0.2999123911014911\n",
      "26000/49000 loss: 0.2688830548926479\n",
      "28000/49000 loss: 0.35426259119485315\n",
      "30000/49000 loss: 0.26547808898205316\n",
      "32000/49000 loss: 0.32363362590592815\n",
      "34000/49000 loss: 0.35286534559262045\n",
      "36000/49000 loss: 0.3698815947581566\n",
      "38000/49000 loss: 0.37629162484415873\n",
      "40000/49000 loss: 0.3892060657943116\n",
      "42000/49000 loss: 0.23187526841968112\n",
      "44000/49000 loss: 0.3340276636546146\n",
      "46000/49000 loss: 0.3093075531206654\n",
      "48000/49000 loss: 0.4016267528334101\n",
      "epoch 25: valid acc = 0.891, new learning rate = 0.00013869478656091682\n",
      "2000/49000 loss: 0.307329714995695\n",
      "4000/49000 loss: 0.2743277656793715\n",
      "6000/49000 loss: 0.32136524147356715\n",
      "8000/49000 loss: 0.3551890685912913\n",
      "10000/49000 loss: 0.27290089855330313\n",
      "12000/49000 loss: 0.3319335437273623\n",
      "14000/49000 loss: 0.3231327789918381\n",
      "16000/49000 loss: 0.2982661388207839\n",
      "18000/49000 loss: 0.43575637903062375\n",
      "20000/49000 loss: 0.2859907148030562\n",
      "22000/49000 loss: 0.27277509173302433\n",
      "24000/49000 loss: 0.27347416439152744\n",
      "26000/49000 loss: 0.28890847294866334\n",
      "28000/49000 loss: 0.3290041765581657\n",
      "30000/49000 loss: 0.22939422963276823\n",
      "32000/49000 loss: 0.29797556024044436\n",
      "34000/49000 loss: 0.26678521607953776\n",
      "36000/49000 loss: 0.26621638301094536\n",
      "38000/49000 loss: 0.26508773294963023\n",
      "40000/49000 loss: 0.36906906619764707\n",
      "42000/49000 loss: 0.30989540942895205\n",
      "44000/49000 loss: 0.31773909467743966\n",
      "46000/49000 loss: 0.20768517486719715\n",
      "48000/49000 loss: 0.24507700482540234\n",
      "epoch 26: valid acc = 0.883, new learning rate = 0.00013176004723287096\n",
      "2000/49000 loss: 0.20506473850911086\n",
      "4000/49000 loss: 0.3199106948378011\n",
      "6000/49000 loss: 0.22723216895508394\n",
      "8000/49000 loss: 0.29084704403704154\n",
      "10000/49000 loss: 0.2723442445160246\n",
      "12000/49000 loss: 0.20004934386890463\n",
      "14000/49000 loss: 0.3185518668318603\n",
      "16000/49000 loss: 0.2596862977939983\n",
      "18000/49000 loss: 0.38060452486152757\n",
      "20000/49000 loss: 0.22152428012825776\n",
      "22000/49000 loss: 0.3168600781822808\n",
      "24000/49000 loss: 0.24570069024246108\n",
      "26000/49000 loss: 0.25265796983699634\n",
      "28000/49000 loss: 0.2919254538639793\n",
      "30000/49000 loss: 0.37336098907101073\n",
      "32000/49000 loss: 0.3836307425621991\n",
      "34000/49000 loss: 0.317085931363367\n",
      "36000/49000 loss: 0.28395055546784104\n",
      "38000/49000 loss: 0.2335868803595155\n",
      "40000/49000 loss: 0.26173652982730344\n",
      "42000/49000 loss: 0.3451861857043069\n",
      "44000/49000 loss: 0.25696028311777847\n",
      "46000/49000 loss: 0.22346860064105736\n",
      "48000/49000 loss: 0.28834669724298206\n",
      "epoch 27: valid acc = 0.886, new learning rate = 0.0001251720448712274\n",
      "2000/49000 loss: 0.37942806069481416\n",
      "4000/49000 loss: 0.31093739236759343\n",
      "6000/49000 loss: 0.27590008380066827\n",
      "8000/49000 loss: 0.3117069710801062\n",
      "10000/49000 loss: 0.29010147750582715\n",
      "12000/49000 loss: 0.2855928021438905\n",
      "14000/49000 loss: 0.3712376432482232\n",
      "16000/49000 loss: 0.2869206771785071\n",
      "18000/49000 loss: 0.26020076449156765\n",
      "20000/49000 loss: 0.3648978730923909\n",
      "22000/49000 loss: 0.2728045347588467\n",
      "24000/49000 loss: 0.24125714611883015\n",
      "26000/49000 loss: 0.247657363078926\n",
      "28000/49000 loss: 0.31491413156532483\n",
      "30000/49000 loss: 0.36702821653587153\n",
      "32000/49000 loss: 0.4140858339290098\n",
      "34000/49000 loss: 0.25386185653936705\n",
      "36000/49000 loss: 0.42766040987682047\n",
      "38000/49000 loss: 0.26350711804874555\n",
      "40000/49000 loss: 0.2247245651942684\n",
      "42000/49000 loss: 0.2800498878426562\n",
      "44000/49000 loss: 0.2799437655590319\n",
      "46000/49000 loss: 0.2628065493077324\n",
      "48000/49000 loss: 0.24660896950220434\n",
      "epoch 28: valid acc = 0.883, new learning rate = 0.00011891344262766602\n",
      "2000/49000 loss: 0.2232577925916584\n",
      "4000/49000 loss: 0.2793501289304727\n",
      "6000/49000 loss: 0.3490484636971073\n",
      "8000/49000 loss: 0.3282877351753446\n",
      "10000/49000 loss: 0.390834702741414\n",
      "12000/49000 loss: 0.21104451226924956\n",
      "14000/49000 loss: 0.3518128953695941\n",
      "16000/49000 loss: 0.32007155140635263\n",
      "18000/49000 loss: 0.34316955015501704\n",
      "20000/49000 loss: 0.3292758285577675\n",
      "22000/49000 loss: 0.22796633256323617\n",
      "24000/49000 loss: 0.28753639173862394\n",
      "26000/49000 loss: 0.2858253237706461\n",
      "28000/49000 loss: 0.3591688510113411\n",
      "30000/49000 loss: 0.29547395350718286\n",
      "32000/49000 loss: 0.34821045586230454\n",
      "34000/49000 loss: 0.31294357424627645\n",
      "36000/49000 loss: 0.28227646857347305\n",
      "38000/49000 loss: 0.2871543005579989\n",
      "40000/49000 loss: 0.24933560341305647\n",
      "42000/49000 loss: 0.2920163044016722\n",
      "44000/49000 loss: 0.34118385553034003\n",
      "46000/49000 loss: 0.388074032892705\n",
      "48000/49000 loss: 0.30913830894196465\n",
      "epoch 29: valid acc = 0.893, new learning rate = 0.00011296777049628272\n",
      "2000/49000 loss: 0.2440165893288852\n",
      "4000/49000 loss: 0.3120083765471578\n",
      "6000/49000 loss: 0.20553709009511958\n",
      "8000/49000 loss: 0.30654558079800354\n",
      "10000/49000 loss: 0.3680477683836487\n",
      "12000/49000 loss: 0.30571354426726155\n",
      "14000/49000 loss: 0.3432721077241665\n",
      "16000/49000 loss: 0.39837721198898274\n",
      "18000/49000 loss: 0.30990955025275435\n",
      "20000/49000 loss: 0.3174198931623833\n",
      "22000/49000 loss: 0.25518792978628957\n",
      "24000/49000 loss: 0.3123373954775255\n",
      "26000/49000 loss: 0.2824950985830336\n",
      "28000/49000 loss: 0.3026191158939668\n",
      "30000/49000 loss: 0.3857739534252181\n",
      "32000/49000 loss: 0.233459566594412\n",
      "34000/49000 loss: 0.30655230491400814\n",
      "36000/49000 loss: 0.33581183043789237\n",
      "38000/49000 loss: 0.38554243598272103\n",
      "40000/49000 loss: 0.3366512077865581\n",
      "42000/49000 loss: 0.29243502176640623\n",
      "44000/49000 loss: 0.32702297852364653\n",
      "46000/49000 loss: 0.31341290314873943\n",
      "48000/49000 loss: 0.26126706724660337\n",
      "epoch 30: valid acc = 0.887, new learning rate = 0.00010731938197146858\n",
      "2000/49000 loss: 0.2641948794793545\n",
      "4000/49000 loss: 0.2856881799090415\n",
      "6000/49000 loss: 0.30489964196101\n",
      "8000/49000 loss: 0.33731411890117113\n",
      "10000/49000 loss: 0.29338907195592256\n",
      "12000/49000 loss: 0.28286552263982767\n",
      "14000/49000 loss: 0.32384947321847185\n",
      "16000/49000 loss: 0.29921499381502686\n",
      "18000/49000 loss: 0.30220116619709886\n",
      "20000/49000 loss: 0.25173538097562853\n",
      "22000/49000 loss: 0.18786335613388377\n",
      "24000/49000 loss: 0.23201693009082394\n",
      "26000/49000 loss: 0.3871607743052813\n",
      "28000/49000 loss: 0.3295403897096276\n",
      "30000/49000 loss: 0.33188068871105864\n",
      "32000/49000 loss: 0.28776582349605145\n",
      "34000/49000 loss: 0.3524794122848546\n",
      "36000/49000 loss: 0.32323830960351824\n",
      "38000/49000 loss: 0.3758363258592908\n",
      "40000/49000 loss: 0.37784133084954213\n",
      "42000/49000 loss: 0.3133466814450976\n",
      "44000/49000 loss: 0.2930963224814625\n",
      "46000/49000 loss: 0.23767821351199075\n",
      "48000/49000 loss: 0.3826614044490812\n",
      "epoch 31: valid acc = 0.889, new learning rate = 0.00010195341287289515\n",
      "2000/49000 loss: 0.2928387508195675\n",
      "4000/49000 loss: 0.34612075515560264\n",
      "6000/49000 loss: 0.2727350507487375\n",
      "8000/49000 loss: 0.25384416637083185\n",
      "10000/49000 loss: 0.3133243755943401\n",
      "12000/49000 loss: 0.26535226498330033\n",
      "14000/49000 loss: 0.30407683758766313\n",
      "16000/49000 loss: 0.3121926971187629\n",
      "18000/49000 loss: 0.23011062698719725\n",
      "20000/49000 loss: 0.27694346725706587\n",
      "22000/49000 loss: 0.2709915606109864\n",
      "24000/49000 loss: 0.3583538226167297\n",
      "26000/49000 loss: 0.31670004980052296\n",
      "28000/49000 loss: 0.3290850981368274\n",
      "30000/49000 loss: 0.257373996501843\n",
      "32000/49000 loss: 0.2807220027294551\n",
      "34000/49000 loss: 0.2584009626498819\n",
      "36000/49000 loss: 0.2512108075103364\n",
      "38000/49000 loss: 0.363995547471422\n",
      "40000/49000 loss: 0.2387727098084256\n",
      "42000/49000 loss: 0.24069844489867787\n",
      "44000/49000 loss: 0.3613167174682884\n",
      "46000/49000 loss: 0.25544273965625885\n",
      "48000/49000 loss: 0.2554848278166331\n",
      "epoch 32: valid acc = 0.888, new learning rate = 9.685574222925039e-05\n",
      "2000/49000 loss: 0.31657441335312425\n",
      "4000/49000 loss: 0.42575954542938377\n",
      "6000/49000 loss: 0.37626361626472526\n",
      "8000/49000 loss: 0.2798129612071851\n",
      "10000/49000 loss: 0.31046360532687867\n",
      "12000/49000 loss: 0.30944999329554695\n",
      "14000/49000 loss: 0.29167032869931614\n",
      "16000/49000 loss: 0.26464810304846953\n",
      "18000/49000 loss: 0.3015502017714317\n",
      "20000/49000 loss: 0.2700434694784682\n",
      "22000/49000 loss: 0.23557711072213788\n",
      "24000/49000 loss: 0.393619546439576\n",
      "26000/49000 loss: 0.2957147201038617\n",
      "28000/49000 loss: 0.25551015524165294\n",
      "30000/49000 loss: 0.27856751611865727\n",
      "32000/49000 loss: 0.31690594208102557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34000/49000 loss: 0.2940508886259284\n",
      "36000/49000 loss: 0.33093814451910547\n",
      "38000/49000 loss: 0.27049128464945\n",
      "40000/49000 loss: 0.2904968996593288\n",
      "42000/49000 loss: 0.2742867145064388\n",
      "44000/49000 loss: 0.33485887239570417\n",
      "46000/49000 loss: 0.26475523568138337\n",
      "48000/49000 loss: 0.267598532279498\n",
      "epoch 33: valid acc = 0.89, new learning rate = 9.201295511778786e-05\n",
      "2000/49000 loss: 0.2906335760378742\n",
      "4000/49000 loss: 0.2666286847595694\n",
      "6000/49000 loss: 0.2596014482440647\n",
      "8000/49000 loss: 0.28292568067339313\n",
      "10000/49000 loss: 0.2881626318158622\n",
      "12000/49000 loss: 0.3259324973254203\n",
      "14000/49000 loss: 0.2996554806606847\n",
      "16000/49000 loss: 0.3129802835688759\n",
      "18000/49000 loss: 0.2651553347276041\n",
      "20000/49000 loss: 0.2829455044559752\n",
      "22000/49000 loss: 0.316104528291666\n",
      "24000/49000 loss: 0.32328972207824264\n",
      "26000/49000 loss: 0.24769241098820305\n",
      "28000/49000 loss: 0.24488577963201533\n",
      "30000/49000 loss: 0.30703614582541644\n",
      "32000/49000 loss: 0.3167574026216324\n",
      "34000/49000 loss: 0.2754728181847429\n",
      "36000/49000 loss: 0.33997743681810505\n",
      "38000/49000 loss: 0.2634337872653231\n",
      "40000/49000 loss: 0.3572523633080744\n",
      "42000/49000 loss: 0.3558108885253561\n",
      "44000/49000 loss: 0.23222550304603515\n",
      "46000/49000 loss: 0.30024964763742606\n",
      "48000/49000 loss: 0.2795484107259405\n",
      "epoch 34: valid acc = 0.885, new learning rate = 8.741230736189846e-05\n",
      "2000/49000 loss: 0.31966363269434567\n",
      "4000/49000 loss: 0.29196236630889616\n",
      "6000/49000 loss: 0.36162841430011694\n",
      "8000/49000 loss: 0.3234701698886866\n",
      "10000/49000 loss: 0.2880465852642074\n",
      "12000/49000 loss: 0.27318615337753144\n",
      "14000/49000 loss: 0.32628651692648425\n",
      "16000/49000 loss: 0.36249977068174893\n",
      "18000/49000 loss: 0.3010156250357969\n",
      "20000/49000 loss: 0.2800404873656067\n",
      "22000/49000 loss: 0.2664660323067736\n",
      "24000/49000 loss: 0.231172398266087\n",
      "26000/49000 loss: 0.27969090903299654\n",
      "28000/49000 loss: 0.2402326755756765\n",
      "30000/49000 loss: 0.3042587871378519\n",
      "32000/49000 loss: 0.3702881235552341\n",
      "34000/49000 loss: 0.27793057983150393\n",
      "36000/49000 loss: 0.30306514695087594\n",
      "38000/49000 loss: 0.38876468968810596\n",
      "40000/49000 loss: 0.23224282576560418\n",
      "42000/49000 loss: 0.350719151800817\n",
      "44000/49000 loss: 0.3227822741977225\n",
      "46000/49000 loss: 0.23542021035447977\n",
      "48000/49000 loss: 0.2766455653240135\n",
      "epoch 35: valid acc = 0.89, new learning rate = 8.304169199380353e-05\n",
      "2000/49000 loss: 0.36233787560192215\n",
      "4000/49000 loss: 0.3160355268306053\n",
      "6000/49000 loss: 0.312102151707136\n",
      "8000/49000 loss: 0.2816965276981607\n",
      "10000/49000 loss: 0.3075304719520349\n",
      "12000/49000 loss: 0.3391767825563312\n",
      "14000/49000 loss: 0.2803370624120364\n",
      "16000/49000 loss: 0.27513524567818637\n",
      "18000/49000 loss: 0.3694923490499534\n",
      "20000/49000 loss: 0.25804344280829716\n",
      "22000/49000 loss: 0.28686868367742735\n",
      "24000/49000 loss: 0.32658656511941453\n",
      "26000/49000 loss: 0.29503373165458485\n",
      "28000/49000 loss: 0.31879206411756694\n",
      "30000/49000 loss: 0.2676344082459069\n",
      "32000/49000 loss: 0.32954327467780836\n",
      "34000/49000 loss: 0.28693296496521925\n",
      "36000/49000 loss: 0.3280458853026042\n",
      "38000/49000 loss: 0.2820941696568374\n",
      "40000/49000 loss: 0.2278924825316484\n",
      "42000/49000 loss: 0.29027448976370446\n",
      "44000/49000 loss: 0.24930671758100678\n",
      "46000/49000 loss: 0.3622085234155453\n",
      "48000/49000 loss: 0.29269588822473563\n",
      "epoch 36: valid acc = 0.89, new learning rate = 7.888960739411335e-05\n",
      "2000/49000 loss: 0.25434163757817235\n",
      "4000/49000 loss: 0.3204404068329256\n",
      "6000/49000 loss: 0.23535633490880656\n",
      "8000/49000 loss: 0.2451652540555628\n",
      "10000/49000 loss: 0.27436821486989815\n",
      "12000/49000 loss: 0.24423584240995638\n",
      "14000/49000 loss: 0.2934632108917561\n",
      "16000/49000 loss: 0.3882513782045652\n",
      "18000/49000 loss: 0.32248843587272835\n",
      "20000/49000 loss: 0.22767214169318814\n",
      "22000/49000 loss: 0.28562542116765177\n",
      "24000/49000 loss: 0.2867675104457518\n",
      "26000/49000 loss: 0.2848865225945099\n",
      "28000/49000 loss: 0.3052954225772291\n",
      "30000/49000 loss: 0.279010001042392\n",
      "32000/49000 loss: 0.30924099949334166\n",
      "34000/49000 loss: 0.3004090485164627\n",
      "36000/49000 loss: 0.35332977717963926\n",
      "38000/49000 loss: 0.2870460431191382\n",
      "40000/49000 loss: 0.306133501231285\n",
      "42000/49000 loss: 0.25526154089333986\n",
      "44000/49000 loss: 0.23629897173098405\n",
      "46000/49000 loss: 0.3313416539043912\n",
      "48000/49000 loss: 0.28840815901409234\n",
      "epoch 37: valid acc = 0.891, new learning rate = 7.494512702440768e-05\n",
      "2000/49000 loss: 0.23153240479607037\n",
      "4000/49000 loss: 0.2774785747144174\n",
      "6000/49000 loss: 0.272320815857098\n",
      "8000/49000 loss: 0.25176865243471735\n",
      "10000/49000 loss: 0.2644855455613034\n",
      "12000/49000 loss: 0.355557229064022\n",
      "14000/49000 loss: 0.33954400358342196\n",
      "16000/49000 loss: 0.2564373826177678\n",
      "18000/49000 loss: 0.3663733081280309\n",
      "20000/49000 loss: 0.28994170902636496\n",
      "22000/49000 loss: 0.305723282581646\n",
      "24000/49000 loss: 0.3268661132962802\n",
      "26000/49000 loss: 0.29837071844253826\n",
      "28000/49000 loss: 0.2632285564583228\n",
      "30000/49000 loss: 0.33192003420788607\n",
      "32000/49000 loss: 0.30778197741803803\n",
      "34000/49000 loss: 0.3965932741587139\n",
      "36000/49000 loss: 0.29397547761158443\n",
      "38000/49000 loss: 0.2926631331915074\n",
      "40000/49000 loss: 0.30148698975188276\n",
      "42000/49000 loss: 0.27602799444299414\n",
      "44000/49000 loss: 0.25180101237680264\n",
      "46000/49000 loss: 0.2520811139360039\n",
      "48000/49000 loss: 0.27133166730083397\n",
      "epoch 38: valid acc = 0.892, new learning rate = 7.119787067318729e-05\n",
      "2000/49000 loss: 0.25775909862530627\n",
      "4000/49000 loss: 0.32256850479724564\n",
      "6000/49000 loss: 0.26487106366698665\n",
      "8000/49000 loss: 0.3109394109594809\n",
      "10000/49000 loss: 0.2664670579957646\n",
      "12000/49000 loss: 0.29341289792669684\n",
      "14000/49000 loss: 0.29972685901465773\n",
      "16000/49000 loss: 0.29636903295584816\n",
      "18000/49000 loss: 0.3336127133247825\n",
      "20000/49000 loss: 0.31584020281736364\n",
      "22000/49000 loss: 0.21427878149897292\n",
      "24000/49000 loss: 0.3456021984590131\n",
      "26000/49000 loss: 0.3333491925757818\n",
      "28000/49000 loss: 0.21770388305380325\n",
      "30000/49000 loss: 0.3168422903185116\n",
      "32000/49000 loss: 0.2977660115411862\n",
      "34000/49000 loss: 0.22432838897908713\n",
      "36000/49000 loss: 0.2328121126851722\n",
      "38000/49000 loss: 0.4083581699973189\n",
      "40000/49000 loss: 0.36386184839997293\n",
      "42000/49000 loss: 0.34418062883609674\n",
      "44000/49000 loss: 0.3078571524180327\n",
      "46000/49000 loss: 0.3406882046574414\n",
      "48000/49000 loss: 0.2152744114620333\n",
      "epoch 39: valid acc = 0.895, new learning rate = 6.763797713952792e-05\n",
      "2000/49000 loss: 0.2853550955360355\n",
      "4000/49000 loss: 0.276113808554452\n",
      "6000/49000 loss: 0.3249966830817843\n",
      "8000/49000 loss: 0.3018805369904851\n",
      "10000/49000 loss: 0.2790761713265351\n",
      "12000/49000 loss: 0.23838842202636054\n",
      "14000/49000 loss: 0.3306579252178497\n",
      "16000/49000 loss: 0.3215098914581338\n",
      "18000/49000 loss: 0.2884590663351818\n",
      "20000/49000 loss: 0.29020024966621605\n",
      "22000/49000 loss: 0.23763782303242884\n",
      "24000/49000 loss: 0.2463189701829876\n",
      "26000/49000 loss: 0.36196502499247724\n",
      "28000/49000 loss: 0.3025123220951534\n",
      "30000/49000 loss: 0.21152648708685526\n",
      "32000/49000 loss: 0.2511149421383332\n",
      "34000/49000 loss: 0.39072046987800735\n",
      "36000/49000 loss: 0.2970581217552553\n",
      "38000/49000 loss: 0.3292979938851332\n",
      "40000/49000 loss: 0.24250052597354332\n",
      "42000/49000 loss: 0.30160579319259195\n",
      "44000/49000 loss: 0.2740623851639352\n",
      "46000/49000 loss: 0.3157831982023568\n",
      "48000/49000 loss: 0.22173364617228505\n",
      "epoch 40: valid acc = 0.892, new learning rate = 6.425607828255152e-05\n",
      "2000/49000 loss: 0.301992345669191\n",
      "4000/49000 loss: 0.252291383323119\n",
      "6000/49000 loss: 0.3205649509681107\n",
      "8000/49000 loss: 0.24269093729525582\n",
      "10000/49000 loss: 0.296324423516423\n",
      "12000/49000 loss: 0.31435970682206066\n",
      "14000/49000 loss: 0.3141370187590249\n",
      "16000/49000 loss: 0.28950077909223126\n",
      "18000/49000 loss: 0.2511490560479414\n",
      "20000/49000 loss: 0.378825703359348\n",
      "22000/49000 loss: 0.2361396065451483\n",
      "24000/49000 loss: 0.2971354556981805\n",
      "26000/49000 loss: 0.23139590609908137\n",
      "28000/49000 loss: 0.21935186227587933\n",
      "30000/49000 loss: 0.34234517776908224\n",
      "32000/49000 loss: 0.3470021196079321\n",
      "34000/49000 loss: 0.3992322652662557\n",
      "36000/49000 loss: 0.28318069516526095\n",
      "38000/49000 loss: 0.3622833755238092\n",
      "40000/49000 loss: 0.30954734436919934\n",
      "42000/49000 loss: 0.29025870688587335\n",
      "44000/49000 loss: 0.29505031301530543\n",
      "46000/49000 loss: 0.2986946494166211\n",
      "48000/49000 loss: 0.25899077818182525\n",
      "epoch 41: valid acc = 0.889, new learning rate = 6.104327436842394e-05\n",
      "2000/49000 loss: 0.3173363732720701\n",
      "4000/49000 loss: 0.2425345821983239\n",
      "6000/49000 loss: 0.30103561934818046\n",
      "8000/49000 loss: 0.2276561014282483\n",
      "10000/49000 loss: 0.25863658171935366\n",
      "12000/49000 loss: 0.26902184424981845\n",
      "14000/49000 loss: 0.2312363158593104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000/49000 loss: 0.33063037370457166\n",
      "18000/49000 loss: 0.364474054828425\n",
      "20000/49000 loss: 0.23023964246167028\n",
      "22000/49000 loss: 0.30679334244993717\n",
      "24000/49000 loss: 0.2963637627985952\n",
      "26000/49000 loss: 0.21516774586855222\n",
      "28000/49000 loss: 0.26249283142345603\n",
      "30000/49000 loss: 0.1972341432173537\n",
      "32000/49000 loss: 0.30273371849526476\n",
      "34000/49000 loss: 0.38007856835241427\n",
      "36000/49000 loss: 0.31779683473572784\n",
      "38000/49000 loss: 0.32850147752568576\n",
      "40000/49000 loss: 0.2885921578219075\n",
      "42000/49000 loss: 0.23453257861560206\n",
      "44000/49000 loss: 0.26756943362172564\n",
      "46000/49000 loss: 0.26986969075539863\n",
      "48000/49000 loss: 0.2623424165884238\n",
      "epoch 42: valid acc = 0.893, new learning rate = 5.799111065000274e-05\n",
      "2000/49000 loss: 0.28532927172481853\n",
      "4000/49000 loss: 0.23085875932626848\n",
      "6000/49000 loss: 0.32996163405761325\n",
      "8000/49000 loss: 0.32666901098293055\n",
      "10000/49000 loss: 0.27576930553771745\n",
      "12000/49000 loss: 0.3218653944553791\n",
      "14000/49000 loss: 0.26238906342511126\n",
      "16000/49000 loss: 0.3755146111969191\n",
      "18000/49000 loss: 0.2729325791959982\n",
      "20000/49000 loss: 0.3669358705401962\n",
      "22000/49000 loss: 0.31141204129500993\n",
      "24000/49000 loss: 0.24433494156080637\n",
      "26000/49000 loss: 0.32614522547309893\n",
      "28000/49000 loss: 0.18228716719304255\n",
      "30000/49000 loss: 0.2602175857733594\n",
      "32000/49000 loss: 0.3530433228871247\n",
      "34000/49000 loss: 0.2658563165356537\n",
      "36000/49000 loss: 0.24353409469084636\n",
      "38000/49000 loss: 0.3035191149472285\n",
      "40000/49000 loss: 0.3642448182698139\n",
      "42000/49000 loss: 0.3828458030253052\n",
      "44000/49000 loss: 0.36895596503320904\n",
      "46000/49000 loss: 0.2813247547170127\n",
      "48000/49000 loss: 0.3346987021519318\n",
      "epoch 43: valid acc = 0.892, new learning rate = 5.5091555117502596e-05\n",
      "2000/49000 loss: 0.35636747674384733\n",
      "4000/49000 loss: 0.30485915121128737\n",
      "6000/49000 loss: 0.24161667826573457\n",
      "8000/49000 loss: 0.28753134496232696\n",
      "10000/49000 loss: 0.25866994860419446\n",
      "12000/49000 loss: 0.22178069196799402\n",
      "14000/49000 loss: 0.33952021203179267\n",
      "16000/49000 loss: 0.33550374610767647\n",
      "18000/49000 loss: 0.27801383728442564\n",
      "20000/49000 loss: 0.2586442658845844\n",
      "22000/49000 loss: 0.25708888304271377\n",
      "24000/49000 loss: 0.29134608383685795\n",
      "26000/49000 loss: 0.2961964652809267\n",
      "28000/49000 loss: 0.3816028022381041\n",
      "30000/49000 loss: 0.28536945561512334\n",
      "32000/49000 loss: 0.3229352659746359\n",
      "34000/49000 loss: 0.30734996981199214\n",
      "36000/49000 loss: 0.2122503707510391\n",
      "38000/49000 loss: 0.1533512519475998\n",
      "40000/49000 loss: 0.276542428205542\n",
      "42000/49000 loss: 0.30642953341187096\n",
      "44000/49000 loss: 0.2549779137851505\n",
      "46000/49000 loss: 0.28764125962874443\n",
      "48000/49000 loss: 0.3668230561255697\n",
      "epoch 44: valid acc = 0.89, new learning rate = 5.2336977361627463e-05\n",
      "2000/49000 loss: 0.291294363285019\n",
      "4000/49000 loss: 0.26338834161871943\n",
      "6000/49000 loss: 0.3204392409485325\n",
      "8000/49000 loss: 0.238102021833881\n",
      "10000/49000 loss: 0.34933631849324287\n",
      "12000/49000 loss: 0.24205463189853157\n",
      "14000/49000 loss: 0.3274795815026891\n",
      "16000/49000 loss: 0.2501287435817349\n",
      "18000/49000 loss: 0.20503805194766772\n",
      "20000/49000 loss: 0.30756137856245064\n",
      "22000/49000 loss: 0.22361507999293004\n",
      "24000/49000 loss: 0.28700866545510045\n",
      "26000/49000 loss: 0.32627104898774684\n",
      "28000/49000 loss: 0.25714022070345477\n",
      "30000/49000 loss: 0.29711450198593625\n",
      "32000/49000 loss: 0.24969320313155277\n",
      "34000/49000 loss: 0.2631675638394263\n",
      "36000/49000 loss: 0.20488337015107613\n",
      "38000/49000 loss: 0.27783225289394387\n",
      "40000/49000 loss: 0.3829533381515669\n",
      "42000/49000 loss: 0.29042363637528407\n",
      "44000/49000 loss: 0.36358942585575327\n",
      "46000/49000 loss: 0.3222514400730508\n",
      "48000/49000 loss: 0.32839974715482095\n",
      "epoch 45: valid acc = 0.891, new learning rate = 4.972012849354609e-05\n",
      "2000/49000 loss: 0.261528352153407\n",
      "4000/49000 loss: 0.29167881323614764\n",
      "6000/49000 loss: 0.3555085344658627\n",
      "8000/49000 loss: 0.37276135768153823\n",
      "10000/49000 loss: 0.29457047233642425\n",
      "12000/49000 loss: 0.2612842821722211\n",
      "14000/49000 loss: 0.30385516021203923\n",
      "16000/49000 loss: 0.21461157971664524\n",
      "18000/49000 loss: 0.24255126193526766\n",
      "20000/49000 loss: 0.28170816669398424\n",
      "22000/49000 loss: 0.22427637065116002\n",
      "24000/49000 loss: 0.2326746774771235\n",
      "26000/49000 loss: 0.2953076406220882\n",
      "28000/49000 loss: 0.24361430751732477\n",
      "30000/49000 loss: 0.2381181328755415\n",
      "32000/49000 loss: 0.2987475148013307\n",
      "34000/49000 loss: 0.3400616083060707\n",
      "36000/49000 loss: 0.2841651667178844\n",
      "38000/49000 loss: 0.2297805000496842\n",
      "40000/49000 loss: 0.3096186756792072\n",
      "42000/49000 loss: 0.30177425696765525\n",
      "44000/49000 loss: 0.3192226598847467\n",
      "46000/49000 loss: 0.313082839223313\n",
      "48000/49000 loss: 0.1936746318341652\n",
      "epoch 46: valid acc = 0.887, new learning rate = 4.723412206886878e-05\n",
      "2000/49000 loss: 0.30397465579444083\n",
      "4000/49000 loss: 0.2598680400555467\n",
      "6000/49000 loss: 0.32485277128805873\n",
      "8000/49000 loss: 0.4113282784419904\n",
      "10000/49000 loss: 0.2678491857887086\n",
      "12000/49000 loss: 0.26503903803082396\n",
      "14000/49000 loss: 0.2727403236066334\n",
      "16000/49000 loss: 0.26885091397025407\n",
      "18000/49000 loss: 0.31532772351392535\n",
      "20000/49000 loss: 0.37152330632987846\n",
      "22000/49000 loss: 0.28456284903227447\n",
      "24000/49000 loss: 0.2911355619072912\n",
      "26000/49000 loss: 0.23340621473206552\n",
      "28000/49000 loss: 0.21350985896073124\n",
      "30000/49000 loss: 0.3405897524565394\n",
      "32000/49000 loss: 0.28198072523900714\n",
      "34000/49000 loss: 0.245628779479419\n",
      "36000/49000 loss: 0.2742521793412587\n",
      "38000/49000 loss: 0.2929197400805366\n",
      "40000/49000 loss: 0.20189448995330797\n",
      "42000/49000 loss: 0.27368425721811446\n",
      "44000/49000 loss: 0.25761647765684903\n",
      "46000/49000 loss: 0.23834779065587075\n",
      "48000/49000 loss: 0.2067478411398094\n",
      "epoch 47: valid acc = 0.892, new learning rate = 4.487241596542534e-05\n",
      "2000/49000 loss: 0.33988046069966044\n",
      "4000/49000 loss: 0.34081225208794313\n",
      "6000/49000 loss: 0.36528317722269166\n",
      "8000/49000 loss: 0.21686884496516967\n",
      "10000/49000 loss: 0.34730475860182813\n",
      "12000/49000 loss: 0.26167200885740893\n",
      "14000/49000 loss: 0.2545096630594675\n",
      "16000/49000 loss: 0.25454683609907724\n",
      "18000/49000 loss: 0.3007256577258535\n",
      "20000/49000 loss: 0.3928595978530634\n",
      "22000/49000 loss: 0.27418457831523646\n",
      "24000/49000 loss: 0.21526764760340555\n",
      "26000/49000 loss: 0.3498147250488156\n",
      "28000/49000 loss: 0.29511452562770785\n",
      "30000/49000 loss: 0.259709078278871\n",
      "32000/49000 loss: 0.3441385712048431\n",
      "34000/49000 loss: 0.36656970712601405\n",
      "36000/49000 loss: 0.2302104386040152\n",
      "38000/49000 loss: 0.2551523977246602\n",
      "40000/49000 loss: 0.31929895175396716\n",
      "42000/49000 loss: 0.22890724485557054\n",
      "44000/49000 loss: 0.34468685500709345\n",
      "46000/49000 loss: 0.2546506386728804\n",
      "48000/49000 loss: 0.3178516921346126\n",
      "epoch 48: valid acc = 0.887, new learning rate = 4.262879516715407e-05\n",
      "2000/49000 loss: 0.19621028120658512\n",
      "4000/49000 loss: 0.2501878430801002\n",
      "6000/49000 loss: 0.22910905564657524\n",
      "8000/49000 loss: 0.23484065723216654\n",
      "10000/49000 loss: 0.2772489503539621\n",
      "12000/49000 loss: 0.3312659422317836\n",
      "14000/49000 loss: 0.2353749923871971\n",
      "16000/49000 loss: 0.28227334435160223\n",
      "18000/49000 loss: 0.38500704888773796\n",
      "20000/49000 loss: 0.2187888042578977\n",
      "22000/49000 loss: 0.2601194832003176\n",
      "24000/49000 loss: 0.2631857139560305\n",
      "26000/49000 loss: 0.1872001618963298\n",
      "28000/49000 loss: 0.2615675444406436\n",
      "30000/49000 loss: 0.2934895551815255\n",
      "32000/49000 loss: 0.43294861484180647\n",
      "34000/49000 loss: 0.24245114330864248\n",
      "36000/49000 loss: 0.3333599233701828\n",
      "38000/49000 loss: 0.28671007988755\n",
      "40000/49000 loss: 0.23386623314654384\n",
      "42000/49000 loss: 0.31347456117024336\n",
      "44000/49000 loss: 0.2524691342894592\n",
      "46000/49000 loss: 0.25961240306649164\n",
      "48000/49000 loss: 0.26560272042100985\n",
      "epoch 49: valid acc = 0.887, new learning rate = 4.049735540879637e-05\n",
      "2000/49000 loss: 0.29627193740198404\n",
      "4000/49000 loss: 0.19873578965868552\n",
      "6000/49000 loss: 0.3092377143961412\n",
      "8000/49000 loss: 0.3747023866248004\n",
      "10000/49000 loss: 0.31325132322085575\n",
      "12000/49000 loss: 0.32493197660760925\n",
      "14000/49000 loss: 0.27666507368748605\n",
      "16000/49000 loss: 0.3061372955132022\n",
      "18000/49000 loss: 0.2812979090622862\n",
      "20000/49000 loss: 0.30721962661157537\n",
      "22000/49000 loss: 0.2854689442990961\n",
      "24000/49000 loss: 0.24356500790510094\n",
      "26000/49000 loss: 0.2746642649401585\n",
      "28000/49000 loss: 0.18939171138821376\n",
      "30000/49000 loss: 0.25757929745880087\n",
      "32000/49000 loss: 0.3392477692115111\n",
      "34000/49000 loss: 0.18334631991004927\n",
      "36000/49000 loss: 0.31006612739424244\n",
      "38000/49000 loss: 0.2701931612171187\n",
      "40000/49000 loss: 0.2305653294853726\n",
      "42000/49000 loss: 0.27313702978984933\n",
      "44000/49000 loss: 0.23063540086473228\n",
      "46000/49000 loss: 0.2747839344823036\n",
      "48000/49000 loss: 0.24217934944938324\n",
      "epoch 50: valid acc = 0.891, new learning rate = 3.847248763835655e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc: 0.900469387755102\n",
      "test acc: 0.891\n",
      "test acc: 0.8718\n",
      "number of batches for training: 245\n",
      "epoch 1: valid acc = 0.751, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.797, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.837, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.849, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.857, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.861, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.867, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.862, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.867, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.868, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.873, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.87, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.869, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.88, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.884, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.877, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.877, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.883, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.877, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.882, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.886, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.883, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.878, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.882, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.89, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.887, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.893, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.891, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.891, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.888, new learning rate = 0.00010731938197146858\n",
      "epoch 31: valid acc = 0.887, new learning rate = 0.00010195341287289515\n",
      "epoch 32: valid acc = 0.886, new learning rate = 9.685574222925039e-05\n",
      "epoch 33: valid acc = 0.889, new learning rate = 9.201295511778786e-05\n",
      "epoch 34: valid acc = 0.889, new learning rate = 8.741230736189846e-05\n",
      "epoch 35: valid acc = 0.893, new learning rate = 8.304169199380353e-05\n",
      "epoch 36: valid acc = 0.882, new learning rate = 7.888960739411335e-05\n",
      "epoch 37: valid acc = 0.884, new learning rate = 7.494512702440768e-05\n",
      "epoch 38: valid acc = 0.889, new learning rate = 7.119787067318729e-05\n",
      "epoch 39: valid acc = 0.89, new learning rate = 6.763797713952792e-05\n",
      "epoch 40: valid acc = 0.89, new learning rate = 6.425607828255152e-05\n",
      "epoch 41: valid acc = 0.887, new learning rate = 6.104327436842394e-05\n",
      "epoch 42: valid acc = 0.89, new learning rate = 5.799111065000274e-05\n",
      "epoch 43: valid acc = 0.889, new learning rate = 5.5091555117502596e-05\n",
      "epoch 44: valid acc = 0.885, new learning rate = 5.2336977361627463e-05\n",
      "epoch 45: valid acc = 0.886, new learning rate = 4.972012849354609e-05\n",
      "epoch 46: valid acc = 0.886, new learning rate = 4.723412206886878e-05\n",
      "epoch 47: valid acc = 0.891, new learning rate = 4.487241596542534e-05\n",
      "epoch 48: valid acc = 0.891, new learning rate = 4.262879516715407e-05\n",
      "epoch 49: valid acc = 0.888, new learning rate = 4.049735540879637e-05\n",
      "epoch 50: valid acc = 0.888, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.9001428571428571\n",
      "test acc: 0.888\n",
      "test acc: 0.8731\n",
      "number of batches for training: 81\n",
      "6000/49000 loss: 2.6626457731259934\n",
      "12000/49000 loss: 2.625369015826425\n",
      "18000/49000 loss: 2.5623370339267666\n",
      "24000/49000 loss: 2.441680282602166\n",
      "30000/49000 loss: 2.2242766475157905\n",
      "36000/49000 loss: 2.0298504344505277\n",
      "42000/49000 loss: 1.9674089289659211\n",
      "48000/49000 loss: 1.7705128321571306\n",
      "epoch 1: valid acc = 0.477, new learning rate = 0.000475\n",
      "6000/49000 loss: 1.4305280328502283\n",
      "12000/49000 loss: 1.3266510828183997\n",
      "18000/49000 loss: 1.2353227395927309\n",
      "24000/49000 loss: 1.1444106921301196\n",
      "30000/49000 loss: 1.1123935411010704\n",
      "36000/49000 loss: 1.0495486011572923\n",
      "42000/49000 loss: 1.023809126218561\n",
      "48000/49000 loss: 0.9737822721234891\n",
      "epoch 2: valid acc = 0.65, new learning rate = 0.00045125\n",
      "6000/49000 loss: 0.9971054234810263\n",
      "12000/49000 loss: 0.8992064173037051\n",
      "18000/49000 loss: 0.9160209101162843\n",
      "24000/49000 loss: 0.8535602155277385\n",
      "30000/49000 loss: 0.742991528826675\n",
      "36000/49000 loss: 0.7680036415135139\n",
      "42000/49000 loss: 0.7839152486419168\n",
      "48000/49000 loss: 0.6659698579201799\n",
      "epoch 3: valid acc = 0.745, new learning rate = 0.0004286875\n",
      "6000/49000 loss: 0.6782471826572509\n",
      "12000/49000 loss: 0.6436397932220829\n",
      "18000/49000 loss: 0.679372704144705\n",
      "24000/49000 loss: 0.6693115902210222\n",
      "30000/49000 loss: 0.6791080606056494\n",
      "36000/49000 loss: 0.6324364031210619\n",
      "42000/49000 loss: 0.7048605424302544\n",
      "48000/49000 loss: 0.6738374625971745\n",
      "epoch 4: valid acc = 0.773, new learning rate = 0.00040725312499999993\n",
      "6000/49000 loss: 0.614939282809562\n",
      "12000/49000 loss: 0.6487541507567367\n",
      "18000/49000 loss: 0.6346118830483496\n",
      "24000/49000 loss: 0.6614349492504121\n",
      "30000/49000 loss: 0.580099898465021\n",
      "36000/49000 loss: 0.6435053321065732\n",
      "42000/49000 loss: 0.5229570072601567\n",
      "48000/49000 loss: 0.5583308427912921\n",
      "epoch 5: valid acc = 0.779, new learning rate = 0.0003868904687499999\n",
      "6000/49000 loss: 0.5477661446514701\n",
      "12000/49000 loss: 0.5624840428517702\n",
      "18000/49000 loss: 0.5372036469525354\n",
      "24000/49000 loss: 0.5684086632002192\n",
      "30000/49000 loss: 0.5419853658683422\n",
      "36000/49000 loss: 0.5088971567028797\n",
      "42000/49000 loss: 0.5651883089464843\n",
      "48000/49000 loss: 0.5325344442472667\n",
      "epoch 6: valid acc = 0.796, new learning rate = 0.0003675459453124999\n",
      "6000/49000 loss: 0.5369132054607783\n",
      "12000/49000 loss: 0.5326280639923037\n",
      "18000/49000 loss: 0.5547984034402146\n",
      "24000/49000 loss: 0.4929081697844893\n",
      "30000/49000 loss: 0.563124687156575\n",
      "36000/49000 loss: 0.5005591269496341\n",
      "42000/49000 loss: 0.4786952089893888\n",
      "48000/49000 loss: 0.4640673271549088\n",
      "epoch 7: valid acc = 0.814, new learning rate = 0.00034916864804687486\n",
      "6000/49000 loss: 0.4550684196583008\n",
      "12000/49000 loss: 0.45923403726274603\n",
      "18000/49000 loss: 0.48362032407591843\n",
      "24000/49000 loss: 0.5158989637951622\n",
      "30000/49000 loss: 0.46935725916654863\n",
      "36000/49000 loss: 0.6090367800675408\n",
      "42000/49000 loss: 0.48766907471689425\n",
      "48000/49000 loss: 0.4720535500574808\n",
      "epoch 8: valid acc = 0.82, new learning rate = 0.0003317102156445311\n",
      "6000/49000 loss: 0.4871235262655073\n",
      "12000/49000 loss: 0.5236454156528186\n",
      "18000/49000 loss: 0.4475779672769818\n",
      "24000/49000 loss: 0.4623261378509121\n",
      "30000/49000 loss: 0.49791541148342167\n",
      "36000/49000 loss: 0.5252112120514708\n",
      "42000/49000 loss: 0.4341019270265325\n",
      "48000/49000 loss: 0.4410406130605939\n",
      "epoch 9: valid acc = 0.827, new learning rate = 0.0003151247048623045\n",
      "6000/49000 loss: 0.5070666162326517\n",
      "12000/49000 loss: 0.4873120553836539\n",
      "18000/49000 loss: 0.49951724270315023\n",
      "24000/49000 loss: 0.4517062198297244\n",
      "30000/49000 loss: 0.4392526681832518\n",
      "36000/49000 loss: 0.5652585420962759\n",
      "42000/49000 loss: 0.50298092442708\n",
      "48000/49000 loss: 0.4723844148091376\n",
      "epoch 10: valid acc = 0.831, new learning rate = 0.00029936846961918924\n",
      "6000/49000 loss: 0.42401562897344214\n",
      "12000/49000 loss: 0.46955543540660366\n",
      "18000/49000 loss: 0.526307834360259\n",
      "24000/49000 loss: 0.46495222706687034\n",
      "30000/49000 loss: 0.44829887942093194\n",
      "36000/49000 loss: 0.46067290527333554\n",
      "42000/49000 loss: 0.47422863868640475\n",
      "48000/49000 loss: 0.4236060748755785\n",
      "epoch 11: valid acc = 0.836, new learning rate = 0.00028440004613822977\n",
      "6000/49000 loss: 0.45388699121398485\n",
      "12000/49000 loss: 0.43702414760315433\n",
      "18000/49000 loss: 0.44190589656803464\n",
      "24000/49000 loss: 0.47401867141624093\n",
      "30000/49000 loss: 0.4528659742696202\n",
      "36000/49000 loss: 0.4359359198929023\n",
      "42000/49000 loss: 0.43233706857667065\n",
      "48000/49000 loss: 0.448220906400426\n",
      "epoch 12: valid acc = 0.839, new learning rate = 0.00027018004383131826\n",
      "6000/49000 loss: 0.41231470559164546\n",
      "12000/49000 loss: 0.3851423181292719\n",
      "18000/49000 loss: 0.4971846102871263\n",
      "24000/49000 loss: 0.4557903275651552\n",
      "30000/49000 loss: 0.4328120080510995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36000/49000 loss: 0.44292817260621453\n",
      "42000/49000 loss: 0.4497532601990827\n",
      "48000/49000 loss: 0.4053513291497169\n",
      "epoch 13: valid acc = 0.839, new learning rate = 0.00025667104163975234\n",
      "6000/49000 loss: 0.446417875103121\n",
      "12000/49000 loss: 0.4057705836417293\n",
      "18000/49000 loss: 0.3633988623134092\n",
      "24000/49000 loss: 0.4676532855419033\n",
      "30000/49000 loss: 0.41201066349557797\n",
      "36000/49000 loss: 0.43979726986540174\n",
      "42000/49000 loss: 0.419505516140252\n",
      "48000/49000 loss: 0.38059714347657003\n",
      "epoch 14: valid acc = 0.839, new learning rate = 0.00024383748955776472\n",
      "6000/49000 loss: 0.46615467421943974\n",
      "12000/49000 loss: 0.38975063156616796\n",
      "18000/49000 loss: 0.3689648703965089\n",
      "24000/49000 loss: 0.450672056502971\n",
      "30000/49000 loss: 0.41928558712906283\n",
      "36000/49000 loss: 0.44536761419017334\n",
      "42000/49000 loss: 0.461622005467273\n",
      "48000/49000 loss: 0.4137744221216604\n",
      "epoch 15: valid acc = 0.842, new learning rate = 0.00023164561507987649\n",
      "6000/49000 loss: 0.37233929841027585\n",
      "12000/49000 loss: 0.4940014179748412\n",
      "18000/49000 loss: 0.48613982261169686\n",
      "24000/49000 loss: 0.4531613687241619\n",
      "30000/49000 loss: 0.40579859708618093\n",
      "36000/49000 loss: 0.4470348365805681\n",
      "42000/49000 loss: 0.41350480200526185\n",
      "48000/49000 loss: 0.4577382796518711\n",
      "epoch 16: valid acc = 0.852, new learning rate = 0.00022006333432588265\n",
      "6000/49000 loss: 0.45260078633034617\n",
      "12000/49000 loss: 0.4434539100530856\n",
      "18000/49000 loss: 0.4435870277977188\n",
      "24000/49000 loss: 0.42401311638119826\n",
      "30000/49000 loss: 0.4383417812441642\n",
      "36000/49000 loss: 0.39991603919805224\n",
      "42000/49000 loss: 0.4309899480293194\n",
      "48000/49000 loss: 0.4011024079962549\n",
      "epoch 17: valid acc = 0.842, new learning rate = 0.00020906016760958852\n",
      "6000/49000 loss: 0.42598453090252014\n",
      "12000/49000 loss: 0.39574412063889586\n",
      "18000/49000 loss: 0.3712649361467758\n",
      "24000/49000 loss: 0.37108770075021486\n",
      "30000/49000 loss: 0.4134816399646239\n",
      "36000/49000 loss: 0.4090189378485319\n",
      "42000/49000 loss: 0.4395853111374642\n",
      "48000/49000 loss: 0.42375576812634647\n",
      "epoch 18: valid acc = 0.853, new learning rate = 0.00019860715922910907\n",
      "6000/49000 loss: 0.3970539623462267\n",
      "12000/49000 loss: 0.3958433310654678\n",
      "18000/49000 loss: 0.3799552860829056\n",
      "24000/49000 loss: 0.40548992600565237\n",
      "30000/49000 loss: 0.4132501219556554\n",
      "36000/49000 loss: 0.4051259085832147\n",
      "42000/49000 loss: 0.43010386188139543\n",
      "48000/49000 loss: 0.3538725668824415\n",
      "epoch 19: valid acc = 0.857, new learning rate = 0.0001886768012676536\n",
      "6000/49000 loss: 0.37440276208249273\n",
      "12000/49000 loss: 0.4689081343293058\n",
      "18000/49000 loss: 0.39126436504856327\n",
      "24000/49000 loss: 0.4029063568061703\n",
      "30000/49000 loss: 0.3869923256458833\n",
      "36000/49000 loss: 0.4473815609362883\n",
      "42000/49000 loss: 0.39299187486910536\n",
      "48000/49000 loss: 0.4022312106799022\n",
      "epoch 20: valid acc = 0.856, new learning rate = 0.0001792429612042709\n",
      "6000/49000 loss: 0.3765975298277153\n",
      "12000/49000 loss: 0.43487204906699445\n",
      "18000/49000 loss: 0.39764227210452546\n",
      "24000/49000 loss: 0.40752173751742493\n",
      "30000/49000 loss: 0.3576841915247565\n",
      "36000/49000 loss: 0.37350321273377984\n",
      "42000/49000 loss: 0.4237433960617729\n",
      "48000/49000 loss: 0.4294161945466391\n",
      "epoch 21: valid acc = 0.859, new learning rate = 0.00017028081314405735\n",
      "6000/49000 loss: 0.4163203661237152\n",
      "12000/49000 loss: 0.41443804906672743\n",
      "18000/49000 loss: 0.4228644785800822\n",
      "24000/49000 loss: 0.42792143657577836\n",
      "30000/49000 loss: 0.43004040312058445\n",
      "36000/49000 loss: 0.4126799463643906\n",
      "42000/49000 loss: 0.41259336641443795\n",
      "48000/49000 loss: 0.39934982059759405\n",
      "epoch 22: valid acc = 0.859, new learning rate = 0.00016176677248685447\n",
      "6000/49000 loss: 0.3950418745851771\n",
      "12000/49000 loss: 0.36311339758743144\n",
      "18000/49000 loss: 0.4120532358945266\n",
      "24000/49000 loss: 0.410460303551689\n",
      "30000/49000 loss: 0.3886048133281961\n",
      "36000/49000 loss: 0.3940547128441719\n",
      "42000/49000 loss: 0.4292519667912605\n",
      "48000/49000 loss: 0.39380973040489\n",
      "epoch 23: valid acc = 0.859, new learning rate = 0.00015367843386251173\n",
      "6000/49000 loss: 0.39320529815561206\n",
      "12000/49000 loss: 0.41668703695178444\n",
      "18000/49000 loss: 0.4539186702160376\n",
      "24000/49000 loss: 0.3966284297499336\n",
      "30000/49000 loss: 0.4723449621227048\n",
      "36000/49000 loss: 0.3977870051335865\n",
      "42000/49000 loss: 0.4124255424110957\n",
      "48000/49000 loss: 0.3656498318660007\n",
      "epoch 24: valid acc = 0.857, new learning rate = 0.00014599451216938612\n",
      "6000/49000 loss: 0.44771156401411266\n",
      "12000/49000 loss: 0.4211801374142343\n",
      "18000/49000 loss: 0.4012419152793181\n",
      "24000/49000 loss: 0.4174762281479593\n",
      "30000/49000 loss: 0.4270348757889929\n",
      "36000/49000 loss: 0.4238994986968948\n",
      "42000/49000 loss: 0.38772807007368554\n",
      "48000/49000 loss: 0.3705146293161941\n",
      "epoch 25: valid acc = 0.857, new learning rate = 0.00013869478656091682\n",
      "6000/49000 loss: 0.4308918371223976\n",
      "12000/49000 loss: 0.3785945079987459\n",
      "18000/49000 loss: 0.42912801100577863\n",
      "24000/49000 loss: 0.3594376570978774\n",
      "30000/49000 loss: 0.35301946274175044\n",
      "36000/49000 loss: 0.3987295354534618\n",
      "42000/49000 loss: 0.5111003324246908\n",
      "48000/49000 loss: 0.3945367361217549\n",
      "epoch 26: valid acc = 0.862, new learning rate = 0.00013176004723287096\n",
      "6000/49000 loss: 0.3759001392707163\n",
      "12000/49000 loss: 0.35959432324273394\n",
      "18000/49000 loss: 0.4725074949604461\n",
      "24000/49000 loss: 0.37687572546974796\n",
      "30000/49000 loss: 0.3814413004592855\n",
      "36000/49000 loss: 0.4048330044972657\n",
      "42000/49000 loss: 0.4061117096516719\n",
      "48000/49000 loss: 0.42086059145449384\n",
      "epoch 27: valid acc = 0.862, new learning rate = 0.0001251720448712274\n",
      "6000/49000 loss: 0.3976192983362913\n",
      "12000/49000 loss: 0.43327481550712965\n",
      "18000/49000 loss: 0.46352585652605016\n",
      "24000/49000 loss: 0.41108822327257927\n",
      "30000/49000 loss: 0.3865540948449597\n",
      "36000/49000 loss: 0.4519584236768973\n",
      "42000/49000 loss: 0.3677380742881748\n",
      "48000/49000 loss: 0.3912855267553616\n",
      "epoch 28: valid acc = 0.862, new learning rate = 0.00011891344262766602\n",
      "6000/49000 loss: 0.41080246333862724\n",
      "12000/49000 loss: 0.39676145172937666\n",
      "18000/49000 loss: 0.3493981093584505\n",
      "24000/49000 loss: 0.40504559079398833\n",
      "30000/49000 loss: 0.40760776060575415\n",
      "36000/49000 loss: 0.3862567813022463\n",
      "42000/49000 loss: 0.3670926443220593\n",
      "48000/49000 loss: 0.4477115079863273\n",
      "epoch 29: valid acc = 0.859, new learning rate = 0.00011296777049628272\n",
      "6000/49000 loss: 0.38865093309281223\n",
      "12000/49000 loss: 0.38635064087193843\n",
      "18000/49000 loss: 0.3754529762186067\n",
      "24000/49000 loss: 0.44828981687052327\n",
      "30000/49000 loss: 0.42431147613754205\n",
      "36000/49000 loss: 0.3758753018018338\n",
      "42000/49000 loss: 0.3912425740156238\n",
      "48000/49000 loss: 0.35745654597487025\n",
      "epoch 30: valid acc = 0.86, new learning rate = 0.00010731938197146858\n",
      "6000/49000 loss: 0.3345286526208303\n",
      "12000/49000 loss: 0.42217802669927923\n",
      "18000/49000 loss: 0.36094969281295064\n",
      "24000/49000 loss: 0.40303199162074876\n",
      "30000/49000 loss: 0.41890337588852483\n",
      "36000/49000 loss: 0.41406035315021167\n",
      "42000/49000 loss: 0.4293964704618126\n",
      "48000/49000 loss: 0.41787763888152096\n",
      "epoch 31: valid acc = 0.867, new learning rate = 0.00010195341287289515\n",
      "6000/49000 loss: 0.3727538823265448\n",
      "12000/49000 loss: 0.4114188275819669\n",
      "18000/49000 loss: 0.39699515741320895\n",
      "24000/49000 loss: 0.38074890456966565\n",
      "30000/49000 loss: 0.3636205017054745\n",
      "36000/49000 loss: 0.3383852356557923\n",
      "42000/49000 loss: 0.3848087849928617\n",
      "48000/49000 loss: 0.3354818594000786\n",
      "epoch 32: valid acc = 0.865, new learning rate = 9.685574222925039e-05\n",
      "6000/49000 loss: 0.361367862347992\n",
      "12000/49000 loss: 0.3367256467296455\n",
      "18000/49000 loss: 0.4114338732640467\n",
      "24000/49000 loss: 0.3730175098479104\n",
      "30000/49000 loss: 0.36550729062704646\n",
      "36000/49000 loss: 0.3128129583826001\n",
      "42000/49000 loss: 0.3528366239092854\n",
      "48000/49000 loss: 0.416057716334011\n",
      "epoch 33: valid acc = 0.864, new learning rate = 9.201295511778786e-05\n",
      "6000/49000 loss: 0.3713729914861904\n",
      "12000/49000 loss: 0.3333001156704915\n",
      "18000/49000 loss: 0.39694851733550407\n",
      "24000/49000 loss: 0.4155955700392426\n",
      "30000/49000 loss: 0.39196846171367977\n",
      "36000/49000 loss: 0.38727290930245484\n",
      "42000/49000 loss: 0.3808644043916606\n",
      "48000/49000 loss: 0.42031299888126145\n",
      "epoch 34: valid acc = 0.866, new learning rate = 8.741230736189846e-05\n",
      "6000/49000 loss: 0.3527745207544855\n",
      "12000/49000 loss: 0.37600096692925433\n",
      "18000/49000 loss: 0.41252215118628033\n",
      "24000/49000 loss: 0.3597301309514182\n",
      "30000/49000 loss: 0.3886526652512862\n",
      "36000/49000 loss: 0.3414985310627503\n",
      "42000/49000 loss: 0.36976831699894114\n",
      "48000/49000 loss: 0.3893445818051218\n",
      "epoch 35: valid acc = 0.866, new learning rate = 8.304169199380353e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/49000 loss: 0.4003962496630371\n",
      "12000/49000 loss: 0.389587812014767\n",
      "18000/49000 loss: 0.3693290771918241\n",
      "24000/49000 loss: 0.41393786879002004\n",
      "30000/49000 loss: 0.32095232770792653\n",
      "36000/49000 loss: 0.40916055097505943\n",
      "42000/49000 loss: 0.40145828768532027\n",
      "48000/49000 loss: 0.371071667746415\n",
      "epoch 36: valid acc = 0.868, new learning rate = 7.888960739411335e-05\n",
      "6000/49000 loss: 0.47753905907416394\n",
      "12000/49000 loss: 0.35785167956536856\n",
      "18000/49000 loss: 0.4604783151156041\n",
      "24000/49000 loss: 0.35847012449386495\n",
      "30000/49000 loss: 0.35974881319585594\n",
      "36000/49000 loss: 0.3790096092886033\n",
      "42000/49000 loss: 0.4193953635034241\n",
      "48000/49000 loss: 0.4179557061343436\n",
      "epoch 37: valid acc = 0.868, new learning rate = 7.494512702440768e-05\n",
      "6000/49000 loss: 0.39968669423960623\n",
      "12000/49000 loss: 0.399794912414056\n",
      "18000/49000 loss: 0.4151894253760264\n",
      "24000/49000 loss: 0.3992378810690752\n",
      "30000/49000 loss: 0.33423891876189676\n",
      "36000/49000 loss: 0.35461574945781993\n",
      "42000/49000 loss: 0.37652676293487175\n",
      "48000/49000 loss: 0.362546404054017\n",
      "epoch 38: valid acc = 0.865, new learning rate = 7.119787067318729e-05\n",
      "6000/49000 loss: 0.4220956277191413\n",
      "12000/49000 loss: 0.4127662667541946\n",
      "18000/49000 loss: 0.4335025719993769\n",
      "24000/49000 loss: 0.3708562778419188\n",
      "30000/49000 loss: 0.4241172085326391\n",
      "36000/49000 loss: 0.3693512156145866\n",
      "42000/49000 loss: 0.36068988680081515\n",
      "48000/49000 loss: 0.37399867574416795\n",
      "epoch 39: valid acc = 0.866, new learning rate = 6.763797713952792e-05\n",
      "6000/49000 loss: 0.39082079784173285\n",
      "12000/49000 loss: 0.38328985166543866\n",
      "18000/49000 loss: 0.3952073318286943\n",
      "24000/49000 loss: 0.3998591621886369\n",
      "30000/49000 loss: 0.37264427790454824\n",
      "36000/49000 loss: 0.39215268599858155\n",
      "42000/49000 loss: 0.39003422658868936\n",
      "48000/49000 loss: 0.35193890579225634\n",
      "epoch 40: valid acc = 0.867, new learning rate = 6.425607828255152e-05\n",
      "6000/49000 loss: 0.38011681935865377\n",
      "12000/49000 loss: 0.41807242304688036\n",
      "18000/49000 loss: 0.3517640204563547\n",
      "24000/49000 loss: 0.29569679188553893\n",
      "30000/49000 loss: 0.39040737117553787\n",
      "36000/49000 loss: 0.42685479942275906\n",
      "42000/49000 loss: 0.4013102926251864\n",
      "48000/49000 loss: 0.3930360720861125\n",
      "epoch 41: valid acc = 0.867, new learning rate = 6.104327436842394e-05\n",
      "6000/49000 loss: 0.4293232575064951\n",
      "12000/49000 loss: 0.430940119548914\n",
      "18000/49000 loss: 0.36245943918976825\n",
      "24000/49000 loss: 0.40336148380882775\n",
      "30000/49000 loss: 0.3070164185015719\n",
      "36000/49000 loss: 0.39631110857801505\n",
      "42000/49000 loss: 0.3688042149923402\n",
      "48000/49000 loss: 0.3490748509890034\n",
      "epoch 42: valid acc = 0.867, new learning rate = 5.799111065000274e-05\n",
      "6000/49000 loss: 0.38194250192503376\n",
      "12000/49000 loss: 0.4560871619946099\n",
      "18000/49000 loss: 0.3782696523154046\n",
      "24000/49000 loss: 0.4185396032202859\n",
      "30000/49000 loss: 0.4406538907734747\n",
      "36000/49000 loss: 0.35578026873202806\n",
      "42000/49000 loss: 0.40558673318694877\n",
      "48000/49000 loss: 0.3648498612171007\n",
      "epoch 43: valid acc = 0.867, new learning rate = 5.5091555117502596e-05\n",
      "6000/49000 loss: 0.39185107812797215\n",
      "12000/49000 loss: 0.3626953372387637\n",
      "18000/49000 loss: 0.3999222041909984\n",
      "24000/49000 loss: 0.4194712888888081\n",
      "30000/49000 loss: 0.34669105679258877\n",
      "36000/49000 loss: 0.3323540480525558\n",
      "42000/49000 loss: 0.41825150895498064\n",
      "48000/49000 loss: 0.3766426318646668\n",
      "epoch 44: valid acc = 0.868, new learning rate = 5.2336977361627463e-05\n",
      "6000/49000 loss: 0.3524692257965085\n",
      "12000/49000 loss: 0.3715786144629369\n",
      "18000/49000 loss: 0.38958465766688166\n",
      "24000/49000 loss: 0.4235672109861715\n",
      "30000/49000 loss: 0.4355540816500405\n",
      "36000/49000 loss: 0.41135847547217075\n",
      "42000/49000 loss: 0.40431511227465533\n",
      "48000/49000 loss: 0.4163823364856383\n",
      "epoch 45: valid acc = 0.867, new learning rate = 4.972012849354609e-05\n",
      "6000/49000 loss: 0.3745723684941791\n",
      "12000/49000 loss: 0.3483717298710956\n",
      "18000/49000 loss: 0.373794939952768\n",
      "24000/49000 loss: 0.35603664627917075\n",
      "30000/49000 loss: 0.3877624524356524\n",
      "36000/49000 loss: 0.43273864199435025\n",
      "42000/49000 loss: 0.39126880085335675\n",
      "48000/49000 loss: 0.3878298143343758\n",
      "epoch 46: valid acc = 0.865, new learning rate = 4.723412206886878e-05\n",
      "6000/49000 loss: 0.3450914845778317\n",
      "12000/49000 loss: 0.355982617470197\n",
      "18000/49000 loss: 0.3837018664829956\n",
      "24000/49000 loss: 0.37277733419316084\n",
      "30000/49000 loss: 0.4036011917076751\n",
      "36000/49000 loss: 0.3615746898280739\n",
      "42000/49000 loss: 0.3717148100881563\n",
      "48000/49000 loss: 0.39423853823936894\n",
      "epoch 47: valid acc = 0.867, new learning rate = 4.487241596542534e-05\n",
      "6000/49000 loss: 0.3892668867621398\n",
      "12000/49000 loss: 0.3965182418466754\n",
      "18000/49000 loss: 0.4136717792937041\n",
      "24000/49000 loss: 0.3417435533801309\n",
      "30000/49000 loss: 0.35607680540682457\n",
      "36000/49000 loss: 0.327983003893062\n",
      "42000/49000 loss: 0.4147461052192888\n",
      "48000/49000 loss: 0.34331970439756165\n",
      "epoch 48: valid acc = 0.867, new learning rate = 4.262879516715407e-05\n",
      "6000/49000 loss: 0.36175436378772974\n",
      "12000/49000 loss: 0.4024240014246154\n",
      "18000/49000 loss: 0.4024835770750348\n",
      "24000/49000 loss: 0.35054222483872743\n",
      "30000/49000 loss: 0.35095615545827186\n",
      "36000/49000 loss: 0.36787571080463055\n",
      "42000/49000 loss: 0.3711395964346886\n",
      "48000/49000 loss: 0.37739449747895215\n",
      "epoch 49: valid acc = 0.869, new learning rate = 4.049735540879637e-05\n",
      "6000/49000 loss: 0.36752122214746874\n",
      "12000/49000 loss: 0.3563527527322481\n",
      "18000/49000 loss: 0.38389507179958815\n",
      "24000/49000 loss: 0.3307537332332414\n",
      "30000/49000 loss: 0.35912832870730177\n",
      "36000/49000 loss: 0.4388753808842646\n",
      "42000/49000 loss: 0.38016912797379443\n",
      "48000/49000 loss: 0.43926354762732644\n",
      "epoch 50: valid acc = 0.867, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.8681020408163266\n",
      "test acc: 0.867\n",
      "test acc: 0.8469\n",
      "number of batches for training: 81\n",
      "epoch 1: valid acc = 0.417, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.668, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.735, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.764, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.778, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.796, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.812, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.819, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.828, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.83, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.829, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.831, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.837, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.846, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.845, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.85, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.849, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.851, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.852, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.858, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.856, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.854, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.856, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.855, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.857, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.858, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.861, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.856, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.861, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.856, new learning rate = 0.00010731938197146858\n",
      "epoch 31: valid acc = 0.857, new learning rate = 0.00010195341287289515\n",
      "epoch 32: valid acc = 0.858, new learning rate = 9.685574222925039e-05\n",
      "epoch 33: valid acc = 0.859, new learning rate = 9.201295511778786e-05\n",
      "epoch 34: valid acc = 0.862, new learning rate = 8.741230736189846e-05\n",
      "epoch 35: valid acc = 0.861, new learning rate = 8.304169199380353e-05\n",
      "epoch 36: valid acc = 0.864, new learning rate = 7.888960739411335e-05\n",
      "epoch 37: valid acc = 0.861, new learning rate = 7.494512702440768e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 38: valid acc = 0.863, new learning rate = 7.119787067318729e-05\n",
      "epoch 39: valid acc = 0.862, new learning rate = 6.763797713952792e-05\n",
      "epoch 40: valid acc = 0.863, new learning rate = 6.425607828255152e-05\n",
      "epoch 41: valid acc = 0.861, new learning rate = 6.104327436842394e-05\n",
      "epoch 42: valid acc = 0.861, new learning rate = 5.799111065000274e-05\n",
      "epoch 43: valid acc = 0.863, new learning rate = 5.5091555117502596e-05\n",
      "epoch 44: valid acc = 0.864, new learning rate = 5.2336977361627463e-05\n",
      "epoch 45: valid acc = 0.86, new learning rate = 4.972012849354609e-05\n",
      "epoch 46: valid acc = 0.861, new learning rate = 4.723412206886878e-05\n",
      "epoch 47: valid acc = 0.863, new learning rate = 4.487241596542534e-05\n",
      "epoch 48: valid acc = 0.862, new learning rate = 4.262879516715407e-05\n",
      "epoch 49: valid acc = 0.863, new learning rate = 4.049735540879637e-05\n",
      "epoch 50: valid acc = 0.862, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.8671632653061224\n",
      "test acc: 0.862\n",
      "test acc: 0.8478\n",
      "number of batches for training: 81\n",
      "6000/49000 loss: 2.6287536986241165\n",
      "12000/49000 loss: 2.584707512972823\n",
      "18000/49000 loss: 2.5478992643942253\n",
      "24000/49000 loss: 2.4658005781036665\n",
      "30000/49000 loss: 2.3093952807791087\n",
      "36000/49000 loss: 2.049790355402418\n",
      "42000/49000 loss: 2.0264539877660352\n",
      "48000/49000 loss: 1.8354161010211292\n",
      "epoch 1: valid acc = 0.47, new learning rate = 0.000475\n",
      "6000/49000 loss: 1.4687260079216837\n",
      "12000/49000 loss: 1.2794400496652483\n",
      "18000/49000 loss: 1.194709381142288\n",
      "24000/49000 loss: 1.1569998893448652\n",
      "30000/49000 loss: 1.0869292301051543\n",
      "36000/49000 loss: 1.0673941921254901\n",
      "42000/49000 loss: 1.0803745676195966\n",
      "48000/49000 loss: 1.027163525460196\n",
      "epoch 2: valid acc = 0.656, new learning rate = 0.00045125\n",
      "6000/49000 loss: 0.8905575831347676\n",
      "12000/49000 loss: 0.9873839363379113\n",
      "18000/49000 loss: 0.9457391510846079\n",
      "24000/49000 loss: 0.8439906021503978\n",
      "30000/49000 loss: 0.8345443022159674\n",
      "36000/49000 loss: 0.8201975884067266\n",
      "42000/49000 loss: 0.8179350764960958\n",
      "48000/49000 loss: 0.7121316432723185\n",
      "epoch 3: valid acc = 0.732, new learning rate = 0.0004286875\n",
      "6000/49000 loss: 0.6919325982227875\n",
      "12000/49000 loss: 0.6546970306309607\n",
      "18000/49000 loss: 0.7059698786161847\n",
      "24000/49000 loss: 0.6958791204281841\n",
      "30000/49000 loss: 0.6655723419262944\n",
      "36000/49000 loss: 0.5986421441586306\n",
      "42000/49000 loss: 0.613437992080277\n",
      "48000/49000 loss: 0.7128848846510957\n",
      "epoch 4: valid acc = 0.766, new learning rate = 0.00040725312499999993\n",
      "6000/49000 loss: 0.5291768441434317\n",
      "12000/49000 loss: 0.6206333700719509\n",
      "18000/49000 loss: 0.6637726663477527\n",
      "24000/49000 loss: 0.6590766995742161\n",
      "30000/49000 loss: 0.6123294334896676\n",
      "36000/49000 loss: 0.5347360568834113\n",
      "42000/49000 loss: 0.5987408982626226\n",
      "48000/49000 loss: 0.5763825293878697\n",
      "epoch 5: valid acc = 0.778, new learning rate = 0.0003868904687499999\n",
      "6000/49000 loss: 0.5609745031676248\n",
      "12000/49000 loss: 0.584971783307096\n",
      "18000/49000 loss: 0.563782099912952\n",
      "24000/49000 loss: 0.54894545916311\n",
      "30000/49000 loss: 0.5722490491375525\n",
      "36000/49000 loss: 0.5465115863715587\n",
      "42000/49000 loss: 0.4640958500268542\n",
      "48000/49000 loss: 0.5532675372381651\n",
      "epoch 6: valid acc = 0.801, new learning rate = 0.0003675459453124999\n",
      "6000/49000 loss: 0.5639340648374872\n",
      "12000/49000 loss: 0.5287946193791171\n",
      "18000/49000 loss: 0.5462655099052384\n",
      "24000/49000 loss: 0.5050959836484143\n",
      "30000/49000 loss: 0.5697503899621337\n",
      "36000/49000 loss: 0.5145616609543975\n",
      "42000/49000 loss: 0.47005926982330726\n",
      "48000/49000 loss: 0.46920475171274095\n",
      "epoch 7: valid acc = 0.812, new learning rate = 0.00034916864804687486\n",
      "6000/49000 loss: 0.5092249062898045\n",
      "12000/49000 loss: 0.49953214137178187\n",
      "18000/49000 loss: 0.4371022709942038\n",
      "24000/49000 loss: 0.5282160787040575\n",
      "30000/49000 loss: 0.5121730575929793\n",
      "36000/49000 loss: 0.5723426144169571\n",
      "42000/49000 loss: 0.46829684709509645\n",
      "48000/49000 loss: 0.4241772490234229\n",
      "epoch 8: valid acc = 0.818, new learning rate = 0.0003317102156445311\n",
      "6000/49000 loss: 0.46216096647098753\n",
      "12000/49000 loss: 0.43644528879502503\n",
      "18000/49000 loss: 0.49982834024548317\n",
      "24000/49000 loss: 0.4904662910697842\n",
      "30000/49000 loss: 0.5525813859072559\n",
      "36000/49000 loss: 0.47427186190537857\n",
      "42000/49000 loss: 0.5336722859938599\n",
      "48000/49000 loss: 0.4557259617001351\n",
      "epoch 9: valid acc = 0.825, new learning rate = 0.0003151247048623045\n",
      "6000/49000 loss: 0.4552412014808841\n",
      "12000/49000 loss: 0.4687666388106098\n",
      "18000/49000 loss: 0.4828559399961739\n",
      "24000/49000 loss: 0.49544023424576766\n",
      "30000/49000 loss: 0.45949903916993445\n",
      "36000/49000 loss: 0.43137739254927077\n",
      "42000/49000 loss: 0.5344794932996317\n",
      "48000/49000 loss: 0.489735903949124\n",
      "epoch 10: valid acc = 0.828, new learning rate = 0.00029936846961918924\n",
      "6000/49000 loss: 0.5422019278739235\n",
      "12000/49000 loss: 0.4910277965464576\n",
      "18000/49000 loss: 0.42222076373995704\n",
      "24000/49000 loss: 0.424652275933318\n",
      "30000/49000 loss: 0.4489269616523902\n",
      "36000/49000 loss: 0.428388168125227\n",
      "42000/49000 loss: 0.4187137370627732\n",
      "48000/49000 loss: 0.4315652234086297\n",
      "epoch 11: valid acc = 0.833, new learning rate = 0.00028440004613822977\n",
      "6000/49000 loss: 0.5046465750102666\n",
      "12000/49000 loss: 0.4447550539402208\n",
      "18000/49000 loss: 0.45015805229074857\n",
      "24000/49000 loss: 0.46653583644945945\n",
      "30000/49000 loss: 0.47655374931692984\n",
      "36000/49000 loss: 0.4798521889914321\n",
      "42000/49000 loss: 0.4225612109524784\n",
      "48000/49000 loss: 0.4496874522467527\n",
      "epoch 12: valid acc = 0.838, new learning rate = 0.00027018004383131826\n",
      "6000/49000 loss: 0.49360044928755836\n",
      "12000/49000 loss: 0.4686934182317082\n",
      "18000/49000 loss: 0.4275717773144449\n",
      "24000/49000 loss: 0.4176943725184331\n",
      "30000/49000 loss: 0.38760744736901775\n",
      "36000/49000 loss: 0.48650620198994904\n",
      "42000/49000 loss: 0.4502462418412716\n",
      "48000/49000 loss: 0.41192145041145856\n",
      "epoch 13: valid acc = 0.838, new learning rate = 0.00025667104163975234\n",
      "6000/49000 loss: 0.4189235333755387\n",
      "12000/49000 loss: 0.48607498543017713\n",
      "18000/49000 loss: 0.4503271698930907\n",
      "24000/49000 loss: 0.4121407949079394\n",
      "30000/49000 loss: 0.4674532503579558\n",
      "36000/49000 loss: 0.40640309305723277\n",
      "42000/49000 loss: 0.4148467169587802\n",
      "48000/49000 loss: 0.368632325367253\n",
      "epoch 14: valid acc = 0.844, new learning rate = 0.00024383748955776472\n",
      "6000/49000 loss: 0.444956868219749\n",
      "12000/49000 loss: 0.4521014152113977\n",
      "18000/49000 loss: 0.38775820686478835\n",
      "24000/49000 loss: 0.4256299497814494\n",
      "30000/49000 loss: 0.43095275653744974\n",
      "36000/49000 loss: 0.4763559126226939\n",
      "42000/49000 loss: 0.46151470005006173\n",
      "48000/49000 loss: 0.4146728228825038\n",
      "epoch 15: valid acc = 0.843, new learning rate = 0.00023164561507987649\n",
      "6000/49000 loss: 0.4368859991354274\n",
      "12000/49000 loss: 0.4810921949997572\n",
      "18000/49000 loss: 0.4469572107197782\n",
      "24000/49000 loss: 0.41553614830619656\n",
      "30000/49000 loss: 0.4279349751418116\n",
      "36000/49000 loss: 0.47768691486211134\n",
      "42000/49000 loss: 0.38591073017847477\n",
      "48000/49000 loss: 0.4389712118620623\n",
      "epoch 16: valid acc = 0.839, new learning rate = 0.00022006333432588265\n",
      "6000/49000 loss: 0.39323650125017906\n",
      "12000/49000 loss: 0.38488224962162937\n",
      "18000/49000 loss: 0.40812016293904213\n",
      "24000/49000 loss: 0.4470552110141453\n",
      "30000/49000 loss: 0.4304031761375674\n",
      "36000/49000 loss: 0.4104877488497559\n",
      "42000/49000 loss: 0.41417434125734304\n",
      "48000/49000 loss: 0.37497653674109915\n",
      "epoch 17: valid acc = 0.847, new learning rate = 0.00020906016760958852\n",
      "6000/49000 loss: 0.4231326649591987\n",
      "12000/49000 loss: 0.3864500724225771\n",
      "18000/49000 loss: 0.44613300855645194\n",
      "24000/49000 loss: 0.41841901784536156\n",
      "30000/49000 loss: 0.4233741905666381\n",
      "36000/49000 loss: 0.3839301834279847\n",
      "42000/49000 loss: 0.4275178272742893\n",
      "48000/49000 loss: 0.4275116937976254\n",
      "epoch 18: valid acc = 0.848, new learning rate = 0.00019860715922910907\n",
      "6000/49000 loss: 0.4114108261188341\n",
      "12000/49000 loss: 0.3601500809678185\n",
      "18000/49000 loss: 0.43022902176700817\n",
      "24000/49000 loss: 0.40190524375754483\n",
      "30000/49000 loss: 0.44940499151090796\n",
      "36000/49000 loss: 0.469109052840788\n",
      "42000/49000 loss: 0.42553200624462223\n",
      "48000/49000 loss: 0.41014463658934824\n",
      "epoch 19: valid acc = 0.85, new learning rate = 0.0001886768012676536\n",
      "6000/49000 loss: 0.39646469884996904\n",
      "12000/49000 loss: 0.40044058050553916\n",
      "18000/49000 loss: 0.43666399213474744\n",
      "24000/49000 loss: 0.3823007226865532\n",
      "30000/49000 loss: 0.3882577233474889\n",
      "36000/49000 loss: 0.40871367474616066\n",
      "42000/49000 loss: 0.42057667130143184\n",
      "48000/49000 loss: 0.4228274100775206\n",
      "epoch 20: valid acc = 0.851, new learning rate = 0.0001792429612042709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/49000 loss: 0.4197895363040935\n",
      "12000/49000 loss: 0.4279580494640189\n",
      "18000/49000 loss: 0.37693240023876057\n",
      "24000/49000 loss: 0.4305959101243169\n",
      "30000/49000 loss: 0.4071103343481227\n",
      "36000/49000 loss: 0.3653084543994869\n",
      "42000/49000 loss: 0.37386301747650724\n",
      "48000/49000 loss: 0.3763459846555171\n",
      "epoch 21: valid acc = 0.855, new learning rate = 0.00017028081314405735\n",
      "6000/49000 loss: 0.42556742465974007\n",
      "12000/49000 loss: 0.4180517627352707\n",
      "18000/49000 loss: 0.3989288114423802\n",
      "24000/49000 loss: 0.435498999196428\n",
      "30000/49000 loss: 0.3463166330610226\n",
      "36000/49000 loss: 0.42818217796097685\n",
      "42000/49000 loss: 0.4369982197952589\n",
      "48000/49000 loss: 0.3877592090988467\n",
      "epoch 22: valid acc = 0.862, new learning rate = 0.00016176677248685447\n",
      "6000/49000 loss: 0.43681929042608164\n",
      "12000/49000 loss: 0.43479987521909613\n",
      "18000/49000 loss: 0.3810124923451359\n",
      "24000/49000 loss: 0.4559376822608056\n",
      "30000/49000 loss: 0.4115733889671707\n",
      "36000/49000 loss: 0.4001241811411535\n",
      "42000/49000 loss: 0.4163932280640448\n",
      "48000/49000 loss: 0.43616325975959486\n",
      "epoch 23: valid acc = 0.854, new learning rate = 0.00015367843386251173\n",
      "6000/49000 loss: 0.4041504471259595\n",
      "12000/49000 loss: 0.40904312888974687\n",
      "18000/49000 loss: 0.3832014330029785\n",
      "24000/49000 loss: 0.44729015800854416\n",
      "30000/49000 loss: 0.4279099066116117\n",
      "36000/49000 loss: 0.3906403603423333\n",
      "42000/49000 loss: 0.38566136867159484\n",
      "48000/49000 loss: 0.36206677201151366\n",
      "epoch 24: valid acc = 0.854, new learning rate = 0.00014599451216938612\n",
      "6000/49000 loss: 0.39189815916411064\n",
      "12000/49000 loss: 0.4162562161214291\n",
      "18000/49000 loss: 0.39729898788594714\n",
      "24000/49000 loss: 0.4000707670747551\n",
      "30000/49000 loss: 0.3805633530871666\n",
      "36000/49000 loss: 0.42032156305049806\n",
      "42000/49000 loss: 0.37986700200754586\n",
      "48000/49000 loss: 0.40325181416307226\n",
      "epoch 25: valid acc = 0.857, new learning rate = 0.00013869478656091682\n",
      "6000/49000 loss: 0.3990972568830749\n",
      "12000/49000 loss: 0.4263089886233287\n",
      "18000/49000 loss: 0.3252479206390415\n",
      "24000/49000 loss: 0.4009361658015123\n",
      "30000/49000 loss: 0.4141837148884876\n",
      "36000/49000 loss: 0.3828749862130327\n",
      "42000/49000 loss: 0.3889654463084992\n",
      "48000/49000 loss: 0.37009940185756507\n",
      "epoch 26: valid acc = 0.861, new learning rate = 0.00013176004723287096\n",
      "6000/49000 loss: 0.41354872126337744\n",
      "12000/49000 loss: 0.3618283136679531\n",
      "18000/49000 loss: 0.4098014726542211\n",
      "24000/49000 loss: 0.41812577318194194\n",
      "30000/49000 loss: 0.3739640192847054\n",
      "36000/49000 loss: 0.4196098780773433\n",
      "42000/49000 loss: 0.38239031481759217\n",
      "48000/49000 loss: 0.34133094274685005\n",
      "epoch 27: valid acc = 0.86, new learning rate = 0.0001251720448712274\n",
      "6000/49000 loss: 0.39023774071841527\n",
      "12000/49000 loss: 0.34022570737259605\n",
      "18000/49000 loss: 0.4154514898585921\n",
      "24000/49000 loss: 0.37865180831193207\n",
      "30000/49000 loss: 0.37492332839959475\n",
      "36000/49000 loss: 0.45176276177216573\n",
      "42000/49000 loss: 0.42484708908719265\n",
      "48000/49000 loss: 0.430747739152152\n",
      "epoch 28: valid acc = 0.861, new learning rate = 0.00011891344262766602\n",
      "6000/49000 loss: 0.43740534450885227\n",
      "12000/49000 loss: 0.3805167085992604\n",
      "18000/49000 loss: 0.4378838095313594\n",
      "24000/49000 loss: 0.3948285748103271\n",
      "30000/49000 loss: 0.3462254675426775\n",
      "36000/49000 loss: 0.38644075025574254\n",
      "42000/49000 loss: 0.4448718955326725\n",
      "48000/49000 loss: 0.42554911362690884\n",
      "epoch 29: valid acc = 0.862, new learning rate = 0.00011296777049628272\n",
      "6000/49000 loss: 0.418805399528407\n",
      "12000/49000 loss: 0.3808394860628488\n",
      "18000/49000 loss: 0.4061292088603401\n",
      "24000/49000 loss: 0.4618599526215183\n",
      "30000/49000 loss: 0.3800947722999339\n",
      "36000/49000 loss: 0.4165801919000234\n",
      "42000/49000 loss: 0.4212344376858978\n",
      "48000/49000 loss: 0.41118937413430023\n",
      "epoch 30: valid acc = 0.861, new learning rate = 0.00010731938197146858\n",
      "6000/49000 loss: 0.4385217825140644\n",
      "12000/49000 loss: 0.3087678371957383\n",
      "18000/49000 loss: 0.4192432013733015\n",
      "24000/49000 loss: 0.4250187082119124\n",
      "30000/49000 loss: 0.379903958759096\n",
      "36000/49000 loss: 0.43915121343466235\n",
      "42000/49000 loss: 0.42690027329836827\n",
      "48000/49000 loss: 0.36048589259347874\n",
      "epoch 31: valid acc = 0.864, new learning rate = 0.00010195341287289515\n",
      "6000/49000 loss: 0.3789307505157253\n",
      "12000/49000 loss: 0.3869007586696887\n",
      "18000/49000 loss: 0.38006084000194046\n",
      "24000/49000 loss: 0.3838183978370431\n",
      "30000/49000 loss: 0.3674387353891351\n",
      "36000/49000 loss: 0.38001242773219945\n",
      "42000/49000 loss: 0.3580139321138793\n",
      "48000/49000 loss: 0.3883999604280846\n",
      "epoch 32: valid acc = 0.86, new learning rate = 9.685574222925039e-05\n",
      "6000/49000 loss: 0.3740585897709525\n",
      "12000/49000 loss: 0.385720202193468\n",
      "18000/49000 loss: 0.4226716788800442\n",
      "24000/49000 loss: 0.4155057863684447\n",
      "30000/49000 loss: 0.3397493112748641\n",
      "36000/49000 loss: 0.40200601533465435\n",
      "42000/49000 loss: 0.3948141106621398\n",
      "48000/49000 loss: 0.3992811198137619\n",
      "epoch 33: valid acc = 0.864, new learning rate = 9.201295511778786e-05\n",
      "6000/49000 loss: 0.37345581076385737\n",
      "12000/49000 loss: 0.33583100472869487\n",
      "18000/49000 loss: 0.3666599335980161\n",
      "24000/49000 loss: 0.4448962243333852\n",
      "30000/49000 loss: 0.33611552675271616\n",
      "36000/49000 loss: 0.39792723307062355\n",
      "42000/49000 loss: 0.347391297218896\n",
      "48000/49000 loss: 0.3989033332049227\n",
      "epoch 34: valid acc = 0.864, new learning rate = 8.741230736189846e-05\n",
      "6000/49000 loss: 0.3702350854010357\n",
      "12000/49000 loss: 0.38727485526611705\n",
      "18000/49000 loss: 0.35348383811685447\n",
      "24000/49000 loss: 0.3481413128492481\n",
      "30000/49000 loss: 0.4092793713499351\n",
      "36000/49000 loss: 0.4681161865926778\n",
      "42000/49000 loss: 0.36492463408426523\n",
      "48000/49000 loss: 0.4158202041314519\n",
      "epoch 35: valid acc = 0.867, new learning rate = 8.304169199380353e-05\n",
      "6000/49000 loss: 0.35050326214223826\n",
      "12000/49000 loss: 0.36537011043652395\n",
      "18000/49000 loss: 0.4030892050524017\n",
      "24000/49000 loss: 0.41719516509244736\n",
      "30000/49000 loss: 0.3728867466208291\n",
      "36000/49000 loss: 0.4542536723317322\n",
      "42000/49000 loss: 0.39544702851228475\n",
      "48000/49000 loss: 0.43412720680606404\n",
      "epoch 36: valid acc = 0.863, new learning rate = 7.888960739411335e-05\n",
      "6000/49000 loss: 0.36262312661682927\n",
      "12000/49000 loss: 0.368255432550347\n",
      "18000/49000 loss: 0.41622412655102486\n",
      "24000/49000 loss: 0.36871029526965016\n",
      "30000/49000 loss: 0.377296894312857\n",
      "36000/49000 loss: 0.38632774553630733\n",
      "42000/49000 loss: 0.39493069521035234\n",
      "48000/49000 loss: 0.3615090674863964\n",
      "epoch 37: valid acc = 0.866, new learning rate = 7.494512702440768e-05\n",
      "6000/49000 loss: 0.3884186597199646\n",
      "12000/49000 loss: 0.40307612557679895\n",
      "18000/49000 loss: 0.33871661514551865\n",
      "24000/49000 loss: 0.39128823354632836\n",
      "30000/49000 loss: 0.3984071215523998\n",
      "36000/49000 loss: 0.43373166106493755\n",
      "42000/49000 loss: 0.41366411364384686\n",
      "48000/49000 loss: 0.4603897115390902\n",
      "epoch 38: valid acc = 0.866, new learning rate = 7.119787067318729e-05\n",
      "6000/49000 loss: 0.3758354388055268\n",
      "12000/49000 loss: 0.35615594497843855\n",
      "18000/49000 loss: 0.32439298465674493\n",
      "24000/49000 loss: 0.36515274818280363\n",
      "30000/49000 loss: 0.4434509043634413\n",
      "36000/49000 loss: 0.36424199711235195\n",
      "42000/49000 loss: 0.3886831828197271\n",
      "48000/49000 loss: 0.4192237943861045\n",
      "epoch 39: valid acc = 0.866, new learning rate = 6.763797713952792e-05\n",
      "6000/49000 loss: 0.3680623396334126\n",
      "12000/49000 loss: 0.4374690491423256\n",
      "18000/49000 loss: 0.41850381216389043\n",
      "24000/49000 loss: 0.3983953308201655\n",
      "30000/49000 loss: 0.36705178097571006\n",
      "36000/49000 loss: 0.4162174344431953\n",
      "42000/49000 loss: 0.36105542309879624\n",
      "48000/49000 loss: 0.3883180830933568\n",
      "epoch 40: valid acc = 0.865, new learning rate = 6.425607828255152e-05\n",
      "6000/49000 loss: 0.35381474017214243\n",
      "12000/49000 loss: 0.3840872383324446\n",
      "18000/49000 loss: 0.37747103829724865\n",
      "24000/49000 loss: 0.3571869433899804\n",
      "30000/49000 loss: 0.35081748733871343\n",
      "36000/49000 loss: 0.35294484938602977\n",
      "42000/49000 loss: 0.37398248888435015\n",
      "48000/49000 loss: 0.4029021821189154\n",
      "epoch 41: valid acc = 0.866, new learning rate = 6.104327436842394e-05\n",
      "6000/49000 loss: 0.3810179598858712\n",
      "12000/49000 loss: 0.3656460895976504\n",
      "18000/49000 loss: 0.3442649325062241\n",
      "24000/49000 loss: 0.3591281448978986\n",
      "30000/49000 loss: 0.3778569354144721\n",
      "36000/49000 loss: 0.33768992146733084\n",
      "42000/49000 loss: 0.39234423627065595\n",
      "48000/49000 loss: 0.35864813862483685\n",
      "epoch 42: valid acc = 0.867, new learning rate = 5.799111065000274e-05\n",
      "6000/49000 loss: 0.40110631295760074\n",
      "12000/49000 loss: 0.41856063693446033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/49000 loss: 0.31159289487454467\n",
      "24000/49000 loss: 0.339447621181797\n",
      "30000/49000 loss: 0.3611990164586902\n",
      "36000/49000 loss: 0.3877359224114741\n",
      "42000/49000 loss: 0.3786757472561818\n",
      "48000/49000 loss: 0.38446804549095126\n",
      "epoch 43: valid acc = 0.865, new learning rate = 5.5091555117502596e-05\n",
      "6000/49000 loss: 0.38617034626820246\n",
      "12000/49000 loss: 0.3668052040093624\n",
      "18000/49000 loss: 0.4073482273621548\n",
      "24000/49000 loss: 0.397889586522223\n",
      "30000/49000 loss: 0.416767808627387\n",
      "36000/49000 loss: 0.4379279891149711\n",
      "42000/49000 loss: 0.38413401229899413\n",
      "48000/49000 loss: 0.363440565425685\n",
      "epoch 44: valid acc = 0.868, new learning rate = 5.2336977361627463e-05\n",
      "6000/49000 loss: 0.4026769592013116\n",
      "12000/49000 loss: 0.3691818622477658\n",
      "18000/49000 loss: 0.3718808760704444\n",
      "24000/49000 loss: 0.37745007821554977\n",
      "30000/49000 loss: 0.4038764677977286\n",
      "36000/49000 loss: 0.3641961031996758\n",
      "42000/49000 loss: 0.40558361313266145\n",
      "48000/49000 loss: 0.4367077352857984\n",
      "epoch 45: valid acc = 0.868, new learning rate = 4.972012849354609e-05\n",
      "6000/49000 loss: 0.44363301724719895\n",
      "12000/49000 loss: 0.35676589208931425\n",
      "18000/49000 loss: 0.37781927119156483\n",
      "24000/49000 loss: 0.38919575293055514\n",
      "30000/49000 loss: 0.369559563570644\n",
      "36000/49000 loss: 0.33933813306190513\n",
      "42000/49000 loss: 0.3709387548917628\n",
      "48000/49000 loss: 0.39446382959887694\n",
      "epoch 46: valid acc = 0.865, new learning rate = 4.723412206886878e-05\n",
      "6000/49000 loss: 0.408316208412651\n",
      "12000/49000 loss: 0.3613619066083424\n",
      "18000/49000 loss: 0.36109616408656675\n",
      "24000/49000 loss: 0.34764409760111514\n",
      "30000/49000 loss: 0.42223393333597364\n",
      "36000/49000 loss: 0.4510300751683511\n",
      "42000/49000 loss: 0.40631432432659076\n",
      "48000/49000 loss: 0.3513603653999011\n",
      "epoch 47: valid acc = 0.866, new learning rate = 4.487241596542534e-05\n",
      "6000/49000 loss: 0.37574890813381107\n",
      "12000/49000 loss: 0.3714070642051476\n",
      "18000/49000 loss: 0.37881562561471765\n",
      "24000/49000 loss: 0.36829525338372104\n",
      "30000/49000 loss: 0.3752598086207127\n",
      "36000/49000 loss: 0.35774871838640854\n",
      "42000/49000 loss: 0.4085574046731259\n",
      "48000/49000 loss: 0.3591040159756311\n",
      "epoch 48: valid acc = 0.867, new learning rate = 4.262879516715407e-05\n",
      "6000/49000 loss: 0.3379320579321578\n",
      "12000/49000 loss: 0.4049395510226319\n",
      "18000/49000 loss: 0.3479350730583193\n",
      "24000/49000 loss: 0.39671431408613583\n",
      "30000/49000 loss: 0.4103273659447742\n",
      "36000/49000 loss: 0.3721031724481063\n",
      "42000/49000 loss: 0.37238236342238357\n",
      "48000/49000 loss: 0.35298195710335634\n",
      "epoch 49: valid acc = 0.865, new learning rate = 4.049735540879637e-05\n",
      "6000/49000 loss: 0.3822708470254713\n",
      "12000/49000 loss: 0.3551826333780284\n",
      "18000/49000 loss: 0.4359986875687152\n",
      "24000/49000 loss: 0.41080324648045685\n",
      "30000/49000 loss: 0.4060586473311339\n",
      "36000/49000 loss: 0.3796772163596765\n",
      "42000/49000 loss: 0.3615542305408933\n",
      "48000/49000 loss: 0.43219371163363\n",
      "epoch 50: valid acc = 0.865, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.8676938775510205\n",
      "test acc: 0.865\n",
      "test acc: 0.8471\n",
      "number of batches for training: 81\n",
      "epoch 1: valid acc = 0.462, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.671, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.733, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.76, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.78, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.801, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.811, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.823, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.83, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.836, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.833, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.838, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.84, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.842, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.844, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.849, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.847, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.852, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.848, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.854, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.854, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.854, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.856, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.862, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.863, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.861, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.863, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.86, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.86, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.864, new learning rate = 0.00010731938197146858\n",
      "epoch 31: valid acc = 0.863, new learning rate = 0.00010195341287289515\n",
      "epoch 32: valid acc = 0.862, new learning rate = 9.685574222925039e-05\n",
      "epoch 33: valid acc = 0.863, new learning rate = 9.201295511778786e-05\n",
      "epoch 34: valid acc = 0.861, new learning rate = 8.741230736189846e-05\n",
      "epoch 35: valid acc = 0.864, new learning rate = 8.304169199380353e-05\n",
      "epoch 36: valid acc = 0.864, new learning rate = 7.888960739411335e-05\n",
      "epoch 37: valid acc = 0.863, new learning rate = 7.494512702440768e-05\n",
      "epoch 38: valid acc = 0.865, new learning rate = 7.119787067318729e-05\n",
      "epoch 39: valid acc = 0.867, new learning rate = 6.763797713952792e-05\n",
      "epoch 40: valid acc = 0.865, new learning rate = 6.425607828255152e-05\n",
      "epoch 41: valid acc = 0.865, new learning rate = 6.104327436842394e-05\n",
      "epoch 42: valid acc = 0.867, new learning rate = 5.799111065000274e-05\n",
      "epoch 43: valid acc = 0.864, new learning rate = 5.5091555117502596e-05\n",
      "epoch 44: valid acc = 0.864, new learning rate = 5.2336977361627463e-05\n",
      "epoch 45: valid acc = 0.866, new learning rate = 4.972012849354609e-05\n",
      "epoch 46: valid acc = 0.866, new learning rate = 4.723412206886878e-05\n",
      "epoch 47: valid acc = 0.868, new learning rate = 4.487241596542534e-05\n",
      "epoch 48: valid acc = 0.867, new learning rate = 4.262879516715407e-05\n",
      "epoch 49: valid acc = 0.867, new learning rate = 4.049735540879637e-05\n",
      "epoch 50: valid acc = 0.868, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.8678979591836735\n",
      "test acc: 0.868\n",
      "test acc: 0.8485\n",
      "number of batches for training: 81\n",
      "6000/49000 loss: 2.6390804133060852\n",
      "12000/49000 loss: 2.569122597568341\n",
      "18000/49000 loss: 2.5569504766145417\n",
      "24000/49000 loss: 2.4775112461340356\n",
      "30000/49000 loss: 2.2930403666199086\n",
      "36000/49000 loss: 2.1167041159508777\n",
      "42000/49000 loss: 1.953975696405212\n",
      "48000/49000 loss: 1.7250598812569073\n",
      "epoch 1: valid acc = 0.44, new learning rate = 0.000475\n",
      "6000/49000 loss: 1.3473493475954361\n",
      "12000/49000 loss: 1.2616573131399633\n",
      "18000/49000 loss: 1.1791579063134188\n",
      "24000/49000 loss: 1.1673607459083215\n",
      "30000/49000 loss: 1.0978224249954802\n",
      "36000/49000 loss: 1.0796672319562308\n",
      "42000/49000 loss: 0.9909663989924758\n",
      "48000/49000 loss: 0.9945551135488094\n",
      "epoch 2: valid acc = 0.678, new learning rate = 0.00045125\n",
      "6000/49000 loss: 0.8891849551324159\n",
      "12000/49000 loss: 0.8064925061662446\n",
      "18000/49000 loss: 0.8941582083947746\n",
      "24000/49000 loss: 0.8381057228555818\n",
      "30000/49000 loss: 0.7585165944164644\n",
      "36000/49000 loss: 0.7254995458382282\n",
      "42000/49000 loss: 0.7447382668820525\n",
      "48000/49000 loss: 0.7062629405313191\n",
      "epoch 3: valid acc = 0.735, new learning rate = 0.0004286875\n",
      "6000/49000 loss: 0.7351702387073732\n",
      "12000/49000 loss: 0.7082982489024903\n",
      "18000/49000 loss: 0.6574500975344203\n",
      "24000/49000 loss: 0.6600396637341217\n",
      "30000/49000 loss: 0.6186687752498793\n",
      "36000/49000 loss: 0.6408489560653061\n",
      "42000/49000 loss: 0.6187626029032253\n",
      "48000/49000 loss: 0.6343070202060507\n",
      "epoch 4: valid acc = 0.76, new learning rate = 0.00040725312499999993\n",
      "6000/49000 loss: 0.6913801086585902\n",
      "12000/49000 loss: 0.5731866597722665\n",
      "18000/49000 loss: 0.6200786250997862\n",
      "24000/49000 loss: 0.5898738970397209\n",
      "30000/49000 loss: 0.5669056654272192\n",
      "36000/49000 loss: 0.5670092520364788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/49000 loss: 0.5579539529921994\n",
      "48000/49000 loss: 0.5883500285567735\n",
      "epoch 5: valid acc = 0.783, new learning rate = 0.0003868904687499999\n",
      "6000/49000 loss: 0.5592315433038715\n",
      "12000/49000 loss: 0.572666233769052\n",
      "18000/49000 loss: 0.5231992902194293\n",
      "24000/49000 loss: 0.5814009649436269\n",
      "30000/49000 loss: 0.5251937806003459\n",
      "36000/49000 loss: 0.6116074986607178\n",
      "42000/49000 loss: 0.5896861637315194\n",
      "48000/49000 loss: 0.576651571556179\n",
      "epoch 6: valid acc = 0.803, new learning rate = 0.0003675459453124999\n",
      "6000/49000 loss: 0.5553788969795344\n",
      "12000/49000 loss: 0.5699856585735154\n",
      "18000/49000 loss: 0.4991578072816833\n",
      "24000/49000 loss: 0.548983587412454\n",
      "30000/49000 loss: 0.5450640784231827\n",
      "36000/49000 loss: 0.5396803753566202\n",
      "42000/49000 loss: 0.497540837563978\n",
      "48000/49000 loss: 0.4998939701594613\n",
      "epoch 7: valid acc = 0.811, new learning rate = 0.00034916864804687486\n",
      "6000/49000 loss: 0.523431843766893\n",
      "12000/49000 loss: 0.48671652264301724\n",
      "18000/49000 loss: 0.44809620936786454\n",
      "24000/49000 loss: 0.49044789240111114\n",
      "30000/49000 loss: 0.4655675786471544\n",
      "36000/49000 loss: 0.47605726746398\n",
      "42000/49000 loss: 0.4637164842680959\n",
      "48000/49000 loss: 0.5041181740915903\n",
      "epoch 8: valid acc = 0.82, new learning rate = 0.0003317102156445311\n",
      "6000/49000 loss: 0.5264235626091736\n",
      "12000/49000 loss: 0.4875120083726498\n",
      "18000/49000 loss: 0.4851173330060213\n",
      "24000/49000 loss: 0.441596148688835\n",
      "30000/49000 loss: 0.5055736262219633\n",
      "36000/49000 loss: 0.49606243028664393\n",
      "42000/49000 loss: 0.4209933766282062\n",
      "48000/49000 loss: 0.459545154958252\n",
      "epoch 9: valid acc = 0.825, new learning rate = 0.0003151247048623045\n",
      "6000/49000 loss: 0.5298296400262019\n",
      "12000/49000 loss: 0.45434960767130367\n",
      "18000/49000 loss: 0.5069848885041368\n",
      "24000/49000 loss: 0.44833692572899836\n",
      "30000/49000 loss: 0.5018416868985757\n",
      "36000/49000 loss: 0.45496211573695433\n",
      "42000/49000 loss: 0.5073871516070342\n",
      "48000/49000 loss: 0.548430260081571\n",
      "epoch 10: valid acc = 0.83, new learning rate = 0.00029936846961918924\n",
      "6000/49000 loss: 0.44477924536436236\n",
      "12000/49000 loss: 0.5493349432214177\n",
      "18000/49000 loss: 0.4644612376819259\n",
      "24000/49000 loss: 0.5119326200429577\n",
      "30000/49000 loss: 0.48615172718283955\n",
      "36000/49000 loss: 0.4549179575260396\n",
      "42000/49000 loss: 0.4251842584874319\n",
      "48000/49000 loss: 0.47951523405738133\n",
      "epoch 11: valid acc = 0.834, new learning rate = 0.00028440004613822977\n",
      "6000/49000 loss: 0.49017468562053534\n",
      "12000/49000 loss: 0.47254018498794376\n",
      "18000/49000 loss: 0.41858558657356143\n",
      "24000/49000 loss: 0.44587894321958627\n",
      "30000/49000 loss: 0.4309987708252031\n",
      "36000/49000 loss: 0.42883613222257627\n",
      "42000/49000 loss: 0.45051335432221584\n",
      "48000/49000 loss: 0.4065491852909541\n",
      "epoch 12: valid acc = 0.836, new learning rate = 0.00027018004383131826\n",
      "6000/49000 loss: 0.44279133913381175\n",
      "12000/49000 loss: 0.45698444174787567\n",
      "18000/49000 loss: 0.44507340694276004\n",
      "24000/49000 loss: 0.441739295641147\n",
      "30000/49000 loss: 0.46588064308315474\n",
      "36000/49000 loss: 0.44858761753887777\n",
      "42000/49000 loss: 0.4189482077106832\n",
      "48000/49000 loss: 0.42239603591144437\n",
      "epoch 13: valid acc = 0.839, new learning rate = 0.00025667104163975234\n",
      "6000/49000 loss: 0.4483128981122807\n",
      "12000/49000 loss: 0.46634401868398156\n",
      "18000/49000 loss: 0.48370156916708645\n",
      "24000/49000 loss: 0.4396577292351259\n",
      "30000/49000 loss: 0.42557460777433426\n",
      "36000/49000 loss: 0.4402721432209504\n",
      "42000/49000 loss: 0.4234364422097675\n",
      "48000/49000 loss: 0.44555624033561303\n",
      "epoch 14: valid acc = 0.845, new learning rate = 0.00024383748955776472\n",
      "6000/49000 loss: 0.4128102473952353\n",
      "12000/49000 loss: 0.41287767971756484\n",
      "18000/49000 loss: 0.4490410370658558\n",
      "24000/49000 loss: 0.4396165142494515\n",
      "30000/49000 loss: 0.3835404297486824\n",
      "36000/49000 loss: 0.39981663100030934\n",
      "42000/49000 loss: 0.40225360989344316\n",
      "48000/49000 loss: 0.43186453189262264\n",
      "epoch 15: valid acc = 0.841, new learning rate = 0.00023164561507987649\n",
      "6000/49000 loss: 0.4283670411912597\n",
      "12000/49000 loss: 0.37740563481322087\n",
      "18000/49000 loss: 0.4251084405731047\n",
      "24000/49000 loss: 0.42216624441412115\n",
      "30000/49000 loss: 0.4143643123756479\n",
      "36000/49000 loss: 0.41960667173337696\n",
      "42000/49000 loss: 0.42700678479710774\n",
      "48000/49000 loss: 0.41103413441155107\n",
      "epoch 16: valid acc = 0.846, new learning rate = 0.00022006333432588265\n",
      "6000/49000 loss: 0.4293191907812513\n",
      "12000/49000 loss: 0.5002559671211003\n",
      "18000/49000 loss: 0.44369259544886885\n",
      "24000/49000 loss: 0.37566259298080756\n",
      "30000/49000 loss: 0.4354326234017757\n",
      "36000/49000 loss: 0.38850632082390524\n",
      "42000/49000 loss: 0.48011057028549936\n",
      "48000/49000 loss: 0.3814049722654511\n",
      "epoch 17: valid acc = 0.85, new learning rate = 0.00020906016760958852\n",
      "6000/49000 loss: 0.4077301827790184\n",
      "12000/49000 loss: 0.4918634746233851\n",
      "18000/49000 loss: 0.43485139402094347\n",
      "24000/49000 loss: 0.43874996013620676\n",
      "30000/49000 loss: 0.3640286345456392\n",
      "36000/49000 loss: 0.4539357259260496\n",
      "42000/49000 loss: 0.3860532379430101\n",
      "48000/49000 loss: 0.4437252400950587\n",
      "epoch 18: valid acc = 0.851, new learning rate = 0.00019860715922910907\n",
      "6000/49000 loss: 0.4737655054691088\n",
      "12000/49000 loss: 0.4406760952473326\n",
      "18000/49000 loss: 0.4389585726929413\n",
      "24000/49000 loss: 0.4117154831684364\n",
      "30000/49000 loss: 0.414034496933366\n",
      "36000/49000 loss: 0.4288713089957422\n",
      "42000/49000 loss: 0.4216246268419286\n",
      "48000/49000 loss: 0.4072002072818155\n",
      "epoch 19: valid acc = 0.851, new learning rate = 0.0001886768012676536\n",
      "6000/49000 loss: 0.4218278766114765\n",
      "12000/49000 loss: 0.3693399462151883\n",
      "18000/49000 loss: 0.3828177428723942\n",
      "24000/49000 loss: 0.3810346001778611\n",
      "30000/49000 loss: 0.42132832581894797\n",
      "36000/49000 loss: 0.408979847881397\n",
      "42000/49000 loss: 0.47733848259186196\n",
      "48000/49000 loss: 0.39032329743912814\n",
      "epoch 20: valid acc = 0.853, new learning rate = 0.0001792429612042709\n",
      "6000/49000 loss: 0.42770221337849545\n",
      "12000/49000 loss: 0.4054043693441506\n",
      "18000/49000 loss: 0.4155183457820026\n",
      "24000/49000 loss: 0.4094030349314282\n",
      "30000/49000 loss: 0.37894949469422295\n",
      "36000/49000 loss: 0.39601877580436345\n",
      "42000/49000 loss: 0.42268615458430137\n",
      "48000/49000 loss: 0.4384849209704808\n",
      "epoch 21: valid acc = 0.848, new learning rate = 0.00017028081314405735\n",
      "6000/49000 loss: 0.4271328841883366\n",
      "12000/49000 loss: 0.3443916915727693\n",
      "18000/49000 loss: 0.34022458928722427\n",
      "24000/49000 loss: 0.33421342879454174\n",
      "30000/49000 loss: 0.42063636524625214\n",
      "36000/49000 loss: 0.4302312824202712\n",
      "42000/49000 loss: 0.4330572178091444\n",
      "48000/49000 loss: 0.43780785230357755\n",
      "epoch 22: valid acc = 0.857, new learning rate = 0.00016176677248685447\n",
      "6000/49000 loss: 0.4393759018416897\n",
      "12000/49000 loss: 0.4003072935215288\n",
      "18000/49000 loss: 0.4126496201588082\n",
      "24000/49000 loss: 0.42765438271231493\n",
      "30000/49000 loss: 0.38929739898594695\n",
      "36000/49000 loss: 0.4947518175514952\n",
      "42000/49000 loss: 0.37624973456734967\n",
      "48000/49000 loss: 0.42278099399778984\n",
      "epoch 23: valid acc = 0.858, new learning rate = 0.00015367843386251173\n",
      "6000/49000 loss: 0.3981525186836805\n",
      "12000/49000 loss: 0.40198140056245013\n",
      "18000/49000 loss: 0.4128784236664412\n",
      "24000/49000 loss: 0.3589878825656843\n",
      "30000/49000 loss: 0.3741916263727634\n",
      "36000/49000 loss: 0.41312215263672936\n",
      "42000/49000 loss: 0.37966705529449474\n",
      "48000/49000 loss: 0.3974680226224692\n",
      "epoch 24: valid acc = 0.86, new learning rate = 0.00014599451216938612\n",
      "6000/49000 loss: 0.39177851785414314\n",
      "12000/49000 loss: 0.4112193109981434\n",
      "18000/49000 loss: 0.39143807749846105\n",
      "24000/49000 loss: 0.35483913389765537\n",
      "30000/49000 loss: 0.3617289354821593\n",
      "36000/49000 loss: 0.41201179716795083\n",
      "42000/49000 loss: 0.3827986164231703\n",
      "48000/49000 loss: 0.4075329878895382\n",
      "epoch 25: valid acc = 0.86, new learning rate = 0.00013869478656091682\n",
      "6000/49000 loss: 0.3397858909084941\n",
      "12000/49000 loss: 0.45292053800207427\n",
      "18000/49000 loss: 0.3918287506594445\n",
      "24000/49000 loss: 0.3999632502059066\n",
      "30000/49000 loss: 0.3972760426852822\n",
      "36000/49000 loss: 0.42516584750293984\n",
      "42000/49000 loss: 0.4560990496489231\n",
      "48000/49000 loss: 0.41159257031445473\n",
      "epoch 26: valid acc = 0.859, new learning rate = 0.00013176004723287096\n",
      "6000/49000 loss: 0.38084930986676513\n",
      "12000/49000 loss: 0.362284611213677\n",
      "18000/49000 loss: 0.3724896645874456\n",
      "24000/49000 loss: 0.4677945476824372\n",
      "30000/49000 loss: 0.38491717945847354\n",
      "36000/49000 loss: 0.36318303437681926\n",
      "42000/49000 loss: 0.3566724818988356\n",
      "48000/49000 loss: 0.3938899884981132\n",
      "epoch 27: valid acc = 0.861, new learning rate = 0.0001251720448712274\n",
      "6000/49000 loss: 0.34806508519444884\n",
      "12000/49000 loss: 0.4370252146836943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/49000 loss: 0.4324971240902201\n",
      "24000/49000 loss: 0.3600762680320935\n",
      "30000/49000 loss: 0.4422168582476765\n",
      "36000/49000 loss: 0.405237916155703\n",
      "42000/49000 loss: 0.379991279560455\n",
      "48000/49000 loss: 0.41374682654622796\n",
      "epoch 28: valid acc = 0.861, new learning rate = 0.00011891344262766602\n",
      "6000/49000 loss: 0.35908181290296187\n",
      "12000/49000 loss: 0.40901320488948384\n",
      "18000/49000 loss: 0.39614593528950015\n",
      "24000/49000 loss: 0.4021359204081901\n",
      "30000/49000 loss: 0.44715932929678737\n",
      "36000/49000 loss: 0.3437612769856887\n",
      "42000/49000 loss: 0.3913911590529503\n",
      "48000/49000 loss: 0.36109868252451754\n",
      "epoch 29: valid acc = 0.864, new learning rate = 0.00011296777049628272\n",
      "6000/49000 loss: 0.4010075931131231\n",
      "12000/49000 loss: 0.36517034670885257\n",
      "18000/49000 loss: 0.4337624742550508\n",
      "24000/49000 loss: 0.39334606469030253\n",
      "30000/49000 loss: 0.43175677843005195\n",
      "36000/49000 loss: 0.4120807469360979\n",
      "42000/49000 loss: 0.3134172408757668\n",
      "48000/49000 loss: 0.36646466571397235\n",
      "epoch 30: valid acc = 0.864, new learning rate = 0.00010731938197146858\n",
      "6000/49000 loss: 0.33196848821438585\n",
      "12000/49000 loss: 0.4394456996918698\n",
      "18000/49000 loss: 0.40333151065528505\n",
      "24000/49000 loss: 0.38460721381305096\n",
      "30000/49000 loss: 0.3837148666358832\n",
      "36000/49000 loss: 0.40180531226905397\n",
      "42000/49000 loss: 0.37294576170259397\n",
      "48000/49000 loss: 0.43540898604012457\n",
      "epoch 31: valid acc = 0.864, new learning rate = 0.00010195341287289515\n",
      "6000/49000 loss: 0.3959324045412519\n",
      "12000/49000 loss: 0.3821723943496107\n",
      "18000/49000 loss: 0.40978932011913033\n",
      "24000/49000 loss: 0.41794369736430204\n",
      "30000/49000 loss: 0.3521535342347677\n",
      "36000/49000 loss: 0.4282003551507121\n",
      "42000/49000 loss: 0.39659101264615254\n",
      "48000/49000 loss: 0.3942156935150746\n",
      "epoch 32: valid acc = 0.86, new learning rate = 9.685574222925039e-05\n",
      "6000/49000 loss: 0.3676266429639653\n",
      "12000/49000 loss: 0.38357241121682983\n",
      "18000/49000 loss: 0.3419146374694204\n",
      "24000/49000 loss: 0.41011102581438164\n",
      "30000/49000 loss: 0.40104607942941\n",
      "36000/49000 loss: 0.38142714602762084\n",
      "42000/49000 loss: 0.4039526107981479\n",
      "48000/49000 loss: 0.36701102326333157\n",
      "epoch 33: valid acc = 0.861, new learning rate = 9.201295511778786e-05\n",
      "6000/49000 loss: 0.4707464729422127\n",
      "12000/49000 loss: 0.3771455088787845\n",
      "18000/49000 loss: 0.38885629797411864\n",
      "24000/49000 loss: 0.3568162154535309\n",
      "30000/49000 loss: 0.350274089772631\n",
      "36000/49000 loss: 0.41815684493903976\n",
      "42000/49000 loss: 0.42264083344107506\n",
      "48000/49000 loss: 0.35331692098609196\n",
      "epoch 34: valid acc = 0.864, new learning rate = 8.741230736189846e-05\n",
      "6000/49000 loss: 0.35119483516274463\n",
      "12000/49000 loss: 0.3927524825070377\n",
      "18000/49000 loss: 0.3978072778808343\n",
      "24000/49000 loss: 0.3536919942845364\n",
      "30000/49000 loss: 0.3934956397707965\n",
      "36000/49000 loss: 0.396734493387904\n",
      "42000/49000 loss: 0.37699661002303414\n",
      "48000/49000 loss: 0.3966350394724701\n",
      "epoch 35: valid acc = 0.866, new learning rate = 8.304169199380353e-05\n",
      "6000/49000 loss: 0.3916028174073182\n",
      "12000/49000 loss: 0.39391363648852856\n",
      "18000/49000 loss: 0.4438278479670335\n",
      "24000/49000 loss: 0.3956924286830758\n",
      "30000/49000 loss: 0.33223508340593555\n",
      "36000/49000 loss: 0.33855204637117403\n",
      "42000/49000 loss: 0.32745985028537017\n",
      "48000/49000 loss: 0.41111217698221303\n",
      "epoch 36: valid acc = 0.864, new learning rate = 7.888960739411335e-05\n",
      "6000/49000 loss: 0.3884462821228671\n",
      "12000/49000 loss: 0.34402260478849767\n",
      "18000/49000 loss: 0.44652655849643996\n",
      "24000/49000 loss: 0.37464617484702895\n",
      "30000/49000 loss: 0.41757557040464977\n",
      "36000/49000 loss: 0.44437488398741776\n",
      "42000/49000 loss: 0.36968418123530244\n",
      "48000/49000 loss: 0.3785360247975567\n",
      "epoch 37: valid acc = 0.864, new learning rate = 7.494512702440768e-05\n",
      "6000/49000 loss: 0.381267531134563\n",
      "12000/49000 loss: 0.425793306391972\n",
      "18000/49000 loss: 0.37661728216524987\n",
      "24000/49000 loss: 0.4480376907303164\n",
      "30000/49000 loss: 0.44666648046067436\n",
      "36000/49000 loss: 0.34625312873519365\n",
      "42000/49000 loss: 0.36818819505138345\n",
      "48000/49000 loss: 0.3902525864307728\n",
      "epoch 38: valid acc = 0.865, new learning rate = 7.119787067318729e-05\n",
      "6000/49000 loss: 0.4084893676543264\n",
      "12000/49000 loss: 0.3322564802403409\n",
      "18000/49000 loss: 0.40212340110699213\n",
      "24000/49000 loss: 0.40702849376940575\n",
      "30000/49000 loss: 0.45039871803948617\n",
      "36000/49000 loss: 0.40574729437799667\n",
      "42000/49000 loss: 0.4029564472789813\n",
      "48000/49000 loss: 0.41900184600575563\n",
      "epoch 39: valid acc = 0.865, new learning rate = 6.763797713952792e-05\n",
      "6000/49000 loss: 0.3672868213886665\n",
      "12000/49000 loss: 0.34010952027210645\n",
      "18000/49000 loss: 0.3902317589245977\n",
      "24000/49000 loss: 0.3666570150367774\n",
      "30000/49000 loss: 0.4561875242864455\n",
      "36000/49000 loss: 0.3524372637525456\n",
      "42000/49000 loss: 0.37329330741581235\n",
      "48000/49000 loss: 0.43343491748245866\n",
      "epoch 40: valid acc = 0.865, new learning rate = 6.425607828255152e-05\n",
      "6000/49000 loss: 0.39698929767215585\n",
      "12000/49000 loss: 0.4136490134133907\n",
      "18000/49000 loss: 0.41550617867328116\n",
      "24000/49000 loss: 0.36243475009593895\n",
      "30000/49000 loss: 0.4025593371104398\n",
      "36000/49000 loss: 0.44581348644285207\n",
      "42000/49000 loss: 0.4154293794482613\n",
      "48000/49000 loss: 0.3927912951870913\n",
      "epoch 41: valid acc = 0.863, new learning rate = 6.104327436842394e-05\n",
      "6000/49000 loss: 0.3857531156832849\n",
      "12000/49000 loss: 0.41571439601176113\n",
      "18000/49000 loss: 0.3550883936951476\n",
      "24000/49000 loss: 0.3660867477863973\n",
      "30000/49000 loss: 0.4519813728386562\n",
      "36000/49000 loss: 0.3936731386516265\n",
      "42000/49000 loss: 0.37607668171430014\n",
      "48000/49000 loss: 0.4113210206835058\n",
      "epoch 42: valid acc = 0.864, new learning rate = 5.799111065000274e-05\n",
      "6000/49000 loss: 0.38231841277688366\n",
      "12000/49000 loss: 0.3516942368235298\n",
      "18000/49000 loss: 0.36657603748526707\n",
      "24000/49000 loss: 0.37184977161507743\n",
      "30000/49000 loss: 0.342130408249808\n",
      "36000/49000 loss: 0.43040933199987697\n",
      "42000/49000 loss: 0.42169886043127547\n",
      "48000/49000 loss: 0.4117869499387913\n",
      "epoch 43: valid acc = 0.863, new learning rate = 5.5091555117502596e-05\n",
      "6000/49000 loss: 0.35303703444800777\n",
      "12000/49000 loss: 0.41351150375728835\n",
      "18000/49000 loss: 0.43479619170443656\n",
      "24000/49000 loss: 0.3577771351566723\n",
      "30000/49000 loss: 0.3465534426710948\n",
      "36000/49000 loss: 0.397416010525198\n",
      "42000/49000 loss: 0.4356668868964239\n",
      "48000/49000 loss: 0.44784963313610393\n",
      "epoch 44: valid acc = 0.863, new learning rate = 5.2336977361627463e-05\n",
      "6000/49000 loss: 0.36579009635390924\n",
      "12000/49000 loss: 0.342069015331238\n",
      "18000/49000 loss: 0.3897565568586798\n",
      "24000/49000 loss: 0.3972128573151637\n",
      "30000/49000 loss: 0.3325124261429921\n",
      "36000/49000 loss: 0.36236622417374886\n",
      "42000/49000 loss: 0.3781337334183261\n",
      "48000/49000 loss: 0.41291636176260355\n",
      "epoch 45: valid acc = 0.866, new learning rate = 4.972012849354609e-05\n",
      "6000/49000 loss: 0.36679958912239996\n",
      "12000/49000 loss: 0.31372419639641697\n",
      "18000/49000 loss: 0.43077288505061134\n",
      "24000/49000 loss: 0.3427456344694171\n",
      "30000/49000 loss: 0.4342348741845118\n",
      "36000/49000 loss: 0.3735405787727983\n",
      "42000/49000 loss: 0.40709079648748797\n",
      "48000/49000 loss: 0.36524469481556593\n",
      "epoch 46: valid acc = 0.865, new learning rate = 4.723412206886878e-05\n",
      "6000/49000 loss: 0.38810823322548976\n",
      "12000/49000 loss: 0.45352294575658186\n",
      "18000/49000 loss: 0.3715096627940279\n",
      "24000/49000 loss: 0.37316829541089636\n",
      "30000/49000 loss: 0.36779524973315786\n",
      "36000/49000 loss: 0.3660912748625238\n",
      "42000/49000 loss: 0.3622534355980182\n",
      "48000/49000 loss: 0.39999028158407995\n",
      "epoch 47: valid acc = 0.865, new learning rate = 4.487241596542534e-05\n",
      "6000/49000 loss: 0.34689766131981126\n",
      "12000/49000 loss: 0.38960302969614363\n",
      "18000/49000 loss: 0.4005839105532719\n",
      "24000/49000 loss: 0.39522415585540055\n",
      "30000/49000 loss: 0.38375084244196767\n",
      "36000/49000 loss: 0.39168193514101746\n",
      "42000/49000 loss: 0.348959102764685\n",
      "48000/49000 loss: 0.3992425835963584\n",
      "epoch 48: valid acc = 0.867, new learning rate = 4.262879516715407e-05\n",
      "6000/49000 loss: 0.3576326658551297\n",
      "12000/49000 loss: 0.40313196001150736\n",
      "18000/49000 loss: 0.36037046849270093\n",
      "24000/49000 loss: 0.39751988120668025\n",
      "30000/49000 loss: 0.3423160044210569\n",
      "36000/49000 loss: 0.41145387301190206\n",
      "42000/49000 loss: 0.4060210538582938\n",
      "48000/49000 loss: 0.3548871036048577\n",
      "epoch 49: valid acc = 0.867, new learning rate = 4.049735540879637e-05\n",
      "6000/49000 loss: 0.390813824897733\n",
      "12000/49000 loss: 0.3803431924121707\n",
      "18000/49000 loss: 0.4057776670096308\n",
      "24000/49000 loss: 0.3603827237740875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000/49000 loss: 0.36953596690365054\n",
      "36000/49000 loss: 0.33344875951932773\n",
      "42000/49000 loss: 0.36877416868370777\n",
      "48000/49000 loss: 0.3962446749796645\n",
      "epoch 50: valid acc = 0.867, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.8670408163265306\n",
      "test acc: 0.867\n",
      "test acc: 0.8472\n",
      "number of batches for training: 81\n",
      "epoch 1: valid acc = 0.448, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.658, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.738, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.762, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.789, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.798, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.813, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.817, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.821, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.829, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.835, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.836, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.839, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.848, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.842, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.843, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.844, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.851, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.857, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.856, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.857, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.858, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.86, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.859, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.862, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.866, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.864, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.865, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.861, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.863, new learning rate = 0.00010731938197146858\n",
      "epoch 31: valid acc = 0.865, new learning rate = 0.00010195341287289515\n",
      "epoch 32: valid acc = 0.866, new learning rate = 9.685574222925039e-05\n",
      "epoch 33: valid acc = 0.864, new learning rate = 9.201295511778786e-05\n",
      "epoch 34: valid acc = 0.865, new learning rate = 8.741230736189846e-05\n",
      "epoch 35: valid acc = 0.868, new learning rate = 8.304169199380353e-05\n",
      "epoch 36: valid acc = 0.868, new learning rate = 7.888960739411335e-05\n",
      "epoch 37: valid acc = 0.865, new learning rate = 7.494512702440768e-05\n",
      "epoch 38: valid acc = 0.866, new learning rate = 7.119787067318729e-05\n",
      "epoch 39: valid acc = 0.867, new learning rate = 6.763797713952792e-05\n",
      "epoch 40: valid acc = 0.866, new learning rate = 6.425607828255152e-05\n",
      "epoch 41: valid acc = 0.868, new learning rate = 6.104327436842394e-05\n",
      "epoch 42: valid acc = 0.867, new learning rate = 5.799111065000274e-05\n",
      "epoch 43: valid acc = 0.866, new learning rate = 5.5091555117502596e-05\n",
      "epoch 44: valid acc = 0.867, new learning rate = 5.2336977361627463e-05\n",
      "epoch 45: valid acc = 0.866, new learning rate = 4.972012849354609e-05\n",
      "epoch 46: valid acc = 0.866, new learning rate = 4.723412206886878e-05\n",
      "epoch 47: valid acc = 0.867, new learning rate = 4.487241596542534e-05\n",
      "epoch 48: valid acc = 0.868, new learning rate = 4.262879516715407e-05\n",
      "epoch 49: valid acc = 0.868, new learning rate = 4.049735540879637e-05\n",
      "epoch 50: valid acc = 0.868, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.8687142857142857\n",
      "test acc: 0.868\n",
      "test acc: 0.8475\n",
      "number of batches for training: 49\n",
      "10000/49000 loss: 2.6796631594682006\n",
      "20000/49000 loss: 2.5951262960161325\n",
      "30000/49000 loss: 2.5927992529598156\n",
      "40000/49000 loss: 2.4526469902305164\n",
      "epoch 1: valid acc = 0.371, new learning rate = 0.000475\n",
      "10000/49000 loss: 2.140043964231601\n",
      "20000/49000 loss: 2.0765264274600623\n",
      "30000/49000 loss: 1.8332217380773073\n",
      "40000/49000 loss: 1.5709208188184225\n",
      "epoch 2: valid acc = 0.518, new learning rate = 0.00045125\n",
      "10000/49000 loss: 1.2586642709325546\n",
      "20000/49000 loss: 1.176729966983745\n",
      "30000/49000 loss: 1.1985791212019319\n",
      "40000/49000 loss: 1.106986839906829\n",
      "epoch 3: valid acc = 0.576, new learning rate = 0.0004286875\n",
      "10000/49000 loss: 0.992559929292028\n",
      "20000/49000 loss: 1.023061323232805\n",
      "30000/49000 loss: 0.9850575654538726\n",
      "40000/49000 loss: 0.9180786477742495\n",
      "epoch 4: valid acc = 0.687, new learning rate = 0.00040725312499999993\n",
      "10000/49000 loss: 0.8741328449130978\n",
      "20000/49000 loss: 0.8564870324285019\n",
      "30000/49000 loss: 0.7696774097211846\n",
      "40000/49000 loss: 0.8066404129467616\n",
      "epoch 5: valid acc = 0.738, new learning rate = 0.0003868904687499999\n",
      "10000/49000 loss: 0.758410574296574\n",
      "20000/49000 loss: 0.707781036587329\n",
      "30000/49000 loss: 0.7005776423567313\n",
      "40000/49000 loss: 0.6762360461655491\n",
      "epoch 6: valid acc = 0.745, new learning rate = 0.0003675459453124999\n",
      "10000/49000 loss: 0.6306793728730391\n",
      "20000/49000 loss: 0.6460546100588251\n",
      "30000/49000 loss: 0.6603216305650297\n",
      "40000/49000 loss: 0.6630514696269597\n",
      "epoch 7: valid acc = 0.756, new learning rate = 0.00034916864804687486\n",
      "10000/49000 loss: 0.610574715738761\n",
      "20000/49000 loss: 0.6222713083288204\n",
      "30000/49000 loss: 0.6871595035243531\n",
      "40000/49000 loss: 0.5905932060727191\n",
      "epoch 8: valid acc = 0.771, new learning rate = 0.0003317102156445311\n",
      "10000/49000 loss: 0.5867975672441511\n",
      "20000/49000 loss: 0.5833818597818604\n",
      "30000/49000 loss: 0.5773604775283616\n",
      "40000/49000 loss: 0.5890716207635706\n",
      "epoch 9: valid acc = 0.783, new learning rate = 0.0003151247048623045\n",
      "10000/49000 loss: 0.5511142033031663\n",
      "20000/49000 loss: 0.5993667206883367\n",
      "30000/49000 loss: 0.5909892272780373\n",
      "40000/49000 loss: 0.5784758526419533\n",
      "epoch 10: valid acc = 0.785, new learning rate = 0.00029936846961918924\n",
      "10000/49000 loss: 0.5394389268529411\n",
      "20000/49000 loss: 0.5575240862098061\n",
      "30000/49000 loss: 0.5507099665169713\n",
      "40000/49000 loss: 0.5497062957453809\n",
      "epoch 11: valid acc = 0.795, new learning rate = 0.00028440004613822977\n",
      "10000/49000 loss: 0.5773195599467678\n",
      "20000/49000 loss: 0.5314861400961649\n",
      "30000/49000 loss: 0.5271693625511554\n",
      "40000/49000 loss: 0.4855804809033644\n",
      "epoch 12: valid acc = 0.809, new learning rate = 0.00027018004383131826\n",
      "10000/49000 loss: 0.5180941286545346\n",
      "20000/49000 loss: 0.5304217573828254\n",
      "30000/49000 loss: 0.5481559752864698\n",
      "40000/49000 loss: 0.49889670893419985\n",
      "epoch 13: valid acc = 0.817, new learning rate = 0.00025667104163975234\n",
      "10000/49000 loss: 0.5250381700212642\n",
      "20000/49000 loss: 0.5329996190752267\n",
      "30000/49000 loss: 0.5138754929576846\n",
      "40000/49000 loss: 0.5122216735592927\n",
      "epoch 14: valid acc = 0.819, new learning rate = 0.00024383748955776472\n",
      "10000/49000 loss: 0.5124837652473603\n",
      "20000/49000 loss: 0.5133583667533226\n",
      "30000/49000 loss: 0.5019792787679657\n",
      "40000/49000 loss: 0.467068653606094\n",
      "epoch 15: valid acc = 0.813, new learning rate = 0.00023164561507987649\n",
      "10000/49000 loss: 0.4662160269014779\n",
      "20000/49000 loss: 0.49769351478241636\n",
      "30000/49000 loss: 0.49485281882238463\n",
      "40000/49000 loss: 0.4859614623250197\n",
      "epoch 16: valid acc = 0.815, new learning rate = 0.00022006333432588265\n",
      "10000/49000 loss: 0.47786793227054464\n",
      "20000/49000 loss: 0.5209551499092138\n",
      "30000/49000 loss: 0.5322146290548109\n",
      "40000/49000 loss: 0.47275505576512966\n",
      "epoch 17: valid acc = 0.822, new learning rate = 0.00020906016760958852\n",
      "10000/49000 loss: 0.49951282175748163\n",
      "20000/49000 loss: 0.5163104099104804\n",
      "30000/49000 loss: 0.4790541406719856\n",
      "40000/49000 loss: 0.4932601262392659\n",
      "epoch 18: valid acc = 0.822, new learning rate = 0.00019860715922910907\n",
      "10000/49000 loss: 0.4546192649451504\n",
      "20000/49000 loss: 0.44891706588476543\n",
      "30000/49000 loss: 0.4992032266683324\n",
      "40000/49000 loss: 0.46771419218227467\n",
      "epoch 19: valid acc = 0.826, new learning rate = 0.0001886768012676536\n",
      "10000/49000 loss: 0.4515303109174642\n",
      "20000/49000 loss: 0.5414781851920003\n",
      "30000/49000 loss: 0.5051235964231836\n",
      "40000/49000 loss: 0.45522036789232617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20: valid acc = 0.829, new learning rate = 0.0001792429612042709\n",
      "10000/49000 loss: 0.4611196814696398\n",
      "20000/49000 loss: 0.5140112440767948\n",
      "30000/49000 loss: 0.47119922759170735\n",
      "40000/49000 loss: 0.4871856383895951\n",
      "epoch 21: valid acc = 0.83, new learning rate = 0.00017028081314405735\n",
      "10000/49000 loss: 0.44723440371009177\n",
      "20000/49000 loss: 0.4672024985336025\n",
      "30000/49000 loss: 0.45278658873136307\n",
      "40000/49000 loss: 0.4317604604267055\n",
      "epoch 22: valid acc = 0.83, new learning rate = 0.00016176677248685447\n",
      "10000/49000 loss: 0.4620157391567211\n",
      "20000/49000 loss: 0.44130768480012733\n",
      "30000/49000 loss: 0.46718986920610445\n",
      "40000/49000 loss: 0.44979388924995073\n",
      "epoch 23: valid acc = 0.834, new learning rate = 0.00015367843386251173\n",
      "10000/49000 loss: 0.4719126274582114\n",
      "20000/49000 loss: 0.4324750684563497\n",
      "30000/49000 loss: 0.45048891443236333\n",
      "40000/49000 loss: 0.4699142440718723\n",
      "epoch 24: valid acc = 0.83, new learning rate = 0.00014599451216938612\n",
      "10000/49000 loss: 0.4462017728180087\n",
      "20000/49000 loss: 0.45908320117820245\n",
      "30000/49000 loss: 0.4135915768287344\n",
      "40000/49000 loss: 0.459852747410163\n",
      "epoch 25: valid acc = 0.836, new learning rate = 0.00013869478656091682\n",
      "10000/49000 loss: 0.47587858362119934\n",
      "20000/49000 loss: 0.47592661022033617\n",
      "30000/49000 loss: 0.40909265772718123\n",
      "40000/49000 loss: 0.47925782687372875\n",
      "epoch 26: valid acc = 0.835, new learning rate = 0.00013176004723287096\n",
      "10000/49000 loss: 0.39297159772679796\n",
      "20000/49000 loss: 0.4877226292078609\n",
      "30000/49000 loss: 0.463596772613864\n",
      "40000/49000 loss: 0.43978222084701296\n",
      "epoch 27: valid acc = 0.835, new learning rate = 0.0001251720448712274\n",
      "10000/49000 loss: 0.4272812720399799\n",
      "20000/49000 loss: 0.47389288233523114\n",
      "30000/49000 loss: 0.40511676829544313\n",
      "40000/49000 loss: 0.45114472093059343\n",
      "epoch 28: valid acc = 0.837, new learning rate = 0.00011891344262766602\n",
      "10000/49000 loss: 0.4782355123732815\n",
      "20000/49000 loss: 0.43494740687170497\n",
      "30000/49000 loss: 0.4649473370568531\n",
      "40000/49000 loss: 0.46110194133516963\n",
      "epoch 29: valid acc = 0.837, new learning rate = 0.00011296777049628272\n",
      "10000/49000 loss: 0.44757578359041034\n",
      "20000/49000 loss: 0.45725506173901487\n",
      "30000/49000 loss: 0.4505871279800208\n",
      "40000/49000 loss: 0.41853258137985744\n",
      "epoch 30: valid acc = 0.841, new learning rate = 0.00010731938197146858\n",
      "10000/49000 loss: 0.4329162348944021\n",
      "20000/49000 loss: 0.46864634252762283\n",
      "30000/49000 loss: 0.4256778589920605\n",
      "40000/49000 loss: 0.4043415860825222\n",
      "epoch 31: valid acc = 0.841, new learning rate = 0.00010195341287289515\n",
      "10000/49000 loss: 0.4624863288600324\n",
      "20000/49000 loss: 0.4326804492664716\n",
      "30000/49000 loss: 0.42123204162966177\n",
      "40000/49000 loss: 0.4234703936978456\n",
      "epoch 32: valid acc = 0.842, new learning rate = 9.685574222925039e-05\n",
      "10000/49000 loss: 0.4215517825212976\n",
      "20000/49000 loss: 0.4613362212699259\n",
      "30000/49000 loss: 0.42801220089018066\n",
      "40000/49000 loss: 0.44222956534397584\n",
      "epoch 33: valid acc = 0.844, new learning rate = 9.201295511778786e-05\n",
      "10000/49000 loss: 0.49837692545864776\n",
      "20000/49000 loss: 0.4338162495638251\n",
      "30000/49000 loss: 0.4202882756830544\n",
      "40000/49000 loss: 0.4388118062747757\n",
      "epoch 34: valid acc = 0.839, new learning rate = 8.741230736189846e-05\n",
      "10000/49000 loss: 0.4567128907365337\n",
      "20000/49000 loss: 0.4054322772855015\n",
      "30000/49000 loss: 0.4623235766622522\n",
      "40000/49000 loss: 0.4271996263120082\n",
      "epoch 35: valid acc = 0.843, new learning rate = 8.304169199380353e-05\n",
      "10000/49000 loss: 0.4551832348998808\n",
      "20000/49000 loss: 0.4264926274815329\n",
      "30000/49000 loss: 0.4892110143832939\n",
      "40000/49000 loss: 0.4154291788457362\n",
      "epoch 36: valid acc = 0.841, new learning rate = 7.888960739411335e-05\n",
      "10000/49000 loss: 0.4396682684167459\n",
      "20000/49000 loss: 0.42317617684485626\n",
      "30000/49000 loss: 0.42734360053253295\n",
      "40000/49000 loss: 0.4912638268281934\n",
      "epoch 37: valid acc = 0.842, new learning rate = 7.494512702440768e-05\n",
      "10000/49000 loss: 0.43993175065849144\n",
      "20000/49000 loss: 0.4050534217683009\n",
      "30000/49000 loss: 0.43025958647391377\n",
      "40000/49000 loss: 0.4817763576887197\n",
      "epoch 38: valid acc = 0.842, new learning rate = 7.119787067318729e-05\n",
      "10000/49000 loss: 0.4229042379409862\n",
      "20000/49000 loss: 0.44084784619376627\n",
      "30000/49000 loss: 0.4284821529920065\n",
      "40000/49000 loss: 0.38518573214381957\n",
      "epoch 39: valid acc = 0.841, new learning rate = 6.763797713952792e-05\n",
      "10000/49000 loss: 0.42487257502410375\n",
      "20000/49000 loss: 0.4313408843483668\n",
      "30000/49000 loss: 0.4281326634595401\n",
      "40000/49000 loss: 0.40291904816261714\n",
      "epoch 40: valid acc = 0.843, new learning rate = 6.425607828255152e-05\n",
      "10000/49000 loss: 0.4281589789239651\n",
      "20000/49000 loss: 0.42282384145066565\n",
      "30000/49000 loss: 0.47286184514153884\n",
      "40000/49000 loss: 0.4133519250285312\n",
      "epoch 41: valid acc = 0.843, new learning rate = 6.104327436842394e-05\n",
      "10000/49000 loss: 0.41667595370118865\n",
      "20000/49000 loss: 0.4235635198021182\n",
      "30000/49000 loss: 0.39155094929209594\n",
      "40000/49000 loss: 0.4486281508964067\n",
      "epoch 42: valid acc = 0.842, new learning rate = 5.799111065000274e-05\n",
      "10000/49000 loss: 0.4260898146649899\n",
      "20000/49000 loss: 0.4627803167846614\n",
      "30000/49000 loss: 0.43432752198250507\n",
      "40000/49000 loss: 0.38355256830481355\n",
      "epoch 43: valid acc = 0.842, new learning rate = 5.5091555117502596e-05\n",
      "10000/49000 loss: 0.43641456788587013\n",
      "20000/49000 loss: 0.426563621551912\n",
      "30000/49000 loss: 0.3901392535974174\n",
      "40000/49000 loss: 0.4356069560656519\n",
      "epoch 44: valid acc = 0.843, new learning rate = 5.2336977361627463e-05\n",
      "10000/49000 loss: 0.4028762782093679\n",
      "20000/49000 loss: 0.4235823679735591\n",
      "30000/49000 loss: 0.4394695479703822\n",
      "40000/49000 loss: 0.4307623005241462\n",
      "epoch 45: valid acc = 0.844, new learning rate = 4.972012849354609e-05\n",
      "10000/49000 loss: 0.428152194241936\n",
      "20000/49000 loss: 0.4345667252778755\n",
      "30000/49000 loss: 0.47230676164925967\n",
      "40000/49000 loss: 0.4644165570592566\n",
      "epoch 46: valid acc = 0.843, new learning rate = 4.723412206886878e-05\n",
      "10000/49000 loss: 0.3873183628875153\n",
      "20000/49000 loss: 0.43982337586664894\n",
      "30000/49000 loss: 0.41784310882257975\n",
      "40000/49000 loss: 0.4289537025770844\n",
      "epoch 47: valid acc = 0.844, new learning rate = 4.487241596542534e-05\n",
      "10000/49000 loss: 0.42251815504363816\n",
      "20000/49000 loss: 0.4032761834475395\n",
      "30000/49000 loss: 0.4408058825827591\n",
      "40000/49000 loss: 0.4444275254093915\n",
      "epoch 48: valid acc = 0.846, new learning rate = 4.262879516715407e-05\n",
      "10000/49000 loss: 0.44406914743553866\n",
      "20000/49000 loss: 0.4532936870203955\n",
      "30000/49000 loss: 0.4280398962827601\n",
      "40000/49000 loss: 0.3737965688722562\n",
      "epoch 49: valid acc = 0.845, new learning rate = 4.049735540879637e-05\n",
      "10000/49000 loss: 0.4183001011706377\n",
      "20000/49000 loss: 0.45741186875795203\n",
      "30000/49000 loss: 0.4373930228890419\n",
      "40000/49000 loss: 0.41931408393020203\n",
      "epoch 50: valid acc = 0.849, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.8499591836734693\n",
      "test acc: 0.849\n",
      "test acc: 0.8355\n",
      "number of batches for training: 49\n",
      "epoch 1: valid acc = 0.37, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.541, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.587, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.707, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.739, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.748, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.765, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.779, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.786, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.791, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.793, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.805, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.809, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.814, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.818, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.818, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.825, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.823, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.829, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.828, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.832, new learning rate = 0.00017028081314405735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22: valid acc = 0.83, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.83, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.828, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.835, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.834, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.835, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.839, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.834, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.834, new learning rate = 0.00010731938197146858\n",
      "epoch 31: valid acc = 0.84, new learning rate = 0.00010195341287289515\n",
      "epoch 32: valid acc = 0.839, new learning rate = 9.685574222925039e-05\n",
      "epoch 33: valid acc = 0.84, new learning rate = 9.201295511778786e-05\n",
      "epoch 34: valid acc = 0.842, new learning rate = 8.741230736189846e-05\n",
      "epoch 35: valid acc = 0.842, new learning rate = 8.304169199380353e-05\n",
      "epoch 36: valid acc = 0.842, new learning rate = 7.888960739411335e-05\n",
      "epoch 37: valid acc = 0.842, new learning rate = 7.494512702440768e-05\n",
      "epoch 38: valid acc = 0.844, new learning rate = 7.119787067318729e-05\n",
      "epoch 39: valid acc = 0.846, new learning rate = 6.763797713952792e-05\n",
      "epoch 40: valid acc = 0.845, new learning rate = 6.425607828255152e-05\n",
      "epoch 41: valid acc = 0.847, new learning rate = 6.104327436842394e-05\n",
      "epoch 42: valid acc = 0.848, new learning rate = 5.799111065000274e-05\n",
      "epoch 43: valid acc = 0.849, new learning rate = 5.5091555117502596e-05\n",
      "epoch 44: valid acc = 0.844, new learning rate = 5.2336977361627463e-05\n",
      "epoch 45: valid acc = 0.848, new learning rate = 4.972012849354609e-05\n",
      "epoch 46: valid acc = 0.847, new learning rate = 4.723412206886878e-05\n",
      "epoch 47: valid acc = 0.847, new learning rate = 4.487241596542534e-05\n",
      "epoch 48: valid acc = 0.847, new learning rate = 4.262879516715407e-05\n",
      "epoch 49: valid acc = 0.845, new learning rate = 4.049735540879637e-05\n",
      "epoch 50: valid acc = 0.849, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.8519591836734693\n",
      "test acc: 0.849\n",
      "test acc: 0.8382\n",
      "number of batches for training: 49\n",
      "10000/49000 loss: 2.667603495663016\n",
      "20000/49000 loss: 2.596894330267099\n",
      "30000/49000 loss: 2.558537989703485\n",
      "40000/49000 loss: 2.4120545892560545\n",
      "epoch 1: valid acc = 0.364, new learning rate = 0.000475\n",
      "10000/49000 loss: 2.0896121597914816\n",
      "20000/49000 loss: 1.9874386834013378\n",
      "30000/49000 loss: 1.9107288306697472\n",
      "40000/49000 loss: 1.577396418165766\n",
      "epoch 2: valid acc = 0.523, new learning rate = 0.00045125\n",
      "10000/49000 loss: 1.2534845941484378\n",
      "20000/49000 loss: 1.1948786446773072\n",
      "30000/49000 loss: 1.099116002481887\n",
      "40000/49000 loss: 1.1463486650931582\n",
      "epoch 3: valid acc = 0.651, new learning rate = 0.0004286875\n",
      "10000/49000 loss: 0.9862522012850002\n",
      "20000/49000 loss: 0.9150600980392937\n",
      "30000/49000 loss: 0.9508263827589599\n",
      "40000/49000 loss: 0.8695460760927959\n",
      "epoch 4: valid acc = 0.714, new learning rate = 0.00040725312499999993\n",
      "10000/49000 loss: 0.8025190552094504\n",
      "20000/49000 loss: 0.8125895835635981\n",
      "30000/49000 loss: 0.7944189250182685\n",
      "40000/49000 loss: 0.7735898918338603\n",
      "epoch 5: valid acc = 0.731, new learning rate = 0.0003868904687499999\n",
      "10000/49000 loss: 0.7412709054913122\n",
      "20000/49000 loss: 0.6832466147011416\n",
      "30000/49000 loss: 0.67773255232098\n",
      "40000/49000 loss: 0.7021121964282186\n",
      "epoch 6: valid acc = 0.752, new learning rate = 0.0003675459453124999\n",
      "10000/49000 loss: 0.6618440072791008\n",
      "20000/49000 loss: 0.6758775718709166\n",
      "30000/49000 loss: 0.6740422127040515\n",
      "40000/49000 loss: 0.642292938953089\n",
      "epoch 7: valid acc = 0.758, new learning rate = 0.00034916864804687486\n",
      "10000/49000 loss: 0.6544903313356715\n",
      "20000/49000 loss: 0.6077601108473756\n",
      "30000/49000 loss: 0.6541094976475802\n",
      "40000/49000 loss: 0.583785255470541\n",
      "epoch 8: valid acc = 0.77, new learning rate = 0.0003317102156445311\n",
      "10000/49000 loss: 0.5850612764899679\n",
      "20000/49000 loss: 0.5382482956075508\n",
      "30000/49000 loss: 0.5770422953053447\n",
      "40000/49000 loss: 0.5668791061335844\n",
      "epoch 9: valid acc = 0.789, new learning rate = 0.0003151247048623045\n",
      "10000/49000 loss: 0.5607240479449965\n",
      "20000/49000 loss: 0.6014267545895141\n",
      "30000/49000 loss: 0.582526422473445\n",
      "40000/49000 loss: 0.5219113273676528\n",
      "epoch 10: valid acc = 0.792, new learning rate = 0.00029936846961918924\n",
      "10000/49000 loss: 0.5203808869873586\n",
      "20000/49000 loss: 0.5005872184739814\n",
      "30000/49000 loss: 0.5996410899582996\n",
      "40000/49000 loss: 0.5687488186277148\n",
      "epoch 11: valid acc = 0.806, new learning rate = 0.00028440004613822977\n",
      "10000/49000 loss: 0.5068977046290646\n",
      "20000/49000 loss: 0.5454238869720672\n",
      "30000/49000 loss: 0.5327236840068852\n",
      "40000/49000 loss: 0.5305097223634829\n",
      "epoch 12: valid acc = 0.809, new learning rate = 0.00027018004383131826\n",
      "10000/49000 loss: 0.4854327378632311\n",
      "20000/49000 loss: 0.5564149356748948\n",
      "30000/49000 loss: 0.545829464094799\n",
      "40000/49000 loss: 0.4825180569612108\n",
      "epoch 13: valid acc = 0.814, new learning rate = 0.00025667104163975234\n",
      "10000/49000 loss: 0.5091981307072146\n",
      "20000/49000 loss: 0.4724040389702389\n",
      "30000/49000 loss: 0.5561371880722449\n",
      "40000/49000 loss: 0.5290273623919793\n",
      "epoch 14: valid acc = 0.816, new learning rate = 0.00024383748955776472\n",
      "10000/49000 loss: 0.51454020165896\n",
      "20000/49000 loss: 0.5054091181190724\n",
      "30000/49000 loss: 0.48851130989388875\n",
      "40000/49000 loss: 0.4839703592229368\n",
      "epoch 15: valid acc = 0.822, new learning rate = 0.00023164561507987649\n",
      "10000/49000 loss: 0.5326684761307509\n",
      "20000/49000 loss: 0.4978460154059383\n",
      "30000/49000 loss: 0.45904805270648863\n",
      "40000/49000 loss: 0.49983259258958407\n",
      "epoch 16: valid acc = 0.822, new learning rate = 0.00022006333432588265\n",
      "10000/49000 loss: 0.4902319443797663\n",
      "20000/49000 loss: 0.490794286062797\n",
      "30000/49000 loss: 0.4640895582508388\n",
      "40000/49000 loss: 0.49535172169335706\n",
      "epoch 17: valid acc = 0.828, new learning rate = 0.00020906016760958852\n",
      "10000/49000 loss: 0.4699420411390664\n",
      "20000/49000 loss: 0.44089743318664504\n",
      "30000/49000 loss: 0.4913323266982129\n",
      "40000/49000 loss: 0.4352762180714611\n",
      "epoch 18: valid acc = 0.825, new learning rate = 0.00019860715922910907\n",
      "10000/49000 loss: 0.48923923279188547\n",
      "20000/49000 loss: 0.4855015907408157\n",
      "30000/49000 loss: 0.5528907825839658\n",
      "40000/49000 loss: 0.42826369639651435\n",
      "epoch 19: valid acc = 0.826, new learning rate = 0.0001886768012676536\n",
      "10000/49000 loss: 0.4531854775532494\n",
      "20000/49000 loss: 0.4635918581209215\n",
      "30000/49000 loss: 0.462679710282161\n",
      "40000/49000 loss: 0.4620583181808912\n",
      "epoch 20: valid acc = 0.83, new learning rate = 0.0001792429612042709\n",
      "10000/49000 loss: 0.507502276158175\n",
      "20000/49000 loss: 0.46739951579860467\n",
      "30000/49000 loss: 0.4827497436114764\n",
      "40000/49000 loss: 0.48658575087725625\n",
      "epoch 21: valid acc = 0.834, new learning rate = 0.00017028081314405735\n",
      "10000/49000 loss: 0.45115038276508834\n",
      "20000/49000 loss: 0.48514038418059596\n",
      "30000/49000 loss: 0.47360347243429796\n",
      "40000/49000 loss: 0.5011125551623281\n",
      "epoch 22: valid acc = 0.837, new learning rate = 0.00016176677248685447\n",
      "10000/49000 loss: 0.4264246057607534\n",
      "20000/49000 loss: 0.45991285095024964\n",
      "30000/49000 loss: 0.4755297544933806\n",
      "40000/49000 loss: 0.47549503292136963\n",
      "epoch 23: valid acc = 0.837, new learning rate = 0.00015367843386251173\n",
      "10000/49000 loss: 0.4905897032692725\n",
      "20000/49000 loss: 0.46470395719567265\n",
      "30000/49000 loss: 0.49824085682335667\n",
      "40000/49000 loss: 0.4560412105822825\n",
      "epoch 24: valid acc = 0.836, new learning rate = 0.00014599451216938612\n",
      "10000/49000 loss: 0.448571753611128\n",
      "20000/49000 loss: 0.40539186620211715\n",
      "30000/49000 loss: 0.4613140182757311\n",
      "40000/49000 loss: 0.45112254117977596\n",
      "epoch 25: valid acc = 0.835, new learning rate = 0.00013869478656091682\n",
      "10000/49000 loss: 0.4390176471468448\n",
      "20000/49000 loss: 0.448427683985924\n",
      "30000/49000 loss: 0.4645099193779908\n",
      "40000/49000 loss: 0.45563885632889006\n",
      "epoch 26: valid acc = 0.839, new learning rate = 0.00013176004723287096\n",
      "10000/49000 loss: 0.4332819395002858\n",
      "20000/49000 loss: 0.4664894193510041\n",
      "30000/49000 loss: 0.44077734897796683\n",
      "40000/49000 loss: 0.4762093892116008\n",
      "epoch 27: valid acc = 0.835, new learning rate = 0.0001251720448712274\n",
      "10000/49000 loss: 0.48083308172854883\n",
      "20000/49000 loss: 0.4416448396501292\n",
      "30000/49000 loss: 0.4560114237469112\n",
      "40000/49000 loss: 0.4531199676745767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 28: valid acc = 0.839, new learning rate = 0.00011891344262766602\n",
      "10000/49000 loss: 0.4747073043875463\n",
      "20000/49000 loss: 0.45844049131271714\n",
      "30000/49000 loss: 0.42785340218955303\n",
      "40000/49000 loss: 0.47900340195322466\n",
      "epoch 29: valid acc = 0.837, new learning rate = 0.00011296777049628272\n",
      "10000/49000 loss: 0.4352261132654513\n",
      "20000/49000 loss: 0.41320064449464533\n",
      "30000/49000 loss: 0.4034759729172537\n",
      "40000/49000 loss: 0.4920226201564145\n",
      "epoch 30: valid acc = 0.843, new learning rate = 0.00010731938197146858\n",
      "10000/49000 loss: 0.47045604177923034\n",
      "20000/49000 loss: 0.41767401432139356\n",
      "30000/49000 loss: 0.4371077634366465\n",
      "40000/49000 loss: 0.4311180428091177\n",
      "epoch 31: valid acc = 0.845, new learning rate = 0.00010195341287289515\n",
      "10000/49000 loss: 0.43842681065423983\n",
      "20000/49000 loss: 0.4454629236778532\n",
      "30000/49000 loss: 0.44174784528095035\n",
      "40000/49000 loss: 0.4068216623935963\n",
      "epoch 32: valid acc = 0.843, new learning rate = 9.685574222925039e-05\n",
      "10000/49000 loss: 0.4707215217328438\n",
      "20000/49000 loss: 0.46921487786359906\n",
      "30000/49000 loss: 0.447999105885173\n",
      "40000/49000 loss: 0.4500918058058325\n",
      "epoch 33: valid acc = 0.84, new learning rate = 9.201295511778786e-05\n",
      "10000/49000 loss: 0.48509054806509405\n",
      "20000/49000 loss: 0.43393997894871644\n",
      "30000/49000 loss: 0.4082603367788594\n",
      "40000/49000 loss: 0.4571919214724243\n",
      "epoch 34: valid acc = 0.838, new learning rate = 8.741230736189846e-05\n",
      "10000/49000 loss: 0.46120464282904333\n",
      "20000/49000 loss: 0.40591965428130317\n",
      "30000/49000 loss: 0.41570590304510896\n",
      "40000/49000 loss: 0.4269904515775494\n",
      "epoch 35: valid acc = 0.84, new learning rate = 8.304169199380353e-05\n",
      "10000/49000 loss: 0.4531344997281203\n",
      "20000/49000 loss: 0.46209789331818624\n",
      "30000/49000 loss: 0.44958596160352254\n",
      "40000/49000 loss: 0.41742715369235156\n",
      "epoch 36: valid acc = 0.841, new learning rate = 7.888960739411335e-05\n",
      "10000/49000 loss: 0.4372216349451452\n",
      "20000/49000 loss: 0.422239742116777\n",
      "30000/49000 loss: 0.3965946975335711\n",
      "40000/49000 loss: 0.43262092387587375\n",
      "epoch 37: valid acc = 0.843, new learning rate = 7.494512702440768e-05\n",
      "10000/49000 loss: 0.4462359218711351\n",
      "20000/49000 loss: 0.4526831986972764\n",
      "30000/49000 loss: 0.45844339346307494\n",
      "40000/49000 loss: 0.4458273941776977\n",
      "epoch 38: valid acc = 0.844, new learning rate = 7.119787067318729e-05\n",
      "10000/49000 loss: 0.44215017536489154\n",
      "20000/49000 loss: 0.3924483610911905\n",
      "30000/49000 loss: 0.4097424472199009\n",
      "40000/49000 loss: 0.4431912622275774\n",
      "epoch 39: valid acc = 0.845, new learning rate = 6.763797713952792e-05\n",
      "10000/49000 loss: 0.4112816338367517\n",
      "20000/49000 loss: 0.4708544391092947\n",
      "30000/49000 loss: 0.4363087481442166\n",
      "40000/49000 loss: 0.4324824285372258\n",
      "epoch 40: valid acc = 0.845, new learning rate = 6.425607828255152e-05\n",
      "10000/49000 loss: 0.4507484408232635\n",
      "20000/49000 loss: 0.41310920793746764\n",
      "30000/49000 loss: 0.46468825092069693\n",
      "40000/49000 loss: 0.44173597233574774\n",
      "epoch 41: valid acc = 0.844, new learning rate = 6.104327436842394e-05\n",
      "10000/49000 loss: 0.45841312765527337\n",
      "20000/49000 loss: 0.4272558743908587\n",
      "30000/49000 loss: 0.42980066848210624\n",
      "40000/49000 loss: 0.4350899032353896\n",
      "epoch 42: valid acc = 0.845, new learning rate = 5.799111065000274e-05\n",
      "10000/49000 loss: 0.4332830124702443\n",
      "20000/49000 loss: 0.4311989581978948\n",
      "30000/49000 loss: 0.35371453582829726\n",
      "40000/49000 loss: 0.3937715542067026\n",
      "epoch 43: valid acc = 0.847, new learning rate = 5.5091555117502596e-05\n",
      "10000/49000 loss: 0.4583820253797535\n",
      "20000/49000 loss: 0.422140691120866\n",
      "30000/49000 loss: 0.3861220650730014\n",
      "40000/49000 loss: 0.3609863281644635\n",
      "epoch 44: valid acc = 0.846, new learning rate = 5.2336977361627463e-05\n",
      "10000/49000 loss: 0.4889374504473514\n",
      "20000/49000 loss: 0.42369082966214494\n",
      "30000/49000 loss: 0.4357386121406297\n",
      "40000/49000 loss: 0.4234807378408182\n",
      "epoch 45: valid acc = 0.847, new learning rate = 4.972012849354609e-05\n",
      "10000/49000 loss: 0.4312907122361857\n",
      "20000/49000 loss: 0.4069247811597323\n",
      "30000/49000 loss: 0.43283283054584837\n",
      "40000/49000 loss: 0.4306916663653875\n",
      "epoch 46: valid acc = 0.846, new learning rate = 4.723412206886878e-05\n",
      "10000/49000 loss: 0.4130794924735792\n",
      "20000/49000 loss: 0.4621430209640001\n",
      "30000/49000 loss: 0.4079947169945047\n",
      "40000/49000 loss: 0.4526788027860353\n",
      "epoch 47: valid acc = 0.846, new learning rate = 4.487241596542534e-05\n",
      "10000/49000 loss: 0.4043912836372318\n",
      "20000/49000 loss: 0.45364744343501784\n",
      "30000/49000 loss: 0.45898495340589374\n",
      "40000/49000 loss: 0.3856640479789949\n",
      "epoch 48: valid acc = 0.847, new learning rate = 4.262879516715407e-05\n",
      "10000/49000 loss: 0.44493538861213644\n",
      "20000/49000 loss: 0.44131373189088074\n",
      "30000/49000 loss: 0.4319706106917205\n",
      "40000/49000 loss: 0.4689597383713471\n",
      "epoch 49: valid acc = 0.848, new learning rate = 4.049735540879637e-05\n",
      "10000/49000 loss: 0.46920594215382944\n",
      "20000/49000 loss: 0.3588844454084164\n",
      "30000/49000 loss: 0.39642453621909046\n",
      "40000/49000 loss: 0.41115911051194104\n",
      "epoch 50: valid acc = 0.848, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.8509183673469388\n",
      "test acc: 0.848\n",
      "test acc: 0.8347\n",
      "number of batches for training: 49\n",
      "epoch 1: valid acc = 0.397, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.514, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.648, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.707, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.737, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.748, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.765, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.779, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.791, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.801, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.804, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.806, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.813, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.822, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.825, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.825, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.833, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.832, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.834, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.835, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.834, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.839, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.834, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.839, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.841, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.841, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.845, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.845, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.843, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.845, new learning rate = 0.00010731938197146858\n",
      "epoch 31: valid acc = 0.842, new learning rate = 0.00010195341287289515\n",
      "epoch 32: valid acc = 0.846, new learning rate = 9.685574222925039e-05\n",
      "epoch 33: valid acc = 0.843, new learning rate = 9.201295511778786e-05\n",
      "epoch 34: valid acc = 0.845, new learning rate = 8.741230736189846e-05\n",
      "epoch 35: valid acc = 0.843, new learning rate = 8.304169199380353e-05\n",
      "epoch 36: valid acc = 0.848, new learning rate = 7.888960739411335e-05\n",
      "epoch 37: valid acc = 0.848, new learning rate = 7.494512702440768e-05\n",
      "epoch 38: valid acc = 0.849, new learning rate = 7.119787067318729e-05\n",
      "epoch 39: valid acc = 0.85, new learning rate = 6.763797713952792e-05\n",
      "epoch 40: valid acc = 0.846, new learning rate = 6.425607828255152e-05\n",
      "epoch 41: valid acc = 0.849, new learning rate = 6.104327436842394e-05\n",
      "epoch 42: valid acc = 0.848, new learning rate = 5.799111065000274e-05\n",
      "epoch 43: valid acc = 0.851, new learning rate = 5.5091555117502596e-05\n",
      "epoch 44: valid acc = 0.846, new learning rate = 5.2336977361627463e-05\n",
      "epoch 45: valid acc = 0.849, new learning rate = 4.972012849354609e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 46: valid acc = 0.849, new learning rate = 4.723412206886878e-05\n",
      "epoch 47: valid acc = 0.849, new learning rate = 4.487241596542534e-05\n",
      "epoch 48: valid acc = 0.849, new learning rate = 4.262879516715407e-05\n",
      "epoch 49: valid acc = 0.849, new learning rate = 4.049735540879637e-05\n",
      "epoch 50: valid acc = 0.851, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.8517551020408163\n",
      "test acc: 0.851\n",
      "test acc: 0.8357\n",
      "number of batches for training: 49\n",
      "10000/49000 loss: 2.6626914544234497\n",
      "20000/49000 loss: 2.6159606894285923\n",
      "30000/49000 loss: 2.5914619874109284\n",
      "40000/49000 loss: 2.4671871753962344\n",
      "epoch 1: valid acc = 0.389, new learning rate = 0.000475\n",
      "10000/49000 loss: 2.1678500235576803\n",
      "20000/49000 loss: 2.037963582404069\n",
      "30000/49000 loss: 1.8018088893056645\n",
      "40000/49000 loss: 1.6308413178913321\n",
      "epoch 2: valid acc = 0.536, new learning rate = 0.00045125\n",
      "10000/49000 loss: 1.2616151436905803\n",
      "20000/49000 loss: 1.1812822762286195\n",
      "30000/49000 loss: 1.117793953817751\n",
      "40000/49000 loss: 1.1039699384089348\n",
      "epoch 3: valid acc = 0.598, new learning rate = 0.0004286875\n",
      "10000/49000 loss: 1.059221502594522\n",
      "20000/49000 loss: 1.021506862780242\n",
      "30000/49000 loss: 0.9692648866863587\n",
      "40000/49000 loss: 0.883574906007037\n",
      "epoch 4: valid acc = 0.706, new learning rate = 0.00040725312499999993\n",
      "10000/49000 loss: 0.8847941408277286\n",
      "20000/49000 loss: 0.8717313532369977\n",
      "30000/49000 loss: 0.7957504173796458\n",
      "40000/49000 loss: 0.7686989845351261\n",
      "epoch 5: valid acc = 0.735, new learning rate = 0.0003868904687499999\n",
      "10000/49000 loss: 0.7146732433898952\n",
      "20000/49000 loss: 0.7376444035501117\n",
      "30000/49000 loss: 0.7336972894885417\n",
      "40000/49000 loss: 0.6771148246596096\n",
      "epoch 6: valid acc = 0.754, new learning rate = 0.0003675459453124999\n",
      "10000/49000 loss: 0.6501258901797496\n",
      "20000/49000 loss: 0.6783127147072441\n",
      "30000/49000 loss: 0.6061051654729293\n",
      "40000/49000 loss: 0.6215751257360569\n",
      "epoch 7: valid acc = 0.755, new learning rate = 0.00034916864804687486\n",
      "10000/49000 loss: 0.6726081125720591\n",
      "20000/49000 loss: 0.6284796918864679\n",
      "30000/49000 loss: 0.6119169005012731\n",
      "40000/49000 loss: 0.6125738712426074\n",
      "epoch 8: valid acc = 0.769, new learning rate = 0.0003317102156445311\n",
      "10000/49000 loss: 0.6290191659981162\n",
      "20000/49000 loss: 0.5889331799483972\n",
      "30000/49000 loss: 0.5714684027538961\n",
      "40000/49000 loss: 0.575896141292568\n",
      "epoch 9: valid acc = 0.774, new learning rate = 0.0003151247048623045\n",
      "10000/49000 loss: 0.5752182902715788\n",
      "20000/49000 loss: 0.6045475189419206\n",
      "30000/49000 loss: 0.5782863448158466\n",
      "40000/49000 loss: 0.573406316011446\n",
      "epoch 10: valid acc = 0.78, new learning rate = 0.00029936846961918924\n",
      "10000/49000 loss: 0.5547700406011691\n",
      "20000/49000 loss: 0.533214379567244\n",
      "30000/49000 loss: 0.5465236958031723\n",
      "40000/49000 loss: 0.5450635856171112\n",
      "epoch 11: valid acc = 0.79, new learning rate = 0.00028440004613822977\n",
      "10000/49000 loss: 0.5244980030231786\n",
      "20000/49000 loss: 0.5162610603787952\n",
      "30000/49000 loss: 0.5154093991203846\n",
      "40000/49000 loss: 0.5299744005958962\n",
      "epoch 12: valid acc = 0.802, new learning rate = 0.00027018004383131826\n",
      "10000/49000 loss: 0.521242743770471\n",
      "20000/49000 loss: 0.5125760968904833\n",
      "30000/49000 loss: 0.5222376247365432\n",
      "40000/49000 loss: 0.5278895478054912\n",
      "epoch 13: valid acc = 0.803, new learning rate = 0.00025667104163975234\n",
      "10000/49000 loss: 0.530368601212234\n",
      "20000/49000 loss: 0.49350846250890207\n",
      "30000/49000 loss: 0.5455400645818715\n",
      "40000/49000 loss: 0.5078784520209786\n",
      "epoch 14: valid acc = 0.813, new learning rate = 0.00024383748955776472\n",
      "10000/49000 loss: 0.5138652216774546\n",
      "20000/49000 loss: 0.4856616318144641\n",
      "30000/49000 loss: 0.5057398290847629\n",
      "40000/49000 loss: 0.4841902161866324\n",
      "epoch 15: valid acc = 0.813, new learning rate = 0.00023164561507987649\n",
      "10000/49000 loss: 0.5164035691102805\n",
      "20000/49000 loss: 0.4744228819031926\n",
      "30000/49000 loss: 0.46902929696973866\n",
      "40000/49000 loss: 0.5090170884204148\n",
      "epoch 16: valid acc = 0.815, new learning rate = 0.00022006333432588265\n",
      "10000/49000 loss: 0.5135559895890376\n",
      "20000/49000 loss: 0.5304364565087156\n",
      "30000/49000 loss: 0.4782608270267118\n",
      "40000/49000 loss: 0.5092906933923224\n",
      "epoch 17: valid acc = 0.819, new learning rate = 0.00020906016760958852\n",
      "10000/49000 loss: 0.4490219058884244\n",
      "20000/49000 loss: 0.47182370280652497\n",
      "30000/49000 loss: 0.5003501171252849\n",
      "40000/49000 loss: 0.4995564445045242\n",
      "epoch 18: valid acc = 0.829, new learning rate = 0.00019860715922910907\n",
      "10000/49000 loss: 0.45915115209588336\n",
      "20000/49000 loss: 0.45964805314145796\n",
      "30000/49000 loss: 0.4573411465799458\n",
      "40000/49000 loss: 0.4904098401749231\n",
      "epoch 19: valid acc = 0.831, new learning rate = 0.0001886768012676536\n",
      "10000/49000 loss: 0.4483810156291781\n",
      "20000/49000 loss: 0.45838632854628003\n",
      "30000/49000 loss: 0.4738461942393178\n",
      "40000/49000 loss: 0.4922864457073128\n",
      "epoch 20: valid acc = 0.831, new learning rate = 0.0001792429612042709\n",
      "10000/49000 loss: 0.4483209897440162\n",
      "20000/49000 loss: 0.5007422330078413\n",
      "30000/49000 loss: 0.4778225122404404\n",
      "40000/49000 loss: 0.4606037987727351\n",
      "epoch 21: valid acc = 0.835, new learning rate = 0.00017028081314405735\n",
      "10000/49000 loss: 0.5001494997382027\n",
      "20000/49000 loss: 0.4725526749066336\n",
      "30000/49000 loss: 0.4374078433548302\n",
      "40000/49000 loss: 0.4715045635789521\n",
      "epoch 22: valid acc = 0.831, new learning rate = 0.00016176677248685447\n",
      "10000/49000 loss: 0.4892791269987646\n",
      "20000/49000 loss: 0.48313171360321694\n",
      "30000/49000 loss: 0.4297778378452571\n",
      "40000/49000 loss: 0.3995438894958053\n",
      "epoch 23: valid acc = 0.834, new learning rate = 0.00015367843386251173\n",
      "10000/49000 loss: 0.4567295573914885\n",
      "20000/49000 loss: 0.4652755271508407\n",
      "30000/49000 loss: 0.4619233768640209\n",
      "40000/49000 loss: 0.4971984027843503\n",
      "epoch 24: valid acc = 0.833, new learning rate = 0.00014599451216938612\n",
      "10000/49000 loss: 0.4045959248479807\n",
      "20000/49000 loss: 0.4756595998372976\n",
      "30000/49000 loss: 0.44356523090403416\n",
      "40000/49000 loss: 0.44713302079645423\n",
      "epoch 25: valid acc = 0.833, new learning rate = 0.00013869478656091682\n",
      "10000/49000 loss: 0.4642274779195585\n",
      "20000/49000 loss: 0.4543352104450078\n",
      "30000/49000 loss: 0.42801703368936467\n",
      "40000/49000 loss: 0.43355575452212275\n",
      "epoch 26: valid acc = 0.835, new learning rate = 0.00013176004723287096\n",
      "10000/49000 loss: 0.4392600090891846\n",
      "20000/49000 loss: 0.48054730638546916\n",
      "30000/49000 loss: 0.4794337250557895\n",
      "40000/49000 loss: 0.42977217190606376\n",
      "epoch 27: valid acc = 0.836, new learning rate = 0.0001251720448712274\n",
      "10000/49000 loss: 0.46526059318749824\n",
      "20000/49000 loss: 0.4762465998378525\n",
      "30000/49000 loss: 0.41654698658355693\n",
      "40000/49000 loss: 0.439146668042427\n",
      "epoch 28: valid acc = 0.837, new learning rate = 0.00011891344262766602\n",
      "10000/49000 loss: 0.481226744097095\n",
      "20000/49000 loss: 0.4682093781312717\n",
      "30000/49000 loss: 0.42020311268271604\n",
      "40000/49000 loss: 0.45419360646465623\n",
      "epoch 29: valid acc = 0.839, new learning rate = 0.00011296777049628272\n",
      "10000/49000 loss: 0.41986431947969377\n",
      "20000/49000 loss: 0.42613012069185524\n",
      "30000/49000 loss: 0.4162744979489689\n",
      "40000/49000 loss: 0.4686379296818647\n",
      "epoch 30: valid acc = 0.844, new learning rate = 0.00010731938197146858\n",
      "10000/49000 loss: 0.4214331640294719\n",
      "20000/49000 loss: 0.4374719365756779\n",
      "30000/49000 loss: 0.46802921703186146\n",
      "40000/49000 loss: 0.47043283223421734\n",
      "epoch 31: valid acc = 0.838, new learning rate = 0.00010195341287289515\n",
      "10000/49000 loss: 0.463164910326864\n",
      "20000/49000 loss: 0.4319680340735238\n",
      "30000/49000 loss: 0.44528758255141193\n",
      "40000/49000 loss: 0.4522020979163555\n",
      "epoch 32: valid acc = 0.836, new learning rate = 9.685574222925039e-05\n",
      "10000/49000 loss: 0.41477818311251824\n",
      "20000/49000 loss: 0.4647637900753948\n",
      "30000/49000 loss: 0.41806823252735686\n",
      "40000/49000 loss: 0.40834687922176877\n",
      "epoch 33: valid acc = 0.838, new learning rate = 9.201295511778786e-05\n",
      "10000/49000 loss: 0.42460748105880747\n",
      "20000/49000 loss: 0.4289689863166539\n",
      "30000/49000 loss: 0.40827612449848893\n",
      "40000/49000 loss: 0.4314895654407781\n",
      "epoch 34: valid acc = 0.838, new learning rate = 8.741230736189846e-05\n",
      "10000/49000 loss: 0.4437333001151516\n",
      "20000/49000 loss: 0.4520357372659034\n",
      "30000/49000 loss: 0.4567120650736353\n",
      "40000/49000 loss: 0.4437236002260181\n",
      "epoch 35: valid acc = 0.844, new learning rate = 8.304169199380353e-05\n",
      "10000/49000 loss: 0.4038933339797196\n",
      "20000/49000 loss: 0.44197093289546174\n",
      "30000/49000 loss: 0.4430241591600435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/49000 loss: 0.5015764056450971\n",
      "epoch 36: valid acc = 0.842, new learning rate = 7.888960739411335e-05\n",
      "10000/49000 loss: 0.46183075626132847\n",
      "20000/49000 loss: 0.3820835444551169\n",
      "30000/49000 loss: 0.39198976959052084\n",
      "40000/49000 loss: 0.3970889194502851\n",
      "epoch 37: valid acc = 0.843, new learning rate = 7.494512702440768e-05\n",
      "10000/49000 loss: 0.43876013813330783\n",
      "20000/49000 loss: 0.4118214197588117\n",
      "30000/49000 loss: 0.41572646153459825\n",
      "40000/49000 loss: 0.42438017535836575\n",
      "epoch 38: valid acc = 0.849, new learning rate = 7.119787067318729e-05\n",
      "10000/49000 loss: 0.42029096247980574\n",
      "20000/49000 loss: 0.4300708693360452\n",
      "30000/49000 loss: 0.41171618308469216\n",
      "40000/49000 loss: 0.42905239191712025\n",
      "epoch 39: valid acc = 0.848, new learning rate = 6.763797713952792e-05\n",
      "10000/49000 loss: 0.4474478374051974\n",
      "20000/49000 loss: 0.42672344747102375\n",
      "30000/49000 loss: 0.42713980250835054\n",
      "40000/49000 loss: 0.4118213106341788\n",
      "epoch 40: valid acc = 0.846, new learning rate = 6.425607828255152e-05\n",
      "10000/49000 loss: 0.3890680598001364\n",
      "20000/49000 loss: 0.41763081704116056\n",
      "30000/49000 loss: 0.45533500788069586\n",
      "40000/49000 loss: 0.4394841991893149\n",
      "epoch 41: valid acc = 0.846, new learning rate = 6.104327436842394e-05\n",
      "10000/49000 loss: 0.4163387020002584\n",
      "20000/49000 loss: 0.4182021602959066\n",
      "30000/49000 loss: 0.45526095727441335\n",
      "40000/49000 loss: 0.4687666281073816\n",
      "epoch 42: valid acc = 0.849, new learning rate = 5.799111065000274e-05\n",
      "10000/49000 loss: 0.4186669008795802\n",
      "20000/49000 loss: 0.39457646462919627\n",
      "30000/49000 loss: 0.4558189690330295\n",
      "40000/49000 loss: 0.430017506279298\n",
      "epoch 43: valid acc = 0.845, new learning rate = 5.5091555117502596e-05\n",
      "10000/49000 loss: 0.4200847443402192\n",
      "20000/49000 loss: 0.4389093992797878\n",
      "30000/49000 loss: 0.4448675467915876\n",
      "40000/49000 loss: 0.4373482999954003\n",
      "epoch 44: valid acc = 0.846, new learning rate = 5.2336977361627463e-05\n",
      "10000/49000 loss: 0.4526844985572112\n",
      "20000/49000 loss: 0.42492341979434944\n",
      "30000/49000 loss: 0.4223759866351032\n",
      "40000/49000 loss: 0.4116779280902961\n",
      "epoch 45: valid acc = 0.848, new learning rate = 4.972012849354609e-05\n",
      "10000/49000 loss: 0.4299561257712621\n",
      "20000/49000 loss: 0.40393902464719944\n",
      "30000/49000 loss: 0.44347267755062625\n",
      "40000/49000 loss: 0.41658557383653666\n",
      "epoch 46: valid acc = 0.848, new learning rate = 4.723412206886878e-05\n",
      "10000/49000 loss: 0.440476562515081\n",
      "20000/49000 loss: 0.4482301000314778\n",
      "30000/49000 loss: 0.45197957614100287\n",
      "40000/49000 loss: 0.4492071880818197\n",
      "epoch 47: valid acc = 0.846, new learning rate = 4.487241596542534e-05\n",
      "10000/49000 loss: 0.3558734183039273\n",
      "20000/49000 loss: 0.4756228351456156\n",
      "30000/49000 loss: 0.4241105269626296\n",
      "40000/49000 loss: 0.419171801667621\n",
      "epoch 48: valid acc = 0.846, new learning rate = 4.262879516715407e-05\n",
      "10000/49000 loss: 0.4001276674745612\n",
      "20000/49000 loss: 0.40064936031018683\n",
      "30000/49000 loss: 0.40717929722709145\n",
      "40000/49000 loss: 0.3717423633826429\n",
      "epoch 49: valid acc = 0.844, new learning rate = 4.049735540879637e-05\n",
      "10000/49000 loss: 0.3791028872268088\n",
      "20000/49000 loss: 0.428576731387658\n",
      "30000/49000 loss: 0.3764902298840836\n",
      "40000/49000 loss: 0.38151229617238885\n",
      "epoch 50: valid acc = 0.845, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.849265306122449\n",
      "test acc: 0.845\n",
      "test acc: 0.8335\n",
      "number of batches for training: 49\n",
      "epoch 1: valid acc = 0.364, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.532, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.649, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.717, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.738, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.753, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.768, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.783, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.786, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.794, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.794, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.803, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.808, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.816, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.823, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.823, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.822, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.829, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.829, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.831, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.829, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.831, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.833, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.836, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.839, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.837, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.835, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.842, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.84, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.844, new learning rate = 0.00010731938197146858\n",
      "epoch 31: valid acc = 0.845, new learning rate = 0.00010195341287289515\n",
      "epoch 32: valid acc = 0.849, new learning rate = 9.685574222925039e-05\n",
      "epoch 33: valid acc = 0.847, new learning rate = 9.201295511778786e-05\n",
      "epoch 34: valid acc = 0.844, new learning rate = 8.741230736189846e-05\n",
      "epoch 35: valid acc = 0.845, new learning rate = 8.304169199380353e-05\n",
      "epoch 36: valid acc = 0.845, new learning rate = 7.888960739411335e-05\n",
      "epoch 37: valid acc = 0.85, new learning rate = 7.494512702440768e-05\n",
      "epoch 38: valid acc = 0.849, new learning rate = 7.119787067318729e-05\n",
      "epoch 39: valid acc = 0.851, new learning rate = 6.763797713952792e-05\n",
      "epoch 40: valid acc = 0.849, new learning rate = 6.425607828255152e-05\n",
      "epoch 41: valid acc = 0.853, new learning rate = 6.104327436842394e-05\n",
      "epoch 42: valid acc = 0.848, new learning rate = 5.799111065000274e-05\n",
      "epoch 43: valid acc = 0.847, new learning rate = 5.5091555117502596e-05\n",
      "epoch 44: valid acc = 0.849, new learning rate = 5.2336977361627463e-05\n",
      "epoch 45: valid acc = 0.85, new learning rate = 4.972012849354609e-05\n",
      "epoch 46: valid acc = 0.851, new learning rate = 4.723412206886878e-05\n",
      "epoch 47: valid acc = 0.85, new learning rate = 4.487241596542534e-05\n",
      "epoch 48: valid acc = 0.852, new learning rate = 4.262879516715407e-05\n",
      "epoch 49: valid acc = 0.853, new learning rate = 4.049735540879637e-05\n",
      "epoch 50: valid acc = 0.853, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.8517755102040816\n",
      "test acc: 0.853\n",
      "test acc: 0.836\n",
      "number of batches for training: 245\n",
      "2000/49000 loss: 2.8478092202364347\n",
      "4000/49000 loss: 3.131540038657124\n",
      "6000/49000 loss: 2.786141983150591\n",
      "8000/49000 loss: 2.3862730298332053\n",
      "10000/49000 loss: 2.1185426108659273\n",
      "12000/49000 loss: 2.131250125219456\n",
      "14000/49000 loss: 1.9175909423828301\n",
      "16000/49000 loss: 1.7818705883253374\n",
      "18000/49000 loss: 1.448179073086061\n",
      "20000/49000 loss: 1.187036571670348\n",
      "22000/49000 loss: 1.1738550497866365\n",
      "24000/49000 loss: 1.0295540774406957\n",
      "26000/49000 loss: 1.027189266745953\n",
      "28000/49000 loss: 1.0060835273170656\n",
      "30000/49000 loss: 1.0370107473464891\n",
      "32000/49000 loss: 0.9777062945382093\n",
      "34000/49000 loss: 0.9417567941874766\n",
      "36000/49000 loss: 0.821631312472547\n",
      "38000/49000 loss: 0.8130281234099088\n",
      "40000/49000 loss: 0.7253866613638423\n",
      "42000/49000 loss: 0.8704382733597449\n",
      "44000/49000 loss: 0.7698335619352072\n",
      "46000/49000 loss: 0.6928538266827172\n",
      "48000/49000 loss: 0.6680436279526054\n",
      "epoch 1: valid acc = 0.757, new learning rate = 0.000475\n",
      "2000/49000 loss: 0.6782444104195912\n",
      "4000/49000 loss: 0.6935198836457444\n",
      "6000/49000 loss: 0.7351599030556749\n",
      "8000/49000 loss: 0.7780494327791543\n",
      "10000/49000 loss: 0.6734119021015871\n",
      "12000/49000 loss: 0.6345974302304523\n",
      "14000/49000 loss: 0.5311063948905791\n",
      "16000/49000 loss: 0.5915610823493063\n",
      "18000/49000 loss: 0.6540007997508454\n",
      "20000/49000 loss: 0.6762835665868866\n",
      "22000/49000 loss: 0.6150619675549467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000/49000 loss: 0.5405371262389552\n",
      "26000/49000 loss: 0.6162154792458934\n",
      "28000/49000 loss: 0.5655648247964297\n",
      "30000/49000 loss: 0.5647518759506088\n",
      "32000/49000 loss: 0.502483104838743\n",
      "34000/49000 loss: 0.5036356648267853\n",
      "36000/49000 loss: 0.6013107984625651\n",
      "38000/49000 loss: 0.5570006727542215\n",
      "40000/49000 loss: 0.576566457969659\n",
      "42000/49000 loss: 0.5635539502613852\n",
      "44000/49000 loss: 0.6261433107376415\n",
      "46000/49000 loss: 0.5737464012440255\n",
      "48000/49000 loss: 0.556127171156203\n",
      "epoch 2: valid acc = 0.796, new learning rate = 0.00045125\n",
      "2000/49000 loss: 0.508013274454308\n",
      "4000/49000 loss: 0.5165481971767634\n",
      "6000/49000 loss: 0.45149746530981133\n",
      "8000/49000 loss: 0.46371921871748584\n",
      "10000/49000 loss: 0.5468503202245842\n",
      "12000/49000 loss: 0.38818956898360735\n",
      "14000/49000 loss: 0.5461999788569367\n",
      "16000/49000 loss: 0.4646733649628976\n",
      "18000/49000 loss: 0.6223920809099724\n",
      "20000/49000 loss: 0.44161732102385365\n",
      "22000/49000 loss: 0.4738264370727535\n",
      "24000/49000 loss: 0.45716936060755725\n",
      "26000/49000 loss: 0.4293434414015617\n",
      "28000/49000 loss: 0.4555699612928785\n",
      "30000/49000 loss: 0.5602304262352752\n",
      "32000/49000 loss: 0.42695281506808197\n",
      "34000/49000 loss: 0.5198362045613593\n",
      "36000/49000 loss: 0.446758945675831\n",
      "38000/49000 loss: 0.4724929383373362\n",
      "40000/49000 loss: 0.5319658904381349\n",
      "42000/49000 loss: 0.47952490596686786\n",
      "44000/49000 loss: 0.4944596275836315\n",
      "46000/49000 loss: 0.483338583491206\n",
      "48000/49000 loss: 0.49541840400881976\n",
      "epoch 3: valid acc = 0.827, new learning rate = 0.0004286875\n",
      "2000/49000 loss: 0.42965330556673736\n",
      "4000/49000 loss: 0.36775588923458946\n",
      "6000/49000 loss: 0.5250160329676656\n",
      "8000/49000 loss: 0.47407650558001446\n",
      "10000/49000 loss: 0.551741059721853\n",
      "12000/49000 loss: 0.42778629316201994\n",
      "14000/49000 loss: 0.377922957018492\n",
      "16000/49000 loss: 0.4049061852066809\n",
      "18000/49000 loss: 0.44079690034904645\n",
      "20000/49000 loss: 0.5306563607112659\n",
      "22000/49000 loss: 0.41715412326133894\n",
      "24000/49000 loss: 0.5266720001596196\n",
      "26000/49000 loss: 0.47119439414399045\n",
      "28000/49000 loss: 0.392932647648367\n",
      "30000/49000 loss: 0.45683732862481324\n",
      "32000/49000 loss: 0.42618242503161163\n",
      "34000/49000 loss: 0.45495783230185577\n",
      "36000/49000 loss: 0.42932447823857\n",
      "38000/49000 loss: 0.44227329128225723\n",
      "40000/49000 loss: 0.42370645651048744\n",
      "42000/49000 loss: 0.3951079327376987\n",
      "44000/49000 loss: 0.4229411318768756\n",
      "46000/49000 loss: 0.4540453082038503\n",
      "48000/49000 loss: 0.4302199601844692\n",
      "epoch 4: valid acc = 0.84, new learning rate = 0.00040725312499999993\n",
      "2000/49000 loss: 0.43066762920500357\n",
      "4000/49000 loss: 0.3288419449942993\n",
      "6000/49000 loss: 0.38753431892049317\n",
      "8000/49000 loss: 0.4321802086364248\n",
      "10000/49000 loss: 0.4388673211279583\n",
      "12000/49000 loss: 0.43606897220754764\n",
      "14000/49000 loss: 0.31826818132748524\n",
      "16000/49000 loss: 0.44481855872762616\n",
      "18000/49000 loss: 0.41952184528633213\n",
      "20000/49000 loss: 0.4819410264862507\n",
      "22000/49000 loss: 0.381915816008627\n",
      "24000/49000 loss: 0.31459060586814197\n",
      "26000/49000 loss: 0.44248105295649753\n",
      "28000/49000 loss: 0.44440556117568186\n",
      "30000/49000 loss: 0.5054948935851088\n",
      "32000/49000 loss: 0.4307467652044083\n",
      "34000/49000 loss: 0.3007937044102491\n",
      "36000/49000 loss: 0.320438520956734\n",
      "38000/49000 loss: 0.47956990798041127\n",
      "40000/49000 loss: 0.4524895083778205\n",
      "42000/49000 loss: 0.5070917946817617\n",
      "44000/49000 loss: 0.4725172717711082\n",
      "46000/49000 loss: 0.4485385582099898\n",
      "48000/49000 loss: 0.4227283627148746\n",
      "epoch 5: valid acc = 0.856, new learning rate = 0.0003868904687499999\n",
      "2000/49000 loss: 0.39622958567563343\n",
      "4000/49000 loss: 0.39870801631473074\n",
      "6000/49000 loss: 0.5195596274366001\n",
      "8000/49000 loss: 0.48822250547701596\n",
      "10000/49000 loss: 0.3606984506077236\n",
      "12000/49000 loss: 0.3641738304384564\n",
      "14000/49000 loss: 0.41910686271977987\n",
      "16000/49000 loss: 0.3771833825457936\n",
      "18000/49000 loss: 0.41522165981859577\n",
      "20000/49000 loss: 0.4373302165384276\n",
      "22000/49000 loss: 0.4670851009742144\n",
      "24000/49000 loss: 0.3657905831507198\n",
      "26000/49000 loss: 0.3442011743007192\n",
      "28000/49000 loss: 0.4401196018790352\n",
      "30000/49000 loss: 0.40925538895423214\n",
      "32000/49000 loss: 0.49049629461703964\n",
      "34000/49000 loss: 0.47730798428645016\n",
      "36000/49000 loss: 0.3442273445509851\n",
      "38000/49000 loss: 0.3792104453682366\n",
      "40000/49000 loss: 0.38543310879705484\n",
      "42000/49000 loss: 0.41512346826934193\n",
      "44000/49000 loss: 0.392732192606228\n",
      "46000/49000 loss: 0.30434028803723745\n",
      "48000/49000 loss: 0.352259626918981\n",
      "epoch 6: valid acc = 0.858, new learning rate = 0.0003675459453124999\n",
      "2000/49000 loss: 0.2907982536786614\n",
      "4000/49000 loss: 0.32300552844362407\n",
      "6000/49000 loss: 0.4495033985664238\n",
      "8000/49000 loss: 0.334163291143215\n",
      "10000/49000 loss: 0.4362046024740454\n",
      "12000/49000 loss: 0.40925824535592414\n",
      "14000/49000 loss: 0.3834215855089611\n",
      "16000/49000 loss: 0.3544737929791507\n",
      "18000/49000 loss: 0.46558221884494344\n",
      "20000/49000 loss: 0.4465463461800869\n",
      "22000/49000 loss: 0.37589470909185196\n",
      "24000/49000 loss: 0.41794893767009594\n",
      "26000/49000 loss: 0.31573317066929396\n",
      "28000/49000 loss: 0.339922648396101\n",
      "30000/49000 loss: 0.36873673936301216\n",
      "32000/49000 loss: 0.31588870643861533\n",
      "34000/49000 loss: 0.338692857662494\n",
      "36000/49000 loss: 0.3501080448881987\n",
      "38000/49000 loss: 0.3912413089176245\n",
      "40000/49000 loss: 0.4602736291974277\n",
      "42000/49000 loss: 0.2786201464428636\n",
      "44000/49000 loss: 0.3633981990899414\n",
      "46000/49000 loss: 0.2900895285454652\n",
      "48000/49000 loss: 0.4421036701618847\n",
      "epoch 7: valid acc = 0.858, new learning rate = 0.00034916864804687486\n",
      "2000/49000 loss: 0.35684457715490386\n",
      "4000/49000 loss: 0.41793936361176953\n",
      "6000/49000 loss: 0.37283864568926106\n",
      "8000/49000 loss: 0.2948427868359333\n",
      "10000/49000 loss: 0.38388216596731023\n",
      "12000/49000 loss: 0.4367612280250278\n",
      "14000/49000 loss: 0.31289482487378567\n",
      "16000/49000 loss: 0.5070992483296679\n",
      "18000/49000 loss: 0.4797415991713202\n",
      "20000/49000 loss: 0.34194519046765\n",
      "22000/49000 loss: 0.3184209896715485\n",
      "24000/49000 loss: 0.4842936441377608\n",
      "26000/49000 loss: 0.3984103520875289\n",
      "28000/49000 loss: 0.3362775210214451\n",
      "30000/49000 loss: 0.3476526671348603\n",
      "32000/49000 loss: 0.4369838004481472\n",
      "34000/49000 loss: 0.44120915197131827\n",
      "36000/49000 loss: 0.31485981857542117\n",
      "38000/49000 loss: 0.34446916282840007\n",
      "40000/49000 loss: 0.3772772711607584\n",
      "42000/49000 loss: 0.3057788833663793\n",
      "44000/49000 loss: 0.29960730623138904\n",
      "46000/49000 loss: 0.35710762621140507\n",
      "48000/49000 loss: 0.36287651969697643\n",
      "epoch 8: valid acc = 0.867, new learning rate = 0.0003317102156445311\n",
      "2000/49000 loss: 0.4516446021590952\n",
      "4000/49000 loss: 0.2866150621124405\n",
      "6000/49000 loss: 0.4039404978378761\n",
      "8000/49000 loss: 0.3284539071623547\n",
      "10000/49000 loss: 0.3374158455080749\n",
      "12000/49000 loss: 0.37774578665911857\n",
      "14000/49000 loss: 0.2575742182858003\n",
      "16000/49000 loss: 0.5175099157721033\n",
      "18000/49000 loss: 0.35569319737265526\n",
      "20000/49000 loss: 0.4682871595229123\n",
      "22000/49000 loss: 0.24950833212018075\n",
      "24000/49000 loss: 0.398582358670281\n",
      "26000/49000 loss: 0.3641669241919901\n",
      "28000/49000 loss: 0.4140153537066765\n",
      "30000/49000 loss: 0.28855907449386564\n",
      "32000/49000 loss: 0.31409980982369357\n",
      "34000/49000 loss: 0.32510683283169606\n",
      "36000/49000 loss: 0.3801731216683535\n",
      "38000/49000 loss: 0.26588694178215727\n",
      "40000/49000 loss: 0.32490696920708756\n",
      "42000/49000 loss: 0.3274565880722357\n",
      "44000/49000 loss: 0.3478445799744673\n",
      "46000/49000 loss: 0.3113940426505085\n",
      "48000/49000 loss: 0.3764628212332533\n",
      "epoch 9: valid acc = 0.871, new learning rate = 0.0003151247048623045\n",
      "2000/49000 loss: 0.2977815122194669\n",
      "4000/49000 loss: 0.3355576548487455\n",
      "6000/49000 loss: 0.284660673757289\n",
      "8000/49000 loss: 0.3511204668421073\n",
      "10000/49000 loss: 0.22145056851953843\n",
      "12000/49000 loss: 0.33942324993283124\n",
      "14000/49000 loss: 0.345031363689897\n",
      "16000/49000 loss: 0.2720721724689708\n",
      "18000/49000 loss: 0.34783281527111787\n",
      "20000/49000 loss: 0.3345282315736574\n",
      "22000/49000 loss: 0.41779098258096536\n",
      "24000/49000 loss: 0.33928151142486923\n",
      "26000/49000 loss: 0.48386344967521105\n",
      "28000/49000 loss: 0.30594432646513053\n",
      "30000/49000 loss: 0.41619353184888097\n",
      "32000/49000 loss: 0.4228692653373599\n",
      "34000/49000 loss: 0.4727307902169805\n",
      "36000/49000 loss: 0.35171851886194866\n",
      "38000/49000 loss: 0.40050698193649525\n",
      "40000/49000 loss: 0.35403831309693623\n",
      "42000/49000 loss: 0.35399455181441847\n",
      "44000/49000 loss: 0.28452347649929116\n",
      "46000/49000 loss: 0.33512245307775884\n",
      "48000/49000 loss: 0.3865397806468514\n",
      "epoch 10: valid acc = 0.871, new learning rate = 0.00029936846961918924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc: 0.8753469387755102\n",
      "test acc: 0.871\n",
      "test acc: 0.8553\n",
      "number of batches for training: 245\n",
      "epoch 1: valid acc = 0.747, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.805, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.837, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.838, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.857, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.857, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.858, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.868, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.868, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.869, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.8754489795918368\n",
      "test acc: 0.869\n",
      "test acc: 0.8556\n",
      "number of batches for training: 245\n",
      "2000/49000 loss: 2.7788539526044795\n",
      "4000/49000 loss: 2.6243418440966226\n",
      "6000/49000 loss: 2.8119772146586137\n",
      "8000/49000 loss: 2.4367724532150947\n",
      "10000/49000 loss: 2.226627204163671\n",
      "12000/49000 loss: 2.045574964681886\n",
      "14000/49000 loss: 1.9133659874072273\n",
      "16000/49000 loss: 1.5769445105290136\n",
      "18000/49000 loss: 1.4960389223212822\n",
      "20000/49000 loss: 1.2413174149385526\n",
      "22000/49000 loss: 1.2354392222218815\n",
      "24000/49000 loss: 1.1362836135075896\n",
      "26000/49000 loss: 1.2342132610138972\n",
      "28000/49000 loss: 1.0734116545855439\n",
      "30000/49000 loss: 0.8985680211935442\n",
      "32000/49000 loss: 0.9519197547048838\n",
      "34000/49000 loss: 0.9418235832270566\n",
      "36000/49000 loss: 0.7281176038680305\n",
      "38000/49000 loss: 0.8042381755290909\n",
      "40000/49000 loss: 0.745499112077635\n",
      "42000/49000 loss: 0.9607594649822265\n",
      "44000/49000 loss: 0.8092936588542907\n",
      "46000/49000 loss: 0.8753200810296176\n",
      "48000/49000 loss: 0.6783086142306687\n",
      "epoch 1: valid acc = 0.744, new learning rate = 0.000475\n",
      "2000/49000 loss: 0.7755900416945828\n",
      "4000/49000 loss: 0.7808188682023695\n",
      "6000/49000 loss: 0.7277639208303879\n",
      "8000/49000 loss: 0.7304095937721632\n",
      "10000/49000 loss: 0.7546857550993038\n",
      "12000/49000 loss: 0.6866890973923895\n",
      "14000/49000 loss: 0.594882945862544\n",
      "16000/49000 loss: 0.654909565304975\n",
      "18000/49000 loss: 0.6587704418553942\n",
      "20000/49000 loss: 0.6118865348296086\n",
      "22000/49000 loss: 0.6628455301504574\n",
      "24000/49000 loss: 0.47862647185435186\n",
      "26000/49000 loss: 0.6310834325279543\n",
      "28000/49000 loss: 0.5287253333113462\n",
      "30000/49000 loss: 0.4886249533879308\n",
      "32000/49000 loss: 0.5083312682940895\n",
      "34000/49000 loss: 0.555343585519825\n",
      "36000/49000 loss: 0.4880875568312637\n",
      "38000/49000 loss: 0.5171970235569144\n",
      "40000/49000 loss: 0.6088469474948351\n",
      "42000/49000 loss: 0.5635541610418108\n",
      "44000/49000 loss: 0.4544721640739715\n",
      "46000/49000 loss: 0.554822652994566\n",
      "48000/49000 loss: 0.5033830109264185\n",
      "epoch 2: valid acc = 0.801, new learning rate = 0.00045125\n",
      "2000/49000 loss: 0.46897033635502466\n",
      "4000/49000 loss: 0.4743841922141538\n",
      "6000/49000 loss: 0.5834723812818101\n",
      "8000/49000 loss: 0.49886939298380223\n",
      "10000/49000 loss: 0.5903164306162471\n",
      "12000/49000 loss: 0.5176442562658039\n",
      "14000/49000 loss: 0.5631533484382991\n",
      "16000/49000 loss: 0.44735046408077894\n",
      "18000/49000 loss: 0.39974124046174797\n",
      "20000/49000 loss: 0.5242579187644354\n",
      "22000/49000 loss: 0.5317539183957706\n",
      "24000/49000 loss: 0.5321336426260148\n",
      "26000/49000 loss: 0.5580765593612002\n",
      "28000/49000 loss: 0.5003665800722638\n",
      "30000/49000 loss: 0.536098544767823\n",
      "32000/49000 loss: 0.4597294751941447\n",
      "34000/49000 loss: 0.5637720990967213\n",
      "36000/49000 loss: 0.5999843015617601\n",
      "38000/49000 loss: 0.4950566700451286\n",
      "40000/49000 loss: 0.39456019551981675\n",
      "42000/49000 loss: 0.5633577043535081\n",
      "44000/49000 loss: 0.47522820961115797\n",
      "46000/49000 loss: 0.5547900792178554\n",
      "48000/49000 loss: 0.5004462634434149\n",
      "epoch 3: valid acc = 0.835, new learning rate = 0.0004286875\n",
      "2000/49000 loss: 0.35819058493309003\n",
      "4000/49000 loss: 0.4863200855792656\n",
      "6000/49000 loss: 0.4513173088887543\n",
      "8000/49000 loss: 0.5348189733213178\n",
      "10000/49000 loss: 0.4653574360524848\n",
      "12000/49000 loss: 0.5072831799432841\n",
      "14000/49000 loss: 0.492165906484226\n",
      "16000/49000 loss: 0.4225329637340065\n",
      "18000/49000 loss: 0.4302908200779591\n",
      "20000/49000 loss: 0.500548938681945\n",
      "22000/49000 loss: 0.43321939350512045\n",
      "24000/49000 loss: 0.5262409713748055\n",
      "26000/49000 loss: 0.4426547098118065\n",
      "28000/49000 loss: 0.4759087644584702\n",
      "30000/49000 loss: 0.459383962158894\n",
      "32000/49000 loss: 0.4168841018665053\n",
      "34000/49000 loss: 0.3689580201279214\n",
      "36000/49000 loss: 0.3664776421441142\n",
      "38000/49000 loss: 0.46781280790276364\n",
      "40000/49000 loss: 0.44585971272278807\n",
      "42000/49000 loss: 0.372346274864833\n",
      "44000/49000 loss: 0.45052140403809476\n",
      "46000/49000 loss: 0.5096755939244165\n",
      "48000/49000 loss: 0.5053116283011878\n",
      "epoch 4: valid acc = 0.847, new learning rate = 0.00040725312499999993\n",
      "2000/49000 loss: 0.40645634705315153\n",
      "4000/49000 loss: 0.5279071668000418\n",
      "6000/49000 loss: 0.4809225080196087\n",
      "8000/49000 loss: 0.4024866237530456\n",
      "10000/49000 loss: 0.32660338910528414\n",
      "12000/49000 loss: 0.33887603688364326\n",
      "14000/49000 loss: 0.41226797520060404\n",
      "16000/49000 loss: 0.33354254790929905\n",
      "18000/49000 loss: 0.39628314671885334\n",
      "20000/49000 loss: 0.44729763673431067\n",
      "22000/49000 loss: 0.3148192322963397\n",
      "24000/49000 loss: 0.46008335415500556\n",
      "26000/49000 loss: 0.43305959976960673\n",
      "28000/49000 loss: 0.3224367292512611\n",
      "30000/49000 loss: 0.49570545301524854\n",
      "32000/49000 loss: 0.3259692844552825\n",
      "34000/49000 loss: 0.3258578907997457\n",
      "36000/49000 loss: 0.34568419395017763\n",
      "38000/49000 loss: 0.40978969913408925\n",
      "40000/49000 loss: 0.40092296361766877\n",
      "42000/49000 loss: 0.3369686435102159\n",
      "44000/49000 loss: 0.35053249636109834\n",
      "46000/49000 loss: 0.3550746441852942\n",
      "48000/49000 loss: 0.35786418349546784\n",
      "epoch 5: valid acc = 0.857, new learning rate = 0.0003868904687499999\n",
      "2000/49000 loss: 0.49168442956807606\n",
      "4000/49000 loss: 0.4540611205826454\n",
      "6000/49000 loss: 0.4462776745710858\n",
      "8000/49000 loss: 0.5373849131405989\n",
      "10000/49000 loss: 0.41598981754106285\n",
      "12000/49000 loss: 0.4556249669128858\n",
      "14000/49000 loss: 0.4506703558365866\n",
      "16000/49000 loss: 0.3114066361294631\n",
      "18000/49000 loss: 0.35631281815684934\n",
      "20000/49000 loss: 0.36413231799431317\n",
      "22000/49000 loss: 0.3394450670734741\n",
      "24000/49000 loss: 0.4725829463197055\n",
      "26000/49000 loss: 0.36220570820257175\n",
      "28000/49000 loss: 0.45938005116825126\n",
      "30000/49000 loss: 0.586921421471134\n",
      "32000/49000 loss: 0.38930350028015503\n",
      "34000/49000 loss: 0.3400895088499276\n",
      "36000/49000 loss: 0.4624547150296146\n",
      "38000/49000 loss: 0.3181814743777938\n",
      "40000/49000 loss: 0.35727415354043185\n",
      "42000/49000 loss: 0.36371733020740743\n",
      "44000/49000 loss: 0.3435341386589598\n",
      "46000/49000 loss: 0.40995762110088024\n",
      "48000/49000 loss: 0.3974063196352378\n",
      "epoch 6: valid acc = 0.867, new learning rate = 0.0003675459453124999\n",
      "2000/49000 loss: 0.3510419045199534\n",
      "4000/49000 loss: 0.4083004229607233\n",
      "6000/49000 loss: 0.42188120263295925\n",
      "8000/49000 loss: 0.4005956129835297\n",
      "10000/49000 loss: 0.3411027779906946\n",
      "12000/49000 loss: 0.3543650312005469\n",
      "14000/49000 loss: 0.3665801610760654\n",
      "16000/49000 loss: 0.3388609885037948\n",
      "18000/49000 loss: 0.5081893361191728\n",
      "20000/49000 loss: 0.3465827691482516\n",
      "22000/49000 loss: 0.3428435789890813\n",
      "24000/49000 loss: 0.3606749928889179\n",
      "26000/49000 loss: 0.45143701925165336\n",
      "28000/49000 loss: 0.4438559061443313\n",
      "30000/49000 loss: 0.5462220133275432\n",
      "32000/49000 loss: 0.4842138235346774\n",
      "34000/49000 loss: 0.4673523921179282\n",
      "36000/49000 loss: 0.37547957639398893\n",
      "38000/49000 loss: 0.4647716881542801\n",
      "40000/49000 loss: 0.3843584402643663\n",
      "42000/49000 loss: 0.34082451758968374\n",
      "44000/49000 loss: 0.3848366401545607\n",
      "46000/49000 loss: 0.33932757183691203\n",
      "48000/49000 loss: 0.38012457630229773\n",
      "epoch 7: valid acc = 0.866, new learning rate = 0.00034916864804687486\n",
      "2000/49000 loss: 0.4503016958441082\n",
      "4000/49000 loss: 0.40121975188583997\n",
      "6000/49000 loss: 0.3377319696190766\n",
      "8000/49000 loss: 0.4336528430065368\n",
      "10000/49000 loss: 0.4144515675645336\n",
      "12000/49000 loss: 0.30712208895888476\n",
      "14000/49000 loss: 0.35553352880315137\n",
      "16000/49000 loss: 0.3675809441144309\n",
      "18000/49000 loss: 0.40564494366296305\n",
      "20000/49000 loss: 0.35092787867017267\n",
      "22000/49000 loss: 0.33832247975626184\n",
      "24000/49000 loss: 0.41093802673140717\n",
      "26000/49000 loss: 0.3248346990233161\n",
      "28000/49000 loss: 0.35180583548608996\n",
      "30000/49000 loss: 0.33370868927981\n",
      "32000/49000 loss: 0.42132986299508773\n",
      "34000/49000 loss: 0.398081332210916\n",
      "36000/49000 loss: 0.3569118370435292\n",
      "38000/49000 loss: 0.3851715437237847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/49000 loss: 0.3834558028739712\n",
      "42000/49000 loss: 0.3708940076319516\n",
      "44000/49000 loss: 0.4580264646369767\n",
      "46000/49000 loss: 0.4832317297591893\n",
      "48000/49000 loss: 0.31873894710030626\n",
      "epoch 8: valid acc = 0.863, new learning rate = 0.0003317102156445311\n",
      "2000/49000 loss: 0.36130223614336\n",
      "4000/49000 loss: 0.3847503018285352\n",
      "6000/49000 loss: 0.3751637950317769\n",
      "8000/49000 loss: 0.4008994637572943\n",
      "10000/49000 loss: 0.37923426132467825\n",
      "12000/49000 loss: 0.3847752079533429\n",
      "14000/49000 loss: 0.40312454416910487\n",
      "16000/49000 loss: 0.31798760930736325\n",
      "18000/49000 loss: 0.3786497809658669\n",
      "20000/49000 loss: 0.3592434769450744\n",
      "22000/49000 loss: 0.39453456762386224\n",
      "24000/49000 loss: 0.348044541118101\n",
      "26000/49000 loss: 0.3059078823643397\n",
      "28000/49000 loss: 0.3436910730708579\n",
      "30000/49000 loss: 0.295936249187005\n",
      "32000/49000 loss: 0.33817039695384854\n",
      "34000/49000 loss: 0.3996675086825567\n",
      "36000/49000 loss: 0.3317638621200731\n",
      "38000/49000 loss: 0.26932960150369806\n",
      "40000/49000 loss: 0.3837550572148818\n",
      "42000/49000 loss: 0.3371882938922094\n",
      "44000/49000 loss: 0.32186073704094725\n",
      "46000/49000 loss: 0.4763387942516325\n",
      "48000/49000 loss: 0.32771276900249807\n",
      "epoch 9: valid acc = 0.872, new learning rate = 0.0003151247048623045\n",
      "2000/49000 loss: 0.33202283936972626\n",
      "4000/49000 loss: 0.30195827880027537\n",
      "6000/49000 loss: 0.3301379332935167\n",
      "8000/49000 loss: 0.2847515605350729\n",
      "10000/49000 loss: 0.4204834532882665\n",
      "12000/49000 loss: 0.4529151225174034\n",
      "14000/49000 loss: 0.34266800050736\n",
      "16000/49000 loss: 0.2778148594268287\n",
      "18000/49000 loss: 0.33668473008683875\n",
      "20000/49000 loss: 0.38954652322604627\n",
      "22000/49000 loss: 0.2697810563354509\n",
      "24000/49000 loss: 0.29287165622561323\n",
      "26000/49000 loss: 0.37716595706472206\n",
      "28000/49000 loss: 0.3513195415517417\n",
      "30000/49000 loss: 0.3438850691915838\n",
      "32000/49000 loss: 0.32937586777048844\n",
      "34000/49000 loss: 0.31943769989678505\n",
      "36000/49000 loss: 0.40149833689443903\n",
      "38000/49000 loss: 0.3109213876586413\n",
      "40000/49000 loss: 0.2941726951481397\n",
      "42000/49000 loss: 0.4875720919492709\n",
      "44000/49000 loss: 0.2384035039571503\n",
      "46000/49000 loss: 0.38116783115751013\n",
      "48000/49000 loss: 0.30656073863639055\n",
      "epoch 10: valid acc = 0.876, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.8755102040816326\n",
      "test acc: 0.876\n",
      "test acc: 0.8564\n",
      "number of batches for training: 245\n",
      "epoch 1: valid acc = 0.744, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.797, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.834, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.84, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.851, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.858, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.858, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.862, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.87, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.874, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.876265306122449\n",
      "test acc: 0.874\n",
      "test acc: 0.8575\n",
      "number of batches for training: 245\n",
      "2000/49000 loss: 2.8883582975487787\n",
      "4000/49000 loss: 2.973968920218763\n",
      "6000/49000 loss: 2.727074609529244\n",
      "8000/49000 loss: 2.3717175613545884\n",
      "10000/49000 loss: 2.1445042038095377\n",
      "12000/49000 loss: 2.067664636272308\n",
      "14000/49000 loss: 1.9270411453066618\n",
      "16000/49000 loss: 1.6264271404663961\n",
      "18000/49000 loss: 1.3410257512167993\n",
      "20000/49000 loss: 1.321659901616736\n",
      "22000/49000 loss: 1.3371440108316273\n",
      "24000/49000 loss: 1.0672631823806937\n",
      "26000/49000 loss: 1.1662314004673762\n",
      "28000/49000 loss: 1.0959772262296468\n",
      "30000/49000 loss: 0.9623789291951758\n",
      "32000/49000 loss: 1.010751175065231\n",
      "34000/49000 loss: 0.9421360593894939\n",
      "36000/49000 loss: 0.9631609623398071\n",
      "38000/49000 loss: 0.9859757752564381\n",
      "40000/49000 loss: 0.774595008247282\n",
      "42000/49000 loss: 0.8717952377918448\n",
      "44000/49000 loss: 0.8921388500839798\n",
      "46000/49000 loss: 0.7406741746607438\n",
      "48000/49000 loss: 0.7746482915101663\n",
      "epoch 1: valid acc = 0.744, new learning rate = 0.000475\n",
      "2000/49000 loss: 0.8011465268403374\n",
      "4000/49000 loss: 0.65519591902673\n",
      "6000/49000 loss: 0.6846906430861948\n",
      "8000/49000 loss: 0.6849013000407108\n",
      "10000/49000 loss: 0.6722219824851705\n",
      "12000/49000 loss: 0.5544231242778861\n",
      "14000/49000 loss: 0.6170866940429486\n",
      "16000/49000 loss: 0.7103466487739444\n",
      "18000/49000 loss: 0.6815344802813481\n",
      "20000/49000 loss: 0.6445270841053986\n",
      "22000/49000 loss: 0.6394773820265514\n",
      "24000/49000 loss: 0.49633054885588174\n",
      "26000/49000 loss: 0.5377070932914428\n",
      "28000/49000 loss: 0.5537391315038069\n",
      "30000/49000 loss: 0.5212677995790602\n",
      "32000/49000 loss: 0.6116184513150461\n",
      "34000/49000 loss: 0.5629112797156745\n",
      "36000/49000 loss: 0.5299407457649653\n",
      "38000/49000 loss: 0.5066166431419042\n",
      "40000/49000 loss: 0.5037321342470285\n",
      "42000/49000 loss: 0.4023129080254221\n",
      "44000/49000 loss: 0.571588614629392\n",
      "46000/49000 loss: 0.49994390222584817\n",
      "48000/49000 loss: 0.5476349380984321\n",
      "epoch 2: valid acc = 0.8, new learning rate = 0.00045125\n",
      "2000/49000 loss: 0.5344400482359108\n",
      "4000/49000 loss: 0.5038012938977141\n",
      "6000/49000 loss: 0.5967215252027692\n",
      "8000/49000 loss: 0.39004680330792435\n",
      "10000/49000 loss: 0.5904776065765265\n",
      "12000/49000 loss: 0.6230947275905288\n",
      "14000/49000 loss: 0.4448068283659714\n",
      "16000/49000 loss: 0.48045912336266533\n",
      "18000/49000 loss: 0.5050451490307447\n",
      "20000/49000 loss: 0.45883722369598595\n",
      "22000/49000 loss: 0.5415928193578895\n",
      "24000/49000 loss: 0.5113186414437381\n",
      "26000/49000 loss: 0.5808848499748437\n",
      "28000/49000 loss: 0.5999864697766726\n",
      "30000/49000 loss: 0.4314827023756807\n",
      "32000/49000 loss: 0.45819751239720385\n",
      "34000/49000 loss: 0.4354429783565594\n",
      "36000/49000 loss: 0.5086947051985001\n",
      "38000/49000 loss: 0.4607884148820946\n",
      "40000/49000 loss: 0.4888574307129047\n",
      "42000/49000 loss: 0.48475619048415275\n",
      "44000/49000 loss: 0.47224252362478\n",
      "46000/49000 loss: 0.5177463190200589\n",
      "48000/49000 loss: 0.473714172708605\n",
      "epoch 3: valid acc = 0.832, new learning rate = 0.0004286875\n",
      "2000/49000 loss: 0.5935905931934496\n",
      "4000/49000 loss: 0.4057492114550582\n",
      "6000/49000 loss: 0.3812473142303937\n",
      "8000/49000 loss: 0.4828421628522865\n",
      "10000/49000 loss: 0.4135610462386891\n",
      "12000/49000 loss: 0.48463120877664034\n",
      "14000/49000 loss: 0.4070267627789318\n",
      "16000/49000 loss: 0.49544519180194097\n",
      "18000/49000 loss: 0.49527008313836945\n",
      "20000/49000 loss: 0.3393125743394658\n",
      "22000/49000 loss: 0.5804264699766913\n",
      "24000/49000 loss: 0.3386362195891583\n",
      "26000/49000 loss: 0.4315206779319549\n",
      "28000/49000 loss: 0.40547596291451277\n",
      "30000/49000 loss: 0.4559009865005876\n",
      "32000/49000 loss: 0.40648620515964856\n",
      "34000/49000 loss: 0.3921718584538844\n",
      "36000/49000 loss: 0.39422661612606136\n",
      "38000/49000 loss: 0.44864158202876325\n",
      "40000/49000 loss: 0.42453930351733804\n",
      "42000/49000 loss: 0.42804395128129324\n",
      "44000/49000 loss: 0.4603018585384076\n",
      "46000/49000 loss: 0.42533410790963405\n",
      "48000/49000 loss: 0.456069880554211\n",
      "epoch 4: valid acc = 0.842, new learning rate = 0.00040725312499999993\n",
      "2000/49000 loss: 0.49408103554595895\n",
      "4000/49000 loss: 0.3252372846997171\n",
      "6000/49000 loss: 0.36019997582093033\n",
      "8000/49000 loss: 0.3691159073559884\n",
      "10000/49000 loss: 0.48559722225617513\n",
      "12000/49000 loss: 0.5063466211928216\n",
      "14000/49000 loss: 0.42364676216203356\n",
      "16000/49000 loss: 0.4554163377834286\n",
      "18000/49000 loss: 0.40338138037920723\n",
      "20000/49000 loss: 0.47121374974734004\n",
      "22000/49000 loss: 0.43043638436128834\n",
      "24000/49000 loss: 0.41174509967971173\n",
      "26000/49000 loss: 0.5373865478351452\n",
      "28000/49000 loss: 0.45873992533907704\n",
      "30000/49000 loss: 0.37728538629032216\n",
      "32000/49000 loss: 0.34559708061117894\n",
      "34000/49000 loss: 0.3324438896106585\n",
      "36000/49000 loss: 0.4096434738598185\n",
      "38000/49000 loss: 0.3909368261031588\n",
      "40000/49000 loss: 0.30906577740249785\n",
      "42000/49000 loss: 0.4300406286638014\n",
      "44000/49000 loss: 0.31172066086130573\n",
      "46000/49000 loss: 0.43736976715652576\n",
      "48000/49000 loss: 0.4169658465434613\n",
      "epoch 5: valid acc = 0.85, new learning rate = 0.0003868904687499999\n",
      "2000/49000 loss: 0.4730598250139136\n",
      "4000/49000 loss: 0.30783927094595886\n",
      "6000/49000 loss: 0.30866022121948306\n",
      "8000/49000 loss: 0.4648244277857066\n",
      "10000/49000 loss: 0.4375818376852079\n",
      "12000/49000 loss: 0.513274145483631\n",
      "14000/49000 loss: 0.34046142645739486\n",
      "16000/49000 loss: 0.587333725106499\n",
      "18000/49000 loss: 0.4659810491840351\n",
      "20000/49000 loss: 0.4361752215780913\n",
      "22000/49000 loss: 0.5159771024595851\n",
      "24000/49000 loss: 0.422353856087317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26000/49000 loss: 0.3998790596725639\n",
      "28000/49000 loss: 0.3829286241378644\n",
      "30000/49000 loss: 0.34185271553030355\n",
      "32000/49000 loss: 0.32376284608905526\n",
      "34000/49000 loss: 0.6037130786310986\n",
      "36000/49000 loss: 0.33522056614354884\n",
      "38000/49000 loss: 0.4162867566257879\n",
      "40000/49000 loss: 0.355288448907404\n",
      "42000/49000 loss: 0.43118429751456716\n",
      "44000/49000 loss: 0.3921705840534399\n",
      "46000/49000 loss: 0.3031488933647704\n",
      "48000/49000 loss: 0.34351441035964\n",
      "epoch 6: valid acc = 0.859, new learning rate = 0.0003675459453124999\n",
      "2000/49000 loss: 0.2849939600610977\n",
      "4000/49000 loss: 0.33205052163734566\n",
      "6000/49000 loss: 0.2807711002548416\n",
      "8000/49000 loss: 0.33007406512413046\n",
      "10000/49000 loss: 0.4050818286882392\n",
      "12000/49000 loss: 0.32858112106946324\n",
      "14000/49000 loss: 0.3991457143557215\n",
      "16000/49000 loss: 0.44816054708641584\n",
      "18000/49000 loss: 0.32840314176081975\n",
      "20000/49000 loss: 0.3898167021676699\n",
      "22000/49000 loss: 0.39092280277629565\n",
      "24000/49000 loss: 0.3779942392085935\n",
      "26000/49000 loss: 0.39252498303569827\n",
      "28000/49000 loss: 0.3865380055324459\n",
      "30000/49000 loss: 0.4581055553581062\n",
      "32000/49000 loss: 0.28617371657748675\n",
      "34000/49000 loss: 0.37502788854620883\n",
      "36000/49000 loss: 0.36486532780275155\n",
      "38000/49000 loss: 0.4025909214395223\n",
      "40000/49000 loss: 0.4421996696388177\n",
      "42000/49000 loss: 0.41397559685993135\n",
      "44000/49000 loss: 0.3767956590163227\n",
      "46000/49000 loss: 0.389569628461958\n",
      "48000/49000 loss: 0.40090503787552534\n",
      "epoch 7: valid acc = 0.859, new learning rate = 0.00034916864804687486\n",
      "2000/49000 loss: 0.4241291371963399\n",
      "4000/49000 loss: 0.46997410945274265\n",
      "6000/49000 loss: 0.37195939893135227\n",
      "8000/49000 loss: 0.48548914616806205\n",
      "10000/49000 loss: 0.3604515718945747\n",
      "12000/49000 loss: 0.3912337866629887\n",
      "14000/49000 loss: 0.3106387167057381\n",
      "16000/49000 loss: 0.3774215107004165\n",
      "18000/49000 loss: 0.4449416449512038\n",
      "20000/49000 loss: 0.39094404135058647\n",
      "22000/49000 loss: 0.3566179309574797\n",
      "24000/49000 loss: 0.40643458067731386\n",
      "26000/49000 loss: 0.3882395466680409\n",
      "28000/49000 loss: 0.3138404213461259\n",
      "30000/49000 loss: 0.33105635641548214\n",
      "32000/49000 loss: 0.32825118073822246\n",
      "34000/49000 loss: 0.328172871803843\n",
      "36000/49000 loss: 0.4088924772561218\n",
      "38000/49000 loss: 0.4792652251323921\n",
      "40000/49000 loss: 0.2561828263383804\n",
      "42000/49000 loss: 0.5757375824373887\n",
      "44000/49000 loss: 0.31281691135683914\n",
      "46000/49000 loss: 0.310209843947699\n",
      "48000/49000 loss: 0.3409647697879413\n",
      "epoch 8: valid acc = 0.868, new learning rate = 0.0003317102156445311\n",
      "2000/49000 loss: 0.3318645554265969\n",
      "4000/49000 loss: 0.34821901293466523\n",
      "6000/49000 loss: 0.36603611078613324\n",
      "8000/49000 loss: 0.4127958867685628\n",
      "10000/49000 loss: 0.3869142155155362\n",
      "12000/49000 loss: 0.3332350901926169\n",
      "14000/49000 loss: 0.35578518527439457\n",
      "16000/49000 loss: 0.44969321135003876\n",
      "18000/49000 loss: 0.40513325877375383\n",
      "20000/49000 loss: 0.3182681204942913\n",
      "22000/49000 loss: 0.39690567587174863\n",
      "24000/49000 loss: 0.3662507939109266\n",
      "26000/49000 loss: 0.368129156644217\n",
      "28000/49000 loss: 0.37011589201490364\n",
      "30000/49000 loss: 0.4265031481454923\n",
      "32000/49000 loss: 0.36436336001634667\n",
      "34000/49000 loss: 0.2190490207654969\n",
      "36000/49000 loss: 0.34870037525278935\n",
      "38000/49000 loss: 0.3743560752688041\n",
      "40000/49000 loss: 0.35113243390348087\n",
      "42000/49000 loss: 0.358882114299924\n",
      "44000/49000 loss: 0.3332678126534973\n",
      "46000/49000 loss: 0.352943319795651\n",
      "48000/49000 loss: 0.469178097001856\n",
      "epoch 9: valid acc = 0.874, new learning rate = 0.0003151247048623045\n",
      "2000/49000 loss: 0.3733700296326801\n",
      "4000/49000 loss: 0.32356618009477595\n",
      "6000/49000 loss: 0.2965595003840537\n",
      "8000/49000 loss: 0.37535290228984003\n",
      "10000/49000 loss: 0.3819897581196875\n",
      "12000/49000 loss: 0.3425338671292024\n",
      "14000/49000 loss: 0.33777860437060403\n",
      "16000/49000 loss: 0.3859924605757769\n",
      "18000/49000 loss: 0.35585401564513486\n",
      "20000/49000 loss: 0.29960509369769794\n",
      "22000/49000 loss: 0.357953619226951\n",
      "24000/49000 loss: 0.31843494671903233\n",
      "26000/49000 loss: 0.31456519035900193\n",
      "28000/49000 loss: 0.36956784289565364\n",
      "30000/49000 loss: 0.4279512934724242\n",
      "32000/49000 loss: 0.3306347242697415\n",
      "34000/49000 loss: 0.385041941873329\n",
      "36000/49000 loss: 0.4230729433196497\n",
      "38000/49000 loss: 0.24762013016052226\n",
      "40000/49000 loss: 0.3558461900319609\n",
      "42000/49000 loss: 0.34116732224652374\n",
      "44000/49000 loss: 0.20501594230366343\n",
      "46000/49000 loss: 0.3375909489319663\n",
      "48000/49000 loss: 0.3262257483908995\n",
      "epoch 10: valid acc = 0.871, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.8757142857142857\n",
      "test acc: 0.871\n",
      "test acc: 0.8551\n",
      "number of batches for training: 245\n",
      "epoch 1: valid acc = 0.742, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.807, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.826, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.839, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.857, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.857, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.855, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.87, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.873, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.875, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.8754081632653061\n",
      "test acc: 0.875\n",
      "test acc: 0.8559\n",
      "number of batches for training: 81\n",
      "6000/49000 loss: 2.7956324892954725\n",
      "12000/49000 loss: 2.7123485769504447\n",
      "18000/49000 loss: 2.7199315947589637\n",
      "24000/49000 loss: 2.3625172353560457\n",
      "30000/49000 loss: 2.141860261738906\n",
      "36000/49000 loss: 2.009997673172707\n",
      "42000/49000 loss: 1.8907169317474122\n",
      "48000/49000 loss: 1.7313060128867042\n",
      "epoch 1: valid acc = 0.508, new learning rate = 0.000475\n",
      "6000/49000 loss: 1.3415125764194418\n",
      "12000/49000 loss: 1.39460416399567\n",
      "18000/49000 loss: 1.333470240852634\n",
      "24000/49000 loss: 1.2126286775568833\n",
      "30000/49000 loss: 1.1842477641892797\n",
      "36000/49000 loss: 1.0319196187571953\n",
      "42000/49000 loss: 1.1019592707044306\n",
      "48000/49000 loss: 1.020282279050205\n",
      "epoch 2: valid acc = 0.651, new learning rate = 0.00045125\n",
      "6000/49000 loss: 0.975147281471504\n",
      "12000/49000 loss: 0.8857824129410369\n",
      "18000/49000 loss: 0.9203533888515151\n",
      "24000/49000 loss: 0.8534064042336235\n",
      "30000/49000 loss: 0.8767363022765319\n",
      "36000/49000 loss: 0.8426525671750088\n",
      "42000/49000 loss: 0.7781495576423827\n",
      "48000/49000 loss: 0.8182401549148184\n",
      "epoch 3: valid acc = 0.729, new learning rate = 0.0004286875\n",
      "6000/49000 loss: 0.7486393107496396\n",
      "12000/49000 loss: 0.7372003126715593\n",
      "18000/49000 loss: 0.7221864451017538\n",
      "24000/49000 loss: 0.6973262160513376\n",
      "30000/49000 loss: 0.687105967804768\n",
      "36000/49000 loss: 0.7149880874553252\n",
      "42000/49000 loss: 0.7144495531524503\n",
      "48000/49000 loss: 0.6695493435106409\n",
      "epoch 4: valid acc = 0.758, new learning rate = 0.00040725312499999993\n",
      "6000/49000 loss: 0.7090671615007388\n",
      "12000/49000 loss: 0.6129191011537475\n",
      "18000/49000 loss: 0.6714025921014796\n",
      "24000/49000 loss: 0.6556644321902079\n",
      "30000/49000 loss: 0.6017827876217259\n",
      "36000/49000 loss: 0.5916038505783263\n",
      "42000/49000 loss: 0.5894109931362989\n",
      "48000/49000 loss: 0.6115167208515276\n",
      "epoch 5: valid acc = 0.79, new learning rate = 0.0003868904687499999\n",
      "6000/49000 loss: 0.6452217215398878\n",
      "12000/49000 loss: 0.5416539023152841\n",
      "18000/49000 loss: 0.5513336973031049\n",
      "24000/49000 loss: 0.6052773972564481\n",
      "30000/49000 loss: 0.5604614701011504\n",
      "36000/49000 loss: 0.5049500051471799\n",
      "42000/49000 loss: 0.5657123655696871\n",
      "48000/49000 loss: 0.5455322032925445\n",
      "epoch 6: valid acc = 0.807, new learning rate = 0.0003675459453124999\n",
      "6000/49000 loss: 0.6199463067200863\n",
      "12000/49000 loss: 0.5852874204995149\n",
      "18000/49000 loss: 0.5170820534318264\n",
      "24000/49000 loss: 0.5115029076940275\n",
      "30000/49000 loss: 0.6130523296874512\n",
      "36000/49000 loss: 0.5462499505653352\n",
      "42000/49000 loss: 0.4473658185683405\n",
      "48000/49000 loss: 0.45644260225684996\n",
      "epoch 7: valid acc = 0.815, new learning rate = 0.00034916864804687486\n",
      "6000/49000 loss: 0.4883231549143943\n",
      "12000/49000 loss: 0.5119348240650888\n",
      "18000/49000 loss: 0.578935525848245\n",
      "24000/49000 loss: 0.5232874554022803\n",
      "30000/49000 loss: 0.5035794126953829\n",
      "36000/49000 loss: 0.5138639553127113\n",
      "42000/49000 loss: 0.5320121325992535\n",
      "48000/49000 loss: 0.5118340164340437\n",
      "epoch 8: valid acc = 0.826, new learning rate = 0.0003317102156445311\n",
      "6000/49000 loss: 0.5028897029235023\n",
      "12000/49000 loss: 0.4937358674163557\n",
      "18000/49000 loss: 0.4706704439396405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000/49000 loss: 0.4609223621246895\n",
      "30000/49000 loss: 0.45263408984770487\n",
      "36000/49000 loss: 0.5223789783731696\n",
      "42000/49000 loss: 0.4795265676026825\n",
      "48000/49000 loss: 0.4495567784034079\n",
      "epoch 9: valid acc = 0.826, new learning rate = 0.0003151247048623045\n",
      "6000/49000 loss: 0.52600080267673\n",
      "12000/49000 loss: 0.5050749251486601\n",
      "18000/49000 loss: 0.46814895575154997\n",
      "24000/49000 loss: 0.5161289276226636\n",
      "30000/49000 loss: 0.4785993171985418\n",
      "36000/49000 loss: 0.5039848942851195\n",
      "42000/49000 loss: 0.510866978302474\n",
      "48000/49000 loss: 0.4691571613032336\n",
      "epoch 10: valid acc = 0.824, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.8368571428571429\n",
      "test acc: 0.824\n",
      "test acc: 0.8224\n",
      "number of batches for training: 81\n",
      "epoch 1: valid acc = 0.491, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.676, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.744, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.761, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.785, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.799, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.812, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.816, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.826, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.828, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.8360204081632653\n",
      "test acc: 0.828\n",
      "test acc: 0.8207\n",
      "number of batches for training: 81\n",
      "6000/49000 loss: 2.851310545283215\n",
      "12000/49000 loss: 2.6629043523845857\n",
      "18000/49000 loss: 2.629290180044444\n",
      "24000/49000 loss: 2.345038928765018\n",
      "30000/49000 loss: 2.1820198326207176\n",
      "36000/49000 loss: 2.1703116102011446\n",
      "42000/49000 loss: 1.927320470189909\n",
      "48000/49000 loss: 1.6364548167476194\n",
      "epoch 1: valid acc = 0.504, new learning rate = 0.000475\n",
      "6000/49000 loss: 1.3415725159652694\n",
      "12000/49000 loss: 1.1958274132261264\n",
      "18000/49000 loss: 1.238853350911631\n",
      "24000/49000 loss: 1.1626066913939566\n",
      "30000/49000 loss: 1.2020896636076221\n",
      "36000/49000 loss: 1.1908568063764802\n",
      "42000/49000 loss: 1.0845723233081108\n",
      "48000/49000 loss: 1.0964581406495175\n",
      "epoch 2: valid acc = 0.656, new learning rate = 0.00045125\n",
      "6000/49000 loss: 0.9389959877250076\n",
      "12000/49000 loss: 0.9488915939410029\n",
      "18000/49000 loss: 0.8094778673616057\n",
      "24000/49000 loss: 0.9690694383914255\n",
      "30000/49000 loss: 0.8982820019530953\n",
      "36000/49000 loss: 0.8542469238421565\n",
      "42000/49000 loss: 0.7867557875788718\n",
      "48000/49000 loss: 0.8081992853270636\n",
      "epoch 3: valid acc = 0.733, new learning rate = 0.0004286875\n",
      "6000/49000 loss: 0.7076862829470966\n",
      "12000/49000 loss: 0.7130132881729978\n",
      "18000/49000 loss: 0.70032371119268\n",
      "24000/49000 loss: 0.7110314451324273\n",
      "30000/49000 loss: 0.6881250909291748\n",
      "36000/49000 loss: 0.6636805024332053\n",
      "42000/49000 loss: 0.6377779646100703\n",
      "48000/49000 loss: 0.7063453597565247\n",
      "epoch 4: valid acc = 0.76, new learning rate = 0.00040725312499999993\n",
      "6000/49000 loss: 0.5869393091273442\n",
      "12000/49000 loss: 0.6784926982868723\n",
      "18000/49000 loss: 0.6615353883135725\n",
      "24000/49000 loss: 0.5919122188891432\n",
      "30000/49000 loss: 0.5664132230500174\n",
      "36000/49000 loss: 0.5726715176387425\n",
      "42000/49000 loss: 0.5299066571577089\n",
      "48000/49000 loss: 0.56483032464973\n",
      "epoch 5: valid acc = 0.787, new learning rate = 0.0003868904687499999\n",
      "6000/49000 loss: 0.5453614506084953\n",
      "12000/49000 loss: 0.5739081856843115\n",
      "18000/49000 loss: 0.6578742638402567\n",
      "24000/49000 loss: 0.5037574002289484\n",
      "30000/49000 loss: 0.5555518302993881\n",
      "36000/49000 loss: 0.5207332723851995\n",
      "42000/49000 loss: 0.5780703186414581\n",
      "48000/49000 loss: 0.48489966134618034\n",
      "epoch 6: valid acc = 0.796, new learning rate = 0.0003675459453124999\n",
      "6000/49000 loss: 0.5022723905853935\n",
      "12000/49000 loss: 0.570569663851175\n",
      "18000/49000 loss: 0.5619678468722641\n",
      "24000/49000 loss: 0.5534080630154551\n",
      "30000/49000 loss: 0.48586297023391256\n",
      "36000/49000 loss: 0.5808178445875359\n",
      "42000/49000 loss: 0.5154764767187607\n",
      "48000/49000 loss: 0.5137877272169293\n",
      "epoch 7: valid acc = 0.8, new learning rate = 0.00034916864804687486\n",
      "6000/49000 loss: 0.49072970132065247\n",
      "12000/49000 loss: 0.5158492748062135\n",
      "18000/49000 loss: 0.4842453526821255\n",
      "24000/49000 loss: 0.4985712064169771\n",
      "30000/49000 loss: 0.5194545366118174\n",
      "36000/49000 loss: 0.5606566516806868\n",
      "42000/49000 loss: 0.5448168766745577\n",
      "48000/49000 loss: 0.4722129698492354\n",
      "epoch 8: valid acc = 0.817, new learning rate = 0.0003317102156445311\n",
      "6000/49000 loss: 0.45198908454416276\n",
      "12000/49000 loss: 0.480513871902747\n",
      "18000/49000 loss: 0.5064612259016981\n",
      "24000/49000 loss: 0.5418552771685972\n",
      "30000/49000 loss: 0.5566615738180857\n",
      "36000/49000 loss: 0.4635419212291726\n",
      "42000/49000 loss: 0.47813546110691224\n",
      "48000/49000 loss: 0.5079299792372417\n",
      "epoch 9: valid acc = 0.824, new learning rate = 0.0003151247048623045\n",
      "6000/49000 loss: 0.4541814067630308\n",
      "12000/49000 loss: 0.47431956959909916\n",
      "18000/49000 loss: 0.4822752860832828\n",
      "24000/49000 loss: 0.47145298468139923\n",
      "30000/49000 loss: 0.49238760661573094\n",
      "36000/49000 loss: 0.4119124023120873\n",
      "42000/49000 loss: 0.43374217901442796\n",
      "48000/49000 loss: 0.4341990596608225\n",
      "epoch 10: valid acc = 0.826, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.835265306122449\n",
      "test acc: 0.826\n",
      "test acc: 0.8212\n",
      "number of batches for training: 81\n",
      "epoch 1: valid acc = 0.51, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.648, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.727, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.764, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.783, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.797, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.811, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.816, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.823, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.829, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.8335918367346938\n",
      "test acc: 0.829\n",
      "test acc: 0.8199\n",
      "number of batches for training: 81\n",
      "6000/49000 loss: 2.782590264812913\n",
      "12000/49000 loss: 2.708047780299491\n",
      "18000/49000 loss: 2.495382767937714\n",
      "24000/49000 loss: 2.4434303652579374\n",
      "30000/49000 loss: 2.1981363402577636\n",
      "36000/49000 loss: 2.1462474016859487\n",
      "42000/49000 loss: 1.9235122694199323\n",
      "48000/49000 loss: 1.6312279545822723\n",
      "epoch 1: valid acc = 0.524, new learning rate = 0.000475\n",
      "6000/49000 loss: 1.3752346261638495\n",
      "12000/49000 loss: 1.2570415546522056\n",
      "18000/49000 loss: 1.1944597566068764\n",
      "24000/49000 loss: 1.1365116723864777\n",
      "30000/49000 loss: 1.1073992751125983\n",
      "36000/49000 loss: 1.0464568689367606\n",
      "42000/49000 loss: 1.0083911369319696\n",
      "48000/49000 loss: 0.9783099099972159\n",
      "epoch 2: valid acc = 0.663, new learning rate = 0.00045125\n",
      "6000/49000 loss: 0.8923634296345991\n",
      "12000/49000 loss: 0.8597270209379223\n",
      "18000/49000 loss: 0.8849263609873912\n",
      "24000/49000 loss: 0.8442599332395327\n",
      "30000/49000 loss: 0.7869690710120946\n",
      "36000/49000 loss: 0.8161442562811029\n",
      "42000/49000 loss: 0.7844981822929512\n",
      "48000/49000 loss: 0.7478671613993326\n",
      "epoch 3: valid acc = 0.741, new learning rate = 0.0004286875\n",
      "6000/49000 loss: 0.6513257049729959\n",
      "12000/49000 loss: 0.6463875463852248\n",
      "18000/49000 loss: 0.6685985319678663\n",
      "24000/49000 loss: 0.6847622453101337\n",
      "30000/49000 loss: 0.6244369259031679\n",
      "36000/49000 loss: 0.6252833906533055\n",
      "42000/49000 loss: 0.6573584621088646\n",
      "48000/49000 loss: 0.6610859852929949\n",
      "epoch 4: valid acc = 0.766, new learning rate = 0.00040725312499999993\n",
      "6000/49000 loss: 0.6609570063441703\n",
      "12000/49000 loss: 0.6439416171797581\n",
      "18000/49000 loss: 0.5555083752102854\n",
      "24000/49000 loss: 0.6517052926296539\n",
      "30000/49000 loss: 0.6065506242344275\n",
      "36000/49000 loss: 0.5559627603122929\n",
      "42000/49000 loss: 0.5439634649675049\n",
      "48000/49000 loss: 0.6241587668049603\n",
      "epoch 5: valid acc = 0.785, new learning rate = 0.0003868904687499999\n",
      "6000/49000 loss: 0.5857623715523675\n",
      "12000/49000 loss: 0.6003886713414498\n",
      "18000/49000 loss: 0.5863737419793333\n",
      "24000/49000 loss: 0.5160046727849538\n",
      "30000/49000 loss: 0.4930940381688514\n",
      "36000/49000 loss: 0.5681193600702186\n",
      "42000/49000 loss: 0.6234774601089709\n",
      "48000/49000 loss: 0.5212335938645604\n",
      "epoch 6: valid acc = 0.801, new learning rate = 0.0003675459453124999\n",
      "6000/49000 loss: 0.56654412800159\n",
      "12000/49000 loss: 0.5754760109834309\n",
      "18000/49000 loss: 0.5114073919946357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000/49000 loss: 0.5671410102654216\n",
      "30000/49000 loss: 0.5530782589280014\n",
      "36000/49000 loss: 0.5650043962671173\n",
      "42000/49000 loss: 0.5347099635600645\n",
      "48000/49000 loss: 0.5633588338045356\n",
      "epoch 7: valid acc = 0.809, new learning rate = 0.00034916864804687486\n",
      "6000/49000 loss: 0.5071209370903158\n",
      "12000/49000 loss: 0.5296093979413081\n",
      "18000/49000 loss: 0.49679358333160245\n",
      "24000/49000 loss: 0.4798362584337945\n",
      "30000/49000 loss: 0.5017514875338802\n",
      "36000/49000 loss: 0.5013967817877519\n",
      "42000/49000 loss: 0.48392550836449844\n",
      "48000/49000 loss: 0.47460683723448394\n",
      "epoch 8: valid acc = 0.821, new learning rate = 0.0003317102156445311\n",
      "6000/49000 loss: 0.5406063954261752\n",
      "12000/49000 loss: 0.5014227504551285\n",
      "18000/49000 loss: 0.47691733417844506\n",
      "24000/49000 loss: 0.5003274728080584\n",
      "30000/49000 loss: 0.4652978428390415\n",
      "36000/49000 loss: 0.42375015154504475\n",
      "42000/49000 loss: 0.4327855198621938\n",
      "48000/49000 loss: 0.46672559299782324\n",
      "epoch 9: valid acc = 0.822, new learning rate = 0.0003151247048623045\n",
      "6000/49000 loss: 0.47300044931215807\n",
      "12000/49000 loss: 0.4566962191580061\n",
      "18000/49000 loss: 0.5147734590404548\n",
      "24000/49000 loss: 0.4668727735597918\n",
      "30000/49000 loss: 0.48090660878840275\n",
      "36000/49000 loss: 0.4470814281110075\n",
      "42000/49000 loss: 0.4736325857220269\n",
      "48000/49000 loss: 0.501422941069457\n",
      "epoch 10: valid acc = 0.83, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.8361224489795919\n",
      "test acc: 0.83\n",
      "test acc: 0.82\n",
      "number of batches for training: 81\n",
      "epoch 1: valid acc = 0.514, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.671, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.733, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.75, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.783, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.801, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.809, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.817, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.821, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.824, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.8349183673469388\n",
      "test acc: 0.824\n",
      "test acc: 0.82\n",
      "number of batches for training: 49\n",
      "10000/49000 loss: 2.8724615886709004\n",
      "20000/49000 loss: 2.6703361928661664\n",
      "30000/49000 loss: 2.741873707002437\n",
      "40000/49000 loss: 2.314317163825239\n",
      "epoch 1: valid acc = 0.343, new learning rate = 0.000475\n",
      "10000/49000 loss: 2.0418002921374687\n",
      "20000/49000 loss: 2.0200858854015276\n",
      "30000/49000 loss: 1.712837031735772\n",
      "40000/49000 loss: 1.4722461358357424\n",
      "epoch 2: valid acc = 0.523, new learning rate = 0.00045125\n",
      "10000/49000 loss: 1.1925967169699623\n",
      "20000/49000 loss: 1.175250198201087\n",
      "30000/49000 loss: 1.142220033937329\n",
      "40000/49000 loss: 1.0630541268809464\n",
      "epoch 3: valid acc = 0.619, new learning rate = 0.0004286875\n",
      "10000/49000 loss: 1.0118024182813945\n",
      "20000/49000 loss: 1.0230524223499093\n",
      "30000/49000 loss: 0.9499970607853011\n",
      "40000/49000 loss: 0.976845075458779\n",
      "epoch 4: valid acc = 0.71, new learning rate = 0.00040725312499999993\n",
      "10000/49000 loss: 0.847929909181537\n",
      "20000/49000 loss: 0.8725549196114033\n",
      "30000/49000 loss: 0.8109532376169906\n",
      "40000/49000 loss: 0.8160082399960779\n",
      "epoch 5: valid acc = 0.734, new learning rate = 0.0003868904687499999\n",
      "10000/49000 loss: 0.773835753020982\n",
      "20000/49000 loss: 0.735488098275412\n",
      "30000/49000 loss: 0.7514804269496278\n",
      "40000/49000 loss: 0.6945921556749978\n",
      "epoch 6: valid acc = 0.742, new learning rate = 0.0003675459453124999\n",
      "10000/49000 loss: 0.6324653666087737\n",
      "20000/49000 loss: 0.6673847093252766\n",
      "30000/49000 loss: 0.6705112065951196\n",
      "40000/49000 loss: 0.6533619733122693\n",
      "epoch 7: valid acc = 0.758, new learning rate = 0.00034916864804687486\n",
      "10000/49000 loss: 0.6172786314786654\n",
      "20000/49000 loss: 0.6584890431355767\n",
      "30000/49000 loss: 0.6465104561188796\n",
      "40000/49000 loss: 0.5963238456082711\n",
      "epoch 8: valid acc = 0.768, new learning rate = 0.0003317102156445311\n",
      "10000/49000 loss: 0.6111151154654472\n",
      "20000/49000 loss: 0.6117923866497359\n",
      "30000/49000 loss: 0.5737303160693652\n",
      "40000/49000 loss: 0.6278174791830091\n",
      "epoch 9: valid acc = 0.781, new learning rate = 0.0003151247048623045\n",
      "10000/49000 loss: 0.5754361788417984\n",
      "20000/49000 loss: 0.5435766162904504\n",
      "30000/49000 loss: 0.520946942395172\n",
      "40000/49000 loss: 0.5601721704886093\n",
      "epoch 10: valid acc = 0.795, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.8016122448979592\n",
      "test acc: 0.795\n",
      "test acc: 0.7941\n",
      "number of batches for training: 49\n",
      "epoch 1: valid acc = 0.374, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.51, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.645, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.691, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.727, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.744, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.768, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.773, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.786, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.793, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.8003469387755102\n",
      "test acc: 0.793\n",
      "test acc: 0.7903\n",
      "number of batches for training: 49\n",
      "10000/49000 loss: 2.6871780035847896\n",
      "20000/49000 loss: 2.697905106326699\n",
      "30000/49000 loss: 2.5819497380431584\n",
      "40000/49000 loss: 2.6850183485853774\n",
      "epoch 1: valid acc = 0.351, new learning rate = 0.000475\n",
      "10000/49000 loss: 2.130065437257909\n",
      "20000/49000 loss: 1.998668523448124\n",
      "30000/49000 loss: 1.7460066155038068\n",
      "40000/49000 loss: 1.529211252697198\n",
      "epoch 2: valid acc = 0.519, new learning rate = 0.00045125\n",
      "10000/49000 loss: 1.2398886552552286\n",
      "20000/49000 loss: 1.1912554159496316\n",
      "30000/49000 loss: 1.2004963218993743\n",
      "40000/49000 loss: 1.0924284725806204\n",
      "epoch 3: valid acc = 0.645, new learning rate = 0.0004286875\n",
      "10000/49000 loss: 1.0045883919725536\n",
      "20000/49000 loss: 1.0020072365563928\n",
      "30000/49000 loss: 0.9775443772394661\n",
      "40000/49000 loss: 0.8725011475779983\n",
      "epoch 4: valid acc = 0.712, new learning rate = 0.00040725312499999993\n",
      "10000/49000 loss: 0.9101316047996924\n",
      "20000/49000 loss: 0.9113345905679607\n",
      "30000/49000 loss: 0.7855061879837981\n",
      "40000/49000 loss: 0.7535135948936752\n",
      "epoch 5: valid acc = 0.734, new learning rate = 0.0003868904687499999\n",
      "10000/49000 loss: 0.7359987280135446\n",
      "20000/49000 loss: 0.6885631619943936\n",
      "30000/49000 loss: 0.7599066816925897\n",
      "40000/49000 loss: 0.7113813036453549\n",
      "epoch 6: valid acc = 0.747, new learning rate = 0.0003675459453124999\n",
      "10000/49000 loss: 0.7011846693885946\n",
      "20000/49000 loss: 0.7203524265266031\n",
      "30000/49000 loss: 0.6508135351974812\n",
      "40000/49000 loss: 0.6713413968308466\n",
      "epoch 7: valid acc = 0.75, new learning rate = 0.00034916864804687486\n",
      "10000/49000 loss: 0.6331891052411427\n",
      "20000/49000 loss: 0.6099743171332165\n",
      "30000/49000 loss: 0.5942378877885146\n",
      "40000/49000 loss: 0.5893991590210295\n",
      "epoch 8: valid acc = 0.769, new learning rate = 0.0003317102156445311\n",
      "10000/49000 loss: 0.6643538053256729\n",
      "20000/49000 loss: 0.5919624597508112\n",
      "30000/49000 loss: 0.5734966639148193\n",
      "40000/49000 loss: 0.5713657201309328\n",
      "epoch 9: valid acc = 0.781, new learning rate = 0.0003151247048623045\n",
      "10000/49000 loss: 0.5469811438071851\n",
      "20000/49000 loss: 0.5549566919021359\n",
      "30000/49000 loss: 0.5278471088732832\n",
      "40000/49000 loss: 0.5957826583687833\n",
      "epoch 10: valid acc = 0.784, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.7994489795918367\n",
      "test acc: 0.784\n",
      "test acc: 0.7904\n",
      "number of batches for training: 49\n",
      "epoch 1: valid acc = 0.372, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.544, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.647, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.695, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.732, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.744, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.756, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.774, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.782, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.789, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.7974489795918367\n",
      "test acc: 0.789\n",
      "test acc: 0.788\n",
      "number of batches for training: 49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/49000 loss: 2.764091043550589\n",
      "20000/49000 loss: 2.857740876252938\n",
      "30000/49000 loss: 2.4678372348163222\n",
      "40000/49000 loss: 2.3198541092505667\n",
      "epoch 1: valid acc = 0.382, new learning rate = 0.000475\n",
      "10000/49000 loss: 2.129326730496013\n",
      "20000/49000 loss: 2.0267443983733293\n",
      "30000/49000 loss: 1.7777688855697538\n",
      "40000/49000 loss: 1.4686797452910096\n",
      "epoch 2: valid acc = 0.52, new learning rate = 0.00045125\n",
      "10000/49000 loss: 1.2148954239984253\n",
      "20000/49000 loss: 1.1302895084413749\n",
      "30000/49000 loss: 1.1595301570062857\n",
      "40000/49000 loss: 1.1390109751165511\n",
      "epoch 3: valid acc = 0.654, new learning rate = 0.0004286875\n",
      "10000/49000 loss: 0.9999595981508161\n",
      "20000/49000 loss: 1.0003041723312809\n",
      "30000/49000 loss: 0.967725009952621\n",
      "40000/49000 loss: 0.9136758363415621\n",
      "epoch 4: valid acc = 0.692, new learning rate = 0.00040725312499999993\n",
      "10000/49000 loss: 0.9180848754248566\n",
      "20000/49000 loss: 0.8833419817080723\n",
      "30000/49000 loss: 0.8279674883568023\n",
      "40000/49000 loss: 0.7824770377306219\n",
      "epoch 5: valid acc = 0.734, new learning rate = 0.0003868904687499999\n",
      "10000/49000 loss: 0.7499383674044015\n",
      "20000/49000 loss: 0.772352571797855\n",
      "30000/49000 loss: 0.7501226586961893\n",
      "40000/49000 loss: 0.7212713213483913\n",
      "epoch 6: valid acc = 0.753, new learning rate = 0.0003675459453124999\n",
      "10000/49000 loss: 0.6591658359720672\n",
      "20000/49000 loss: 0.712737503732655\n",
      "30000/49000 loss: 0.602416787507828\n",
      "40000/49000 loss: 0.6340215188643852\n",
      "epoch 7: valid acc = 0.759, new learning rate = 0.00034916864804687486\n",
      "10000/49000 loss: 0.6557510997354482\n",
      "20000/49000 loss: 0.6291584635574137\n",
      "30000/49000 loss: 0.607523982118169\n",
      "40000/49000 loss: 0.6387869506480598\n",
      "epoch 8: valid acc = 0.775, new learning rate = 0.0003317102156445311\n",
      "10000/49000 loss: 0.5976974679285963\n",
      "20000/49000 loss: 0.588149452592132\n",
      "30000/49000 loss: 0.6241618933253877\n",
      "40000/49000 loss: 0.5906183203610068\n",
      "epoch 9: valid acc = 0.782, new learning rate = 0.0003151247048623045\n",
      "10000/49000 loss: 0.6068642552544774\n",
      "20000/49000 loss: 0.525902682215014\n",
      "30000/49000 loss: 0.5921311935594273\n",
      "40000/49000 loss: 0.5559313274080957\n",
      "epoch 10: valid acc = 0.794, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.7999183673469388\n",
      "test acc: 0.794\n",
      "test acc: 0.7897\n",
      "number of batches for training: 49\n",
      "epoch 1: valid acc = 0.427, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.518, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.637, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.695, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.72, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.738, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.749, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.767, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.779, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.779, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.7971632653061225\n",
      "test acc: 0.779\n",
      "test acc: 0.7888\n",
      "number of batches for training: 245\n",
      "2000/49000 loss: 3.3307016011760116\n",
      "4000/49000 loss: 2.9406209409623187\n",
      "6000/49000 loss: 2.4900567413771917\n",
      "8000/49000 loss: 2.4370988289887094\n",
      "10000/49000 loss: 2.293315473280831\n",
      "12000/49000 loss: 2.105488124793551\n",
      "14000/49000 loss: 1.9299552883507611\n",
      "16000/49000 loss: 1.5337266916365921\n",
      "18000/49000 loss: 1.4742209688597192\n",
      "20000/49000 loss: 1.2464571822916595\n",
      "22000/49000 loss: 1.2861683874982253\n",
      "24000/49000 loss: 1.2365937875939543\n",
      "26000/49000 loss: 1.0969976029266972\n",
      "28000/49000 loss: 1.0744801691580919\n",
      "30000/49000 loss: 0.9487792277406195\n",
      "32000/49000 loss: 1.023970220523002\n",
      "34000/49000 loss: 0.9935120719490557\n",
      "36000/49000 loss: 1.0071884776536528\n",
      "38000/49000 loss: 0.8434881202620335\n",
      "40000/49000 loss: 0.739820002177791\n",
      "42000/49000 loss: 0.8766975432688083\n",
      "44000/49000 loss: 0.6761525067365955\n",
      "46000/49000 loss: 0.8435550196241023\n",
      "48000/49000 loss: 0.7495607044901523\n",
      "epoch 1: valid acc = 0.746, new learning rate = 0.000475\n",
      "2000/49000 loss: 0.6465505845363841\n",
      "4000/49000 loss: 0.7358829649820737\n",
      "6000/49000 loss: 0.6641897331038359\n",
      "8000/49000 loss: 0.6229585507427728\n",
      "10000/49000 loss: 0.6531738684968278\n",
      "12000/49000 loss: 0.7398890802996357\n",
      "14000/49000 loss: 0.6445794052077833\n",
      "16000/49000 loss: 0.6809747334703631\n",
      "18000/49000 loss: 0.6311864377596762\n",
      "20000/49000 loss: 0.5673711057746975\n",
      "22000/49000 loss: 0.5330568906484072\n",
      "24000/49000 loss: 0.5018440527370106\n",
      "26000/49000 loss: 0.605296596118495\n",
      "28000/49000 loss: 0.5166855027330656\n",
      "30000/49000 loss: 0.5721258462279122\n",
      "32000/49000 loss: 0.5047489872322991\n",
      "34000/49000 loss: 0.5244238328450966\n",
      "36000/49000 loss: 0.5197865512571913\n",
      "38000/49000 loss: 0.5513850356981792\n",
      "40000/49000 loss: 0.5868431274663926\n",
      "42000/49000 loss: 0.547665214741198\n",
      "44000/49000 loss: 0.5292229024402246\n",
      "46000/49000 loss: 0.4340909690806388\n",
      "48000/49000 loss: 0.4856543354329299\n",
      "epoch 2: valid acc = 0.812, new learning rate = 0.00045125\n",
      "2000/49000 loss: 0.46883695423701705\n",
      "4000/49000 loss: 0.5173526384133262\n",
      "6000/49000 loss: 0.4647367145673293\n",
      "8000/49000 loss: 0.43380427637082686\n",
      "10000/49000 loss: 0.5946597129081204\n",
      "12000/49000 loss: 0.43736668110035404\n",
      "14000/49000 loss: 0.6128150872508756\n",
      "16000/49000 loss: 0.5164729648625614\n",
      "18000/49000 loss: 0.662054076438288\n",
      "20000/49000 loss: 0.41230310402407283\n",
      "22000/49000 loss: 0.5901735608091798\n",
      "24000/49000 loss: 0.5167746405422072\n",
      "26000/49000 loss: 0.3994739385148956\n",
      "28000/49000 loss: 0.5019965832860436\n",
      "30000/49000 loss: 0.5089298798476466\n",
      "32000/49000 loss: 0.47230453482241325\n",
      "34000/49000 loss: 0.5550670425693728\n",
      "36000/49000 loss: 0.5764220170732234\n",
      "38000/49000 loss: 0.4576720418196794\n",
      "40000/49000 loss: 0.4936700949683328\n",
      "42000/49000 loss: 0.4228515285080665\n",
      "44000/49000 loss: 0.4884540118206647\n",
      "46000/49000 loss: 0.5199300184364327\n",
      "48000/49000 loss: 0.4789916208693066\n",
      "epoch 3: valid acc = 0.837, new learning rate = 0.0004286875\n",
      "2000/49000 loss: 0.5329266680841052\n",
      "4000/49000 loss: 0.5209327734411735\n",
      "6000/49000 loss: 0.4693318886391144\n",
      "8000/49000 loss: 0.38961266565169433\n",
      "10000/49000 loss: 0.3467724384450527\n",
      "12000/49000 loss: 0.4646880219402395\n",
      "14000/49000 loss: 0.37603749719432406\n",
      "16000/49000 loss: 0.5304385014975348\n",
      "18000/49000 loss: 0.42801925106114874\n",
      "20000/49000 loss: 0.430704808678849\n",
      "22000/49000 loss: 0.42513541583079034\n",
      "24000/49000 loss: 0.3490999022734872\n",
      "26000/49000 loss: 0.3877639998490215\n",
      "28000/49000 loss: 0.39433120491947615\n",
      "30000/49000 loss: 0.47511030832969803\n",
      "32000/49000 loss: 0.4892533046644399\n",
      "34000/49000 loss: 0.37464903112628234\n",
      "36000/49000 loss: 0.43618602879743984\n",
      "38000/49000 loss: 0.47205353482064005\n",
      "40000/49000 loss: 0.5026306311057347\n",
      "42000/49000 loss: 0.5220746746349484\n",
      "44000/49000 loss: 0.34678288539701246\n",
      "46000/49000 loss: 0.3509672716054839\n",
      "48000/49000 loss: 0.43507637369426544\n",
      "epoch 4: valid acc = 0.833, new learning rate = 0.00040725312499999993\n",
      "2000/49000 loss: 0.45128223081883767\n",
      "4000/49000 loss: 0.3936632524314117\n",
      "6000/49000 loss: 0.43246794048520404\n",
      "8000/49000 loss: 0.47388453022325067\n",
      "10000/49000 loss: 0.38587676463585446\n",
      "12000/49000 loss: 0.5464518157158532\n",
      "14000/49000 loss: 0.4732291902036132\n",
      "16000/49000 loss: 0.44723961648120986\n",
      "18000/49000 loss: 0.420632215083436\n",
      "20000/49000 loss: 0.37037433178963736\n",
      "22000/49000 loss: 0.4618340422908443\n",
      "24000/49000 loss: 0.3721430353222633\n",
      "26000/49000 loss: 0.41854283342437054\n",
      "28000/49000 loss: 0.36138286194121044\n",
      "30000/49000 loss: 0.49978024533721727\n",
      "32000/49000 loss: 0.42457683930612783\n",
      "34000/49000 loss: 0.4323459725520827\n",
      "36000/49000 loss: 0.46206146822723787\n",
      "38000/49000 loss: 0.3578979302724491\n",
      "40000/49000 loss: 0.41262105879114425\n",
      "42000/49000 loss: 0.3396010337766241\n",
      "44000/49000 loss: 0.4116472851737441\n",
      "46000/49000 loss: 0.39565197081463926\n",
      "48000/49000 loss: 0.47678667075529285\n",
      "epoch 5: valid acc = 0.856, new learning rate = 0.0003868904687499999\n",
      "2000/49000 loss: 0.2552854490210682\n",
      "4000/49000 loss: 0.43519934612658606\n",
      "6000/49000 loss: 0.4551670295199911\n",
      "8000/49000 loss: 0.3330325852251558\n",
      "10000/49000 loss: 0.3610971720883208\n",
      "12000/49000 loss: 0.4028522690682144\n",
      "14000/49000 loss: 0.41288167613509474\n",
      "16000/49000 loss: 0.39379570699471483\n",
      "18000/49000 loss: 0.4568565497636706\n",
      "20000/49000 loss: 0.4863852561128637\n",
      "22000/49000 loss: 0.4581706287812476\n",
      "24000/49000 loss: 0.33003733962428683\n",
      "26000/49000 loss: 0.4559107017333454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28000/49000 loss: 0.4527987148228932\n",
      "30000/49000 loss: 0.4323798300525109\n",
      "32000/49000 loss: 0.35372494845201535\n",
      "34000/49000 loss: 0.37403676079086456\n",
      "36000/49000 loss: 0.48409653627805116\n",
      "38000/49000 loss: 0.3790757004467612\n",
      "40000/49000 loss: 0.40362400809241344\n",
      "42000/49000 loss: 0.4308803311272009\n",
      "44000/49000 loss: 0.4860966674313749\n",
      "46000/49000 loss: 0.44513954231636793\n",
      "48000/49000 loss: 0.297157686769066\n",
      "epoch 6: valid acc = 0.861, new learning rate = 0.0003675459453124999\n",
      "2000/49000 loss: 0.4216701576247689\n",
      "4000/49000 loss: 0.35393760404679797\n",
      "6000/49000 loss: 0.48772499044047796\n",
      "8000/49000 loss: 0.41101235472758374\n",
      "10000/49000 loss: 0.5166990254148457\n",
      "12000/49000 loss: 0.3266756317726008\n",
      "14000/49000 loss: 0.3318270852647675\n",
      "16000/49000 loss: 0.3253826088250547\n",
      "18000/49000 loss: 0.3490766692305669\n",
      "20000/49000 loss: 0.48006766730990885\n",
      "22000/49000 loss: 0.45068007255325376\n",
      "24000/49000 loss: 0.27561770519757484\n",
      "26000/49000 loss: 0.40833757907078927\n",
      "28000/49000 loss: 0.38120888155684735\n",
      "30000/49000 loss: 0.36629575559335825\n",
      "32000/49000 loss: 0.3690850931454729\n",
      "34000/49000 loss: 0.4702626050569992\n",
      "36000/49000 loss: 0.3472676401749067\n",
      "38000/49000 loss: 0.4506614512427735\n",
      "40000/49000 loss: 0.4283204317613675\n",
      "42000/49000 loss: 0.4365773981098821\n",
      "44000/49000 loss: 0.3338967512755727\n",
      "46000/49000 loss: 0.37493434465649694\n",
      "48000/49000 loss: 0.460109781807875\n",
      "epoch 7: valid acc = 0.859, new learning rate = 0.00034916864804687486\n",
      "2000/49000 loss: 0.31306973734253374\n",
      "4000/49000 loss: 0.34996718964609747\n",
      "6000/49000 loss: 0.39050943087159457\n",
      "8000/49000 loss: 0.25462914737264863\n",
      "10000/49000 loss: 0.46191440120708804\n",
      "12000/49000 loss: 0.3710035484820673\n",
      "14000/49000 loss: 0.38693220209020435\n",
      "16000/49000 loss: 0.3462374071898355\n",
      "18000/49000 loss: 0.4487959775858464\n",
      "20000/49000 loss: 0.3225384657595164\n",
      "22000/49000 loss: 0.36437255386792244\n",
      "24000/49000 loss: 0.3271874186737991\n",
      "26000/49000 loss: 0.36082481514591414\n",
      "28000/49000 loss: 0.36693456820422354\n",
      "30000/49000 loss: 0.3657056028696804\n",
      "32000/49000 loss: 0.36162342730620756\n",
      "34000/49000 loss: 0.35377481947410827\n",
      "36000/49000 loss: 0.3483311579446308\n",
      "38000/49000 loss: 0.38596029822954914\n",
      "40000/49000 loss: 0.3851374383081844\n",
      "42000/49000 loss: 0.5065546544749192\n",
      "44000/49000 loss: 0.33984942080768243\n",
      "46000/49000 loss: 0.4230134443002241\n",
      "48000/49000 loss: 0.31016675803651583\n",
      "epoch 8: valid acc = 0.866, new learning rate = 0.0003317102156445311\n",
      "2000/49000 loss: 0.3266047304787804\n",
      "4000/49000 loss: 0.38436696696456013\n",
      "6000/49000 loss: 0.32904398630106935\n",
      "8000/49000 loss: 0.42081364987280406\n",
      "10000/49000 loss: 0.3641893985004798\n",
      "12000/49000 loss: 0.3094712742653296\n",
      "14000/49000 loss: 0.477532050857349\n",
      "16000/49000 loss: 0.3432139166866427\n",
      "18000/49000 loss: 0.3230564022862373\n",
      "20000/49000 loss: 0.255382435927217\n",
      "22000/49000 loss: 0.5183107319215717\n",
      "24000/49000 loss: 0.4527294863490078\n",
      "26000/49000 loss: 0.4245319699150045\n",
      "28000/49000 loss: 0.32110061154288766\n",
      "30000/49000 loss: 0.39004999763757\n",
      "32000/49000 loss: 0.37277215000274877\n",
      "34000/49000 loss: 0.40944647324423594\n",
      "36000/49000 loss: 0.4246007669009546\n",
      "38000/49000 loss: 0.3707182640411998\n",
      "40000/49000 loss: 0.33305879248881415\n",
      "42000/49000 loss: 0.4008507975437731\n",
      "44000/49000 loss: 0.43707494614560927\n",
      "46000/49000 loss: 0.3734041349932617\n",
      "48000/49000 loss: 0.26341281761454244\n",
      "epoch 9: valid acc = 0.863, new learning rate = 0.0003151247048623045\n",
      "2000/49000 loss: 0.3691792977727664\n",
      "4000/49000 loss: 0.337804376697502\n",
      "6000/49000 loss: 0.46883242074023956\n",
      "8000/49000 loss: 0.42674028863233576\n",
      "10000/49000 loss: 0.3636551839745261\n",
      "12000/49000 loss: 0.30335042394823175\n",
      "14000/49000 loss: 0.277011981164706\n",
      "16000/49000 loss: 0.27895619231959345\n",
      "18000/49000 loss: 0.3612758737102641\n",
      "20000/49000 loss: 0.4369979516581323\n",
      "22000/49000 loss: 0.38124906296760647\n",
      "24000/49000 loss: 0.38200796027030787\n",
      "26000/49000 loss: 0.2956829507371625\n",
      "28000/49000 loss: 0.27247784442869843\n",
      "30000/49000 loss: 0.4558640806234604\n",
      "32000/49000 loss: 0.3480575560300736\n",
      "34000/49000 loss: 0.35402724129809743\n",
      "36000/49000 loss: 0.40510049614985444\n",
      "38000/49000 loss: 0.387453169085111\n",
      "40000/49000 loss: 0.42040941999156456\n",
      "42000/49000 loss: 0.3960975390895697\n",
      "44000/49000 loss: 0.43409957985118586\n",
      "46000/49000 loss: 0.3992900356308534\n",
      "48000/49000 loss: 0.28768557540733536\n",
      "epoch 10: valid acc = 0.87, new learning rate = 0.00029936846961918924\n",
      "2000/49000 loss: 0.38981494494689645\n",
      "4000/49000 loss: 0.40695301291969693\n",
      "6000/49000 loss: 0.3583343982023071\n",
      "8000/49000 loss: 0.36917314827334163\n",
      "10000/49000 loss: 0.3537943625036017\n",
      "12000/49000 loss: 0.3591816122928597\n",
      "14000/49000 loss: 0.3398920642434853\n",
      "16000/49000 loss: 0.37317534344198783\n",
      "18000/49000 loss: 0.33921912097626944\n",
      "20000/49000 loss: 0.4492005656212492\n",
      "22000/49000 loss: 0.32743637198924447\n",
      "24000/49000 loss: 0.31968353571524843\n",
      "26000/49000 loss: 0.4879803712871046\n",
      "28000/49000 loss: 0.3104891919342507\n",
      "30000/49000 loss: 0.25312767090235766\n",
      "32000/49000 loss: 0.39397665272192983\n",
      "34000/49000 loss: 0.373852444019386\n",
      "36000/49000 loss: 0.37097179597996455\n",
      "38000/49000 loss: 0.3503397330595918\n",
      "40000/49000 loss: 0.39014822229824364\n",
      "42000/49000 loss: 0.45531293656104105\n",
      "44000/49000 loss: 0.3619091430098125\n",
      "46000/49000 loss: 0.31344582771483326\n",
      "48000/49000 loss: 0.3046208682820808\n",
      "epoch 11: valid acc = 0.867, new learning rate = 0.00028440004613822977\n",
      "2000/49000 loss: 0.5025006289502372\n",
      "4000/49000 loss: 0.2688517429861697\n",
      "6000/49000 loss: 0.33888947528674346\n",
      "8000/49000 loss: 0.346530051295184\n",
      "10000/49000 loss: 0.4560540823637115\n",
      "12000/49000 loss: 0.39147557816484635\n",
      "14000/49000 loss: 0.36865494961050715\n",
      "16000/49000 loss: 0.298843516448369\n",
      "18000/49000 loss: 0.37333842227033814\n",
      "20000/49000 loss: 0.3263319753679453\n",
      "22000/49000 loss: 0.24063400448098154\n",
      "24000/49000 loss: 0.45925424575353485\n",
      "26000/49000 loss: 0.31101860075584964\n",
      "28000/49000 loss: 0.35311525435439994\n",
      "30000/49000 loss: 0.32936501288731895\n",
      "32000/49000 loss: 0.32401037435650293\n",
      "34000/49000 loss: 0.31614905281187855\n",
      "36000/49000 loss: 0.33683235631661657\n",
      "38000/49000 loss: 0.37082272403430644\n",
      "40000/49000 loss: 0.2588778632777181\n",
      "42000/49000 loss: 0.3263468640536283\n",
      "44000/49000 loss: 0.3933179321237301\n",
      "46000/49000 loss: 0.31763556855914044\n",
      "48000/49000 loss: 0.33676443029592923\n",
      "epoch 12: valid acc = 0.875, new learning rate = 0.00027018004383131826\n",
      "2000/49000 loss: 0.27064951975917595\n",
      "4000/49000 loss: 0.3073761717209649\n",
      "6000/49000 loss: 0.2933120100779233\n",
      "8000/49000 loss: 0.2443929344977701\n",
      "10000/49000 loss: 0.36379709944569555\n",
      "12000/49000 loss: 0.36297958365948807\n",
      "14000/49000 loss: 0.2890667829383895\n",
      "16000/49000 loss: 0.28222072745132126\n",
      "18000/49000 loss: 0.3438403673516053\n",
      "20000/49000 loss: 0.294698095227042\n",
      "22000/49000 loss: 0.2993551554152953\n",
      "24000/49000 loss: 0.40783379050342533\n",
      "26000/49000 loss: 0.3877160520400296\n",
      "28000/49000 loss: 0.24086795070113884\n",
      "30000/49000 loss: 0.41380083060754863\n",
      "32000/49000 loss: 0.41188665650180944\n",
      "34000/49000 loss: 0.3704592453272498\n",
      "36000/49000 loss: 0.3008697443602403\n",
      "38000/49000 loss: 0.30440039362364457\n",
      "40000/49000 loss: 0.31340406759466544\n",
      "42000/49000 loss: 0.2651691942249628\n",
      "44000/49000 loss: 0.3154131052017967\n",
      "46000/49000 loss: 0.22239736789025238\n",
      "48000/49000 loss: 0.31644583233857826\n",
      "epoch 13: valid acc = 0.874, new learning rate = 0.00025667104163975234\n",
      "2000/49000 loss: 0.27206026904117653\n",
      "4000/49000 loss: 0.35914874381221834\n",
      "6000/49000 loss: 0.3260666094710218\n",
      "8000/49000 loss: 0.28704116829177867\n",
      "10000/49000 loss: 0.34865107222945174\n",
      "12000/49000 loss: 0.38592580133172594\n",
      "14000/49000 loss: 0.3390812751316655\n",
      "16000/49000 loss: 0.4046896149104452\n",
      "18000/49000 loss: 0.29197693305384026\n",
      "20000/49000 loss: 0.2854357234959523\n",
      "22000/49000 loss: 0.28122969042358803\n",
      "24000/49000 loss: 0.3639998769483761\n",
      "26000/49000 loss: 0.3490639448997071\n",
      "28000/49000 loss: 0.33113722476591306\n",
      "30000/49000 loss: 0.47572775019245817\n",
      "32000/49000 loss: 0.2854771850060685\n",
      "34000/49000 loss: 0.3465590966985855\n",
      "36000/49000 loss: 0.4152062239532673\n",
      "38000/49000 loss: 0.2547971991037079\n",
      "40000/49000 loss: 0.27948790384807715\n",
      "42000/49000 loss: 0.41160258342186584\n",
      "44000/49000 loss: 0.3109281204156768\n",
      "46000/49000 loss: 0.3235079839750743\n",
      "48000/49000 loss: 0.264205255873196\n",
      "epoch 14: valid acc = 0.882, new learning rate = 0.00024383748955776472\n",
      "2000/49000 loss: 0.3910509433968934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/49000 loss: 0.30603571166668425\n",
      "6000/49000 loss: 0.3912317371103514\n",
      "8000/49000 loss: 0.2895434760360887\n",
      "10000/49000 loss: 0.32104709377819307\n",
      "12000/49000 loss: 0.24881908933528002\n",
      "14000/49000 loss: 0.42290411088038965\n",
      "16000/49000 loss: 0.2534649857467344\n",
      "18000/49000 loss: 0.3467767835144877\n",
      "20000/49000 loss: 0.34203091451710554\n",
      "22000/49000 loss: 0.23275139173688367\n",
      "24000/49000 loss: 0.39276182012780175\n",
      "26000/49000 loss: 0.2763573254239073\n",
      "28000/49000 loss: 0.27067088710202586\n",
      "30000/49000 loss: 0.3140015738950314\n",
      "32000/49000 loss: 0.29097620281365005\n",
      "34000/49000 loss: 0.3670348534220829\n",
      "36000/49000 loss: 0.34159269676907134\n",
      "38000/49000 loss: 0.30660027891578695\n",
      "40000/49000 loss: 0.3579990174701811\n",
      "42000/49000 loss: 0.3644495295179475\n",
      "44000/49000 loss: 0.3711454870080494\n",
      "46000/49000 loss: 0.31917755676674675\n",
      "48000/49000 loss: 0.3184487337456259\n",
      "epoch 15: valid acc = 0.884, new learning rate = 0.00023164561507987649\n",
      "2000/49000 loss: 0.4436430720982456\n",
      "4000/49000 loss: 0.33834285592262037\n",
      "6000/49000 loss: 0.34639606587988175\n",
      "8000/49000 loss: 0.3825302022745792\n",
      "10000/49000 loss: 0.27540576833986635\n",
      "12000/49000 loss: 0.3051625956716803\n",
      "14000/49000 loss: 0.3363776667313462\n",
      "16000/49000 loss: 0.2937716835009397\n",
      "18000/49000 loss: 0.3128623574267642\n",
      "20000/49000 loss: 0.3343369913286542\n",
      "22000/49000 loss: 0.32602899448211964\n",
      "24000/49000 loss: 0.29175090446792096\n",
      "26000/49000 loss: 0.2590771359508775\n",
      "28000/49000 loss: 0.2535767715453112\n",
      "30000/49000 loss: 0.3212600824288376\n",
      "32000/49000 loss: 0.2665046393700959\n",
      "34000/49000 loss: 0.3148573747505107\n",
      "36000/49000 loss: 0.35873308255250724\n",
      "38000/49000 loss: 0.2965997699696753\n",
      "40000/49000 loss: 0.2953467440035683\n",
      "42000/49000 loss: 0.2697277585406402\n",
      "44000/49000 loss: 0.3799668030724079\n",
      "46000/49000 loss: 0.4014300158935102\n",
      "48000/49000 loss: 0.2663933441288726\n",
      "epoch 16: valid acc = 0.883, new learning rate = 0.00022006333432588265\n",
      "2000/49000 loss: 0.3003772749490104\n",
      "4000/49000 loss: 0.4029815981082723\n",
      "6000/49000 loss: 0.3716288940522462\n",
      "8000/49000 loss: 0.3181816249341632\n",
      "10000/49000 loss: 0.36438936436401587\n",
      "12000/49000 loss: 0.3797400916681286\n",
      "14000/49000 loss: 0.40661181440268246\n",
      "16000/49000 loss: 0.37551910214560535\n",
      "18000/49000 loss: 0.30690747458298273\n",
      "20000/49000 loss: 0.36035432463610684\n",
      "22000/49000 loss: 0.2317409503052164\n",
      "24000/49000 loss: 0.3588667036694862\n",
      "26000/49000 loss: 0.30676277679733616\n",
      "28000/49000 loss: 0.39479212310081696\n",
      "30000/49000 loss: 0.35060081654289943\n",
      "32000/49000 loss: 0.24150535913449395\n",
      "34000/49000 loss: 0.3065146800294683\n",
      "36000/49000 loss: 0.33353053988850473\n",
      "38000/49000 loss: 0.32900564466480076\n",
      "40000/49000 loss: 0.315866946556346\n",
      "42000/49000 loss: 0.3388778846777508\n",
      "44000/49000 loss: 0.23315164413354364\n",
      "46000/49000 loss: 0.34194134287838746\n",
      "48000/49000 loss: 0.3095629946824139\n",
      "epoch 17: valid acc = 0.885, new learning rate = 0.00020906016760958852\n",
      "2000/49000 loss: 0.35468316258329735\n",
      "4000/49000 loss: 0.2810330277239955\n",
      "6000/49000 loss: 0.27683037686733747\n",
      "8000/49000 loss: 0.3249555505333855\n",
      "10000/49000 loss: 0.3674507774147356\n",
      "12000/49000 loss: 0.2702018918546519\n",
      "14000/49000 loss: 0.3470897445679396\n",
      "16000/49000 loss: 0.294455391933672\n",
      "18000/49000 loss: 0.3004903697969621\n",
      "20000/49000 loss: 0.34601685155352807\n",
      "22000/49000 loss: 0.37718540183716215\n",
      "24000/49000 loss: 0.37412649030603334\n",
      "26000/49000 loss: 0.26448484782889\n",
      "28000/49000 loss: 0.21827106295156948\n",
      "30000/49000 loss: 0.3261545537747596\n",
      "32000/49000 loss: 0.3346118016511944\n",
      "34000/49000 loss: 0.34655330687528696\n",
      "36000/49000 loss: 0.30949478785429396\n",
      "38000/49000 loss: 0.30063947773254873\n",
      "40000/49000 loss: 0.30923361944029837\n",
      "42000/49000 loss: 0.3402656012925336\n",
      "44000/49000 loss: 0.32477727449951427\n",
      "46000/49000 loss: 0.3201846240795121\n",
      "48000/49000 loss: 0.3972673468656286\n",
      "epoch 18: valid acc = 0.885, new learning rate = 0.00019860715922910907\n",
      "2000/49000 loss: 0.21582838316155492\n",
      "4000/49000 loss: 0.2527620052144987\n",
      "6000/49000 loss: 0.2992460584282158\n",
      "8000/49000 loss: 0.34562791690224726\n",
      "10000/49000 loss: 0.32456227880896693\n",
      "12000/49000 loss: 0.3374695164658467\n",
      "14000/49000 loss: 0.34666908809714253\n",
      "16000/49000 loss: 0.3768443942804931\n",
      "18000/49000 loss: 0.35879026448202317\n",
      "20000/49000 loss: 0.30241862213659026\n",
      "22000/49000 loss: 0.2679615012050479\n",
      "24000/49000 loss: 0.21533635207210133\n",
      "26000/49000 loss: 0.3043870822094819\n",
      "28000/49000 loss: 0.24653521288000854\n",
      "30000/49000 loss: 0.4222478929542429\n",
      "32000/49000 loss: 0.27329962537216357\n",
      "34000/49000 loss: 0.34616503626199135\n",
      "36000/49000 loss: 0.3929860420580899\n",
      "38000/49000 loss: 0.33038839525590386\n",
      "40000/49000 loss: 0.2448586799883162\n",
      "42000/49000 loss: 0.3129542952360519\n",
      "44000/49000 loss: 0.292322570814074\n",
      "46000/49000 loss: 0.40237740577071757\n",
      "48000/49000 loss: 0.2569665497433365\n",
      "epoch 19: valid acc = 0.883, new learning rate = 0.0001886768012676536\n",
      "2000/49000 loss: 0.3911727929186326\n",
      "4000/49000 loss: 0.19612328511948685\n",
      "6000/49000 loss: 0.34793361251703564\n",
      "8000/49000 loss: 0.3398051154294107\n",
      "10000/49000 loss: 0.3185118659244631\n",
      "12000/49000 loss: 0.24371813153850264\n",
      "14000/49000 loss: 0.3450308460382471\n",
      "16000/49000 loss: 0.4159974133471815\n",
      "18000/49000 loss: 0.33576493703431864\n",
      "20000/49000 loss: 0.26524678968732773\n",
      "22000/49000 loss: 0.2663292772760586\n",
      "24000/49000 loss: 0.31172050999270906\n",
      "26000/49000 loss: 0.2706978104803881\n",
      "28000/49000 loss: 0.2846184154171443\n",
      "30000/49000 loss: 0.2976636591365986\n",
      "32000/49000 loss: 0.305495124184557\n",
      "34000/49000 loss: 0.25127488487434707\n",
      "36000/49000 loss: 0.26059390613922784\n",
      "38000/49000 loss: 0.2921249796545892\n",
      "40000/49000 loss: 0.3162955262553221\n",
      "42000/49000 loss: 0.2427172034171066\n",
      "44000/49000 loss: 0.24147485775176789\n",
      "46000/49000 loss: 0.3478638733550443\n",
      "48000/49000 loss: 0.358055527003028\n",
      "epoch 20: valid acc = 0.884, new learning rate = 0.0001792429612042709\n",
      "2000/49000 loss: 0.20888286315532256\n",
      "4000/49000 loss: 0.3058141840054258\n",
      "6000/49000 loss: 0.3006604304765516\n",
      "8000/49000 loss: 0.23281389300279454\n",
      "10000/49000 loss: 0.26490657897038156\n",
      "12000/49000 loss: 0.3179391646437257\n",
      "14000/49000 loss: 0.2852082766222143\n",
      "16000/49000 loss: 0.32432978928796385\n",
      "18000/49000 loss: 0.31146606855809994\n",
      "20000/49000 loss: 0.2649516569137527\n",
      "22000/49000 loss: 0.26244701735960435\n",
      "24000/49000 loss: 0.337616404954222\n",
      "26000/49000 loss: 0.2757844337231166\n",
      "28000/49000 loss: 0.310383151067966\n",
      "30000/49000 loss: 0.23318067265768758\n",
      "32000/49000 loss: 0.2927632373124122\n",
      "34000/49000 loss: 0.2590597075150204\n",
      "36000/49000 loss: 0.3336979768914066\n",
      "38000/49000 loss: 0.2889145207931937\n",
      "40000/49000 loss: 0.27996368511863434\n",
      "42000/49000 loss: 0.2582534787358703\n",
      "44000/49000 loss: 0.4062856399745041\n",
      "46000/49000 loss: 0.22371124035463455\n",
      "48000/49000 loss: 0.2824730401761212\n",
      "epoch 21: valid acc = 0.889, new learning rate = 0.00017028081314405735\n",
      "2000/49000 loss: 0.3349152383326377\n",
      "4000/49000 loss: 0.33324817071806584\n",
      "6000/49000 loss: 0.28754145082195837\n",
      "8000/49000 loss: 0.301990274819091\n",
      "10000/49000 loss: 0.40111362255072336\n",
      "12000/49000 loss: 0.2698403586045289\n",
      "14000/49000 loss: 0.35527393610097013\n",
      "16000/49000 loss: 0.29377534339637107\n",
      "18000/49000 loss: 0.32248458699823995\n",
      "20000/49000 loss: 0.33453257315446255\n",
      "22000/49000 loss: 0.2933470825203112\n",
      "24000/49000 loss: 0.4273528560264933\n",
      "26000/49000 loss: 0.2755504919214767\n",
      "28000/49000 loss: 0.2754942811318312\n",
      "30000/49000 loss: 0.21385916629826252\n",
      "32000/49000 loss: 0.3627629341531516\n",
      "34000/49000 loss: 0.3032640385020609\n",
      "36000/49000 loss: 0.30258724813530447\n",
      "38000/49000 loss: 0.24664035725640734\n",
      "40000/49000 loss: 0.3391192995907287\n",
      "42000/49000 loss: 0.3109452561880938\n",
      "44000/49000 loss: 0.3210355589437912\n",
      "46000/49000 loss: 0.29292358137980684\n",
      "48000/49000 loss: 0.37513781720140627\n",
      "epoch 22: valid acc = 0.882, new learning rate = 0.00016176677248685447\n",
      "2000/49000 loss: 0.3201543017920705\n",
      "4000/49000 loss: 0.37309004178064\n",
      "6000/49000 loss: 0.3096651191502949\n",
      "8000/49000 loss: 0.2729690280543675\n",
      "10000/49000 loss: 0.3085969433454939\n",
      "12000/49000 loss: 0.2577651330021758\n",
      "14000/49000 loss: 0.21299697482601646\n",
      "16000/49000 loss: 0.25635352693478825\n",
      "18000/49000 loss: 0.3379064692784009\n",
      "20000/49000 loss: 0.3166576347669068\n",
      "22000/49000 loss: 0.30927283701820807\n",
      "24000/49000 loss: 0.322899265762062\n",
      "26000/49000 loss: 0.31785837398076744\n",
      "28000/49000 loss: 0.2682827578758993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000/49000 loss: 0.2821651350478871\n",
      "32000/49000 loss: 0.24288662513811027\n",
      "34000/49000 loss: 0.20899705315501885\n",
      "36000/49000 loss: 0.2966426554457589\n",
      "38000/49000 loss: 0.3311470797478525\n",
      "40000/49000 loss: 0.29114784865288634\n",
      "42000/49000 loss: 0.2822982494734637\n",
      "44000/49000 loss: 0.2702357587377881\n",
      "46000/49000 loss: 0.3548629429213744\n",
      "48000/49000 loss: 0.27700611729653696\n",
      "epoch 23: valid acc = 0.884, new learning rate = 0.00015367843386251173\n",
      "2000/49000 loss: 0.2926289861306008\n",
      "4000/49000 loss: 0.2378944856311474\n",
      "6000/49000 loss: 0.25429702436655927\n",
      "8000/49000 loss: 0.32790132179552545\n",
      "10000/49000 loss: 0.2719563583588178\n",
      "12000/49000 loss: 0.31453010176099616\n",
      "14000/49000 loss: 0.2494329292378244\n",
      "16000/49000 loss: 0.4530131200981582\n",
      "18000/49000 loss: 0.3808200582041933\n",
      "20000/49000 loss: 0.38629212063285895\n",
      "22000/49000 loss: 0.2756079741423086\n",
      "24000/49000 loss: 0.35487346866728786\n",
      "26000/49000 loss: 0.32968811222822625\n",
      "28000/49000 loss: 0.21111957543231707\n",
      "30000/49000 loss: 0.18915803044454513\n",
      "32000/49000 loss: 0.24325835437364363\n",
      "34000/49000 loss: 0.3105139572770494\n",
      "36000/49000 loss: 0.2865070553728431\n",
      "38000/49000 loss: 0.2875625697536097\n",
      "40000/49000 loss: 0.2976358666128004\n",
      "42000/49000 loss: 0.22388442117501975\n",
      "44000/49000 loss: 0.22222934614453438\n",
      "46000/49000 loss: 0.24361420420270483\n",
      "48000/49000 loss: 0.3079485116704918\n",
      "epoch 24: valid acc = 0.888, new learning rate = 0.00014599451216938612\n",
      "2000/49000 loss: 0.3890076992020618\n",
      "4000/49000 loss: 0.36788879713622535\n",
      "6000/49000 loss: 0.4015319076388587\n",
      "8000/49000 loss: 0.2929305998601348\n",
      "10000/49000 loss: 0.2941488925305747\n",
      "12000/49000 loss: 0.21211671626303463\n",
      "14000/49000 loss: 0.34673055469260483\n",
      "16000/49000 loss: 0.36667907114381904\n",
      "18000/49000 loss: 0.31103927966272915\n",
      "20000/49000 loss: 0.33928054222042425\n",
      "22000/49000 loss: 0.3453638429561312\n",
      "24000/49000 loss: 0.25826661398636364\n",
      "26000/49000 loss: 0.33103140964611927\n",
      "28000/49000 loss: 0.29216812973946543\n",
      "30000/49000 loss: 0.3328572497589758\n",
      "32000/49000 loss: 0.29237430684445986\n",
      "34000/49000 loss: 0.3082595243426995\n",
      "36000/49000 loss: 0.317496881944407\n",
      "38000/49000 loss: 0.2935922888760739\n",
      "40000/49000 loss: 0.4302904684338874\n",
      "42000/49000 loss: 0.30465220824268874\n",
      "44000/49000 loss: 0.34326091817097576\n",
      "46000/49000 loss: 0.3063017784085205\n",
      "48000/49000 loss: 0.36798674998154635\n",
      "epoch 25: valid acc = 0.884, new learning rate = 0.00013869478656091682\n",
      "2000/49000 loss: 0.29417315423804885\n",
      "4000/49000 loss: 0.2412594830171676\n",
      "6000/49000 loss: 0.32107270890316514\n",
      "8000/49000 loss: 0.3303209039604591\n",
      "10000/49000 loss: 0.24564756192804088\n",
      "12000/49000 loss: 0.29527569771543805\n",
      "14000/49000 loss: 0.4142484923340014\n",
      "16000/49000 loss: 0.2926190018681181\n",
      "18000/49000 loss: 0.29144310493505693\n",
      "20000/49000 loss: 0.3163979877607394\n",
      "22000/49000 loss: 0.33746944038159565\n",
      "24000/49000 loss: 0.3235826410375248\n",
      "26000/49000 loss: 0.28865473929584434\n",
      "28000/49000 loss: 0.3413630174939998\n",
      "30000/49000 loss: 0.305560314117562\n",
      "32000/49000 loss: 0.2019263836578408\n",
      "34000/49000 loss: 0.26896011274252224\n",
      "36000/49000 loss: 0.26337842419080454\n",
      "38000/49000 loss: 0.2720154888189837\n",
      "40000/49000 loss: 0.2850730626190863\n",
      "42000/49000 loss: 0.20716736733523922\n",
      "44000/49000 loss: 0.23162596304246658\n",
      "46000/49000 loss: 0.24783346408999837\n",
      "48000/49000 loss: 0.2696795606937286\n",
      "epoch 26: valid acc = 0.882, new learning rate = 0.00013176004723287096\n",
      "2000/49000 loss: 0.3884779830857636\n",
      "4000/49000 loss: 0.2821187009412011\n",
      "6000/49000 loss: 0.21443696907140036\n",
      "8000/49000 loss: 0.2985285865798779\n",
      "10000/49000 loss: 0.32857027854759574\n",
      "12000/49000 loss: 0.22163813386311906\n",
      "14000/49000 loss: 0.23890094570624246\n",
      "16000/49000 loss: 0.34432478201233185\n",
      "18000/49000 loss: 0.2632832000628401\n",
      "20000/49000 loss: 0.37605926480520047\n",
      "22000/49000 loss: 0.3280966497626801\n",
      "24000/49000 loss: 0.2683657702161338\n",
      "26000/49000 loss: 0.27297127527258025\n",
      "28000/49000 loss: 0.27493597084145527\n",
      "30000/49000 loss: 0.24478603217107503\n",
      "32000/49000 loss: 0.2749103625129042\n",
      "34000/49000 loss: 0.3196488859538395\n",
      "36000/49000 loss: 0.2990893903016319\n",
      "38000/49000 loss: 0.3156965217782983\n",
      "40000/49000 loss: 0.3492476190864359\n",
      "42000/49000 loss: 0.23486411022727333\n",
      "44000/49000 loss: 0.2838561248011608\n",
      "46000/49000 loss: 0.348996367027202\n",
      "48000/49000 loss: 0.31620743406957424\n",
      "epoch 27: valid acc = 0.886, new learning rate = 0.0001251720448712274\n",
      "2000/49000 loss: 0.2350457799362551\n",
      "4000/49000 loss: 0.3785603827956525\n",
      "6000/49000 loss: 0.34832859139344735\n",
      "8000/49000 loss: 0.3159437755828343\n",
      "10000/49000 loss: 0.24967765053375146\n",
      "12000/49000 loss: 0.36743061922869613\n",
      "14000/49000 loss: 0.3059764303819237\n",
      "16000/49000 loss: 0.3023317424702486\n",
      "18000/49000 loss: 0.25303031084992633\n",
      "20000/49000 loss: 0.3017535287190518\n",
      "22000/49000 loss: 0.2888698123762502\n",
      "24000/49000 loss: 0.2885512066079497\n",
      "26000/49000 loss: 0.3096697511623903\n",
      "28000/49000 loss: 0.3511501147535505\n",
      "30000/49000 loss: 0.30052688507349895\n",
      "32000/49000 loss: 0.258488258913695\n",
      "34000/49000 loss: 0.25362102648346824\n",
      "36000/49000 loss: 0.24660103072950906\n",
      "38000/49000 loss: 0.4380370935181221\n",
      "40000/49000 loss: 0.3351448938421557\n",
      "42000/49000 loss: 0.28887633232108156\n",
      "44000/49000 loss: 0.2809716480989976\n",
      "46000/49000 loss: 0.21301738357564057\n",
      "48000/49000 loss: 0.42675195205971367\n",
      "epoch 28: valid acc = 0.88, new learning rate = 0.00011891344262766602\n",
      "2000/49000 loss: 0.2446274661978976\n",
      "4000/49000 loss: 0.32303273685623873\n",
      "6000/49000 loss: 0.31130006161878004\n",
      "8000/49000 loss: 0.19350144263376753\n",
      "10000/49000 loss: 0.2478638396733403\n",
      "12000/49000 loss: 0.22551522474969585\n",
      "14000/49000 loss: 0.3148584344589247\n",
      "16000/49000 loss: 0.3268121025215256\n",
      "18000/49000 loss: 0.2759652444280995\n",
      "20000/49000 loss: 0.24853286410573\n",
      "22000/49000 loss: 0.2894204856440454\n",
      "24000/49000 loss: 0.3093960271198072\n",
      "26000/49000 loss: 0.22860730249314917\n",
      "28000/49000 loss: 0.31605042869113414\n",
      "30000/49000 loss: 0.28208419519635275\n",
      "32000/49000 loss: 0.24755139971324247\n",
      "34000/49000 loss: 0.27127193955799506\n",
      "36000/49000 loss: 0.37190727594531975\n",
      "38000/49000 loss: 0.3020703274043427\n",
      "40000/49000 loss: 0.260380757552484\n",
      "42000/49000 loss: 0.3441902334508971\n",
      "44000/49000 loss: 0.3510450274651553\n",
      "46000/49000 loss: 0.3220859522032309\n",
      "48000/49000 loss: 0.32682415499624273\n",
      "epoch 29: valid acc = 0.881, new learning rate = 0.00011296777049628272\n",
      "2000/49000 loss: 0.4373551018288838\n",
      "4000/49000 loss: 0.22457710904749578\n",
      "6000/49000 loss: 0.3130142531668346\n",
      "8000/49000 loss: 0.24184065768796068\n",
      "10000/49000 loss: 0.22689016234889647\n",
      "12000/49000 loss: 0.30636006756003387\n",
      "14000/49000 loss: 0.2887379320573085\n",
      "16000/49000 loss: 0.2419266563357863\n",
      "18000/49000 loss: 0.36025989160948046\n",
      "20000/49000 loss: 0.3223204951026655\n",
      "22000/49000 loss: 0.22198426651122968\n",
      "24000/49000 loss: 0.26399742665351256\n",
      "26000/49000 loss: 0.27716378259788377\n",
      "28000/49000 loss: 0.39312384996141847\n",
      "30000/49000 loss: 0.3335161865881889\n",
      "32000/49000 loss: 0.3046010284110325\n",
      "34000/49000 loss: 0.36252968780899714\n",
      "36000/49000 loss: 0.4213221197158324\n",
      "38000/49000 loss: 0.2920590252501626\n",
      "40000/49000 loss: 0.2551809475399882\n",
      "42000/49000 loss: 0.30870672445529757\n",
      "44000/49000 loss: 0.2814144127720562\n",
      "46000/49000 loss: 0.3340091216717785\n",
      "48000/49000 loss: 0.37849203560837513\n",
      "epoch 30: valid acc = 0.884, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.8973061224489796\n",
      "test acc: 0.884\n",
      "test acc: 0.8673\n",
      "number of batches for training: 245\n",
      "epoch 1: valid acc = 0.744, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.809, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.823, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.848, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.847, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.86, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.86, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.863, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.869, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.865, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.879, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.868, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.866, new learning rate = 0.00025667104163975234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14: valid acc = 0.874, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.879, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.881, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.879, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.873, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.884, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.885, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.892, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.881, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.881, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.884, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.888, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.891, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.887, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.888, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.89, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.89, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.8978367346938776\n",
      "test acc: 0.89\n",
      "test acc: 0.8688\n",
      "number of batches for training: 245\n",
      "2000/49000 loss: 3.009534503028067\n",
      "4000/49000 loss: 2.5633628505090718\n",
      "6000/49000 loss: 2.501811211572785\n",
      "8000/49000 loss: 2.3595380980607232\n",
      "10000/49000 loss: 2.2786196170731756\n",
      "12000/49000 loss: 1.9296799669864197\n",
      "14000/49000 loss: 2.102924559534777\n",
      "16000/49000 loss: 1.5757060065541586\n",
      "18000/49000 loss: 1.253985855039219\n",
      "20000/49000 loss: 1.2610688927649916\n",
      "22000/49000 loss: 1.0980901059618955\n",
      "24000/49000 loss: 1.0727125263632125\n",
      "26000/49000 loss: 1.034451765120367\n",
      "28000/49000 loss: 1.1671799373055096\n",
      "30000/49000 loss: 1.1157479666478225\n",
      "32000/49000 loss: 0.9224421595084854\n",
      "34000/49000 loss: 0.8734323268184938\n",
      "36000/49000 loss: 0.8622251967168362\n",
      "38000/49000 loss: 0.8246752544324196\n",
      "40000/49000 loss: 0.7526897158425084\n",
      "42000/49000 loss: 0.6804801054931114\n",
      "44000/49000 loss: 0.7413173344777448\n",
      "46000/49000 loss: 0.7345594395218945\n",
      "48000/49000 loss: 0.6986437301360904\n",
      "epoch 1: valid acc = 0.736, new learning rate = 0.000475\n",
      "2000/49000 loss: 0.7385196018735579\n",
      "4000/49000 loss: 0.6843141263452754\n",
      "6000/49000 loss: 0.6602359346912733\n",
      "8000/49000 loss: 0.786441520262213\n",
      "10000/49000 loss: 0.6641225691829586\n",
      "12000/49000 loss: 0.631199633567173\n",
      "14000/49000 loss: 0.584824158796578\n",
      "16000/49000 loss: 0.5792917926499248\n",
      "18000/49000 loss: 0.5192699618344746\n",
      "20000/49000 loss: 0.5073229225495159\n",
      "22000/49000 loss: 0.5997434603888208\n",
      "24000/49000 loss: 0.6835548017880013\n",
      "26000/49000 loss: 0.5870364442573962\n",
      "28000/49000 loss: 0.5589672898076473\n",
      "30000/49000 loss: 0.5278828521549812\n",
      "32000/49000 loss: 0.5581284762063967\n",
      "34000/49000 loss: 0.5366837247236323\n",
      "36000/49000 loss: 0.5604116353640684\n",
      "38000/49000 loss: 0.5283484179682684\n",
      "40000/49000 loss: 0.5547585045375939\n",
      "42000/49000 loss: 0.5552851654519324\n",
      "44000/49000 loss: 0.44625265843305073\n",
      "46000/49000 loss: 0.41983954232460996\n",
      "48000/49000 loss: 0.4420836293391553\n",
      "epoch 2: valid acc = 0.804, new learning rate = 0.00045125\n",
      "2000/49000 loss: 0.5327831020368915\n",
      "4000/49000 loss: 0.48824277278361017\n",
      "6000/49000 loss: 0.6164942364909236\n",
      "8000/49000 loss: 0.4381577640543513\n",
      "10000/49000 loss: 0.5074404715671653\n",
      "12000/49000 loss: 0.5411061702976654\n",
      "14000/49000 loss: 0.598720172976255\n",
      "16000/49000 loss: 0.5089680483836266\n",
      "18000/49000 loss: 0.5426798188811819\n",
      "20000/49000 loss: 0.41516907454611135\n",
      "22000/49000 loss: 0.4134766671828971\n",
      "24000/49000 loss: 0.49720062089967054\n",
      "26000/49000 loss: 0.38700650170238887\n",
      "28000/49000 loss: 0.5366362593478768\n",
      "30000/49000 loss: 0.47794606832246234\n",
      "32000/49000 loss: 0.5329232329785545\n",
      "34000/49000 loss: 0.5045408659282914\n",
      "36000/49000 loss: 0.49694096804229776\n",
      "38000/49000 loss: 0.39674117581986346\n",
      "40000/49000 loss: 0.4271395809133246\n",
      "42000/49000 loss: 0.3789483188765012\n",
      "44000/49000 loss: 0.4273625096910967\n",
      "46000/49000 loss: 0.4649259409889422\n",
      "48000/49000 loss: 0.42297774424335444\n",
      "epoch 3: valid acc = 0.833, new learning rate = 0.0004286875\n",
      "2000/49000 loss: 0.416106840539148\n",
      "4000/49000 loss: 0.4656213548874701\n",
      "6000/49000 loss: 0.5730648587932708\n",
      "8000/49000 loss: 0.51119732162931\n",
      "10000/49000 loss: 0.4309656782821369\n",
      "12000/49000 loss: 0.5650632668669623\n",
      "14000/49000 loss: 0.3983726671326796\n",
      "16000/49000 loss: 0.42711178445119713\n",
      "18000/49000 loss: 0.4446799501854809\n",
      "20000/49000 loss: 0.49344434471628323\n",
      "22000/49000 loss: 0.4349344644816526\n",
      "24000/49000 loss: 0.4799847985894475\n",
      "26000/49000 loss: 0.4211150087941713\n",
      "28000/49000 loss: 0.4402427530967833\n",
      "30000/49000 loss: 0.5262482396015518\n",
      "32000/49000 loss: 0.5014544179386036\n",
      "34000/49000 loss: 0.41534808428350317\n",
      "36000/49000 loss: 0.40360232468067\n",
      "38000/49000 loss: 0.4552778385831491\n",
      "40000/49000 loss: 0.5021974786101405\n",
      "42000/49000 loss: 0.36340749814883777\n",
      "44000/49000 loss: 0.4535727044026576\n",
      "46000/49000 loss: 0.3451013751571474\n",
      "48000/49000 loss: 0.3232286838616831\n",
      "epoch 4: valid acc = 0.852, new learning rate = 0.00040725312499999993\n",
      "2000/49000 loss: 0.4617206184532415\n",
      "4000/49000 loss: 0.4395573966643412\n",
      "6000/49000 loss: 0.4291631685754215\n",
      "8000/49000 loss: 0.4959029653375918\n",
      "10000/49000 loss: 0.3603240143787219\n",
      "12000/49000 loss: 0.37325571557910253\n",
      "14000/49000 loss: 0.527714235990518\n",
      "16000/49000 loss: 0.5652801765688364\n",
      "18000/49000 loss: 0.4052147581901014\n",
      "20000/49000 loss: 0.34448102579850864\n",
      "22000/49000 loss: 0.4117942058918602\n",
      "24000/49000 loss: 0.3498101338631389\n",
      "26000/49000 loss: 0.45201823994489526\n",
      "28000/49000 loss: 0.39836536890884\n",
      "30000/49000 loss: 0.35927790945480487\n",
      "32000/49000 loss: 0.5043792913883727\n",
      "34000/49000 loss: 0.46565291454609176\n",
      "36000/49000 loss: 0.3287022461644096\n",
      "38000/49000 loss: 0.39006873509301987\n",
      "40000/49000 loss: 0.4209923536712668\n",
      "42000/49000 loss: 0.3484156406640758\n",
      "44000/49000 loss: 0.31495165259924957\n",
      "46000/49000 loss: 0.4189957417484958\n",
      "48000/49000 loss: 0.29715915262164144\n",
      "epoch 5: valid acc = 0.845, new learning rate = 0.0003868904687499999\n",
      "2000/49000 loss: 0.3652700781638307\n",
      "4000/49000 loss: 0.4502485346238406\n",
      "6000/49000 loss: 0.403520585975258\n",
      "8000/49000 loss: 0.44160051819285273\n",
      "10000/49000 loss: 0.3999669519709134\n",
      "12000/49000 loss: 0.3618402474952407\n",
      "14000/49000 loss: 0.4264220520459477\n",
      "16000/49000 loss: 0.4728859107773016\n",
      "18000/49000 loss: 0.468498189420983\n",
      "20000/49000 loss: 0.3936600565638005\n",
      "22000/49000 loss: 0.3596922414558767\n",
      "24000/49000 loss: 0.44538936909661053\n",
      "26000/49000 loss: 0.4273590167042307\n",
      "28000/49000 loss: 0.34903786867347747\n",
      "30000/49000 loss: 0.37983105004395007\n",
      "32000/49000 loss: 0.32039997356733163\n",
      "34000/49000 loss: 0.3784286282047078\n",
      "36000/49000 loss: 0.47970860611272736\n",
      "38000/49000 loss: 0.39445490886186035\n",
      "40000/49000 loss: 0.3823070776426317\n",
      "42000/49000 loss: 0.4048653589613494\n",
      "44000/49000 loss: 0.40685049051971933\n",
      "46000/49000 loss: 0.36811064730588666\n",
      "48000/49000 loss: 0.41479213463131354\n",
      "epoch 6: valid acc = 0.862, new learning rate = 0.0003675459453124999\n",
      "2000/49000 loss: 0.3549237201559349\n",
      "4000/49000 loss: 0.4387194685909972\n",
      "6000/49000 loss: 0.465955951699928\n",
      "8000/49000 loss: 0.44598461744253337\n",
      "10000/49000 loss: 0.5321102907901087\n",
      "12000/49000 loss: 0.34723609983044923\n",
      "14000/49000 loss: 0.40316361059264644\n",
      "16000/49000 loss: 0.27844087458059585\n",
      "18000/49000 loss: 0.42436616958591794\n",
      "20000/49000 loss: 0.39483271089991917\n",
      "22000/49000 loss: 0.3956582166593997\n",
      "24000/49000 loss: 0.431260595605911\n",
      "26000/49000 loss: 0.44688533033741656\n",
      "28000/49000 loss: 0.3453790542565593\n",
      "30000/49000 loss: 0.35200431456625425\n",
      "32000/49000 loss: 0.3585594522054495\n",
      "34000/49000 loss: 0.43546154484732785\n",
      "36000/49000 loss: 0.31173424177255604\n",
      "38000/49000 loss: 0.34109535605036556\n",
      "40000/49000 loss: 0.3717557987741139\n",
      "42000/49000 loss: 0.38954248127639685\n",
      "44000/49000 loss: 0.34489169738777103\n",
      "46000/49000 loss: 0.32592935256552863\n",
      "48000/49000 loss: 0.3958864405124476\n",
      "epoch 7: valid acc = 0.857, new learning rate = 0.00034916864804687486\n",
      "2000/49000 loss: 0.352150277394808\n",
      "4000/49000 loss: 0.2668268527902575\n",
      "6000/49000 loss: 0.41402665626279644\n",
      "8000/49000 loss: 0.4625052010034238\n",
      "10000/49000 loss: 0.5228676491247903\n",
      "12000/49000 loss: 0.3178483052280261\n",
      "14000/49000 loss: 0.37099032880403543\n",
      "16000/49000 loss: 0.37864227679799484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/49000 loss: 0.39135025133987533\n",
      "20000/49000 loss: 0.4647444859588126\n",
      "22000/49000 loss: 0.4650699613195676\n",
      "24000/49000 loss: 0.4313218362529056\n",
      "26000/49000 loss: 0.44700596492602457\n",
      "28000/49000 loss: 0.4138931297745783\n",
      "30000/49000 loss: 0.2878344352555859\n",
      "32000/49000 loss: 0.32494806947510724\n",
      "34000/49000 loss: 0.40178167822231303\n",
      "36000/49000 loss: 0.361599507904367\n",
      "38000/49000 loss: 0.30841738072343905\n",
      "40000/49000 loss: 0.36471058384446897\n",
      "42000/49000 loss: 0.37576203516648743\n",
      "44000/49000 loss: 0.2783872754317453\n",
      "46000/49000 loss: 0.36626418558755064\n",
      "48000/49000 loss: 0.3977073446688548\n",
      "epoch 8: valid acc = 0.865, new learning rate = 0.0003317102156445311\n",
      "2000/49000 loss: 0.38239259365792005\n",
      "4000/49000 loss: 0.428850676487532\n",
      "6000/49000 loss: 0.4340392870999815\n",
      "8000/49000 loss: 0.3406858872697701\n",
      "10000/49000 loss: 0.33690857137249064\n",
      "12000/49000 loss: 0.2827837011667934\n",
      "14000/49000 loss: 0.3881311293285144\n",
      "16000/49000 loss: 0.44643349624344386\n",
      "18000/49000 loss: 0.3624987779077641\n",
      "20000/49000 loss: 0.34786897104983455\n",
      "22000/49000 loss: 0.33728044183046163\n",
      "24000/49000 loss: 0.3491453035415995\n",
      "26000/49000 loss: 0.36160473983211566\n",
      "28000/49000 loss: 0.32952787234110775\n",
      "30000/49000 loss: 0.4244289507398648\n",
      "32000/49000 loss: 0.353835510970335\n",
      "34000/49000 loss: 0.3613151207520371\n",
      "36000/49000 loss: 0.31727020829018476\n",
      "38000/49000 loss: 0.3287416696938158\n",
      "40000/49000 loss: 0.30091347874585067\n",
      "42000/49000 loss: 0.3716087376173537\n",
      "44000/49000 loss: 0.31553943726735356\n",
      "46000/49000 loss: 0.3809945492740589\n",
      "48000/49000 loss: 0.3119994806864893\n",
      "epoch 9: valid acc = 0.872, new learning rate = 0.0003151247048623045\n",
      "2000/49000 loss: 0.32602247627672\n",
      "4000/49000 loss: 0.3359365400940894\n",
      "6000/49000 loss: 0.34158738003829103\n",
      "8000/49000 loss: 0.4413126023370912\n",
      "10000/49000 loss: 0.30061563617875176\n",
      "12000/49000 loss: 0.2504285428498283\n",
      "14000/49000 loss: 0.43535947157816085\n",
      "16000/49000 loss: 0.31964124637039115\n",
      "18000/49000 loss: 0.4307498813579733\n",
      "20000/49000 loss: 0.4006121850940005\n",
      "22000/49000 loss: 0.3021137446971335\n",
      "24000/49000 loss: 0.3336902193440436\n",
      "26000/49000 loss: 0.40239916460672126\n",
      "28000/49000 loss: 0.37496693835365824\n",
      "30000/49000 loss: 0.3294177002847652\n",
      "32000/49000 loss: 0.3756718435069939\n",
      "34000/49000 loss: 0.3720193769319082\n",
      "36000/49000 loss: 0.30162335488698927\n",
      "38000/49000 loss: 0.39085229082267997\n",
      "40000/49000 loss: 0.36339855671869153\n",
      "42000/49000 loss: 0.2790288605151789\n",
      "44000/49000 loss: 0.33453696460007154\n",
      "46000/49000 loss: 0.35491681849119616\n",
      "48000/49000 loss: 0.3967454807570677\n",
      "epoch 10: valid acc = 0.873, new learning rate = 0.00029936846961918924\n",
      "2000/49000 loss: 0.3720565400684091\n",
      "4000/49000 loss: 0.3768398675877479\n",
      "6000/49000 loss: 0.31137299311725225\n",
      "8000/49000 loss: 0.3159758958872271\n",
      "10000/49000 loss: 0.34142000705009595\n",
      "12000/49000 loss: 0.38486724912904713\n",
      "14000/49000 loss: 0.3434615684656252\n",
      "16000/49000 loss: 0.4204344688671964\n",
      "18000/49000 loss: 0.5741092465850894\n",
      "20000/49000 loss: 0.35957216671382247\n",
      "22000/49000 loss: 0.36760756726673566\n",
      "24000/49000 loss: 0.37803888779532824\n",
      "26000/49000 loss: 0.3947909876592142\n",
      "28000/49000 loss: 0.27463428999840744\n",
      "30000/49000 loss: 0.3364448023605995\n",
      "32000/49000 loss: 0.3828750072323593\n",
      "34000/49000 loss: 0.4158592466559706\n",
      "36000/49000 loss: 0.3457525104208126\n",
      "38000/49000 loss: 0.3936588843859834\n",
      "40000/49000 loss: 0.3005841028750789\n",
      "42000/49000 loss: 0.38202448364213737\n",
      "44000/49000 loss: 0.3220069291957679\n",
      "46000/49000 loss: 0.3110455031015225\n",
      "48000/49000 loss: 0.3783108701739334\n",
      "epoch 11: valid acc = 0.871, new learning rate = 0.00028440004613822977\n",
      "2000/49000 loss: 0.2613578802978886\n",
      "4000/49000 loss: 0.33949892469223286\n",
      "6000/49000 loss: 0.28527841651492425\n",
      "8000/49000 loss: 0.3421555966038003\n",
      "10000/49000 loss: 0.28193365266266307\n",
      "12000/49000 loss: 0.24583754000329291\n",
      "14000/49000 loss: 0.30768699670305605\n",
      "16000/49000 loss: 0.42029390591546195\n",
      "18000/49000 loss: 0.38321020834032066\n",
      "20000/49000 loss: 0.402112567958732\n",
      "22000/49000 loss: 0.3716366851821651\n",
      "24000/49000 loss: 0.3291518266014859\n",
      "26000/49000 loss: 0.2962107178702274\n",
      "28000/49000 loss: 0.27078074030957294\n",
      "30000/49000 loss: 0.28832379476245656\n",
      "32000/49000 loss: 0.33341675115744074\n",
      "34000/49000 loss: 0.2640946683478672\n",
      "36000/49000 loss: 0.3657189232624791\n",
      "38000/49000 loss: 0.3039733903119811\n",
      "40000/49000 loss: 0.3871735456415446\n",
      "42000/49000 loss: 0.2748578620476325\n",
      "44000/49000 loss: 0.3558385407543489\n",
      "46000/49000 loss: 0.3019629456413645\n",
      "48000/49000 loss: 0.406236671009933\n",
      "epoch 12: valid acc = 0.877, new learning rate = 0.00027018004383131826\n",
      "2000/49000 loss: 0.3135563545849349\n",
      "4000/49000 loss: 0.290871947592117\n",
      "6000/49000 loss: 0.4719387824981486\n",
      "8000/49000 loss: 0.398160556353573\n",
      "10000/49000 loss: 0.38610408010788355\n",
      "12000/49000 loss: 0.3128386209260102\n",
      "14000/49000 loss: 0.3786926448557356\n",
      "16000/49000 loss: 0.3150547656215504\n",
      "18000/49000 loss: 0.3995759953574411\n",
      "20000/49000 loss: 0.28324809175361687\n",
      "22000/49000 loss: 0.2881039459797454\n",
      "24000/49000 loss: 0.3096022057242735\n",
      "26000/49000 loss: 0.2987853154941125\n",
      "28000/49000 loss: 0.3210381640717203\n",
      "30000/49000 loss: 0.40283981882285136\n",
      "32000/49000 loss: 0.29166575952868773\n",
      "34000/49000 loss: 0.37002647024924745\n",
      "36000/49000 loss: 0.42949135196160393\n",
      "38000/49000 loss: 0.2997880016256018\n",
      "40000/49000 loss: 0.36676276257286344\n",
      "42000/49000 loss: 0.2938613175152657\n",
      "44000/49000 loss: 0.4624528187987573\n",
      "46000/49000 loss: 0.3462405608884655\n",
      "48000/49000 loss: 0.34183883132010096\n",
      "epoch 13: valid acc = 0.879, new learning rate = 0.00025667104163975234\n",
      "2000/49000 loss: 0.3727470905047382\n",
      "4000/49000 loss: 0.3549930864850576\n",
      "6000/49000 loss: 0.28403346992280526\n",
      "8000/49000 loss: 0.3799042023359026\n",
      "10000/49000 loss: 0.2768147876518132\n",
      "12000/49000 loss: 0.2954942923369001\n",
      "14000/49000 loss: 0.3046143333601082\n",
      "16000/49000 loss: 0.3368675142487266\n",
      "18000/49000 loss: 0.30540228772473244\n",
      "20000/49000 loss: 0.3363611030598622\n",
      "22000/49000 loss: 0.29641172929793547\n",
      "24000/49000 loss: 0.3093553146635434\n",
      "26000/49000 loss: 0.3480813806537496\n",
      "28000/49000 loss: 0.3776568780513416\n",
      "30000/49000 loss: 0.3531676516147343\n",
      "32000/49000 loss: 0.3166882760904581\n",
      "34000/49000 loss: 0.3249987377649114\n",
      "36000/49000 loss: 0.42153916309786016\n",
      "38000/49000 loss: 0.3142514323992094\n",
      "40000/49000 loss: 0.2245877905239048\n",
      "42000/49000 loss: 0.3025786767991325\n",
      "44000/49000 loss: 0.40755766250327913\n",
      "46000/49000 loss: 0.3691540983017848\n",
      "48000/49000 loss: 0.2567009115212868\n",
      "epoch 14: valid acc = 0.869, new learning rate = 0.00024383748955776472\n",
      "2000/49000 loss: 0.3604889431297657\n",
      "4000/49000 loss: 0.30663267835341745\n",
      "6000/49000 loss: 0.2711447520197889\n",
      "8000/49000 loss: 0.3468560505049441\n",
      "10000/49000 loss: 0.31712604476943557\n",
      "12000/49000 loss: 0.33683738761693394\n",
      "14000/49000 loss: 0.3417978146126536\n",
      "16000/49000 loss: 0.3602374035778874\n",
      "18000/49000 loss: 0.32277490133024866\n",
      "20000/49000 loss: 0.3318273580260219\n",
      "22000/49000 loss: 0.42566478983709366\n",
      "24000/49000 loss: 0.290682683732258\n",
      "26000/49000 loss: 0.24623349546447787\n",
      "28000/49000 loss: 0.32055645877079025\n",
      "30000/49000 loss: 0.3516135752853472\n",
      "32000/49000 loss: 0.37036840963454754\n",
      "34000/49000 loss: 0.3821586606204041\n",
      "36000/49000 loss: 0.4137250608053469\n",
      "38000/49000 loss: 0.4406604615070877\n",
      "40000/49000 loss: 0.3018854663098257\n",
      "42000/49000 loss: 0.3417438327896186\n",
      "44000/49000 loss: 0.41104548252712486\n",
      "46000/49000 loss: 0.35542197539766285\n",
      "48000/49000 loss: 0.33194922412827393\n",
      "epoch 15: valid acc = 0.876, new learning rate = 0.00023164561507987649\n",
      "2000/49000 loss: 0.40677410629845734\n",
      "4000/49000 loss: 0.4550804246500235\n",
      "6000/49000 loss: 0.30386130163438974\n",
      "8000/49000 loss: 0.31924066754822383\n",
      "10000/49000 loss: 0.34131695907514603\n",
      "12000/49000 loss: 0.252506594534896\n",
      "14000/49000 loss: 0.2974988652740651\n",
      "16000/49000 loss: 0.3803971363194956\n",
      "18000/49000 loss: 0.35795345972167214\n",
      "20000/49000 loss: 0.4039572013139431\n",
      "22000/49000 loss: 0.31894557831950066\n",
      "24000/49000 loss: 0.38149248915879147\n",
      "26000/49000 loss: 0.22517161550111203\n",
      "28000/49000 loss: 0.36860221074394134\n",
      "30000/49000 loss: 0.3519957441606643\n",
      "32000/49000 loss: 0.36350179419556167\n",
      "34000/49000 loss: 0.3092067251250012\n",
      "36000/49000 loss: 0.2995373239501214\n",
      "38000/49000 loss: 0.21490612655775448\n",
      "40000/49000 loss: 0.3657672837307869\n",
      "42000/49000 loss: 0.3155009020582035\n",
      "44000/49000 loss: 0.365048911072046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46000/49000 loss: 0.3500576603321367\n",
      "48000/49000 loss: 0.3431577128475577\n",
      "epoch 16: valid acc = 0.879, new learning rate = 0.00022006333432588265\n",
      "2000/49000 loss: 0.3673125886941419\n",
      "4000/49000 loss: 0.3919058258605611\n",
      "6000/49000 loss: 0.32759099558052346\n",
      "8000/49000 loss: 0.4017317099370224\n",
      "10000/49000 loss: 0.348180384853721\n",
      "12000/49000 loss: 0.27731153270959336\n",
      "14000/49000 loss: 0.31690225824665375\n",
      "16000/49000 loss: 0.2490730421352754\n",
      "18000/49000 loss: 0.2790401178685162\n",
      "20000/49000 loss: 0.33152390681888777\n",
      "22000/49000 loss: 0.3147398103944268\n",
      "24000/49000 loss: 0.3461112266379509\n",
      "26000/49000 loss: 0.2251042672354043\n",
      "28000/49000 loss: 0.23922311974076482\n",
      "30000/49000 loss: 0.2760034587665526\n",
      "32000/49000 loss: 0.3237467866512232\n",
      "34000/49000 loss: 0.2949755924305738\n",
      "36000/49000 loss: 0.2555573312982758\n",
      "38000/49000 loss: 0.40154392672322853\n",
      "40000/49000 loss: 0.27473539114286644\n",
      "42000/49000 loss: 0.3423745205000637\n",
      "44000/49000 loss: 0.33958062932298455\n",
      "46000/49000 loss: 0.26008957270376615\n",
      "48000/49000 loss: 0.40862635176851614\n",
      "epoch 17: valid acc = 0.882, new learning rate = 0.00020906016760958852\n",
      "2000/49000 loss: 0.3055984620307154\n",
      "4000/49000 loss: 0.3567147744681649\n",
      "6000/49000 loss: 0.2844161633222413\n",
      "8000/49000 loss: 0.28071615354599605\n",
      "10000/49000 loss: 0.31484456635338637\n",
      "12000/49000 loss: 0.31089426131162134\n",
      "14000/49000 loss: 0.4135419854735004\n",
      "16000/49000 loss: 0.336247762658936\n",
      "18000/49000 loss: 0.29301940460060766\n",
      "20000/49000 loss: 0.4045681004715612\n",
      "22000/49000 loss: 0.2893643563261053\n",
      "24000/49000 loss: 0.379229164373131\n",
      "26000/49000 loss: 0.4136164343941938\n",
      "28000/49000 loss: 0.3341452096588105\n",
      "30000/49000 loss: 0.3441187089625837\n",
      "32000/49000 loss: 0.3500297925011933\n",
      "34000/49000 loss: 0.2973513618536313\n",
      "36000/49000 loss: 0.4367518447896828\n",
      "38000/49000 loss: 0.4312916207380689\n",
      "40000/49000 loss: 0.3007555909353835\n",
      "42000/49000 loss: 0.3173247886112765\n",
      "44000/49000 loss: 0.2833008532374938\n",
      "46000/49000 loss: 0.3662802117380437\n",
      "48000/49000 loss: 0.3750621650952742\n",
      "epoch 18: valid acc = 0.881, new learning rate = 0.00019860715922910907\n",
      "2000/49000 loss: 0.3464155860039591\n",
      "4000/49000 loss: 0.2619525006580068\n",
      "6000/49000 loss: 0.3159003293295311\n",
      "8000/49000 loss: 0.2692714358375112\n",
      "10000/49000 loss: 0.26479413814134234\n",
      "12000/49000 loss: 0.27236626908295236\n",
      "14000/49000 loss: 0.3344049784995034\n",
      "16000/49000 loss: 0.2702987271016048\n",
      "18000/49000 loss: 0.28337810413991\n",
      "20000/49000 loss: 0.35254152470616523\n",
      "22000/49000 loss: 0.2843819996784881\n",
      "24000/49000 loss: 0.33108376592287037\n",
      "26000/49000 loss: 0.3297320900897344\n",
      "28000/49000 loss: 0.2762923371680358\n",
      "30000/49000 loss: 0.29417383066106717\n",
      "32000/49000 loss: 0.2975701502788481\n",
      "34000/49000 loss: 0.34632001492969167\n",
      "36000/49000 loss: 0.36665121984274907\n",
      "38000/49000 loss: 0.2280346746859954\n",
      "40000/49000 loss: 0.2952154253095101\n",
      "42000/49000 loss: 0.2578627213090481\n",
      "44000/49000 loss: 0.36034807619507997\n",
      "46000/49000 loss: 0.2537875259015644\n",
      "48000/49000 loss: 0.30653336813635707\n",
      "epoch 19: valid acc = 0.886, new learning rate = 0.0001886768012676536\n",
      "2000/49000 loss: 0.2823853416919005\n",
      "4000/49000 loss: 0.2241166014252438\n",
      "6000/49000 loss: 0.25213304714952484\n",
      "8000/49000 loss: 0.28729620746394086\n",
      "10000/49000 loss: 0.31994726607734536\n",
      "12000/49000 loss: 0.40495416807114104\n",
      "14000/49000 loss: 0.29427313820978646\n",
      "16000/49000 loss: 0.2983404329176617\n",
      "18000/49000 loss: 0.3147277353105629\n",
      "20000/49000 loss: 0.2672388531853802\n",
      "22000/49000 loss: 0.3183387045719944\n",
      "24000/49000 loss: 0.27062548237341544\n",
      "26000/49000 loss: 0.35148423630383774\n",
      "28000/49000 loss: 0.28916726350081623\n",
      "30000/49000 loss: 0.28481399601625446\n",
      "32000/49000 loss: 0.2638830056769413\n",
      "34000/49000 loss: 0.2874428874393881\n",
      "36000/49000 loss: 0.3101018472255241\n",
      "38000/49000 loss: 0.28559265155044905\n",
      "40000/49000 loss: 0.3111823522632936\n",
      "42000/49000 loss: 0.2921915808427645\n",
      "44000/49000 loss: 0.3820493499333296\n",
      "46000/49000 loss: 0.3485133476580639\n",
      "48000/49000 loss: 0.35354710396326416\n",
      "epoch 20: valid acc = 0.883, new learning rate = 0.0001792429612042709\n",
      "2000/49000 loss: 0.296006220077221\n",
      "4000/49000 loss: 0.38396042246550915\n",
      "6000/49000 loss: 0.29941094152324843\n",
      "8000/49000 loss: 0.30711573777433326\n",
      "10000/49000 loss: 0.2758158811721272\n",
      "12000/49000 loss: 0.32696129418011727\n",
      "14000/49000 loss: 0.31239037985605195\n",
      "16000/49000 loss: 0.40709900018219825\n",
      "18000/49000 loss: 0.23585527525606428\n",
      "20000/49000 loss: 0.36403612903059035\n",
      "22000/49000 loss: 0.20824843562760348\n",
      "24000/49000 loss: 0.2593062027480047\n",
      "26000/49000 loss: 0.3845634991313883\n",
      "28000/49000 loss: 0.3150604341828973\n",
      "30000/49000 loss: 0.4248469428132967\n",
      "32000/49000 loss: 0.31209116180252383\n",
      "34000/49000 loss: 0.3387772532672635\n",
      "36000/49000 loss: 0.29084703081706553\n",
      "38000/49000 loss: 0.3797643680255542\n",
      "40000/49000 loss: 0.29399501938331846\n",
      "42000/49000 loss: 0.35133728550588766\n",
      "44000/49000 loss: 0.24049032155234049\n",
      "46000/49000 loss: 0.3517832289157603\n",
      "48000/49000 loss: 0.3020912385762671\n",
      "epoch 21: valid acc = 0.887, new learning rate = 0.00017028081314405735\n",
      "2000/49000 loss: 0.2921130253564983\n",
      "4000/49000 loss: 0.20166634956879304\n",
      "6000/49000 loss: 0.4797046084610104\n",
      "8000/49000 loss: 0.2729301401612769\n",
      "10000/49000 loss: 0.44517244214383295\n",
      "12000/49000 loss: 0.352844342040879\n",
      "14000/49000 loss: 0.319353830123535\n",
      "16000/49000 loss: 0.3221876539921161\n",
      "18000/49000 loss: 0.29128920706236194\n",
      "20000/49000 loss: 0.22735274583743112\n",
      "22000/49000 loss: 0.2948607431783557\n",
      "24000/49000 loss: 0.34032924230596234\n",
      "26000/49000 loss: 0.3818424339280699\n",
      "28000/49000 loss: 0.29626945624354073\n",
      "30000/49000 loss: 0.3094378487484225\n",
      "32000/49000 loss: 0.278018224230212\n",
      "34000/49000 loss: 0.2624420798356513\n",
      "36000/49000 loss: 0.2589868056529811\n",
      "38000/49000 loss: 0.27250746466277576\n",
      "40000/49000 loss: 0.31426429826377567\n",
      "42000/49000 loss: 0.37851687265542583\n",
      "44000/49000 loss: 0.2570821705779926\n",
      "46000/49000 loss: 0.2899844951933774\n",
      "48000/49000 loss: 0.30496771766011277\n",
      "epoch 22: valid acc = 0.891, new learning rate = 0.00016176677248685447\n",
      "2000/49000 loss: 0.2740484854017192\n",
      "4000/49000 loss: 0.25119246039784154\n",
      "6000/49000 loss: 0.2692729226095629\n",
      "8000/49000 loss: 0.21454305845235183\n",
      "10000/49000 loss: 0.27578264756834764\n",
      "12000/49000 loss: 0.3162946170675452\n",
      "14000/49000 loss: 0.34509480339643145\n",
      "16000/49000 loss: 0.27225818388274264\n",
      "18000/49000 loss: 0.34858337995827815\n",
      "20000/49000 loss: 0.2888380149409064\n",
      "22000/49000 loss: 0.2320463467283111\n",
      "24000/49000 loss: 0.3214021403269364\n",
      "26000/49000 loss: 0.28385266757445143\n",
      "28000/49000 loss: 0.3065950825060667\n",
      "30000/49000 loss: 0.3080839454429247\n",
      "32000/49000 loss: 0.336394918624543\n",
      "34000/49000 loss: 0.26942302962077597\n",
      "36000/49000 loss: 0.2982213374677571\n",
      "38000/49000 loss: 0.35367956402295847\n",
      "40000/49000 loss: 0.2777011045400771\n",
      "42000/49000 loss: 0.2924273093085851\n",
      "44000/49000 loss: 0.2210794278504814\n",
      "46000/49000 loss: 0.2728182069707917\n",
      "48000/49000 loss: 0.3380246765017506\n",
      "epoch 23: valid acc = 0.889, new learning rate = 0.00015367843386251173\n",
      "2000/49000 loss: 0.28798200943992636\n",
      "4000/49000 loss: 0.29248565198776033\n",
      "6000/49000 loss: 0.2807811475784046\n",
      "8000/49000 loss: 0.37587709158611704\n",
      "10000/49000 loss: 0.3515262947755266\n",
      "12000/49000 loss: 0.29085777594559387\n",
      "14000/49000 loss: 0.2713917888967398\n",
      "16000/49000 loss: 0.2147064873605745\n",
      "18000/49000 loss: 0.36617177979000887\n",
      "20000/49000 loss: 0.25880285547749865\n",
      "22000/49000 loss: 0.30521394301478694\n",
      "24000/49000 loss: 0.21088471205369083\n",
      "26000/49000 loss: 0.3218421524079876\n",
      "28000/49000 loss: 0.24636129651442043\n",
      "30000/49000 loss: 0.35999747218774997\n",
      "32000/49000 loss: 0.36861882366377274\n",
      "34000/49000 loss: 0.28550971654605956\n",
      "36000/49000 loss: 0.25446980083236026\n",
      "38000/49000 loss: 0.3014165591524651\n",
      "40000/49000 loss: 0.28991816249893115\n",
      "42000/49000 loss: 0.3060787862747362\n",
      "44000/49000 loss: 0.21288152453048315\n",
      "46000/49000 loss: 0.38701001667777063\n",
      "48000/49000 loss: 0.2710845629441652\n",
      "epoch 24: valid acc = 0.889, new learning rate = 0.00014599451216938612\n",
      "2000/49000 loss: 0.3290312712731279\n",
      "4000/49000 loss: 0.2019595391880627\n",
      "6000/49000 loss: 0.2329958459944131\n",
      "8000/49000 loss: 0.2832705210085751\n",
      "10000/49000 loss: 0.2787864806680368\n",
      "12000/49000 loss: 0.279416221550573\n",
      "14000/49000 loss: 0.2949828575042918\n",
      "16000/49000 loss: 0.19975459269635892\n",
      "18000/49000 loss: 0.3256673310553671\n",
      "20000/49000 loss: 0.261462366228072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22000/49000 loss: 0.4008639946139169\n",
      "24000/49000 loss: 0.34082089549629474\n",
      "26000/49000 loss: 0.3447102642059971\n",
      "28000/49000 loss: 0.31457048714277164\n",
      "30000/49000 loss: 0.2431395328509038\n",
      "32000/49000 loss: 0.24364325865733205\n",
      "34000/49000 loss: 0.3330323966046916\n",
      "36000/49000 loss: 0.27524359603990195\n",
      "38000/49000 loss: 0.24618609807387248\n",
      "40000/49000 loss: 0.2705232511458825\n",
      "42000/49000 loss: 0.3271010010509452\n",
      "44000/49000 loss: 0.3167252043546412\n",
      "46000/49000 loss: 0.3065663292655367\n",
      "48000/49000 loss: 0.3051991096964169\n",
      "epoch 25: valid acc = 0.889, new learning rate = 0.00013869478656091682\n",
      "2000/49000 loss: 0.2910314752561375\n",
      "4000/49000 loss: 0.31078826134097626\n",
      "6000/49000 loss: 0.29935014544421934\n",
      "8000/49000 loss: 0.287220024794361\n",
      "10000/49000 loss: 0.29232478043254073\n",
      "12000/49000 loss: 0.28180568877994466\n",
      "14000/49000 loss: 0.34164288348521227\n",
      "16000/49000 loss: 0.27741729088755446\n",
      "18000/49000 loss: 0.29505486741795295\n",
      "20000/49000 loss: 0.4345946326931486\n",
      "22000/49000 loss: 0.24784396682704565\n",
      "24000/49000 loss: 0.2294260404433869\n",
      "26000/49000 loss: 0.3021401850691177\n",
      "28000/49000 loss: 0.3031362664175651\n",
      "30000/49000 loss: 0.2041735739786489\n",
      "32000/49000 loss: 0.2249983949800929\n",
      "34000/49000 loss: 0.32003140496161125\n",
      "36000/49000 loss: 0.25580953921003785\n",
      "38000/49000 loss: 0.362957387370509\n",
      "40000/49000 loss: 0.36697067494127883\n",
      "42000/49000 loss: 0.3244649343669168\n",
      "44000/49000 loss: 0.33854299543996114\n",
      "46000/49000 loss: 0.2886034437094108\n",
      "48000/49000 loss: 0.1919371491295868\n",
      "epoch 26: valid acc = 0.89, new learning rate = 0.00013176004723287096\n",
      "2000/49000 loss: 0.3241861670557456\n",
      "4000/49000 loss: 0.22280895014812355\n",
      "6000/49000 loss: 0.39663908838648954\n",
      "8000/49000 loss: 0.3152106690475306\n",
      "10000/49000 loss: 0.3380413350009656\n",
      "12000/49000 loss: 0.2436653044608013\n",
      "14000/49000 loss: 0.30214292171662327\n",
      "16000/49000 loss: 0.34844238513819337\n",
      "18000/49000 loss: 0.2550564915968931\n",
      "20000/49000 loss: 0.20102724119427534\n",
      "22000/49000 loss: 0.2518322259277377\n",
      "24000/49000 loss: 0.2957119611678168\n",
      "26000/49000 loss: 0.30402091731963277\n",
      "28000/49000 loss: 0.3225879476583951\n",
      "30000/49000 loss: 0.2970841297671549\n",
      "32000/49000 loss: 0.3995057736021239\n",
      "34000/49000 loss: 0.3310651301480748\n",
      "36000/49000 loss: 0.3193349360391016\n",
      "38000/49000 loss: 0.352794942547186\n",
      "40000/49000 loss: 0.3416436686790827\n",
      "42000/49000 loss: 0.24540433336487316\n",
      "44000/49000 loss: 0.355899563238726\n",
      "46000/49000 loss: 0.3119344671542384\n",
      "48000/49000 loss: 0.2529924208844317\n",
      "epoch 27: valid acc = 0.889, new learning rate = 0.0001251720448712274\n",
      "2000/49000 loss: 0.2031273816826668\n",
      "4000/49000 loss: 0.24695591104529455\n",
      "6000/49000 loss: 0.2789274168985805\n",
      "8000/49000 loss: 0.3345109290357742\n",
      "10000/49000 loss: 0.3254114068619547\n",
      "12000/49000 loss: 0.37048763353423814\n",
      "14000/49000 loss: 0.29664168516481804\n",
      "16000/49000 loss: 0.32130419808837296\n",
      "18000/49000 loss: 0.2902728176315124\n",
      "20000/49000 loss: 0.3046564504182134\n",
      "22000/49000 loss: 0.259691449659298\n",
      "24000/49000 loss: 0.28591511625596294\n",
      "26000/49000 loss: 0.19925650306205075\n",
      "28000/49000 loss: 0.2565196052325386\n",
      "30000/49000 loss: 0.2814037516029482\n",
      "32000/49000 loss: 0.3797235554689482\n",
      "34000/49000 loss: 0.3181511211523422\n",
      "36000/49000 loss: 0.2976471322787969\n",
      "38000/49000 loss: 0.27877785837711516\n",
      "40000/49000 loss: 0.2633565229589902\n",
      "42000/49000 loss: 0.29294630941463773\n",
      "44000/49000 loss: 0.3155417396483609\n",
      "46000/49000 loss: 0.24892540091218202\n",
      "48000/49000 loss: 0.2998146790407654\n",
      "epoch 28: valid acc = 0.896, new learning rate = 0.00011891344262766602\n",
      "2000/49000 loss: 0.2872719226422938\n",
      "4000/49000 loss: 0.3180640597134645\n",
      "6000/49000 loss: 0.20904445954642947\n",
      "8000/49000 loss: 0.2721885233764144\n",
      "10000/49000 loss: 0.24902792818626274\n",
      "12000/49000 loss: 0.2667672905288209\n",
      "14000/49000 loss: 0.2792168170148576\n",
      "16000/49000 loss: 0.3232669725663703\n",
      "18000/49000 loss: 0.3294956971636769\n",
      "20000/49000 loss: 0.30296492482303844\n",
      "22000/49000 loss: 0.24756135669249502\n",
      "24000/49000 loss: 0.3396891158162146\n",
      "26000/49000 loss: 0.349188599972961\n",
      "28000/49000 loss: 0.1889532475087661\n",
      "30000/49000 loss: 0.2812135046736632\n",
      "32000/49000 loss: 0.26105070207036635\n",
      "34000/49000 loss: 0.29450898958502825\n",
      "36000/49000 loss: 0.25820838321503153\n",
      "38000/49000 loss: 0.2654373015394072\n",
      "40000/49000 loss: 0.2673030027662398\n",
      "42000/49000 loss: 0.282797138144439\n",
      "44000/49000 loss: 0.3840459949901981\n",
      "46000/49000 loss: 0.2760647892341687\n",
      "48000/49000 loss: 0.29968705637850424\n",
      "epoch 29: valid acc = 0.895, new learning rate = 0.00011296777049628272\n",
      "2000/49000 loss: 0.37550695192679007\n",
      "4000/49000 loss: 0.24845549513297313\n",
      "6000/49000 loss: 0.3238780527533126\n",
      "8000/49000 loss: 0.29307794799532155\n",
      "10000/49000 loss: 0.28446365440296806\n",
      "12000/49000 loss: 0.3652170977171213\n",
      "14000/49000 loss: 0.31399790885214657\n",
      "16000/49000 loss: 0.4014049398889088\n",
      "18000/49000 loss: 0.3568821146158231\n",
      "20000/49000 loss: 0.30944902958897885\n",
      "22000/49000 loss: 0.26338531441174795\n",
      "24000/49000 loss: 0.23141754009111618\n",
      "26000/49000 loss: 0.34398584666057536\n",
      "28000/49000 loss: 0.3204307770436732\n",
      "30000/49000 loss: 0.38880540767139543\n",
      "32000/49000 loss: 0.334885763374256\n",
      "34000/49000 loss: 0.3162284770622442\n",
      "36000/49000 loss: 0.2950059790851679\n",
      "38000/49000 loss: 0.3169727985792293\n",
      "40000/49000 loss: 0.32503340832992367\n",
      "42000/49000 loss: 0.33300504332112774\n",
      "44000/49000 loss: 0.23136307212270507\n",
      "46000/49000 loss: 0.23587030779019819\n",
      "48000/49000 loss: 0.3231090679277035\n",
      "epoch 30: valid acc = 0.891, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.8971224489795918\n",
      "test acc: 0.891\n",
      "test acc: 0.8702\n",
      "number of batches for training: 245\n",
      "epoch 1: valid acc = 0.732, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.8, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.831, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.837, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.85, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.856, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.861, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.866, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.868, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.876, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.875, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.875, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.881, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.886, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.884, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.88, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.88, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.884, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.882, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.888, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.888, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.88, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.89, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.886, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.883, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.884, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.891, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.886, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.88, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.883, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.8971632653061224\n",
      "test acc: 0.883\n",
      "test acc: 0.8682\n",
      "number of batches for training: 245\n",
      "2000/49000 loss: 2.695754043853\n",
      "4000/49000 loss: 2.656392495763939\n",
      "6000/49000 loss: 2.732056294087839\n",
      "8000/49000 loss: 2.1706402711333035\n",
      "10000/49000 loss: 2.2005736488406145\n",
      "12000/49000 loss: 2.0348702558540035\n",
      "14000/49000 loss: 2.2369473859126234\n",
      "16000/49000 loss: 1.4354825731588363\n",
      "18000/49000 loss: 1.3369171205126587\n",
      "20000/49000 loss: 1.2743598844497144\n",
      "22000/49000 loss: 1.235792996727912\n",
      "24000/49000 loss: 1.2887206608545578\n",
      "26000/49000 loss: 1.0666637787245754\n",
      "28000/49000 loss: 1.176242718898537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000/49000 loss: 1.073546459329717\n",
      "32000/49000 loss: 1.078109847518841\n",
      "34000/49000 loss: 0.8935013662602777\n",
      "36000/49000 loss: 0.8895408965613394\n",
      "38000/49000 loss: 0.8390834332807646\n",
      "40000/49000 loss: 0.8432687812773118\n",
      "42000/49000 loss: 0.7560034429450413\n",
      "44000/49000 loss: 0.614414903722234\n",
      "46000/49000 loss: 0.8071710311592072\n",
      "48000/49000 loss: 0.7649496338756386\n",
      "epoch 1: valid acc = 0.742, new learning rate = 0.000475\n",
      "2000/49000 loss: 0.757864877945659\n",
      "4000/49000 loss: 0.6894466261397367\n",
      "6000/49000 loss: 0.6280618126123712\n",
      "8000/49000 loss: 0.6509296481717494\n",
      "10000/49000 loss: 0.6177933083013867\n",
      "12000/49000 loss: 0.561386977881618\n",
      "14000/49000 loss: 0.573350394145264\n",
      "16000/49000 loss: 0.5450950505153341\n",
      "18000/49000 loss: 0.650599227877048\n",
      "20000/49000 loss: 0.6489636377409271\n",
      "22000/49000 loss: 0.5957481331747403\n",
      "24000/49000 loss: 0.6568460585428592\n",
      "26000/49000 loss: 0.5384219034167613\n",
      "28000/49000 loss: 0.5758978859411985\n",
      "30000/49000 loss: 0.5987732939058207\n",
      "32000/49000 loss: 0.6103052746892662\n",
      "34000/49000 loss: 0.69686912294162\n",
      "36000/49000 loss: 0.7310587670086734\n",
      "38000/49000 loss: 0.5614522828049315\n",
      "40000/49000 loss: 0.5296177148207778\n",
      "42000/49000 loss: 0.5141536747337024\n",
      "44000/49000 loss: 0.5954604456430441\n",
      "46000/49000 loss: 0.5638326657049461\n",
      "48000/49000 loss: 0.4645594654513777\n",
      "epoch 2: valid acc = 0.807, new learning rate = 0.00045125\n",
      "2000/49000 loss: 0.3776960709794172\n",
      "4000/49000 loss: 0.4731679639136333\n",
      "6000/49000 loss: 0.530721432766927\n",
      "8000/49000 loss: 0.47504350608075724\n",
      "10000/49000 loss: 0.4566303592038536\n",
      "12000/49000 loss: 0.5832544715578786\n",
      "14000/49000 loss: 0.4228506933095266\n",
      "16000/49000 loss: 0.4458320487901349\n",
      "18000/49000 loss: 0.46437291037071493\n",
      "20000/49000 loss: 0.48495022934033405\n",
      "22000/49000 loss: 0.43787731642247757\n",
      "24000/49000 loss: 0.48167921935767954\n",
      "26000/49000 loss: 0.5542382985631127\n",
      "28000/49000 loss: 0.4789451538764827\n",
      "30000/49000 loss: 0.46782461842223627\n",
      "32000/49000 loss: 0.491400320130678\n",
      "34000/49000 loss: 0.38999017360309757\n",
      "36000/49000 loss: 0.45859329543973004\n",
      "38000/49000 loss: 0.5193903959283\n",
      "40000/49000 loss: 0.4842144522633299\n",
      "42000/49000 loss: 0.5438563288470125\n",
      "44000/49000 loss: 0.4495630498641324\n",
      "46000/49000 loss: 0.41936013009188366\n",
      "48000/49000 loss: 0.5038707530710087\n",
      "epoch 3: valid acc = 0.837, new learning rate = 0.0004286875\n",
      "2000/49000 loss: 0.5287962108500464\n",
      "4000/49000 loss: 0.47619714865689916\n",
      "6000/49000 loss: 0.45258214642552735\n",
      "8000/49000 loss: 0.44036619691649026\n",
      "10000/49000 loss: 0.5038259882679781\n",
      "12000/49000 loss: 0.35723872834376963\n",
      "14000/49000 loss: 0.4365951495833222\n",
      "16000/49000 loss: 0.47118421289864093\n",
      "18000/49000 loss: 0.436084660341348\n",
      "20000/49000 loss: 0.4158014806415039\n",
      "22000/49000 loss: 0.535476795560617\n",
      "24000/49000 loss: 0.4953717602396993\n",
      "26000/49000 loss: 0.3738167642263961\n",
      "28000/49000 loss: 0.47584446940043906\n",
      "30000/49000 loss: 0.3962153223951077\n",
      "32000/49000 loss: 0.45438373557549705\n",
      "34000/49000 loss: 0.3734839820498648\n",
      "36000/49000 loss: 0.4165376815706212\n",
      "38000/49000 loss: 0.44639564679296756\n",
      "40000/49000 loss: 0.44605471327288576\n",
      "42000/49000 loss: 0.45159452272430733\n",
      "44000/49000 loss: 0.35116699137335533\n",
      "46000/49000 loss: 0.435866162264909\n",
      "48000/49000 loss: 0.31132595581191225\n",
      "epoch 4: valid acc = 0.838, new learning rate = 0.00040725312499999993\n",
      "2000/49000 loss: 0.4175353616536653\n",
      "4000/49000 loss: 0.4060031052798588\n",
      "6000/49000 loss: 0.4759539345286629\n",
      "8000/49000 loss: 0.38467183925029214\n",
      "10000/49000 loss: 0.47253590768809345\n",
      "12000/49000 loss: 0.37010870898521325\n",
      "14000/49000 loss: 0.352276184105243\n",
      "16000/49000 loss: 0.4017301236319798\n",
      "18000/49000 loss: 0.4375130613350016\n",
      "20000/49000 loss: 0.47433764715801335\n",
      "22000/49000 loss: 0.34970429789352014\n",
      "24000/49000 loss: 0.3359512450952737\n",
      "26000/49000 loss: 0.4839700308274719\n",
      "28000/49000 loss: 0.4259276367840382\n",
      "30000/49000 loss: 0.3602793429579809\n",
      "32000/49000 loss: 0.40726353870948645\n",
      "34000/49000 loss: 0.42121166959776696\n",
      "36000/49000 loss: 0.4045356049472787\n",
      "38000/49000 loss: 0.5077204413048542\n",
      "40000/49000 loss: 0.4148598799900524\n",
      "42000/49000 loss: 0.3284606938090426\n",
      "44000/49000 loss: 0.39001274700544214\n",
      "46000/49000 loss: 0.4243558666614085\n",
      "48000/49000 loss: 0.37603158716628476\n",
      "epoch 5: valid acc = 0.848, new learning rate = 0.0003868904687499999\n",
      "2000/49000 loss: 0.38272816059886056\n",
      "4000/49000 loss: 0.433702817498394\n",
      "6000/49000 loss: 0.33746046360550025\n",
      "8000/49000 loss: 0.29526856838767246\n",
      "10000/49000 loss: 0.386095691022619\n",
      "12000/49000 loss: 0.3232920964393484\n",
      "14000/49000 loss: 0.3132542279078392\n",
      "16000/49000 loss: 0.4497849153242927\n",
      "18000/49000 loss: 0.3690241337691326\n",
      "20000/49000 loss: 0.4476466132702286\n",
      "22000/49000 loss: 0.36992594553324065\n",
      "24000/49000 loss: 0.3422019360462652\n",
      "26000/49000 loss: 0.40841175962463405\n",
      "28000/49000 loss: 0.38913792109005396\n",
      "30000/49000 loss: 0.46640226468628215\n",
      "32000/49000 loss: 0.48445111724958356\n",
      "34000/49000 loss: 0.38484908175955296\n",
      "36000/49000 loss: 0.4418063053194825\n",
      "38000/49000 loss: 0.4533302912830015\n",
      "40000/49000 loss: 0.2714538009557237\n",
      "42000/49000 loss: 0.3844937707105443\n",
      "44000/49000 loss: 0.3224630757400668\n",
      "46000/49000 loss: 0.38938461577538963\n",
      "48000/49000 loss: 0.39237199872553535\n",
      "epoch 6: valid acc = 0.866, new learning rate = 0.0003675459453124999\n",
      "2000/49000 loss: 0.4174563295168724\n",
      "4000/49000 loss: 0.3822665702531601\n",
      "6000/49000 loss: 0.45497273187539344\n",
      "8000/49000 loss: 0.4076282693910178\n",
      "10000/49000 loss: 0.3541520342702455\n",
      "12000/49000 loss: 0.36212501683572756\n",
      "14000/49000 loss: 0.4660484203078339\n",
      "16000/49000 loss: 0.4329950863453081\n",
      "18000/49000 loss: 0.42627339156636185\n",
      "20000/49000 loss: 0.39403127611857\n",
      "22000/49000 loss: 0.4614003149273406\n",
      "24000/49000 loss: 0.3978988804261973\n",
      "26000/49000 loss: 0.3325706835286318\n",
      "28000/49000 loss: 0.37171367092257035\n",
      "30000/49000 loss: 0.4808284817364737\n",
      "32000/49000 loss: 0.5406539658946988\n",
      "34000/49000 loss: 0.4373810788232651\n",
      "36000/49000 loss: 0.42126231161756716\n",
      "38000/49000 loss: 0.3380760807038022\n",
      "40000/49000 loss: 0.40957635540794757\n",
      "42000/49000 loss: 0.3754124500699827\n",
      "44000/49000 loss: 0.3899259129737219\n",
      "46000/49000 loss: 0.37539207459087615\n",
      "48000/49000 loss: 0.3492489288315859\n",
      "epoch 7: valid acc = 0.86, new learning rate = 0.00034916864804687486\n",
      "2000/49000 loss: 0.44704490288306487\n",
      "4000/49000 loss: 0.3146664285547778\n",
      "6000/49000 loss: 0.44446109225184777\n",
      "8000/49000 loss: 0.47263076576223106\n",
      "10000/49000 loss: 0.3710548176592807\n",
      "12000/49000 loss: 0.4185871713439202\n",
      "14000/49000 loss: 0.36847624902487874\n",
      "16000/49000 loss: 0.355921155140729\n",
      "18000/49000 loss: 0.2989209754312074\n",
      "20000/49000 loss: 0.38588215677556337\n",
      "22000/49000 loss: 0.3739500362005768\n",
      "24000/49000 loss: 0.46092537205550826\n",
      "26000/49000 loss: 0.2912820995675799\n",
      "28000/49000 loss: 0.3309188982076687\n",
      "30000/49000 loss: 0.3734772584559495\n",
      "32000/49000 loss: 0.3240581058406984\n",
      "34000/49000 loss: 0.36279982078267836\n",
      "36000/49000 loss: 0.37455563206258136\n",
      "38000/49000 loss: 0.43757724220254407\n",
      "40000/49000 loss: 0.3629922714293131\n",
      "42000/49000 loss: 0.3986153677430202\n",
      "44000/49000 loss: 0.4090428639082066\n",
      "46000/49000 loss: 0.46802781731449317\n",
      "48000/49000 loss: 0.39169159838723155\n",
      "epoch 8: valid acc = 0.87, new learning rate = 0.0003317102156445311\n",
      "2000/49000 loss: 0.35469509077662204\n",
      "4000/49000 loss: 0.3181020855833636\n",
      "6000/49000 loss: 0.3639333646052609\n",
      "8000/49000 loss: 0.3328817094199652\n",
      "10000/49000 loss: 0.42315228664880405\n",
      "12000/49000 loss: 0.35252421993672606\n",
      "14000/49000 loss: 0.3654302021919881\n",
      "16000/49000 loss: 0.3597826030838789\n",
      "18000/49000 loss: 0.41777734393152366\n",
      "20000/49000 loss: 0.33696975731372714\n",
      "22000/49000 loss: 0.40989097729766244\n",
      "24000/49000 loss: 0.2859395899848189\n",
      "26000/49000 loss: 0.35158072161428516\n",
      "28000/49000 loss: 0.3217049448921862\n",
      "30000/49000 loss: 0.42997999052812197\n",
      "32000/49000 loss: 0.4933660889035995\n",
      "34000/49000 loss: 0.37065608425059904\n",
      "36000/49000 loss: 0.3338691546940929\n",
      "38000/49000 loss: 0.4794333929256648\n",
      "40000/49000 loss: 0.3812967559914545\n",
      "42000/49000 loss: 0.4523957858703027\n",
      "44000/49000 loss: 0.40862089888845515\n",
      "46000/49000 loss: 0.38969980415676053\n",
      "48000/49000 loss: 0.4777921232590552\n",
      "epoch 9: valid acc = 0.867, new learning rate = 0.0003151247048623045\n",
      "2000/49000 loss: 0.4321581105728152\n",
      "4000/49000 loss: 0.32212270424673517\n",
      "6000/49000 loss: 0.3925636802770402\n",
      "8000/49000 loss: 0.3322094280345065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/49000 loss: 0.27954332169350965\n",
      "12000/49000 loss: 0.402158666072911\n",
      "14000/49000 loss: 0.2661860012473226\n",
      "16000/49000 loss: 0.3093145731202395\n",
      "18000/49000 loss: 0.3046873495087801\n",
      "20000/49000 loss: 0.39996019418834583\n",
      "22000/49000 loss: 0.23704173035909162\n",
      "24000/49000 loss: 0.36666603896474725\n",
      "26000/49000 loss: 0.37284854653467153\n",
      "28000/49000 loss: 0.35688159186360524\n",
      "30000/49000 loss: 0.3325140329234775\n",
      "32000/49000 loss: 0.33758717704918273\n",
      "34000/49000 loss: 0.31210396072146745\n",
      "36000/49000 loss: 0.32079215546302237\n",
      "38000/49000 loss: 0.3722996431877784\n",
      "40000/49000 loss: 0.4071507252155639\n",
      "42000/49000 loss: 0.3130089050106296\n",
      "44000/49000 loss: 0.2722600729181308\n",
      "46000/49000 loss: 0.33645121639012576\n",
      "48000/49000 loss: 0.4055868458971858\n",
      "epoch 10: valid acc = 0.873, new learning rate = 0.00029936846961918924\n",
      "2000/49000 loss: 0.3369466515922733\n",
      "4000/49000 loss: 0.3271064710130515\n",
      "6000/49000 loss: 0.33742490963326865\n",
      "8000/49000 loss: 0.3985166872422514\n",
      "10000/49000 loss: 0.39011606747855665\n",
      "12000/49000 loss: 0.29355530068471686\n",
      "14000/49000 loss: 0.41896473399507567\n",
      "16000/49000 loss: 0.3443514494661541\n",
      "18000/49000 loss: 0.2921214046791818\n",
      "20000/49000 loss: 0.3913774727366595\n",
      "22000/49000 loss: 0.3483257641289185\n",
      "24000/49000 loss: 0.363582528102445\n",
      "26000/49000 loss: 0.2993810895939548\n",
      "28000/49000 loss: 0.4184629711106836\n",
      "30000/49000 loss: 0.28579898426058614\n",
      "32000/49000 loss: 0.4143838758833271\n",
      "34000/49000 loss: 0.43374118377967275\n",
      "36000/49000 loss: 0.3928218948191189\n",
      "38000/49000 loss: 0.4178370510379321\n",
      "40000/49000 loss: 0.36171847650980066\n",
      "42000/49000 loss: 0.310533550557536\n",
      "44000/49000 loss: 0.30750653748757056\n",
      "46000/49000 loss: 0.32443517704274444\n",
      "48000/49000 loss: 0.3024820176476671\n",
      "epoch 11: valid acc = 0.879, new learning rate = 0.00028440004613822977\n",
      "2000/49000 loss: 0.3556690149528838\n",
      "4000/49000 loss: 0.29387938717851664\n",
      "6000/49000 loss: 0.29538198492598644\n",
      "8000/49000 loss: 0.36156371082099076\n",
      "10000/49000 loss: 0.3125103369808019\n",
      "12000/49000 loss: 0.3787579667943159\n",
      "14000/49000 loss: 0.315656529473184\n",
      "16000/49000 loss: 0.31371817150352554\n",
      "18000/49000 loss: 0.37708055193991735\n",
      "20000/49000 loss: 0.352742554264188\n",
      "22000/49000 loss: 0.4424018407253689\n",
      "24000/49000 loss: 0.34829149558979117\n",
      "26000/49000 loss: 0.2592179494167355\n",
      "28000/49000 loss: 0.3115845105888714\n",
      "30000/49000 loss: 0.3092899601026054\n",
      "32000/49000 loss: 0.4478908186640037\n",
      "34000/49000 loss: 0.36895287254899556\n",
      "36000/49000 loss: 0.39641630801897726\n",
      "38000/49000 loss: 0.4282270140472398\n",
      "40000/49000 loss: 0.3935484902187404\n",
      "42000/49000 loss: 0.41806163010593655\n",
      "44000/49000 loss: 0.33412484663823466\n",
      "46000/49000 loss: 0.3485032824873375\n",
      "48000/49000 loss: 0.2783748927378215\n",
      "epoch 12: valid acc = 0.874, new learning rate = 0.00027018004383131826\n",
      "2000/49000 loss: 0.309625309624016\n",
      "4000/49000 loss: 0.3420082502745996\n",
      "6000/49000 loss: 0.40713222451237263\n",
      "8000/49000 loss: 0.34418041254258036\n",
      "10000/49000 loss: 0.3137957870980969\n",
      "12000/49000 loss: 0.3672676874567089\n",
      "14000/49000 loss: 0.30293578572466767\n",
      "16000/49000 loss: 0.3353584979236874\n",
      "18000/49000 loss: 0.28053977843580125\n",
      "20000/49000 loss: 0.2951289653772063\n",
      "22000/49000 loss: 0.3382817457887009\n",
      "24000/49000 loss: 0.3200515706518909\n",
      "26000/49000 loss: 0.314669097732458\n",
      "28000/49000 loss: 0.2924924748438617\n",
      "30000/49000 loss: 0.3884813767366427\n",
      "32000/49000 loss: 0.31106317353727775\n",
      "34000/49000 loss: 0.29004315221827465\n",
      "36000/49000 loss: 0.3210482406447742\n",
      "38000/49000 loss: 0.24724572719435292\n",
      "40000/49000 loss: 0.3677799512115087\n",
      "42000/49000 loss: 0.27646722000286844\n",
      "44000/49000 loss: 0.4100313634180572\n",
      "46000/49000 loss: 0.28474234220913675\n",
      "48000/49000 loss: 0.3321686022695089\n",
      "epoch 13: valid acc = 0.875, new learning rate = 0.00025667104163975234\n",
      "2000/49000 loss: 0.27534413501252636\n",
      "4000/49000 loss: 0.3432226900573164\n",
      "6000/49000 loss: 0.43227783668364905\n",
      "8000/49000 loss: 0.30864893661388076\n",
      "10000/49000 loss: 0.28826666010689106\n",
      "12000/49000 loss: 0.373572185660087\n",
      "14000/49000 loss: 0.32069997738633815\n",
      "16000/49000 loss: 0.3538235351044747\n",
      "18000/49000 loss: 0.3530515009009125\n",
      "20000/49000 loss: 0.3846120251887581\n",
      "22000/49000 loss: 0.31072865925177456\n",
      "24000/49000 loss: 0.2784807698442514\n",
      "26000/49000 loss: 0.3159129342822912\n",
      "28000/49000 loss: 0.4846615121029327\n",
      "30000/49000 loss: 0.2833451429000997\n",
      "32000/49000 loss: 0.393992583172663\n",
      "34000/49000 loss: 0.40680537964525243\n",
      "36000/49000 loss: 0.2638873319580434\n",
      "38000/49000 loss: 0.3114408685631322\n",
      "40000/49000 loss: 0.2415012990157042\n",
      "42000/49000 loss: 0.3260464116513583\n",
      "44000/49000 loss: 0.27368487104431316\n",
      "46000/49000 loss: 0.29240649018620013\n",
      "48000/49000 loss: 0.31739819420824694\n",
      "epoch 14: valid acc = 0.88, new learning rate = 0.00024383748955776472\n",
      "2000/49000 loss: 0.2352379922877509\n",
      "4000/49000 loss: 0.3990709214129332\n",
      "6000/49000 loss: 0.34408903839817795\n",
      "8000/49000 loss: 0.24811562917749522\n",
      "10000/49000 loss: 0.30374117230535497\n",
      "12000/49000 loss: 0.41153042777728366\n",
      "14000/49000 loss: 0.30068838432647776\n",
      "16000/49000 loss: 0.2732261663733367\n",
      "18000/49000 loss: 0.3295878336224197\n",
      "20000/49000 loss: 0.3216535541379053\n",
      "22000/49000 loss: 0.3614135130999456\n",
      "24000/49000 loss: 0.3936145742586578\n",
      "26000/49000 loss: 0.3274892731949372\n",
      "28000/49000 loss: 0.3912340982079281\n",
      "30000/49000 loss: 0.27222435099349934\n",
      "32000/49000 loss: 0.3194344540308594\n",
      "34000/49000 loss: 0.34490635050539215\n",
      "36000/49000 loss: 0.35190580058525034\n",
      "38000/49000 loss: 0.2826405309747932\n",
      "40000/49000 loss: 0.35728871360107484\n",
      "42000/49000 loss: 0.2767553673753153\n",
      "44000/49000 loss: 0.42646735711979894\n",
      "46000/49000 loss: 0.32497204003929714\n",
      "48000/49000 loss: 0.3616801429407085\n",
      "epoch 15: valid acc = 0.881, new learning rate = 0.00023164561507987649\n",
      "2000/49000 loss: 0.30063077967369817\n",
      "4000/49000 loss: 0.2745553472882525\n",
      "6000/49000 loss: 0.3775909194258849\n",
      "8000/49000 loss: 0.43146288337946537\n",
      "10000/49000 loss: 0.3927255152445112\n",
      "12000/49000 loss: 0.4222054678507238\n",
      "14000/49000 loss: 0.25596389322206187\n",
      "16000/49000 loss: 0.35287180984341787\n",
      "18000/49000 loss: 0.3760900311813331\n",
      "20000/49000 loss: 0.3655684657266001\n",
      "22000/49000 loss: 0.31374194185449916\n",
      "24000/49000 loss: 0.24728671853082002\n",
      "26000/49000 loss: 0.2913657614774421\n",
      "28000/49000 loss: 0.32040928588251\n",
      "30000/49000 loss: 0.2456284249074096\n",
      "32000/49000 loss: 0.32729118676365015\n",
      "34000/49000 loss: 0.3194568124460213\n",
      "36000/49000 loss: 0.35858611818290836\n",
      "38000/49000 loss: 0.3146294787018297\n",
      "40000/49000 loss: 0.2721088330203133\n",
      "42000/49000 loss: 0.37127573134828246\n",
      "44000/49000 loss: 0.3387299063960347\n",
      "46000/49000 loss: 0.33684879227614123\n",
      "48000/49000 loss: 0.2899847775627238\n",
      "epoch 16: valid acc = 0.873, new learning rate = 0.00022006333432588265\n",
      "2000/49000 loss: 0.3448383568018866\n",
      "4000/49000 loss: 0.35941506077490787\n",
      "6000/49000 loss: 0.2554026936800497\n",
      "8000/49000 loss: 0.3800618533298926\n",
      "10000/49000 loss: 0.3447515604589054\n",
      "12000/49000 loss: 0.35818511383402896\n",
      "14000/49000 loss: 0.3655527628579967\n",
      "16000/49000 loss: 0.3734350074403116\n",
      "18000/49000 loss: 0.4188376448263327\n",
      "20000/49000 loss: 0.3046334923042994\n",
      "22000/49000 loss: 0.32628510494648716\n",
      "24000/49000 loss: 0.26966720721204124\n",
      "26000/49000 loss: 0.26043955363108595\n",
      "28000/49000 loss: 0.4270967578098367\n",
      "30000/49000 loss: 0.23024324885251365\n",
      "32000/49000 loss: 0.25531996193058\n",
      "34000/49000 loss: 0.29122755523997507\n",
      "36000/49000 loss: 0.4327871584356905\n",
      "38000/49000 loss: 0.3526116662900675\n",
      "40000/49000 loss: 0.3052922966682847\n",
      "42000/49000 loss: 0.3467687334937623\n",
      "44000/49000 loss: 0.30950992729832294\n",
      "46000/49000 loss: 0.3993156294570282\n",
      "48000/49000 loss: 0.2385260500312729\n",
      "epoch 17: valid acc = 0.891, new learning rate = 0.00020906016760958852\n",
      "2000/49000 loss: 0.21507202022912442\n",
      "4000/49000 loss: 0.3172408645170763\n",
      "6000/49000 loss: 0.2657423946323568\n",
      "8000/49000 loss: 0.2737026734159717\n",
      "10000/49000 loss: 0.28455007328545245\n",
      "12000/49000 loss: 0.3417547267136821\n",
      "14000/49000 loss: 0.3637250716716646\n",
      "16000/49000 loss: 0.2989775121787359\n",
      "18000/49000 loss: 0.3175674881420143\n",
      "20000/49000 loss: 0.32443570786242265\n",
      "22000/49000 loss: 0.36356076219053396\n",
      "24000/49000 loss: 0.3210453302731747\n",
      "26000/49000 loss: 0.3226455455222056\n",
      "28000/49000 loss: 0.3088051207521633\n",
      "30000/49000 loss: 0.2864316658512936\n",
      "32000/49000 loss: 0.31756343177118335\n",
      "34000/49000 loss: 0.26461409860595847\n",
      "36000/49000 loss: 0.2553187436952298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38000/49000 loss: 0.26150853537663776\n",
      "40000/49000 loss: 0.4006410359628237\n",
      "42000/49000 loss: 0.2484063565323708\n",
      "44000/49000 loss: 0.313031528020128\n",
      "46000/49000 loss: 0.2657376983495099\n",
      "48000/49000 loss: 0.22466040330279718\n",
      "epoch 18: valid acc = 0.882, new learning rate = 0.00019860715922910907\n",
      "2000/49000 loss: 0.318851086618108\n",
      "4000/49000 loss: 0.3480069330660409\n",
      "6000/49000 loss: 0.28352716664383415\n",
      "8000/49000 loss: 0.2852649322668771\n",
      "10000/49000 loss: 0.3071171876316869\n",
      "12000/49000 loss: 0.34380012544274485\n",
      "14000/49000 loss: 0.35498786027047646\n",
      "16000/49000 loss: 0.30748730545744013\n",
      "18000/49000 loss: 0.29105031070574433\n",
      "20000/49000 loss: 0.2718718347439587\n",
      "22000/49000 loss: 0.32342930691085403\n",
      "24000/49000 loss: 0.27513779880483225\n",
      "26000/49000 loss: 0.2536787013057004\n",
      "28000/49000 loss: 0.3135842607843308\n",
      "30000/49000 loss: 0.3777697462569234\n",
      "32000/49000 loss: 0.3111706159226627\n",
      "34000/49000 loss: 0.3039310687282519\n",
      "36000/49000 loss: 0.419922386508361\n",
      "38000/49000 loss: 0.2620189760413254\n",
      "40000/49000 loss: 0.32233372319248177\n",
      "42000/49000 loss: 0.37138367930854227\n",
      "44000/49000 loss: 0.4026939908469093\n",
      "46000/49000 loss: 0.303876832825858\n",
      "48000/49000 loss: 0.36285436868004467\n",
      "epoch 19: valid acc = 0.886, new learning rate = 0.0001886768012676536\n",
      "2000/49000 loss: 0.29692416066272487\n",
      "4000/49000 loss: 0.293909082605106\n",
      "6000/49000 loss: 0.4251661164278616\n",
      "8000/49000 loss: 0.4009802886934751\n",
      "10000/49000 loss: 0.24066861448841184\n",
      "12000/49000 loss: 0.27343286641021336\n",
      "14000/49000 loss: 0.3018124395198511\n",
      "16000/49000 loss: 0.4445580636099697\n",
      "18000/49000 loss: 0.3958363838833306\n",
      "20000/49000 loss: 0.2667358628788454\n",
      "22000/49000 loss: 0.32874988741631\n",
      "24000/49000 loss: 0.3143112830785704\n",
      "26000/49000 loss: 0.31142265678001996\n",
      "28000/49000 loss: 0.3601357610881276\n",
      "30000/49000 loss: 0.3606099336530465\n",
      "32000/49000 loss: 0.29297477148079387\n",
      "34000/49000 loss: 0.22929640231230816\n",
      "36000/49000 loss: 0.2969639565675491\n",
      "38000/49000 loss: 0.4231183113490698\n",
      "40000/49000 loss: 0.3320518171310128\n",
      "42000/49000 loss: 0.2474455366641854\n",
      "44000/49000 loss: 0.2936173002527453\n",
      "46000/49000 loss: 0.2913856448631523\n",
      "48000/49000 loss: 0.2799715209819776\n",
      "epoch 20: valid acc = 0.881, new learning rate = 0.0001792429612042709\n",
      "2000/49000 loss: 0.31400457344111643\n",
      "4000/49000 loss: 0.28267986046973737\n",
      "6000/49000 loss: 0.31348004531094736\n",
      "8000/49000 loss: 0.35690845507572444\n",
      "10000/49000 loss: 0.2880158381289087\n",
      "12000/49000 loss: 0.24111221135490574\n",
      "14000/49000 loss: 0.24814355381626105\n",
      "16000/49000 loss: 0.23332978072835509\n",
      "18000/49000 loss: 0.24519912814808337\n",
      "20000/49000 loss: 0.23828162408492184\n",
      "22000/49000 loss: 0.2767446147068753\n",
      "24000/49000 loss: 0.31331436983588273\n",
      "26000/49000 loss: 0.27908506253538073\n",
      "28000/49000 loss: 0.2854530880732001\n",
      "30000/49000 loss: 0.2603188776510641\n",
      "32000/49000 loss: 0.3251117270789093\n",
      "34000/49000 loss: 0.42973929471957967\n",
      "36000/49000 loss: 0.2980765873368658\n",
      "38000/49000 loss: 0.38867204898638363\n",
      "40000/49000 loss: 0.2488851502008317\n",
      "42000/49000 loss: 0.26492620228092906\n",
      "44000/49000 loss: 0.41677313229942253\n",
      "46000/49000 loss: 0.2961778301942272\n",
      "48000/49000 loss: 0.2564484329125158\n",
      "epoch 21: valid acc = 0.886, new learning rate = 0.00017028081314405735\n",
      "2000/49000 loss: 0.33853852354613695\n",
      "4000/49000 loss: 0.27994655451937217\n",
      "6000/49000 loss: 0.32655458077698724\n",
      "8000/49000 loss: 0.30259502966252494\n",
      "10000/49000 loss: 0.34409306196541756\n",
      "12000/49000 loss: 0.35160298706832704\n",
      "14000/49000 loss: 0.3150484599456828\n",
      "16000/49000 loss: 0.28826039420121585\n",
      "18000/49000 loss: 0.2859214213080069\n",
      "20000/49000 loss: 0.27103015938996733\n",
      "22000/49000 loss: 0.23338720871209045\n",
      "24000/49000 loss: 0.2820105432792637\n",
      "26000/49000 loss: 0.41662609808623885\n",
      "28000/49000 loss: 0.3186352987579485\n",
      "30000/49000 loss: 0.2725448944083855\n",
      "32000/49000 loss: 0.3060847559780127\n",
      "34000/49000 loss: 0.3021150624724777\n",
      "36000/49000 loss: 0.3513780239408374\n",
      "38000/49000 loss: 0.33276387360100046\n",
      "40000/49000 loss: 0.30107594032945006\n",
      "42000/49000 loss: 0.23161069305545212\n",
      "44000/49000 loss: 0.31762450943377835\n",
      "46000/49000 loss: 0.2437943283682873\n",
      "48000/49000 loss: 0.24974181709833326\n",
      "epoch 22: valid acc = 0.886, new learning rate = 0.00016176677248685447\n",
      "2000/49000 loss: 0.2710931542164132\n",
      "4000/49000 loss: 0.18912078059392184\n",
      "6000/49000 loss: 0.31301921835650853\n",
      "8000/49000 loss: 0.24577645601011916\n",
      "10000/49000 loss: 0.34477186741099225\n",
      "12000/49000 loss: 0.25671695659320026\n",
      "14000/49000 loss: 0.2842311018286276\n",
      "16000/49000 loss: 0.2616109852998281\n",
      "18000/49000 loss: 0.2742215197550053\n",
      "20000/49000 loss: 0.3680196954672877\n",
      "22000/49000 loss: 0.2865277888041675\n",
      "24000/49000 loss: 0.3328133282528865\n",
      "26000/49000 loss: 0.2355385571909977\n",
      "28000/49000 loss: 0.3257078159670321\n",
      "30000/49000 loss: 0.37746640745314064\n",
      "32000/49000 loss: 0.2647498829747712\n",
      "34000/49000 loss: 0.3627111380924412\n",
      "36000/49000 loss: 0.24036002227846848\n",
      "38000/49000 loss: 0.3002275914575425\n",
      "40000/49000 loss: 0.3370393189241065\n",
      "42000/49000 loss: 0.34017575395570726\n",
      "44000/49000 loss: 0.3348949690268851\n",
      "46000/49000 loss: 0.24300109453274504\n",
      "48000/49000 loss: 0.30428951110210545\n",
      "epoch 23: valid acc = 0.887, new learning rate = 0.00015367843386251173\n",
      "2000/49000 loss: 0.25454476041831814\n",
      "4000/49000 loss: 0.2549736358906019\n",
      "6000/49000 loss: 0.344068028044576\n",
      "8000/49000 loss: 0.3094173082577014\n",
      "10000/49000 loss: 0.23312405706036896\n",
      "12000/49000 loss: 0.21231453657135693\n",
      "14000/49000 loss: 0.2803021770065569\n",
      "16000/49000 loss: 0.26196922570003284\n",
      "18000/49000 loss: 0.31137382892473786\n",
      "20000/49000 loss: 0.3252441396685382\n",
      "22000/49000 loss: 0.3091601317301869\n",
      "24000/49000 loss: 0.21699785832847635\n",
      "26000/49000 loss: 0.29701557461465633\n",
      "28000/49000 loss: 0.35850887671506615\n",
      "30000/49000 loss: 0.30261822234304453\n",
      "32000/49000 loss: 0.2713834825263834\n",
      "34000/49000 loss: 0.23133072148255374\n",
      "36000/49000 loss: 0.3748233491108122\n",
      "38000/49000 loss: 0.30842662716225344\n",
      "40000/49000 loss: 0.21307527901949874\n",
      "42000/49000 loss: 0.268723196259014\n",
      "44000/49000 loss: 0.32180437603922485\n",
      "46000/49000 loss: 0.24749331221311346\n",
      "48000/49000 loss: 0.2925262450123292\n",
      "epoch 24: valid acc = 0.886, new learning rate = 0.00014599451216938612\n",
      "2000/49000 loss: 0.2760276640791962\n",
      "4000/49000 loss: 0.3230766618423375\n",
      "6000/49000 loss: 0.2974918743785208\n",
      "8000/49000 loss: 0.32377326344711566\n",
      "10000/49000 loss: 0.3136803776291672\n",
      "12000/49000 loss: 0.3095617160883814\n",
      "14000/49000 loss: 0.3026825871571458\n",
      "16000/49000 loss: 0.2640993619098686\n",
      "18000/49000 loss: 0.3616557314369339\n",
      "20000/49000 loss: 0.2891892754246429\n",
      "22000/49000 loss: 0.24092931950612886\n",
      "24000/49000 loss: 0.36288789611867955\n",
      "26000/49000 loss: 0.35741025184473624\n",
      "28000/49000 loss: 0.23747234410886794\n",
      "30000/49000 loss: 0.2754162871210373\n",
      "32000/49000 loss: 0.3098957107755068\n",
      "34000/49000 loss: 0.2657345467007749\n",
      "36000/49000 loss: 0.3127117979978137\n",
      "38000/49000 loss: 0.2874226498130917\n",
      "40000/49000 loss: 0.43704725370263375\n",
      "42000/49000 loss: 0.38806226055209087\n",
      "44000/49000 loss: 0.24811851201772103\n",
      "46000/49000 loss: 0.2661134933406138\n",
      "48000/49000 loss: 0.2602962337908466\n",
      "epoch 25: valid acc = 0.887, new learning rate = 0.00013869478656091682\n",
      "2000/49000 loss: 0.22165944072854676\n",
      "4000/49000 loss: 0.21752760332310564\n",
      "6000/49000 loss: 0.3164115516255338\n",
      "8000/49000 loss: 0.2592885344343373\n",
      "10000/49000 loss: 0.3024414995254874\n",
      "12000/49000 loss: 0.2872526515995262\n",
      "14000/49000 loss: 0.3319508407728363\n",
      "16000/49000 loss: 0.24686675650723103\n",
      "18000/49000 loss: 0.2145093939792953\n",
      "20000/49000 loss: 0.39920934818950987\n",
      "22000/49000 loss: 0.24646791682567978\n",
      "24000/49000 loss: 0.19939408086840268\n",
      "26000/49000 loss: 0.39675288081131554\n",
      "28000/49000 loss: 0.42729876553812235\n",
      "30000/49000 loss: 0.3054936591946004\n",
      "32000/49000 loss: 0.2841337640527764\n",
      "34000/49000 loss: 0.296691362884326\n",
      "36000/49000 loss: 0.2700510471659983\n",
      "38000/49000 loss: 0.3792363368251013\n",
      "40000/49000 loss: 0.3421208356796001\n",
      "42000/49000 loss: 0.29832818040897663\n",
      "44000/49000 loss: 0.2225264080723694\n",
      "46000/49000 loss: 0.3322260598736331\n",
      "48000/49000 loss: 0.3123406351015607\n",
      "epoch 26: valid acc = 0.886, new learning rate = 0.00013176004723287096\n",
      "2000/49000 loss: 0.32023209960978766\n",
      "4000/49000 loss: 0.26492083485374035\n",
      "6000/49000 loss: 0.3764706430870018\n",
      "8000/49000 loss: 0.356113290555654\n",
      "10000/49000 loss: 0.2943729291188004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/49000 loss: 0.24394204692325383\n",
      "14000/49000 loss: 0.2779236855667836\n",
      "16000/49000 loss: 0.4462508834280619\n",
      "18000/49000 loss: 0.30195876070116734\n",
      "20000/49000 loss: 0.2981151653135633\n",
      "22000/49000 loss: 0.33688704171103134\n",
      "24000/49000 loss: 0.34407498636779926\n",
      "26000/49000 loss: 0.31126932665034207\n",
      "28000/49000 loss: 0.33127818159698064\n",
      "30000/49000 loss: 0.30200752672267067\n",
      "32000/49000 loss: 0.25879084162910265\n",
      "34000/49000 loss: 0.30256184107017947\n",
      "36000/49000 loss: 0.2196028174311367\n",
      "38000/49000 loss: 0.3215363526699328\n",
      "40000/49000 loss: 0.23939342157321708\n",
      "42000/49000 loss: 0.26003505093562723\n",
      "44000/49000 loss: 0.3004739299404232\n",
      "46000/49000 loss: 0.3956092717238713\n",
      "48000/49000 loss: 0.3845524912708319\n",
      "epoch 27: valid acc = 0.888, new learning rate = 0.0001251720448712274\n",
      "2000/49000 loss: 0.22769962388426604\n",
      "4000/49000 loss: 0.2939582286602677\n",
      "6000/49000 loss: 0.22733719893067655\n",
      "8000/49000 loss: 0.3038117292877857\n",
      "10000/49000 loss: 0.19225318092162416\n",
      "12000/49000 loss: 0.21239761210272864\n",
      "14000/49000 loss: 0.25838222738506184\n",
      "16000/49000 loss: 0.20527166857574797\n",
      "18000/49000 loss: 0.27458287112158036\n",
      "20000/49000 loss: 0.3731670691134627\n",
      "22000/49000 loss: 0.32818303351362693\n",
      "24000/49000 loss: 0.27643135004555747\n",
      "26000/49000 loss: 0.34103901840630496\n",
      "28000/49000 loss: 0.2575135690493828\n",
      "30000/49000 loss: 0.30983686618203937\n",
      "32000/49000 loss: 0.27536107681011873\n",
      "34000/49000 loss: 0.2661276506273073\n",
      "36000/49000 loss: 0.3424700015346916\n",
      "38000/49000 loss: 0.31908388379548086\n",
      "40000/49000 loss: 0.3129072366409659\n",
      "42000/49000 loss: 0.36882920662856455\n",
      "44000/49000 loss: 0.25341817542714085\n",
      "46000/49000 loss: 0.2961553532374016\n",
      "48000/49000 loss: 0.3423190731923395\n",
      "epoch 28: valid acc = 0.884, new learning rate = 0.00011891344262766602\n",
      "2000/49000 loss: 0.2677831116878944\n",
      "4000/49000 loss: 0.30667513780940825\n",
      "6000/49000 loss: 0.34589919027530097\n",
      "8000/49000 loss: 0.29067925746204804\n",
      "10000/49000 loss: 0.28431764513341745\n",
      "12000/49000 loss: 0.3077211036775888\n",
      "14000/49000 loss: 0.2754800543312225\n",
      "16000/49000 loss: 0.21637138722695445\n",
      "18000/49000 loss: 0.30614735811881133\n",
      "20000/49000 loss: 0.3001845242368202\n",
      "22000/49000 loss: 0.23097495122740866\n",
      "24000/49000 loss: 0.31494290128010266\n",
      "26000/49000 loss: 0.2905371469598836\n",
      "28000/49000 loss: 0.22469732121023556\n",
      "30000/49000 loss: 0.2507957632578221\n",
      "32000/49000 loss: 0.36464560483102326\n",
      "34000/49000 loss: 0.4852136530128223\n",
      "36000/49000 loss: 0.3391366069753405\n",
      "38000/49000 loss: 0.2657408269735586\n",
      "40000/49000 loss: 0.3108666197912981\n",
      "42000/49000 loss: 0.3074966094892313\n",
      "44000/49000 loss: 0.22818108478345892\n",
      "46000/49000 loss: 0.30878909942160215\n",
      "48000/49000 loss: 0.2526723748947906\n",
      "epoch 29: valid acc = 0.886, new learning rate = 0.00011296777049628272\n",
      "2000/49000 loss: 0.2736381276411233\n",
      "4000/49000 loss: 0.26777095731103295\n",
      "6000/49000 loss: 0.37208629871359616\n",
      "8000/49000 loss: 0.23871005992676414\n",
      "10000/49000 loss: 0.24785463304436778\n",
      "12000/49000 loss: 0.2548316339834015\n",
      "14000/49000 loss: 0.27304661150667653\n",
      "16000/49000 loss: 0.3063361369354384\n",
      "18000/49000 loss: 0.2849293487766004\n",
      "20000/49000 loss: 0.3146419174068891\n",
      "22000/49000 loss: 0.2783638586078425\n",
      "24000/49000 loss: 0.252465756822471\n",
      "26000/49000 loss: 0.28286669104985934\n",
      "28000/49000 loss: 0.3646979164161902\n",
      "30000/49000 loss: 0.3056808140519508\n",
      "32000/49000 loss: 0.3071065979609806\n",
      "34000/49000 loss: 0.3571282532272148\n",
      "36000/49000 loss: 0.2797785996727046\n",
      "38000/49000 loss: 0.27817524506424574\n",
      "40000/49000 loss: 0.35141722282253507\n",
      "42000/49000 loss: 0.24979534295653694\n",
      "44000/49000 loss: 0.27105820511705353\n",
      "46000/49000 loss: 0.2485583099727676\n",
      "48000/49000 loss: 0.2799656984835721\n",
      "epoch 30: valid acc = 0.885, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.898061224489796\n",
      "test acc: 0.885\n",
      "test acc: 0.8709\n",
      "number of batches for training: 245\n",
      "epoch 1: valid acc = 0.749, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.816, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.82, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.836, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.857, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.858, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.862, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.865, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.872, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.871, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.873, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.878, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.882, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.878, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.881, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.886, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.88, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.883, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.889, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.882, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.887, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.887, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.885, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.878, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.89, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.883, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.885, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.886, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.886, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.888, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.898\n",
      "test acc: 0.888\n",
      "test acc: 0.8692\n",
      "number of batches for training: 81\n",
      "6000/49000 loss: 2.694774162974481\n",
      "12000/49000 loss: 2.7155548038894324\n",
      "18000/49000 loss: 2.474188924994556\n",
      "24000/49000 loss: 2.399111356367386\n",
      "30000/49000 loss: 2.2382431852980056\n",
      "36000/49000 loss: 2.047489458332261\n",
      "42000/49000 loss: 2.0826547295081417\n",
      "48000/49000 loss: 1.686967695274915\n",
      "epoch 1: valid acc = 0.51, new learning rate = 0.000475\n",
      "6000/49000 loss: 1.2862695247209426\n",
      "12000/49000 loss: 1.3816638657541775\n",
      "18000/49000 loss: 1.1963000704136262\n",
      "24000/49000 loss: 1.1905135076259816\n",
      "30000/49000 loss: 1.145196683349017\n",
      "36000/49000 loss: 1.0430838157052746\n",
      "42000/49000 loss: 1.010813648798839\n",
      "48000/49000 loss: 0.9355337803332756\n",
      "epoch 2: valid acc = 0.662, new learning rate = 0.00045125\n",
      "6000/49000 loss: 0.9470890349081938\n",
      "12000/49000 loss: 0.8742516813565271\n",
      "18000/49000 loss: 0.973286580188195\n",
      "24000/49000 loss: 0.8896252571933799\n",
      "30000/49000 loss: 0.7814080265116994\n",
      "36000/49000 loss: 0.8636030238281495\n",
      "42000/49000 loss: 0.8197145204567496\n",
      "48000/49000 loss: 0.74876796952245\n",
      "epoch 3: valid acc = 0.727, new learning rate = 0.0004286875\n",
      "6000/49000 loss: 0.7707726249871817\n",
      "12000/49000 loss: 0.7695190967804079\n",
      "18000/49000 loss: 0.704120271133844\n",
      "24000/49000 loss: 0.6397739081026547\n",
      "30000/49000 loss: 0.693115389151428\n",
      "36000/49000 loss: 0.6406008579693981\n",
      "42000/49000 loss: 0.6568727406527813\n",
      "48000/49000 loss: 0.6528948360929308\n",
      "epoch 4: valid acc = 0.755, new learning rate = 0.00040725312499999993\n",
      "6000/49000 loss: 0.633815478485312\n",
      "12000/49000 loss: 0.6517750255869409\n",
      "18000/49000 loss: 0.6340922834366678\n",
      "24000/49000 loss: 0.6278808285140096\n",
      "30000/49000 loss: 0.6321546081419367\n",
      "36000/49000 loss: 0.5974439510659474\n",
      "42000/49000 loss: 0.5773267247271842\n",
      "48000/49000 loss: 0.5833138692465639\n",
      "epoch 5: valid acc = 0.794, new learning rate = 0.0003868904687499999\n",
      "6000/49000 loss: 0.5564710395291206\n",
      "12000/49000 loss: 0.5845314424715139\n",
      "18000/49000 loss: 0.664481673329961\n",
      "24000/49000 loss: 0.6375690445972718\n",
      "30000/49000 loss: 0.5433766329776946\n",
      "36000/49000 loss: 0.6019327188007978\n",
      "42000/49000 loss: 0.5302213419056835\n",
      "48000/49000 loss: 0.5348392072665481\n",
      "epoch 6: valid acc = 0.799, new learning rate = 0.0003675459453124999\n",
      "6000/49000 loss: 0.5243399537872049\n",
      "12000/49000 loss: 0.5031543093415101\n",
      "18000/49000 loss: 0.5170548973827731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000/49000 loss: 0.5780443138096588\n",
      "30000/49000 loss: 0.4710368981657957\n",
      "36000/49000 loss: 0.4783055433216159\n",
      "42000/49000 loss: 0.5152898003549535\n",
      "48000/49000 loss: 0.4619610571913817\n",
      "epoch 7: valid acc = 0.808, new learning rate = 0.00034916864804687486\n",
      "6000/49000 loss: 0.5173193493024507\n",
      "12000/49000 loss: 0.506403177167381\n",
      "18000/49000 loss: 0.510836080968368\n",
      "24000/49000 loss: 0.496937592678826\n",
      "30000/49000 loss: 0.492353688727218\n",
      "36000/49000 loss: 0.4778301539071131\n",
      "42000/49000 loss: 0.4869086831700045\n",
      "48000/49000 loss: 0.4377165645933873\n",
      "epoch 8: valid acc = 0.826, new learning rate = 0.0003317102156445311\n",
      "6000/49000 loss: 0.5628370066842253\n",
      "12000/49000 loss: 0.5209687821674951\n",
      "18000/49000 loss: 0.48438871032439423\n",
      "24000/49000 loss: 0.4672041431144505\n",
      "30000/49000 loss: 0.5025224049940572\n",
      "36000/49000 loss: 0.5009552053271498\n",
      "42000/49000 loss: 0.44633512855840113\n",
      "48000/49000 loss: 0.48385515412623237\n",
      "epoch 9: valid acc = 0.825, new learning rate = 0.0003151247048623045\n",
      "6000/49000 loss: 0.49983459696438237\n",
      "12000/49000 loss: 0.543841251766606\n",
      "18000/49000 loss: 0.4643203354308518\n",
      "24000/49000 loss: 0.512036510577176\n",
      "30000/49000 loss: 0.47254720047699256\n",
      "36000/49000 loss: 0.4977148445616428\n",
      "42000/49000 loss: 0.4844809011122614\n",
      "48000/49000 loss: 0.462470629478049\n",
      "epoch 10: valid acc = 0.829, new learning rate = 0.00029936846961918924\n",
      "6000/49000 loss: 0.45073591343596425\n",
      "12000/49000 loss: 0.4671411266668338\n",
      "18000/49000 loss: 0.45057422848419904\n",
      "24000/49000 loss: 0.482349090869726\n",
      "30000/49000 loss: 0.433733184636179\n",
      "36000/49000 loss: 0.46529629267148936\n",
      "42000/49000 loss: 0.48033525073732736\n",
      "48000/49000 loss: 0.5239105772524746\n",
      "epoch 11: valid acc = 0.838, new learning rate = 0.00028440004613822977\n",
      "6000/49000 loss: 0.4995834934258772\n",
      "12000/49000 loss: 0.412224860131416\n",
      "18000/49000 loss: 0.49801143446882123\n",
      "24000/49000 loss: 0.4171619639955591\n",
      "30000/49000 loss: 0.46823265299086625\n",
      "36000/49000 loss: 0.45521913033551514\n",
      "42000/49000 loss: 0.4492869744969823\n",
      "48000/49000 loss: 0.4599865206466238\n",
      "epoch 12: valid acc = 0.839, new learning rate = 0.00027018004383131826\n",
      "6000/49000 loss: 0.4549132792964324\n",
      "12000/49000 loss: 0.43300266018273603\n",
      "18000/49000 loss: 0.42132054712196454\n",
      "24000/49000 loss: 0.463007125586461\n",
      "30000/49000 loss: 0.46728243958027504\n",
      "36000/49000 loss: 0.42979970227099373\n",
      "42000/49000 loss: 0.41311318499126787\n",
      "48000/49000 loss: 0.4124248866040535\n",
      "epoch 13: valid acc = 0.833, new learning rate = 0.00025667104163975234\n",
      "6000/49000 loss: 0.40664311057414193\n",
      "12000/49000 loss: 0.45082235169578816\n",
      "18000/49000 loss: 0.3748170437117824\n",
      "24000/49000 loss: 0.46034370405570774\n",
      "30000/49000 loss: 0.3758968957211796\n",
      "36000/49000 loss: 0.47474384835325784\n",
      "42000/49000 loss: 0.440879027153031\n",
      "48000/49000 loss: 0.47414613625182406\n",
      "epoch 14: valid acc = 0.844, new learning rate = 0.00024383748955776472\n",
      "6000/49000 loss: 0.41295896452570946\n",
      "12000/49000 loss: 0.3901035566444702\n",
      "18000/49000 loss: 0.48086288019509027\n",
      "24000/49000 loss: 0.48533873512698067\n",
      "30000/49000 loss: 0.46110513511192924\n",
      "36000/49000 loss: 0.49627635224963934\n",
      "42000/49000 loss: 0.4615737772250931\n",
      "48000/49000 loss: 0.5031136924023706\n",
      "epoch 15: valid acc = 0.841, new learning rate = 0.00023164561507987649\n",
      "6000/49000 loss: 0.425262374180128\n",
      "12000/49000 loss: 0.4218404875515314\n",
      "18000/49000 loss: 0.39572597639110735\n",
      "24000/49000 loss: 0.4150536674094242\n",
      "30000/49000 loss: 0.36133892409249535\n",
      "36000/49000 loss: 0.4774229450051932\n",
      "42000/49000 loss: 0.3977498787044714\n",
      "48000/49000 loss: 0.4438114528826263\n",
      "epoch 16: valid acc = 0.842, new learning rate = 0.00022006333432588265\n",
      "6000/49000 loss: 0.4166774577471893\n",
      "12000/49000 loss: 0.3953102365688437\n",
      "18000/49000 loss: 0.40103260977518906\n",
      "24000/49000 loss: 0.4338810439398783\n",
      "30000/49000 loss: 0.36438876608269655\n",
      "36000/49000 loss: 0.3680203843064986\n",
      "42000/49000 loss: 0.43973307829727515\n",
      "48000/49000 loss: 0.41031110766743617\n",
      "epoch 17: valid acc = 0.846, new learning rate = 0.00020906016760958852\n",
      "6000/49000 loss: 0.4112644233043511\n",
      "12000/49000 loss: 0.42223179775299385\n",
      "18000/49000 loss: 0.4414800995735232\n",
      "24000/49000 loss: 0.38855936946361563\n",
      "30000/49000 loss: 0.4379556881595848\n",
      "36000/49000 loss: 0.4233851195941448\n",
      "42000/49000 loss: 0.39020916007576817\n",
      "48000/49000 loss: 0.4041157988087888\n",
      "epoch 18: valid acc = 0.844, new learning rate = 0.00019860715922910907\n",
      "6000/49000 loss: 0.40955491201045746\n",
      "12000/49000 loss: 0.40989226332167406\n",
      "18000/49000 loss: 0.4189025835486145\n",
      "24000/49000 loss: 0.4136690519259576\n",
      "30000/49000 loss: 0.44945918385978006\n",
      "36000/49000 loss: 0.34151891386244093\n",
      "42000/49000 loss: 0.38968358428151356\n",
      "48000/49000 loss: 0.37462076368141206\n",
      "epoch 19: valid acc = 0.857, new learning rate = 0.0001886768012676536\n",
      "6000/49000 loss: 0.354416366090344\n",
      "12000/49000 loss: 0.3680008295424404\n",
      "18000/49000 loss: 0.39358374382092376\n",
      "24000/49000 loss: 0.4378896531045033\n",
      "30000/49000 loss: 0.450199031894069\n",
      "36000/49000 loss: 0.43875161464557644\n",
      "42000/49000 loss: 0.4459199842846548\n",
      "48000/49000 loss: 0.3933965325273396\n",
      "epoch 20: valid acc = 0.857, new learning rate = 0.0001792429612042709\n",
      "6000/49000 loss: 0.330133261483296\n",
      "12000/49000 loss: 0.43221060549601187\n",
      "18000/49000 loss: 0.4281013058355734\n",
      "24000/49000 loss: 0.403868066145615\n",
      "30000/49000 loss: 0.3864348325728439\n",
      "36000/49000 loss: 0.4619735035696419\n",
      "42000/49000 loss: 0.3884404300784991\n",
      "48000/49000 loss: 0.4653981969255445\n",
      "epoch 21: valid acc = 0.856, new learning rate = 0.00017028081314405735\n",
      "6000/49000 loss: 0.3875574969502807\n",
      "12000/49000 loss: 0.457514106264027\n",
      "18000/49000 loss: 0.37672570990016574\n",
      "24000/49000 loss: 0.37756266913716957\n",
      "30000/49000 loss: 0.41360408737727494\n",
      "36000/49000 loss: 0.44709003013146165\n",
      "42000/49000 loss: 0.42275671736951587\n",
      "48000/49000 loss: 0.36251467748961735\n",
      "epoch 22: valid acc = 0.86, new learning rate = 0.00016176677248685447\n",
      "6000/49000 loss: 0.4449479647918689\n",
      "12000/49000 loss: 0.385807393664438\n",
      "18000/49000 loss: 0.42625444999862877\n",
      "24000/49000 loss: 0.37447558623831606\n",
      "30000/49000 loss: 0.400206882466714\n",
      "36000/49000 loss: 0.39170615973650985\n",
      "42000/49000 loss: 0.3481936134634958\n",
      "48000/49000 loss: 0.39699800958830805\n",
      "epoch 23: valid acc = 0.859, new learning rate = 0.00015367843386251173\n",
      "6000/49000 loss: 0.3708870973239073\n",
      "12000/49000 loss: 0.4389039827182751\n",
      "18000/49000 loss: 0.4006093727350867\n",
      "24000/49000 loss: 0.42002067622572514\n",
      "30000/49000 loss: 0.44217280160403316\n",
      "36000/49000 loss: 0.3711090991929381\n",
      "42000/49000 loss: 0.3599553216114726\n",
      "48000/49000 loss: 0.3990492578419098\n",
      "epoch 24: valid acc = 0.853, new learning rate = 0.00014599451216938612\n",
      "6000/49000 loss: 0.41391119734491605\n",
      "12000/49000 loss: 0.47696036013264315\n",
      "18000/49000 loss: 0.43651476903068764\n",
      "24000/49000 loss: 0.3761332943248862\n",
      "30000/49000 loss: 0.40642975940996484\n",
      "36000/49000 loss: 0.3968215871972535\n",
      "42000/49000 loss: 0.43429180196820116\n",
      "48000/49000 loss: 0.3983500604393642\n",
      "epoch 25: valid acc = 0.86, new learning rate = 0.00013869478656091682\n",
      "6000/49000 loss: 0.3854708752288761\n",
      "12000/49000 loss: 0.3774386302978794\n",
      "18000/49000 loss: 0.421649541066465\n",
      "24000/49000 loss: 0.4474424004397265\n",
      "30000/49000 loss: 0.4037191070400521\n",
      "36000/49000 loss: 0.37664832823464844\n",
      "42000/49000 loss: 0.36901498675919514\n",
      "48000/49000 loss: 0.45803772796174697\n",
      "epoch 26: valid acc = 0.852, new learning rate = 0.00013176004723287096\n",
      "6000/49000 loss: 0.3770672950016379\n",
      "12000/49000 loss: 0.3920287822750804\n",
      "18000/49000 loss: 0.38154728935669735\n",
      "24000/49000 loss: 0.3699805787107487\n",
      "30000/49000 loss: 0.42417504756503166\n",
      "36000/49000 loss: 0.4194748370025662\n",
      "42000/49000 loss: 0.3908539535779927\n",
      "48000/49000 loss: 0.4338430327719147\n",
      "epoch 27: valid acc = 0.857, new learning rate = 0.0001251720448712274\n",
      "6000/49000 loss: 0.37850957068787716\n",
      "12000/49000 loss: 0.3768620351198263\n",
      "18000/49000 loss: 0.3943304035980575\n",
      "24000/49000 loss: 0.38757312966343055\n",
      "30000/49000 loss: 0.4164668012067098\n",
      "36000/49000 loss: 0.3883263650771181\n",
      "42000/49000 loss: 0.40107226760546444\n",
      "48000/49000 loss: 0.4177030089222257\n",
      "epoch 28: valid acc = 0.856, new learning rate = 0.00011891344262766602\n",
      "6000/49000 loss: 0.38329039934439074\n",
      "12000/49000 loss: 0.3805493053511776\n",
      "18000/49000 loss: 0.42123636804637277\n",
      "24000/49000 loss: 0.41161112839438624\n",
      "30000/49000 loss: 0.41594800558149103\n",
      "36000/49000 loss: 0.4189500868599301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/49000 loss: 0.403775205488294\n",
      "48000/49000 loss: 0.38420301543546503\n",
      "epoch 29: valid acc = 0.858, new learning rate = 0.00011296777049628272\n",
      "6000/49000 loss: 0.3807804289868305\n",
      "12000/49000 loss: 0.4035245478615944\n",
      "18000/49000 loss: 0.4311681643439593\n",
      "24000/49000 loss: 0.46282636099974817\n",
      "30000/49000 loss: 0.3564879825484464\n",
      "36000/49000 loss: 0.37105162475690795\n",
      "42000/49000 loss: 0.4014964958531836\n",
      "48000/49000 loss: 0.3533662849999624\n",
      "epoch 30: valid acc = 0.854, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.8628979591836735\n",
      "test acc: 0.854\n",
      "test acc: 0.8437\n",
      "number of batches for training: 81\n",
      "epoch 1: valid acc = 0.448, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.675, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.74, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.754, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.793, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.803, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.807, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.818, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.827, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.828, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.838, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.835, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.839, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.838, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.843, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.852, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.847, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.843, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.852, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.853, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.855, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.854, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.854, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.859, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.863, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.863, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.863, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.861, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.861, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.859, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.8633877551020408\n",
      "test acc: 0.859\n",
      "test acc: 0.8455\n",
      "number of batches for training: 81\n",
      "6000/49000 loss: 2.6569118549283126\n",
      "12000/49000 loss: 2.8625364721723985\n",
      "18000/49000 loss: 2.655413768146496\n",
      "24000/49000 loss: 2.477945192271611\n",
      "30000/49000 loss: 2.300779060482929\n",
      "36000/49000 loss: 2.1743522593740274\n",
      "42000/49000 loss: 1.8794342408195965\n",
      "48000/49000 loss: 1.6359146718521673\n",
      "epoch 1: valid acc = 0.499, new learning rate = 0.000475\n",
      "6000/49000 loss: 1.261055299738341\n",
      "12000/49000 loss: 1.2571192858687623\n",
      "18000/49000 loss: 1.2359919046509964\n",
      "24000/49000 loss: 1.179104409158067\n",
      "30000/49000 loss: 1.1737166575217894\n",
      "36000/49000 loss: 1.1267778749913095\n",
      "42000/49000 loss: 1.0326151259992407\n",
      "48000/49000 loss: 1.0101831029496107\n",
      "epoch 2: valid acc = 0.661, new learning rate = 0.00045125\n",
      "6000/49000 loss: 0.9960528451985622\n",
      "12000/49000 loss: 0.98232908313758\n",
      "18000/49000 loss: 0.8850239716917773\n",
      "24000/49000 loss: 0.8960024914531035\n",
      "30000/49000 loss: 0.8423331342960038\n",
      "36000/49000 loss: 0.8483955515700049\n",
      "42000/49000 loss: 0.8079926166179072\n",
      "48000/49000 loss: 0.7166414497243705\n",
      "epoch 3: valid acc = 0.735, new learning rate = 0.0004286875\n",
      "6000/49000 loss: 0.729826087054627\n",
      "12000/49000 loss: 0.7054494998065975\n",
      "18000/49000 loss: 0.6349494628275769\n",
      "24000/49000 loss: 0.8077381655896195\n",
      "30000/49000 loss: 0.6623705600236731\n",
      "36000/49000 loss: 0.5970854896849642\n",
      "42000/49000 loss: 0.6574882143983245\n",
      "48000/49000 loss: 0.6341168166016419\n",
      "epoch 4: valid acc = 0.758, new learning rate = 0.00040725312499999993\n",
      "6000/49000 loss: 0.6473826328053938\n",
      "12000/49000 loss: 0.6135662481947942\n",
      "18000/49000 loss: 0.6393745440949653\n",
      "24000/49000 loss: 0.5825214070719634\n",
      "30000/49000 loss: 0.6931958588824317\n",
      "36000/49000 loss: 0.6116891536258698\n",
      "42000/49000 loss: 0.59655122018597\n",
      "48000/49000 loss: 0.5042467121019909\n",
      "epoch 5: valid acc = 0.785, new learning rate = 0.0003868904687499999\n",
      "6000/49000 loss: 0.6062160376220941\n",
      "12000/49000 loss: 0.5954988636576625\n",
      "18000/49000 loss: 0.5552782532449708\n",
      "24000/49000 loss: 0.6070931088419581\n",
      "30000/49000 loss: 0.5281586452668307\n",
      "36000/49000 loss: 0.5017989019656892\n",
      "42000/49000 loss: 0.5190305673533682\n",
      "48000/49000 loss: 0.5751997752694634\n",
      "epoch 6: valid acc = 0.795, new learning rate = 0.0003675459453124999\n",
      "6000/49000 loss: 0.566969157608117\n",
      "12000/49000 loss: 0.5569423110354806\n",
      "18000/49000 loss: 0.5134361064398957\n",
      "24000/49000 loss: 0.4776824976789607\n",
      "30000/49000 loss: 0.5133380291702719\n",
      "36000/49000 loss: 0.49957063689432973\n",
      "42000/49000 loss: 0.49206810046599353\n",
      "48000/49000 loss: 0.5064478551932743\n",
      "epoch 7: valid acc = 0.808, new learning rate = 0.00034916864804687486\n",
      "6000/49000 loss: 0.5419834052320321\n",
      "12000/49000 loss: 0.545244835890145\n",
      "18000/49000 loss: 0.5517881858619996\n",
      "24000/49000 loss: 0.5060658984108366\n",
      "30000/49000 loss: 0.551368320308118\n",
      "36000/49000 loss: 0.47032459696009793\n",
      "42000/49000 loss: 0.5104164472956747\n",
      "48000/49000 loss: 0.5095114756063047\n",
      "epoch 8: valid acc = 0.81, new learning rate = 0.0003317102156445311\n",
      "6000/49000 loss: 0.546385128165158\n",
      "12000/49000 loss: 0.5273243819000544\n",
      "18000/49000 loss: 0.4974335097168922\n",
      "24000/49000 loss: 0.4981042565440753\n",
      "30000/49000 loss: 0.5259776777367808\n",
      "36000/49000 loss: 0.4624569412011602\n",
      "42000/49000 loss: 0.5426523145671663\n",
      "48000/49000 loss: 0.44394494221928843\n",
      "epoch 9: valid acc = 0.819, new learning rate = 0.0003151247048623045\n",
      "6000/49000 loss: 0.42502975925124437\n",
      "12000/49000 loss: 0.48350123638976955\n",
      "18000/49000 loss: 0.3910507036381738\n",
      "24000/49000 loss: 0.4960064013764599\n",
      "30000/49000 loss: 0.5043282640389024\n",
      "36000/49000 loss: 0.4593902118360111\n",
      "42000/49000 loss: 0.45761221076357683\n",
      "48000/49000 loss: 0.4543348624501385\n",
      "epoch 10: valid acc = 0.825, new learning rate = 0.00029936846961918924\n",
      "6000/49000 loss: 0.45618060131127236\n",
      "12000/49000 loss: 0.44153095249603014\n",
      "18000/49000 loss: 0.460578247432323\n",
      "24000/49000 loss: 0.517019808809555\n",
      "30000/49000 loss: 0.4720772434320764\n",
      "36000/49000 loss: 0.48084909178020724\n",
      "42000/49000 loss: 0.4735333741243\n",
      "48000/49000 loss: 0.4459844337989055\n",
      "epoch 11: valid acc = 0.829, new learning rate = 0.00028440004613822977\n",
      "6000/49000 loss: 0.43148292186315645\n",
      "12000/49000 loss: 0.4953186878937426\n",
      "18000/49000 loss: 0.48398215709841663\n",
      "24000/49000 loss: 0.4595338215631063\n",
      "30000/49000 loss: 0.4135455026680594\n",
      "36000/49000 loss: 0.5135909815485135\n",
      "42000/49000 loss: 0.4732015332710405\n",
      "48000/49000 loss: 0.43447028786244224\n",
      "epoch 12: valid acc = 0.834, new learning rate = 0.00027018004383131826\n",
      "6000/49000 loss: 0.4107015581835471\n",
      "12000/49000 loss: 0.4566166245654948\n",
      "18000/49000 loss: 0.4175513971879892\n",
      "24000/49000 loss: 0.4211158515070983\n",
      "30000/49000 loss: 0.4524599486033948\n",
      "36000/49000 loss: 0.4064362900491184\n",
      "42000/49000 loss: 0.44611429659439933\n",
      "48000/49000 loss: 0.4269722480337009\n",
      "epoch 13: valid acc = 0.836, new learning rate = 0.00025667104163975234\n",
      "6000/49000 loss: 0.4782649208380782\n",
      "12000/49000 loss: 0.4054390634182094\n",
      "18000/49000 loss: 0.4719020637243955\n",
      "24000/49000 loss: 0.4504276642146257\n",
      "30000/49000 loss: 0.43202863692987425\n",
      "36000/49000 loss: 0.3790016428352875\n",
      "42000/49000 loss: 0.4641072787247551\n",
      "48000/49000 loss: 0.43862138036554216\n",
      "epoch 14: valid acc = 0.839, new learning rate = 0.00024383748955776472\n",
      "6000/49000 loss: 0.43696042086029696\n",
      "12000/49000 loss: 0.4602103343821171\n",
      "18000/49000 loss: 0.441437172493929\n",
      "24000/49000 loss: 0.38444480338204173\n",
      "30000/49000 loss: 0.38542421443865577\n",
      "36000/49000 loss: 0.4296703062433175\n",
      "42000/49000 loss: 0.43648986050069266\n",
      "48000/49000 loss: 0.42859030251725805\n",
      "epoch 15: valid acc = 0.841, new learning rate = 0.00023164561507987649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/49000 loss: 0.40405084845586065\n",
      "12000/49000 loss: 0.4449786503376794\n",
      "18000/49000 loss: 0.4183366683685074\n",
      "24000/49000 loss: 0.43496121686843214\n",
      "30000/49000 loss: 0.430749279688728\n",
      "36000/49000 loss: 0.4352247402724872\n",
      "42000/49000 loss: 0.38311947502638566\n",
      "48000/49000 loss: 0.41895745861396017\n",
      "epoch 16: valid acc = 0.844, new learning rate = 0.00022006333432588265\n",
      "6000/49000 loss: 0.4029974349904762\n",
      "12000/49000 loss: 0.4448518551838603\n",
      "18000/49000 loss: 0.4061632573430686\n",
      "24000/49000 loss: 0.4421744036040723\n",
      "30000/49000 loss: 0.38162771262464007\n",
      "36000/49000 loss: 0.4222093431015567\n",
      "42000/49000 loss: 0.5481860761918461\n",
      "48000/49000 loss: 0.43252467484928375\n",
      "epoch 17: valid acc = 0.845, new learning rate = 0.00020906016760958852\n",
      "6000/49000 loss: 0.4685799758085926\n",
      "12000/49000 loss: 0.4343848459603845\n",
      "18000/49000 loss: 0.42287137456795554\n",
      "24000/49000 loss: 0.41798151501344993\n",
      "30000/49000 loss: 0.43763277557522184\n",
      "36000/49000 loss: 0.4275251735934629\n",
      "42000/49000 loss: 0.4392920412025346\n",
      "48000/49000 loss: 0.41368615089879335\n",
      "epoch 18: valid acc = 0.85, new learning rate = 0.00019860715922910907\n",
      "6000/49000 loss: 0.45898975290869864\n",
      "12000/49000 loss: 0.38647596984463933\n",
      "18000/49000 loss: 0.43673954806394905\n",
      "24000/49000 loss: 0.44393545318633376\n",
      "30000/49000 loss: 0.4163308556195749\n",
      "36000/49000 loss: 0.43596253008829455\n",
      "42000/49000 loss: 0.4099868883225663\n",
      "48000/49000 loss: 0.4195087038194308\n",
      "epoch 19: valid acc = 0.854, new learning rate = 0.0001886768012676536\n",
      "6000/49000 loss: 0.46250195166506164\n",
      "12000/49000 loss: 0.3897678094567633\n",
      "18000/49000 loss: 0.39016051189476114\n",
      "24000/49000 loss: 0.4375420826899365\n",
      "30000/49000 loss: 0.35867553617340814\n",
      "36000/49000 loss: 0.43585565687735556\n",
      "42000/49000 loss: 0.4065348759675985\n",
      "48000/49000 loss: 0.47872286724813784\n",
      "epoch 20: valid acc = 0.853, new learning rate = 0.0001792429612042709\n",
      "6000/49000 loss: 0.3883266371281335\n",
      "12000/49000 loss: 0.41772456293181837\n",
      "18000/49000 loss: 0.3866284705338557\n",
      "24000/49000 loss: 0.4450470899176682\n",
      "30000/49000 loss: 0.35303353191258824\n",
      "36000/49000 loss: 0.43500710652937985\n",
      "42000/49000 loss: 0.42374525026395726\n",
      "48000/49000 loss: 0.4313590881241736\n",
      "epoch 21: valid acc = 0.855, new learning rate = 0.00017028081314405735\n",
      "6000/49000 loss: 0.34274280980303573\n",
      "12000/49000 loss: 0.4127489162851501\n",
      "18000/49000 loss: 0.3679059704022015\n",
      "24000/49000 loss: 0.4374856130774947\n",
      "30000/49000 loss: 0.40138512507984586\n",
      "36000/49000 loss: 0.4166018849845014\n",
      "42000/49000 loss: 0.40345263125450354\n",
      "48000/49000 loss: 0.42557221978259774\n",
      "epoch 22: valid acc = 0.852, new learning rate = 0.00016176677248685447\n",
      "6000/49000 loss: 0.42060684943501786\n",
      "12000/49000 loss: 0.3515232448518163\n",
      "18000/49000 loss: 0.4324124137682748\n",
      "24000/49000 loss: 0.4726576346339609\n",
      "30000/49000 loss: 0.4162331094860164\n",
      "36000/49000 loss: 0.4000437282745938\n",
      "42000/49000 loss: 0.3696606497690103\n",
      "48000/49000 loss: 0.3967557184757285\n",
      "epoch 23: valid acc = 0.856, new learning rate = 0.00015367843386251173\n",
      "6000/49000 loss: 0.43272034956530786\n",
      "12000/49000 loss: 0.4337880513580699\n",
      "18000/49000 loss: 0.39528626784764337\n",
      "24000/49000 loss: 0.43542444667311186\n",
      "30000/49000 loss: 0.41094382399725204\n",
      "36000/49000 loss: 0.46815944416955296\n",
      "42000/49000 loss: 0.4071390886183938\n",
      "48000/49000 loss: 0.43325188489074246\n",
      "epoch 24: valid acc = 0.855, new learning rate = 0.00014599451216938612\n",
      "6000/49000 loss: 0.44121360149635647\n",
      "12000/49000 loss: 0.38729160773804616\n",
      "18000/49000 loss: 0.3983539586302325\n",
      "24000/49000 loss: 0.44604196192138057\n",
      "30000/49000 loss: 0.42269214835159274\n",
      "36000/49000 loss: 0.41050724247337755\n",
      "42000/49000 loss: 0.438907707798045\n",
      "48000/49000 loss: 0.36368253863542277\n",
      "epoch 25: valid acc = 0.855, new learning rate = 0.00013869478656091682\n",
      "6000/49000 loss: 0.37013476358213804\n",
      "12000/49000 loss: 0.4219204092108686\n",
      "18000/49000 loss: 0.38829515825193467\n",
      "24000/49000 loss: 0.40305701636011687\n",
      "30000/49000 loss: 0.48628170439457297\n",
      "36000/49000 loss: 0.3728194342735161\n",
      "42000/49000 loss: 0.3800914507857214\n",
      "48000/49000 loss: 0.4251046182254233\n",
      "epoch 26: valid acc = 0.855, new learning rate = 0.00013176004723287096\n",
      "6000/49000 loss: 0.443956868948842\n",
      "12000/49000 loss: 0.3842013742259124\n",
      "18000/49000 loss: 0.35198930375030457\n",
      "24000/49000 loss: 0.36626067066978696\n",
      "30000/49000 loss: 0.37626858357029613\n",
      "36000/49000 loss: 0.3929828680251891\n",
      "42000/49000 loss: 0.3811789253083418\n",
      "48000/49000 loss: 0.3912547717220967\n",
      "epoch 27: valid acc = 0.86, new learning rate = 0.0001251720448712274\n",
      "6000/49000 loss: 0.439546784684217\n",
      "12000/49000 loss: 0.3825562787356204\n",
      "18000/49000 loss: 0.34430152448847895\n",
      "24000/49000 loss: 0.3914193567558875\n",
      "30000/49000 loss: 0.3562830921443224\n",
      "36000/49000 loss: 0.3847238250103941\n",
      "42000/49000 loss: 0.4296341894223671\n",
      "48000/49000 loss: 0.41125884669083107\n",
      "epoch 28: valid acc = 0.862, new learning rate = 0.00011891344262766602\n",
      "6000/49000 loss: 0.4367015662169211\n",
      "12000/49000 loss: 0.4085725654794294\n",
      "18000/49000 loss: 0.4405540174970761\n",
      "24000/49000 loss: 0.3639517715469667\n",
      "30000/49000 loss: 0.3942633415685729\n",
      "36000/49000 loss: 0.3892185368597338\n",
      "42000/49000 loss: 0.3505479061517225\n",
      "48000/49000 loss: 0.4184211663177107\n",
      "epoch 29: valid acc = 0.862, new learning rate = 0.00011296777049628272\n",
      "6000/49000 loss: 0.38766621462359613\n",
      "12000/49000 loss: 0.37705672493640807\n",
      "18000/49000 loss: 0.3614631735362977\n",
      "24000/49000 loss: 0.39083641301194416\n",
      "30000/49000 loss: 0.342787215322509\n",
      "36000/49000 loss: 0.38660286923999543\n",
      "42000/49000 loss: 0.3421046888719199\n",
      "48000/49000 loss: 0.4074048936262757\n",
      "epoch 30: valid acc = 0.86, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.8627959183673469\n",
      "test acc: 0.86\n",
      "test acc: 0.8436\n",
      "number of batches for training: 81\n",
      "epoch 1: valid acc = 0.518, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.66, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.731, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.758, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.776, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.797, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.803, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.811, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.824, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.832, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.831, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.834, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.839, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.845, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.841, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.849, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.848, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.851, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.854, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.85, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.855, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.859, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.854, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.855, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.857, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.862, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.862, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.862, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.862, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.857, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.862938775510204\n",
      "test acc: 0.857\n",
      "test acc: 0.8442\n",
      "number of batches for training: 81\n",
      "6000/49000 loss: 2.958004657328217\n",
      "12000/49000 loss: 2.640381644355543\n",
      "18000/49000 loss: 2.5700534470627607\n",
      "24000/49000 loss: 2.4364993562244717\n",
      "30000/49000 loss: 2.1586615997885192\n",
      "36000/49000 loss: 1.9852644937895512\n",
      "42000/49000 loss: 1.7927974379887104\n",
      "48000/49000 loss: 1.4301583068887065\n",
      "epoch 1: valid acc = 0.505, new learning rate = 0.000475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/49000 loss: 1.4128371999557032\n",
      "12000/49000 loss: 1.1845203311667292\n",
      "18000/49000 loss: 1.2025678010028364\n",
      "24000/49000 loss: 1.2091261371793698\n",
      "30000/49000 loss: 1.1149770710683524\n",
      "36000/49000 loss: 1.0738050121383382\n",
      "42000/49000 loss: 1.026858155208672\n",
      "48000/49000 loss: 1.0553126738081182\n",
      "epoch 2: valid acc = 0.655, new learning rate = 0.00045125\n",
      "6000/49000 loss: 1.0221489809509692\n",
      "12000/49000 loss: 0.9429933613894692\n",
      "18000/49000 loss: 0.9115460925332551\n",
      "24000/49000 loss: 0.9255175684776759\n",
      "30000/49000 loss: 0.8294899050151385\n",
      "36000/49000 loss: 0.7965335868566898\n",
      "42000/49000 loss: 0.8059512610229211\n",
      "48000/49000 loss: 0.777629251120899\n",
      "epoch 3: valid acc = 0.731, new learning rate = 0.0004286875\n",
      "6000/49000 loss: 0.7137741643227346\n",
      "12000/49000 loss: 0.6885533620133308\n",
      "18000/49000 loss: 0.7058730321978678\n",
      "24000/49000 loss: 0.6213972322240026\n",
      "30000/49000 loss: 0.683719115821379\n",
      "36000/49000 loss: 0.6224664442205928\n",
      "42000/49000 loss: 0.6765096350136729\n",
      "48000/49000 loss: 0.6121597227924396\n",
      "epoch 4: valid acc = 0.768, new learning rate = 0.00040725312499999993\n",
      "6000/49000 loss: 0.6910741718302548\n",
      "12000/49000 loss: 0.6703179456176255\n",
      "18000/49000 loss: 0.6297885064292599\n",
      "24000/49000 loss: 0.5706151364495049\n",
      "30000/49000 loss: 0.672288716628159\n",
      "36000/49000 loss: 0.6449431011829908\n",
      "42000/49000 loss: 0.6424041172181528\n",
      "48000/49000 loss: 0.5460066389421552\n",
      "epoch 5: valid acc = 0.79, new learning rate = 0.0003868904687499999\n",
      "6000/49000 loss: 0.544412973205497\n",
      "12000/49000 loss: 0.6422012740520207\n",
      "18000/49000 loss: 0.6029190425591427\n",
      "24000/49000 loss: 0.5489283712804743\n",
      "30000/49000 loss: 0.5754353678342389\n",
      "36000/49000 loss: 0.5415670245216471\n",
      "42000/49000 loss: 0.5741606355534297\n",
      "48000/49000 loss: 0.5243280673960643\n",
      "epoch 6: valid acc = 0.796, new learning rate = 0.0003675459453124999\n",
      "6000/49000 loss: 0.5270781814816323\n",
      "12000/49000 loss: 0.5226807096273705\n",
      "18000/49000 loss: 0.49791868531266453\n",
      "24000/49000 loss: 0.531998494955742\n",
      "30000/49000 loss: 0.5459321257295345\n",
      "36000/49000 loss: 0.4966153651366129\n",
      "42000/49000 loss: 0.482522768307726\n",
      "48000/49000 loss: 0.5537865670971601\n",
      "epoch 7: valid acc = 0.809, new learning rate = 0.00034916864804687486\n",
      "6000/49000 loss: 0.5110604717101698\n",
      "12000/49000 loss: 0.534663831118971\n",
      "18000/49000 loss: 0.5315975291063527\n",
      "24000/49000 loss: 0.45614897026244694\n",
      "30000/49000 loss: 0.5230230431339521\n",
      "36000/49000 loss: 0.5402900790219831\n",
      "42000/49000 loss: 0.5351507583615039\n",
      "48000/49000 loss: 0.49244457002404474\n",
      "epoch 8: valid acc = 0.813, new learning rate = 0.0003317102156445311\n",
      "6000/49000 loss: 0.4574780909866078\n",
      "12000/49000 loss: 0.481664392739271\n",
      "18000/49000 loss: 0.5246039948447311\n",
      "24000/49000 loss: 0.5049647842325559\n",
      "30000/49000 loss: 0.5243398358558536\n",
      "36000/49000 loss: 0.5476181559224863\n",
      "42000/49000 loss: 0.44251046652514275\n",
      "48000/49000 loss: 0.4230617813952669\n",
      "epoch 9: valid acc = 0.821, new learning rate = 0.0003151247048623045\n",
      "6000/49000 loss: 0.5303388298704691\n",
      "12000/49000 loss: 0.48740923097179295\n",
      "18000/49000 loss: 0.5352964794213243\n",
      "24000/49000 loss: 0.45710327267251116\n",
      "30000/49000 loss: 0.42431587328090936\n",
      "36000/49000 loss: 0.45423302073989064\n",
      "42000/49000 loss: 0.48686926701039973\n",
      "48000/49000 loss: 0.5154520988872374\n",
      "epoch 10: valid acc = 0.826, new learning rate = 0.00029936846961918924\n",
      "6000/49000 loss: 0.4768186613368525\n",
      "12000/49000 loss: 0.4587280545458523\n",
      "18000/49000 loss: 0.42321201169817635\n",
      "24000/49000 loss: 0.4421060606786346\n",
      "30000/49000 loss: 0.47146998140814245\n",
      "36000/49000 loss: 0.45074095779808393\n",
      "42000/49000 loss: 0.45392403653526325\n",
      "48000/49000 loss: 0.4596805409867128\n",
      "epoch 11: valid acc = 0.827, new learning rate = 0.00028440004613822977\n",
      "6000/49000 loss: 0.4210277791302467\n",
      "12000/49000 loss: 0.4531914401050537\n",
      "18000/49000 loss: 0.41572713454887494\n",
      "24000/49000 loss: 0.3987661534025848\n",
      "30000/49000 loss: 0.4449266463407542\n",
      "36000/49000 loss: 0.42923033248518067\n",
      "42000/49000 loss: 0.4310441948026579\n",
      "48000/49000 loss: 0.4494251693513354\n",
      "epoch 12: valid acc = 0.838, new learning rate = 0.00027018004383131826\n",
      "6000/49000 loss: 0.3947821856477857\n",
      "12000/49000 loss: 0.4704794303109037\n",
      "18000/49000 loss: 0.45068417973692454\n",
      "24000/49000 loss: 0.4758909731309605\n",
      "30000/49000 loss: 0.444530924181321\n",
      "36000/49000 loss: 0.4612421849128075\n",
      "42000/49000 loss: 0.4313497848982936\n",
      "48000/49000 loss: 0.39451617925868393\n",
      "epoch 13: valid acc = 0.843, new learning rate = 0.00025667104163975234\n",
      "6000/49000 loss: 0.41329548128251326\n",
      "12000/49000 loss: 0.411903900610436\n",
      "18000/49000 loss: 0.4513745665995896\n",
      "24000/49000 loss: 0.5043187775293387\n",
      "30000/49000 loss: 0.49904558342541544\n",
      "36000/49000 loss: 0.43275289114921356\n",
      "42000/49000 loss: 0.3813824470443475\n",
      "48000/49000 loss: 0.4594483579271364\n",
      "epoch 14: valid acc = 0.84, new learning rate = 0.00024383748955776472\n",
      "6000/49000 loss: 0.407575372897311\n",
      "12000/49000 loss: 0.42965258522230887\n",
      "18000/49000 loss: 0.46576874847303124\n",
      "24000/49000 loss: 0.4619607731243963\n",
      "30000/49000 loss: 0.43209960872255454\n",
      "36000/49000 loss: 0.4547395869328529\n",
      "42000/49000 loss: 0.43874139889866864\n",
      "48000/49000 loss: 0.42215035695727127\n",
      "epoch 15: valid acc = 0.841, new learning rate = 0.00023164561507987649\n",
      "6000/49000 loss: 0.3673701535955617\n",
      "12000/49000 loss: 0.41640585428424326\n",
      "18000/49000 loss: 0.47240917640929425\n",
      "24000/49000 loss: 0.41560891568658054\n",
      "30000/49000 loss: 0.523146784539406\n",
      "36000/49000 loss: 0.44032814511765966\n",
      "42000/49000 loss: 0.460608150595133\n",
      "48000/49000 loss: 0.40367645489229464\n",
      "epoch 16: valid acc = 0.848, new learning rate = 0.00022006333432588265\n",
      "6000/49000 loss: 0.5354697458082623\n",
      "12000/49000 loss: 0.3799243155456927\n",
      "18000/49000 loss: 0.43431959382666024\n",
      "24000/49000 loss: 0.4472509905666396\n",
      "30000/49000 loss: 0.44221668051849533\n",
      "36000/49000 loss: 0.3740558831202562\n",
      "42000/49000 loss: 0.4069075892388519\n",
      "48000/49000 loss: 0.40928174738010625\n",
      "epoch 17: valid acc = 0.847, new learning rate = 0.00020906016760958852\n",
      "6000/49000 loss: 0.46765022287716584\n",
      "12000/49000 loss: 0.40659827007301186\n",
      "18000/49000 loss: 0.41014236569464463\n",
      "24000/49000 loss: 0.4330059517131279\n",
      "30000/49000 loss: 0.4056702510989621\n",
      "36000/49000 loss: 0.3716717135192744\n",
      "42000/49000 loss: 0.3703740027242859\n",
      "48000/49000 loss: 0.3826768021325696\n",
      "epoch 18: valid acc = 0.85, new learning rate = 0.00019860715922910907\n",
      "6000/49000 loss: 0.387870433850909\n",
      "12000/49000 loss: 0.40261758951804105\n",
      "18000/49000 loss: 0.42189070249544336\n",
      "24000/49000 loss: 0.37814605830761955\n",
      "30000/49000 loss: 0.4201672359948072\n",
      "36000/49000 loss: 0.4349797983069735\n",
      "42000/49000 loss: 0.4328118261893972\n",
      "48000/49000 loss: 0.43919426174890625\n",
      "epoch 19: valid acc = 0.855, new learning rate = 0.0001886768012676536\n",
      "6000/49000 loss: 0.42274153108069235\n",
      "12000/49000 loss: 0.4115230751921163\n",
      "18000/49000 loss: 0.45533776417337973\n",
      "24000/49000 loss: 0.3775438696511783\n",
      "30000/49000 loss: 0.41409344187418545\n",
      "36000/49000 loss: 0.3755946119730937\n",
      "42000/49000 loss: 0.41436960160508707\n",
      "48000/49000 loss: 0.40567137784203405\n",
      "epoch 20: valid acc = 0.855, new learning rate = 0.0001792429612042709\n",
      "6000/49000 loss: 0.4413052429934432\n",
      "12000/49000 loss: 0.45909895589742583\n",
      "18000/49000 loss: 0.3802242676414338\n",
      "24000/49000 loss: 0.4263892203458738\n",
      "30000/49000 loss: 0.4398528687188084\n",
      "36000/49000 loss: 0.41368948427481383\n",
      "42000/49000 loss: 0.38626473250267024\n",
      "48000/49000 loss: 0.4097682084320591\n",
      "epoch 21: valid acc = 0.853, new learning rate = 0.00017028081314405735\n",
      "6000/49000 loss: 0.43195661790247897\n",
      "12000/49000 loss: 0.39173780750050363\n",
      "18000/49000 loss: 0.4238537135697465\n",
      "24000/49000 loss: 0.4187388438238658\n",
      "30000/49000 loss: 0.3830044413890395\n",
      "36000/49000 loss: 0.4054006126139581\n",
      "42000/49000 loss: 0.37847126178650037\n",
      "48000/49000 loss: 0.4008115731515317\n",
      "epoch 22: valid acc = 0.854, new learning rate = 0.00016176677248685447\n",
      "6000/49000 loss: 0.4368054878177658\n",
      "12000/49000 loss: 0.41383363158681546\n",
      "18000/49000 loss: 0.37447698822645886\n",
      "24000/49000 loss: 0.32798309561025496\n",
      "30000/49000 loss: 0.4041594603624246\n",
      "36000/49000 loss: 0.38554260622790126\n",
      "42000/49000 loss: 0.48249668544096797\n",
      "48000/49000 loss: 0.4022437017314938\n",
      "epoch 23: valid acc = 0.854, new learning rate = 0.00015367843386251173\n",
      "6000/49000 loss: 0.428391047581069\n",
      "12000/49000 loss: 0.36782219411564804\n",
      "18000/49000 loss: 0.3724729662596141\n",
      "24000/49000 loss: 0.39533052621655074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000/49000 loss: 0.3376411747452805\n",
      "36000/49000 loss: 0.4133851388330173\n",
      "42000/49000 loss: 0.3906848621529525\n",
      "48000/49000 loss: 0.34778183412774805\n",
      "epoch 24: valid acc = 0.858, new learning rate = 0.00014599451216938612\n",
      "6000/49000 loss: 0.4074419953213807\n",
      "12000/49000 loss: 0.3886700327367204\n",
      "18000/49000 loss: 0.4245411144379931\n",
      "24000/49000 loss: 0.4439072438151041\n",
      "30000/49000 loss: 0.3944710393728727\n",
      "36000/49000 loss: 0.40586171620285666\n",
      "42000/49000 loss: 0.3889858640438691\n",
      "48000/49000 loss: 0.40235832442557934\n",
      "epoch 25: valid acc = 0.858, new learning rate = 0.00013869478656091682\n",
      "6000/49000 loss: 0.4327373162423697\n",
      "12000/49000 loss: 0.3402685798692926\n",
      "18000/49000 loss: 0.3587127325731779\n",
      "24000/49000 loss: 0.3823319084342092\n",
      "30000/49000 loss: 0.41646380280392564\n",
      "36000/49000 loss: 0.42097584138545\n",
      "42000/49000 loss: 0.42370682076418\n",
      "48000/49000 loss: 0.4273110750785361\n",
      "epoch 26: valid acc = 0.856, new learning rate = 0.00013176004723287096\n",
      "6000/49000 loss: 0.3491597661036948\n",
      "12000/49000 loss: 0.3855289458644551\n",
      "18000/49000 loss: 0.3899301408152121\n",
      "24000/49000 loss: 0.3737182013746788\n",
      "30000/49000 loss: 0.3579478779431403\n",
      "36000/49000 loss: 0.42285389372329296\n",
      "42000/49000 loss: 0.4065968918649336\n",
      "48000/49000 loss: 0.38407303355097056\n",
      "epoch 27: valid acc = 0.858, new learning rate = 0.0001251720448712274\n",
      "6000/49000 loss: 0.4120839080154951\n",
      "12000/49000 loss: 0.3487032018024144\n",
      "18000/49000 loss: 0.396212232951306\n",
      "24000/49000 loss: 0.38465755589003736\n",
      "30000/49000 loss: 0.46691240743668727\n",
      "36000/49000 loss: 0.40095271623141565\n",
      "42000/49000 loss: 0.4368353097098305\n",
      "48000/49000 loss: 0.4009848831045828\n",
      "epoch 28: valid acc = 0.86, new learning rate = 0.00011891344262766602\n",
      "6000/49000 loss: 0.38331293839684266\n",
      "12000/49000 loss: 0.3689294815440772\n",
      "18000/49000 loss: 0.4303185398646797\n",
      "24000/49000 loss: 0.31068236157013174\n",
      "30000/49000 loss: 0.34890576418745484\n",
      "36000/49000 loss: 0.40259417977523365\n",
      "42000/49000 loss: 0.4288436900546478\n",
      "48000/49000 loss: 0.4065779056725717\n",
      "epoch 29: valid acc = 0.858, new learning rate = 0.00011296777049628272\n",
      "6000/49000 loss: 0.44825481855509947\n",
      "12000/49000 loss: 0.4025504570502479\n",
      "18000/49000 loss: 0.39848089465633063\n",
      "24000/49000 loss: 0.404317741579652\n",
      "30000/49000 loss: 0.3704048147960177\n",
      "36000/49000 loss: 0.40720164083734584\n",
      "42000/49000 loss: 0.48942047705401837\n",
      "48000/49000 loss: 0.35446340420565076\n",
      "epoch 30: valid acc = 0.861, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.8625714285714285\n",
      "test acc: 0.861\n",
      "test acc: 0.8451\n",
      "number of batches for training: 81\n",
      "epoch 1: valid acc = 0.502, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.676, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.737, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.768, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.788, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.805, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.809, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.812, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.823, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.83, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.837, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.834, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.841, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.839, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.845, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.843, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.848, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.857, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.857, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.858, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.858, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.857, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.861, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.859, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.863, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.863, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.862, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.863, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.861, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.865, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.8643061224489796\n",
      "test acc: 0.865\n",
      "test acc: 0.8441\n",
      "number of batches for training: 49\n",
      "10000/49000 loss: 2.861557915726248\n",
      "20000/49000 loss: 2.683610305254537\n",
      "30000/49000 loss: 2.4954839362321084\n",
      "40000/49000 loss: 2.495571384349823\n",
      "epoch 1: valid acc = 0.386, new learning rate = 0.000475\n",
      "10000/49000 loss: 2.036440584521903\n",
      "20000/49000 loss: 1.9634468794784459\n",
      "30000/49000 loss: 1.666668893245265\n",
      "40000/49000 loss: 1.3937601839148388\n",
      "epoch 2: valid acc = 0.525, new learning rate = 0.00045125\n",
      "10000/49000 loss: 1.2394983717209074\n",
      "20000/49000 loss: 1.2197877395931886\n",
      "30000/49000 loss: 1.1758350713876862\n",
      "40000/49000 loss: 1.078828644947951\n",
      "epoch 3: valid acc = 0.633, new learning rate = 0.0004286875\n",
      "10000/49000 loss: 1.110244642590876\n",
      "20000/49000 loss: 0.9877629216063343\n",
      "30000/49000 loss: 0.9321281239310425\n",
      "40000/49000 loss: 0.9290077808358401\n",
      "epoch 4: valid acc = 0.702, new learning rate = 0.00040725312499999993\n",
      "10000/49000 loss: 0.8736188619082956\n",
      "20000/49000 loss: 0.8280799906039058\n",
      "30000/49000 loss: 0.8470565264755153\n",
      "40000/49000 loss: 0.8154883806171932\n",
      "epoch 5: valid acc = 0.726, new learning rate = 0.0003868904687499999\n",
      "10000/49000 loss: 0.763097968715792\n",
      "20000/49000 loss: 0.7015154605037333\n",
      "30000/49000 loss: 0.7896251800583567\n",
      "40000/49000 loss: 0.7296436453316788\n",
      "epoch 6: valid acc = 0.744, new learning rate = 0.0003675459453124999\n",
      "10000/49000 loss: 0.6843520507038294\n",
      "20000/49000 loss: 0.6910923451706705\n",
      "30000/49000 loss: 0.6542279565620249\n",
      "40000/49000 loss: 0.6823288936590597\n",
      "epoch 7: valid acc = 0.759, new learning rate = 0.00034916864804687486\n",
      "10000/49000 loss: 0.6499740533605646\n",
      "20000/49000 loss: 0.6785032390443154\n",
      "30000/49000 loss: 0.6389797124325546\n",
      "40000/49000 loss: 0.6113277350265822\n",
      "epoch 8: valid acc = 0.772, new learning rate = 0.0003317102156445311\n",
      "10000/49000 loss: 0.617689208584918\n",
      "20000/49000 loss: 0.6411274299649313\n",
      "30000/49000 loss: 0.611527692770887\n",
      "40000/49000 loss: 0.5891374758240214\n",
      "epoch 9: valid acc = 0.773, new learning rate = 0.0003151247048623045\n",
      "10000/49000 loss: 0.5687865866352585\n",
      "20000/49000 loss: 0.6018917980298354\n",
      "30000/49000 loss: 0.5833691421033794\n",
      "40000/49000 loss: 0.5955494759684475\n",
      "epoch 10: valid acc = 0.79, new learning rate = 0.00029936846961918924\n",
      "10000/49000 loss: 0.5650113911625112\n",
      "20000/49000 loss: 0.5890605017113963\n",
      "30000/49000 loss: 0.5882387001519662\n",
      "40000/49000 loss: 0.5540011583662661\n",
      "epoch 11: valid acc = 0.8, new learning rate = 0.00028440004613822977\n",
      "10000/49000 loss: 0.5633035700237169\n",
      "20000/49000 loss: 0.525618695795248\n",
      "30000/49000 loss: 0.5456702814114983\n",
      "40000/49000 loss: 0.5557835852345178\n",
      "epoch 12: valid acc = 0.799, new learning rate = 0.00027018004383131826\n",
      "10000/49000 loss: 0.4927629719065597\n",
      "20000/49000 loss: 0.5343123203557703\n",
      "30000/49000 loss: 0.5608009650804316\n",
      "40000/49000 loss: 0.5224347312455155\n",
      "epoch 13: valid acc = 0.811, new learning rate = 0.00025667104163975234\n",
      "10000/49000 loss: 0.5565373703186053\n",
      "20000/49000 loss: 0.5043116928562686\n",
      "30000/49000 loss: 0.5080747776770888\n",
      "40000/49000 loss: 0.47597782835828556\n",
      "epoch 14: valid acc = 0.811, new learning rate = 0.00024383748955776472\n",
      "10000/49000 loss: 0.5218689345051566\n",
      "20000/49000 loss: 0.4880615310340478\n",
      "30000/49000 loss: 0.5043203916560336\n",
      "40000/49000 loss: 0.5219971053717052\n",
      "epoch 15: valid acc = 0.807, new learning rate = 0.00023164561507987649\n",
      "10000/49000 loss: 0.5090353071464738\n",
      "20000/49000 loss: 0.5251835009201913\n",
      "30000/49000 loss: 0.4993942734099877\n",
      "40000/49000 loss: 0.4872246206459486\n",
      "epoch 16: valid acc = 0.816, new learning rate = 0.00022006333432588265\n",
      "10000/49000 loss: 0.5015101619255686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/49000 loss: 0.5428930945417957\n",
      "30000/49000 loss: 0.48849092589215726\n",
      "40000/49000 loss: 0.41600812756353284\n",
      "epoch 17: valid acc = 0.817, new learning rate = 0.00020906016760958852\n",
      "10000/49000 loss: 0.517037759144436\n",
      "20000/49000 loss: 0.5113517885273641\n",
      "30000/49000 loss: 0.48395030703754977\n",
      "40000/49000 loss: 0.48152496150551743\n",
      "epoch 18: valid acc = 0.818, new learning rate = 0.00019860715922910907\n",
      "10000/49000 loss: 0.4831122816468055\n",
      "20000/49000 loss: 0.4586268340915648\n",
      "30000/49000 loss: 0.470817981939634\n",
      "40000/49000 loss: 0.46215489564721324\n",
      "epoch 19: valid acc = 0.822, new learning rate = 0.0001886768012676536\n",
      "10000/49000 loss: 0.525802107379852\n",
      "20000/49000 loss: 0.48102947402112295\n",
      "30000/49000 loss: 0.4650091468159944\n",
      "40000/49000 loss: 0.5171390133895037\n",
      "epoch 20: valid acc = 0.824, new learning rate = 0.0001792429612042709\n",
      "10000/49000 loss: 0.5079598924564422\n",
      "20000/49000 loss: 0.48130742735215115\n",
      "30000/49000 loss: 0.46209020546119955\n",
      "40000/49000 loss: 0.4363227988476361\n",
      "epoch 21: valid acc = 0.827, new learning rate = 0.00017028081314405735\n",
      "10000/49000 loss: 0.49600864003649603\n",
      "20000/49000 loss: 0.5174005299387892\n",
      "30000/49000 loss: 0.47397063585557886\n",
      "40000/49000 loss: 0.4358563991761837\n",
      "epoch 22: valid acc = 0.826, new learning rate = 0.00016176677248685447\n",
      "10000/49000 loss: 0.48711506675405086\n",
      "20000/49000 loss: 0.4652792608714723\n",
      "30000/49000 loss: 0.44722774841818574\n",
      "40000/49000 loss: 0.47677140482480435\n",
      "epoch 23: valid acc = 0.83, new learning rate = 0.00015367843386251173\n",
      "10000/49000 loss: 0.470831273962496\n",
      "20000/49000 loss: 0.44946567603928356\n",
      "30000/49000 loss: 0.47330215364611206\n",
      "40000/49000 loss: 0.44929855301079\n",
      "epoch 24: valid acc = 0.827, new learning rate = 0.00014599451216938612\n",
      "10000/49000 loss: 0.4968953944115186\n",
      "20000/49000 loss: 0.47831815761969376\n",
      "30000/49000 loss: 0.4761784770269146\n",
      "40000/49000 loss: 0.4417618322682377\n",
      "epoch 25: valid acc = 0.828, new learning rate = 0.00013869478656091682\n",
      "10000/49000 loss: 0.4741086144782903\n",
      "20000/49000 loss: 0.39762409729866505\n",
      "30000/49000 loss: 0.47885887579708153\n",
      "40000/49000 loss: 0.47502505863446615\n",
      "epoch 26: valid acc = 0.833, new learning rate = 0.00013176004723287096\n",
      "10000/49000 loss: 0.41526544806360555\n",
      "20000/49000 loss: 0.4423851514551596\n",
      "30000/49000 loss: 0.4504046081981511\n",
      "40000/49000 loss: 0.4420944828703092\n",
      "epoch 27: valid acc = 0.833, new learning rate = 0.0001251720448712274\n",
      "10000/49000 loss: 0.4421550152668093\n",
      "20000/49000 loss: 0.44950352245999214\n",
      "30000/49000 loss: 0.42696496083381325\n",
      "40000/49000 loss: 0.44934484981762046\n",
      "epoch 28: valid acc = 0.831, new learning rate = 0.00011891344262766602\n",
      "10000/49000 loss: 0.47426667190792615\n",
      "20000/49000 loss: 0.44914033417716515\n",
      "30000/49000 loss: 0.4624602030283016\n",
      "40000/49000 loss: 0.41850752415911613\n",
      "epoch 29: valid acc = 0.834, new learning rate = 0.00011296777049628272\n",
      "10000/49000 loss: 0.4494733729420147\n",
      "20000/49000 loss: 0.4508861214468609\n",
      "30000/49000 loss: 0.374484571191486\n",
      "40000/49000 loss: 0.436756045985227\n",
      "epoch 30: valid acc = 0.833, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.8440408163265306\n",
      "test acc: 0.833\n",
      "test acc: 0.8294\n",
      "number of batches for training: 49\n",
      "epoch 1: valid acc = 0.389, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.507, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.647, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.695, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.73, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.755, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.753, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.761, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.781, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.78, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.794, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.805, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.809, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.811, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.812, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.82, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.821, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.824, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.823, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.825, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.825, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.828, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.831, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.832, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.831, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.828, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.832, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.829, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.832, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.833, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.8441020408163266\n",
      "test acc: 0.833\n",
      "test acc: 0.8293\n",
      "number of batches for training: 49\n",
      "10000/49000 loss: 2.6688702851336994\n",
      "20000/49000 loss: 2.7213279345459096\n",
      "30000/49000 loss: 2.562366454360111\n",
      "40000/49000 loss: 2.4363414572105024\n",
      "epoch 1: valid acc = 0.381, new learning rate = 0.000475\n",
      "10000/49000 loss: 2.026678061035203\n",
      "20000/49000 loss: 1.8908372196087142\n",
      "30000/49000 loss: 1.710769998842215\n",
      "40000/49000 loss: 1.4937951085618408\n",
      "epoch 2: valid acc = 0.529, new learning rate = 0.00045125\n",
      "10000/49000 loss: 1.2676689661182887\n",
      "20000/49000 loss: 1.2538468729058574\n",
      "30000/49000 loss: 1.1594644755402896\n",
      "40000/49000 loss: 1.1372319257120749\n",
      "epoch 3: valid acc = 0.63, new learning rate = 0.0004286875\n",
      "10000/49000 loss: 1.0407940937274345\n",
      "20000/49000 loss: 1.0093822282351348\n",
      "30000/49000 loss: 1.0011689600078313\n",
      "40000/49000 loss: 0.9674727089237158\n",
      "epoch 4: valid acc = 0.692, new learning rate = 0.00040725312499999993\n",
      "10000/49000 loss: 0.8898669759167941\n",
      "20000/49000 loss: 0.8420995566878587\n",
      "30000/49000 loss: 0.8305505582904259\n",
      "40000/49000 loss: 0.8095621403488835\n",
      "epoch 5: valid acc = 0.728, new learning rate = 0.0003868904687499999\n",
      "10000/49000 loss: 0.8062179387181435\n",
      "20000/49000 loss: 0.795322254888813\n",
      "30000/49000 loss: 0.7147349087402934\n",
      "40000/49000 loss: 0.6842523727785601\n",
      "epoch 6: valid acc = 0.738, new learning rate = 0.0003675459453124999\n",
      "10000/49000 loss: 0.6917360551074864\n",
      "20000/49000 loss: 0.6280015756245854\n",
      "30000/49000 loss: 0.7592340861124596\n",
      "40000/49000 loss: 0.6964522189766853\n",
      "epoch 7: valid acc = 0.761, new learning rate = 0.00034916864804687486\n",
      "10000/49000 loss: 0.63476633442134\n",
      "20000/49000 loss: 0.7162784437457783\n",
      "30000/49000 loss: 0.5843937852426426\n",
      "40000/49000 loss: 0.5711183682864563\n",
      "epoch 8: valid acc = 0.774, new learning rate = 0.0003317102156445311\n",
      "10000/49000 loss: 0.5987983107568593\n",
      "20000/49000 loss: 0.6288266794473731\n",
      "30000/49000 loss: 0.581106407564046\n",
      "40000/49000 loss: 0.6156812203048264\n",
      "epoch 9: valid acc = 0.782, new learning rate = 0.0003151247048623045\n",
      "10000/49000 loss: 0.564863119966419\n",
      "20000/49000 loss: 0.6075943730194866\n",
      "30000/49000 loss: 0.6130845043306875\n",
      "40000/49000 loss: 0.5405686556915731\n",
      "epoch 10: valid acc = 0.788, new learning rate = 0.00029936846961918924\n",
      "10000/49000 loss: 0.5675906747910803\n",
      "20000/49000 loss: 0.5814761933558075\n",
      "30000/49000 loss: 0.5975364785819592\n",
      "40000/49000 loss: 0.5080521208768266\n",
      "epoch 11: valid acc = 0.795, new learning rate = 0.00028440004613822977\n",
      "10000/49000 loss: 0.5393550647228578\n",
      "20000/49000 loss: 0.5315432740662588\n",
      "30000/49000 loss: 0.5267359816119079\n",
      "40000/49000 loss: 0.5143481239392729\n",
      "epoch 12: valid acc = 0.804, new learning rate = 0.00027018004383131826\n",
      "10000/49000 loss: 0.4878077143544243\n",
      "20000/49000 loss: 0.5223416963006986\n",
      "30000/49000 loss: 0.5324311508427434\n",
      "40000/49000 loss: 0.5142340179317415\n",
      "epoch 13: valid acc = 0.805, new learning rate = 0.00025667104163975234\n",
      "10000/49000 loss: 0.5306239228959286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/49000 loss: 0.5135178248072143\n",
      "30000/49000 loss: 0.5259155446880155\n",
      "40000/49000 loss: 0.541955730649409\n",
      "epoch 14: valid acc = 0.808, new learning rate = 0.00024383748955776472\n",
      "10000/49000 loss: 0.4993318906957332\n",
      "20000/49000 loss: 0.5458843188341999\n",
      "30000/49000 loss: 0.47701678999331804\n",
      "40000/49000 loss: 0.5092236972430529\n",
      "epoch 15: valid acc = 0.815, new learning rate = 0.00023164561507987649\n",
      "10000/49000 loss: 0.5165447318613043\n",
      "20000/49000 loss: 0.5439805683045992\n",
      "30000/49000 loss: 0.4864704795376647\n",
      "40000/49000 loss: 0.49518771307175086\n",
      "epoch 16: valid acc = 0.817, new learning rate = 0.00022006333432588265\n",
      "10000/49000 loss: 0.516074263783376\n",
      "20000/49000 loss: 0.49052008161225696\n",
      "30000/49000 loss: 0.46167296745786474\n",
      "40000/49000 loss: 0.491237262141097\n",
      "epoch 17: valid acc = 0.824, new learning rate = 0.00020906016760958852\n",
      "10000/49000 loss: 0.4750369110726287\n",
      "20000/49000 loss: 0.4645950214399163\n",
      "30000/49000 loss: 0.43965141631945465\n",
      "40000/49000 loss: 0.4986055688738156\n",
      "epoch 18: valid acc = 0.826, new learning rate = 0.00019860715922910907\n",
      "10000/49000 loss: 0.46255940837345344\n",
      "20000/49000 loss: 0.5199090068849546\n",
      "30000/49000 loss: 0.48582918704859296\n",
      "40000/49000 loss: 0.49718299985946757\n",
      "epoch 19: valid acc = 0.824, new learning rate = 0.0001886768012676536\n",
      "10000/49000 loss: 0.4999943928574905\n",
      "20000/49000 loss: 0.5081854912712044\n",
      "30000/49000 loss: 0.4856949295815317\n",
      "40000/49000 loss: 0.4823804687237851\n",
      "epoch 20: valid acc = 0.828, new learning rate = 0.0001792429612042709\n",
      "10000/49000 loss: 0.4987237343542797\n",
      "20000/49000 loss: 0.5513398286233104\n",
      "30000/49000 loss: 0.4598361139003901\n",
      "40000/49000 loss: 0.4744066688761821\n",
      "epoch 21: valid acc = 0.828, new learning rate = 0.00017028081314405735\n",
      "10000/49000 loss: 0.4528097864477949\n",
      "20000/49000 loss: 0.4542491833991838\n",
      "30000/49000 loss: 0.4707559821450798\n",
      "40000/49000 loss: 0.4998494509938435\n",
      "epoch 22: valid acc = 0.828, new learning rate = 0.00016176677248685447\n",
      "10000/49000 loss: 0.47862494388664234\n",
      "20000/49000 loss: 0.45867418673212124\n",
      "30000/49000 loss: 0.4793501423911556\n",
      "40000/49000 loss: 0.4554611659589704\n",
      "epoch 23: valid acc = 0.828, new learning rate = 0.00015367843386251173\n",
      "10000/49000 loss: 0.47373889607208314\n",
      "20000/49000 loss: 0.46331597232146543\n",
      "30000/49000 loss: 0.42736238022541717\n",
      "40000/49000 loss: 0.4764817349157494\n",
      "epoch 24: valid acc = 0.832, new learning rate = 0.00014599451216938612\n",
      "10000/49000 loss: 0.45829515934240544\n",
      "20000/49000 loss: 0.4505858415635724\n",
      "30000/49000 loss: 0.49333553709713773\n",
      "40000/49000 loss: 0.4225653750397278\n",
      "epoch 25: valid acc = 0.832, new learning rate = 0.00013869478656091682\n",
      "10000/49000 loss: 0.4524090683315603\n",
      "20000/49000 loss: 0.4430554846784283\n",
      "30000/49000 loss: 0.41099385383097736\n",
      "40000/49000 loss: 0.4692165021172243\n",
      "epoch 26: valid acc = 0.832, new learning rate = 0.00013176004723287096\n",
      "10000/49000 loss: 0.4202328446477371\n",
      "20000/49000 loss: 0.4425654861445581\n",
      "30000/49000 loss: 0.4476472860678931\n",
      "40000/49000 loss: 0.4439041867878918\n",
      "epoch 27: valid acc = 0.836, new learning rate = 0.0001251720448712274\n",
      "10000/49000 loss: 0.41783526403754195\n",
      "20000/49000 loss: 0.45968766185419563\n",
      "30000/49000 loss: 0.4248637565067848\n",
      "40000/49000 loss: 0.452902439896127\n",
      "epoch 28: valid acc = 0.836, new learning rate = 0.00011891344262766602\n",
      "10000/49000 loss: 0.4232021674494642\n",
      "20000/49000 loss: 0.4301923051041225\n",
      "30000/49000 loss: 0.47769328899283714\n",
      "40000/49000 loss: 0.4168457988079858\n",
      "epoch 29: valid acc = 0.835, new learning rate = 0.00011296777049628272\n",
      "10000/49000 loss: 0.4155966722600323\n",
      "20000/49000 loss: 0.4628073121752453\n",
      "30000/49000 loss: 0.4044165385040247\n",
      "40000/49000 loss: 0.3899371071114361\n",
      "epoch 30: valid acc = 0.835, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.8429795918367347\n",
      "test acc: 0.835\n",
      "test acc: 0.8283\n",
      "number of batches for training: 49\n",
      "epoch 1: valid acc = 0.381, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.527, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.619, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.685, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.725, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.735, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.761, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.769, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.78, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.798, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.802, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.808, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.81, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.813, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.816, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.817, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.818, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.818, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.824, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.828, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.826, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.83, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.831, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.834, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.835, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.836, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.832, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.836, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.837, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.84, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.8438571428571429\n",
      "test acc: 0.84\n",
      "test acc: 0.8282\n",
      "number of batches for training: 49\n",
      "10000/49000 loss: 2.8930583931146563\n",
      "20000/49000 loss: 2.599676911860467\n",
      "30000/49000 loss: 2.5673517710571594\n",
      "40000/49000 loss: 2.3821496051136912\n",
      "epoch 1: valid acc = 0.375, new learning rate = 0.000475\n",
      "10000/49000 loss: 2.1445746136374537\n",
      "20000/49000 loss: 2.065397980493194\n",
      "30000/49000 loss: 1.8287636740135027\n",
      "40000/49000 loss: 1.4526469305189316\n",
      "epoch 2: valid acc = 0.517, new learning rate = 0.00045125\n",
      "10000/49000 loss: 1.2591276175485087\n",
      "20000/49000 loss: 1.2171276992033595\n",
      "30000/49000 loss: 1.0976164415493443\n",
      "40000/49000 loss: 1.1231704555278914\n",
      "epoch 3: valid acc = 0.638, new learning rate = 0.0004286875\n",
      "10000/49000 loss: 0.9986374956384065\n",
      "20000/49000 loss: 1.0097423928247664\n",
      "30000/49000 loss: 1.0000726852708877\n",
      "40000/49000 loss: 0.9475542788994802\n",
      "epoch 4: valid acc = 0.684, new learning rate = 0.00040725312499999993\n",
      "10000/49000 loss: 0.8779064201973472\n",
      "20000/49000 loss: 0.8157276506637686\n",
      "30000/49000 loss: 0.8881045256351989\n",
      "40000/49000 loss: 0.8382317282016867\n",
      "epoch 5: valid acc = 0.735, new learning rate = 0.0003868904687499999\n",
      "10000/49000 loss: 0.7841787185444539\n",
      "20000/49000 loss: 0.7283386773405691\n",
      "30000/49000 loss: 0.7574603215001282\n",
      "40000/49000 loss: 0.6635139901430541\n",
      "epoch 6: valid acc = 0.748, new learning rate = 0.0003675459453124999\n",
      "10000/49000 loss: 0.7098872255480534\n",
      "20000/49000 loss: 0.6806981448754393\n",
      "30000/49000 loss: 0.665063823462388\n",
      "40000/49000 loss: 0.7389116303681339\n",
      "epoch 7: valid acc = 0.753, new learning rate = 0.00034916864804687486\n",
      "10000/49000 loss: 0.6473347624361288\n",
      "20000/49000 loss: 0.6342957705986825\n",
      "30000/49000 loss: 0.6425728225189314\n",
      "40000/49000 loss: 0.6562493277094391\n",
      "epoch 8: valid acc = 0.777, new learning rate = 0.0003317102156445311\n",
      "10000/49000 loss: 0.6265799783544007\n",
      "20000/49000 loss: 0.5598296918078849\n",
      "30000/49000 loss: 0.6408320398468023\n",
      "40000/49000 loss: 0.63357547715476\n",
      "epoch 9: valid acc = 0.781, new learning rate = 0.0003151247048623045\n",
      "10000/49000 loss: 0.559378486988762\n",
      "20000/49000 loss: 0.5607884396309372\n",
      "30000/49000 loss: 0.5901695193166123\n",
      "40000/49000 loss: 0.5457752568976745\n",
      "epoch 10: valid acc = 0.795, new learning rate = 0.00029936846961918924\n",
      "10000/49000 loss: 0.559552760440894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/49000 loss: 0.5050069394065866\n",
      "30000/49000 loss: 0.5678855003626483\n",
      "40000/49000 loss: 0.553013192348744\n",
      "epoch 11: valid acc = 0.798, new learning rate = 0.00028440004613822977\n",
      "10000/49000 loss: 0.568348082985878\n",
      "20000/49000 loss: 0.606234005873001\n",
      "30000/49000 loss: 0.5454364034246495\n",
      "40000/49000 loss: 0.5298152260401551\n",
      "epoch 12: valid acc = 0.801, new learning rate = 0.00027018004383131826\n",
      "10000/49000 loss: 0.5383664840093375\n",
      "20000/49000 loss: 0.5236971432939168\n",
      "30000/49000 loss: 0.4483577470421782\n",
      "40000/49000 loss: 0.5201212245405116\n",
      "epoch 13: valid acc = 0.81, new learning rate = 0.00025667104163975234\n",
      "10000/49000 loss: 0.5383543134961966\n",
      "20000/49000 loss: 0.5467097388867646\n",
      "30000/49000 loss: 0.5245749390748067\n",
      "40000/49000 loss: 0.5675853005334771\n",
      "epoch 14: valid acc = 0.812, new learning rate = 0.00024383748955776472\n",
      "10000/49000 loss: 0.48882153146962015\n",
      "20000/49000 loss: 0.5153095746954919\n",
      "30000/49000 loss: 0.5404929260387893\n",
      "40000/49000 loss: 0.4592616960504042\n",
      "epoch 15: valid acc = 0.816, new learning rate = 0.00023164561507987649\n",
      "10000/49000 loss: 0.4924218976983434\n",
      "20000/49000 loss: 0.4944958930767498\n",
      "30000/49000 loss: 0.46414061400797424\n",
      "40000/49000 loss: 0.4944995259295594\n",
      "epoch 16: valid acc = 0.813, new learning rate = 0.00022006333432588265\n",
      "10000/49000 loss: 0.5023195811199809\n",
      "20000/49000 loss: 0.5445841537136842\n",
      "30000/49000 loss: 0.5078262812915542\n",
      "40000/49000 loss: 0.5014206747111503\n",
      "epoch 17: valid acc = 0.819, new learning rate = 0.00020906016760958852\n",
      "10000/49000 loss: 0.4432662305979382\n",
      "20000/49000 loss: 0.5110105422146453\n",
      "30000/49000 loss: 0.48435642287843766\n",
      "40000/49000 loss: 0.47845314337004985\n",
      "epoch 18: valid acc = 0.82, new learning rate = 0.00019860715922910907\n",
      "10000/49000 loss: 0.4798530204120796\n",
      "20000/49000 loss: 0.5116343520506268\n",
      "30000/49000 loss: 0.46465706440692184\n",
      "40000/49000 loss: 0.46558772891626043\n",
      "epoch 19: valid acc = 0.828, new learning rate = 0.0001886768012676536\n",
      "10000/49000 loss: 0.5133692153922585\n",
      "20000/49000 loss: 0.45150925000457476\n",
      "30000/49000 loss: 0.4886629605771016\n",
      "40000/49000 loss: 0.46730066009812665\n",
      "epoch 20: valid acc = 0.828, new learning rate = 0.0001792429612042709\n",
      "10000/49000 loss: 0.45638152050067393\n",
      "20000/49000 loss: 0.45056656167314457\n",
      "30000/49000 loss: 0.46060378143642344\n",
      "40000/49000 loss: 0.4680330474396942\n",
      "epoch 21: valid acc = 0.827, new learning rate = 0.00017028081314405735\n",
      "10000/49000 loss: 0.420734003048458\n",
      "20000/49000 loss: 0.44184742093632046\n",
      "30000/49000 loss: 0.4664310615727178\n",
      "40000/49000 loss: 0.48143178191410546\n",
      "epoch 22: valid acc = 0.83, new learning rate = 0.00016176677248685447\n",
      "10000/49000 loss: 0.4352160404190868\n",
      "20000/49000 loss: 0.43570183079016467\n",
      "30000/49000 loss: 0.5213194172759378\n",
      "40000/49000 loss: 0.48530054233883047\n",
      "epoch 23: valid acc = 0.832, new learning rate = 0.00015367843386251173\n",
      "10000/49000 loss: 0.42951597579232204\n",
      "20000/49000 loss: 0.48468439062265717\n",
      "30000/49000 loss: 0.43504845644831963\n",
      "40000/49000 loss: 0.45697552029155936\n",
      "epoch 24: valid acc = 0.837, new learning rate = 0.00014599451216938612\n",
      "10000/49000 loss: 0.4631111007614162\n",
      "20000/49000 loss: 0.5049437326872906\n",
      "30000/49000 loss: 0.5109602114654159\n",
      "40000/49000 loss: 0.39947139189006087\n",
      "epoch 25: valid acc = 0.837, new learning rate = 0.00013869478656091682\n",
      "10000/49000 loss: 0.4885880928775922\n",
      "20000/49000 loss: 0.44520915562417956\n",
      "30000/49000 loss: 0.48373656392903613\n",
      "40000/49000 loss: 0.4777846615304386\n",
      "epoch 26: valid acc = 0.838, new learning rate = 0.00013176004723287096\n",
      "10000/49000 loss: 0.4719689286272654\n",
      "20000/49000 loss: 0.46928335860702586\n",
      "30000/49000 loss: 0.43201322558137595\n",
      "40000/49000 loss: 0.47408548765025843\n",
      "epoch 27: valid acc = 0.836, new learning rate = 0.0001251720448712274\n",
      "10000/49000 loss: 0.46087589737051926\n",
      "20000/49000 loss: 0.5001656534809985\n",
      "30000/49000 loss: 0.4721724198086011\n",
      "40000/49000 loss: 0.4540208939918284\n",
      "epoch 28: valid acc = 0.84, new learning rate = 0.00011891344262766602\n",
      "10000/49000 loss: 0.4322314036276504\n",
      "20000/49000 loss: 0.46351126254346275\n",
      "30000/49000 loss: 0.45167213445004994\n",
      "40000/49000 loss: 0.42604633742032305\n",
      "epoch 29: valid acc = 0.838, new learning rate = 0.00011296777049628272\n",
      "10000/49000 loss: 0.44590976350159317\n",
      "20000/49000 loss: 0.4442694862079956\n",
      "30000/49000 loss: 0.4283385618887587\n",
      "40000/49000 loss: 0.45609363574039685\n",
      "epoch 30: valid acc = 0.838, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.8441428571428572\n",
      "test acc: 0.838\n",
      "test acc: 0.8286\n",
      "number of batches for training: 49\n",
      "epoch 1: valid acc = 0.387, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.518, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.607, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.675, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.721, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.742, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.753, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.777, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.783, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.786, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.8, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.802, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.811, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.816, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.811, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.822, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.816, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.825, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.828, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.829, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.832, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.827, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.828, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.832, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.832, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.831, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.836, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.834, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.835, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.835, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.8445714285714285\n",
      "test acc: 0.835\n",
      "test acc: 0.8294\n",
      "number of batches for training: 245\n",
      "2000/49000 loss: 3.021973433106971\n",
      "4000/49000 loss: 2.6894749886891605\n",
      "6000/49000 loss: 2.44875569136776\n",
      "8000/49000 loss: 2.5863922984771355\n",
      "10000/49000 loss: 2.5340200206586743\n",
      "12000/49000 loss: 2.1656848069664023\n",
      "14000/49000 loss: 1.9992116714808592\n",
      "16000/49000 loss: 1.6713286572294004\n",
      "18000/49000 loss: 1.3952390305860325\n",
      "20000/49000 loss: 1.2449204872338264\n",
      "22000/49000 loss: 1.1080364847295405\n",
      "24000/49000 loss: 1.301349161033171\n",
      "26000/49000 loss: 1.1905650085096258\n",
      "28000/49000 loss: 1.1761639840647138\n",
      "30000/49000 loss: 0.9660162166246961\n",
      "32000/49000 loss: 1.060612876773407\n",
      "34000/49000 loss: 0.9822505490147316\n",
      "36000/49000 loss: 0.8916497425132375\n",
      "38000/49000 loss: 0.7877277510754602\n",
      "40000/49000 loss: 0.7578025152463288\n",
      "42000/49000 loss: 0.7685347582269592\n",
      "44000/49000 loss: 0.8197091511547548\n",
      "46000/49000 loss: 0.831119564212391\n",
      "48000/49000 loss: 0.7893746322648084\n",
      "epoch 1: valid acc = 0.736, new learning rate = 0.000475\n",
      "2000/49000 loss: 0.6681969789804508\n",
      "4000/49000 loss: 0.7729666174558874\n",
      "6000/49000 loss: 0.7260408879389623\n",
      "8000/49000 loss: 0.6708818353493913\n",
      "10000/49000 loss: 0.6832502293198858\n",
      "12000/49000 loss: 0.6663347321954649\n",
      "14000/49000 loss: 0.6221115631479578\n",
      "16000/49000 loss: 0.49202597560687494\n",
      "18000/49000 loss: 0.5009655727511865\n",
      "20000/49000 loss: 0.6358763503531568\n",
      "22000/49000 loss: 0.653383228323371\n",
      "24000/49000 loss: 0.597142177731618\n",
      "26000/49000 loss: 0.5660867724304816\n",
      "28000/49000 loss: 0.726657372111184\n",
      "30000/49000 loss: 0.6104092727805875\n",
      "32000/49000 loss: 0.47666084393405855\n",
      "34000/49000 loss: 0.5466751320821068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36000/49000 loss: 0.5908876106609601\n",
      "38000/49000 loss: 0.5919804792287178\n",
      "40000/49000 loss: 0.494837732871207\n",
      "42000/49000 loss: 0.44428605926450215\n",
      "44000/49000 loss: 0.5173049924611983\n",
      "46000/49000 loss: 0.45760308699656665\n",
      "48000/49000 loss: 0.4962070108659454\n",
      "epoch 2: valid acc = 0.812, new learning rate = 0.00045125\n",
      "2000/49000 loss: 0.47078276126836094\n",
      "4000/49000 loss: 0.4743035197985478\n",
      "6000/49000 loss: 0.5323444525219765\n",
      "8000/49000 loss: 0.6948254642962645\n",
      "10000/49000 loss: 0.43869462554420946\n",
      "12000/49000 loss: 0.44225235223847864\n",
      "14000/49000 loss: 0.49364467326266526\n",
      "16000/49000 loss: 0.5700835243420911\n",
      "18000/49000 loss: 0.46189225631985836\n",
      "20000/49000 loss: 0.48393783477147967\n",
      "22000/49000 loss: 0.4688263664529988\n",
      "24000/49000 loss: 0.46636719460302856\n",
      "26000/49000 loss: 0.4184482659953054\n",
      "28000/49000 loss: 0.4514631066565467\n",
      "30000/49000 loss: 0.47379058053679624\n",
      "32000/49000 loss: 0.5092684352599055\n",
      "34000/49000 loss: 0.5252831935695769\n",
      "36000/49000 loss: 0.41308046473919385\n",
      "38000/49000 loss: 0.5616261117973804\n",
      "40000/49000 loss: 0.4056521573848682\n",
      "42000/49000 loss: 0.4282790750420183\n",
      "44000/49000 loss: 0.47189711156144154\n",
      "46000/49000 loss: 0.46551837832965515\n",
      "48000/49000 loss: 0.4389857402564817\n",
      "epoch 3: valid acc = 0.839, new learning rate = 0.0004286875\n",
      "2000/49000 loss: 0.4461418854695959\n",
      "4000/49000 loss: 0.429871901321557\n",
      "6000/49000 loss: 0.38622758006361907\n",
      "8000/49000 loss: 0.4386730203871131\n",
      "10000/49000 loss: 0.44251851891226657\n",
      "12000/49000 loss: 0.3776870088132416\n",
      "14000/49000 loss: 0.37711860982779244\n",
      "16000/49000 loss: 0.4496376715929178\n",
      "18000/49000 loss: 0.34987963020431795\n",
      "20000/49000 loss: 0.5451629572096517\n",
      "22000/49000 loss: 0.4057323221153062\n",
      "24000/49000 loss: 0.4138270081866733\n",
      "26000/49000 loss: 0.5224598960193412\n",
      "28000/49000 loss: 0.42696305853806865\n",
      "30000/49000 loss: 0.43068433416800794\n",
      "32000/49000 loss: 0.42036484034433974\n",
      "34000/49000 loss: 0.4726719759902818\n",
      "36000/49000 loss: 0.47010944839317853\n",
      "38000/49000 loss: 0.4146022518999789\n",
      "40000/49000 loss: 0.3813133113835883\n",
      "42000/49000 loss: 0.5090308340954617\n",
      "44000/49000 loss: 0.46840513184837856\n",
      "46000/49000 loss: 0.39052723730066113\n",
      "48000/49000 loss: 0.5406322998746094\n",
      "epoch 4: valid acc = 0.843, new learning rate = 0.00040725312499999993\n",
      "2000/49000 loss: 0.44821670122737073\n",
      "4000/49000 loss: 0.4326217430955476\n",
      "6000/49000 loss: 0.41433227617616714\n",
      "8000/49000 loss: 0.43749201524234066\n",
      "10000/49000 loss: 0.3768219471162423\n",
      "12000/49000 loss: 0.42039416881837327\n",
      "14000/49000 loss: 0.3425375487048027\n",
      "16000/49000 loss: 0.47591194464180564\n",
      "18000/49000 loss: 0.40912053518599206\n",
      "20000/49000 loss: 0.41828951502692746\n",
      "22000/49000 loss: 0.4585865776500492\n",
      "24000/49000 loss: 0.44175032124095487\n",
      "26000/49000 loss: 0.48731058487740353\n",
      "28000/49000 loss: 0.47044620850418273\n",
      "30000/49000 loss: 0.27225213487270766\n",
      "32000/49000 loss: 0.4157220900141605\n",
      "34000/49000 loss: 0.5107141650244602\n",
      "36000/49000 loss: 0.4270343098398869\n",
      "38000/49000 loss: 0.41968457926665936\n",
      "40000/49000 loss: 0.3710692897657658\n",
      "42000/49000 loss: 0.3411541697269323\n",
      "44000/49000 loss: 0.43082408933430827\n",
      "46000/49000 loss: 0.3773084654988084\n",
      "48000/49000 loss: 0.416270988512965\n",
      "epoch 5: valid acc = 0.852, new learning rate = 0.0003868904687499999\n",
      "2000/49000 loss: 0.4859194546516256\n",
      "4000/49000 loss: 0.4428253290152125\n",
      "6000/49000 loss: 0.4424159837475145\n",
      "8000/49000 loss: 0.3904863707087278\n",
      "10000/49000 loss: 0.40030655730993864\n",
      "12000/49000 loss: 0.4133985677851967\n",
      "14000/49000 loss: 0.32403492766629155\n",
      "16000/49000 loss: 0.3906996510412239\n",
      "18000/49000 loss: 0.41638618795478766\n",
      "20000/49000 loss: 0.357077393876231\n",
      "22000/49000 loss: 0.43665223084537047\n",
      "24000/49000 loss: 0.4293577334323709\n",
      "26000/49000 loss: 0.3711879732054128\n",
      "28000/49000 loss: 0.40492011690870183\n",
      "30000/49000 loss: 0.3857514099383256\n",
      "32000/49000 loss: 0.4069638720351927\n",
      "34000/49000 loss: 0.3572868338745634\n",
      "36000/49000 loss: 0.40821769979450584\n",
      "38000/49000 loss: 0.3650920827465235\n",
      "40000/49000 loss: 0.34482665249667266\n",
      "42000/49000 loss: 0.2826961897052008\n",
      "44000/49000 loss: 0.44443532601836694\n",
      "46000/49000 loss: 0.41299972201551904\n",
      "48000/49000 loss: 0.42844181892621674\n",
      "epoch 6: valid acc = 0.861, new learning rate = 0.0003675459453124999\n",
      "2000/49000 loss: 0.4232353704695219\n",
      "4000/49000 loss: 0.4253196510764991\n",
      "6000/49000 loss: 0.3537701665821776\n",
      "8000/49000 loss: 0.34574527824291607\n",
      "10000/49000 loss: 0.48219611627265646\n",
      "12000/49000 loss: 0.3620099875065369\n",
      "14000/49000 loss: 0.42860152858112077\n",
      "16000/49000 loss: 0.2967043393975636\n",
      "18000/49000 loss: 0.42995091383709855\n",
      "20000/49000 loss: 0.4035481701901752\n",
      "22000/49000 loss: 0.4277321460959101\n",
      "24000/49000 loss: 0.3447522917372349\n",
      "26000/49000 loss: 0.324152957608133\n",
      "28000/49000 loss: 0.2963121188358724\n",
      "30000/49000 loss: 0.3763906500745299\n",
      "32000/49000 loss: 0.32486238480368607\n",
      "34000/49000 loss: 0.43058236465138344\n",
      "36000/49000 loss: 0.3839924464117613\n",
      "38000/49000 loss: 0.37599401677486005\n",
      "40000/49000 loss: 0.3541425152200794\n",
      "42000/49000 loss: 0.35222713209786055\n",
      "44000/49000 loss: 0.3457802696771284\n",
      "46000/49000 loss: 0.35338777417827105\n",
      "48000/49000 loss: 0.32384319193769967\n",
      "epoch 7: valid acc = 0.858, new learning rate = 0.00034916864804687486\n",
      "2000/49000 loss: 0.4127424192996399\n",
      "4000/49000 loss: 0.32997081315113597\n",
      "6000/49000 loss: 0.3828052096304698\n",
      "8000/49000 loss: 0.3607308788125384\n",
      "10000/49000 loss: 0.3244907720776263\n",
      "12000/49000 loss: 0.3974948388694203\n",
      "14000/49000 loss: 0.27358634637683915\n",
      "16000/49000 loss: 0.4389190505994774\n",
      "18000/49000 loss: 0.34340541688213094\n",
      "20000/49000 loss: 0.31806010874803015\n",
      "22000/49000 loss: 0.32536581765045486\n",
      "24000/49000 loss: 0.35843960586215695\n",
      "26000/49000 loss: 0.2736104431123361\n",
      "28000/49000 loss: 0.38890971937730545\n",
      "30000/49000 loss: 0.37900856702252417\n",
      "32000/49000 loss: 0.3629419262987511\n",
      "34000/49000 loss: 0.33254727345691815\n",
      "36000/49000 loss: 0.41434064220173783\n",
      "38000/49000 loss: 0.343460645618012\n",
      "40000/49000 loss: 0.38606624724370164\n",
      "42000/49000 loss: 0.44834813487304964\n",
      "44000/49000 loss: 0.4534135255466127\n",
      "46000/49000 loss: 0.44116158098154934\n",
      "48000/49000 loss: 0.37694952698333684\n",
      "epoch 8: valid acc = 0.87, new learning rate = 0.0003317102156445311\n",
      "2000/49000 loss: 0.3703788789079213\n",
      "4000/49000 loss: 0.3329672791516311\n",
      "6000/49000 loss: 0.32014255450900814\n",
      "8000/49000 loss: 0.3454716606867419\n",
      "10000/49000 loss: 0.27897399167965814\n",
      "12000/49000 loss: 0.32106020648506683\n",
      "14000/49000 loss: 0.2931679581532979\n",
      "16000/49000 loss: 0.30065861076007905\n",
      "18000/49000 loss: 0.4491056095712674\n",
      "20000/49000 loss: 0.3844558626871891\n",
      "22000/49000 loss: 0.4354833011151897\n",
      "24000/49000 loss: 0.36619621883124837\n",
      "26000/49000 loss: 0.3581652893694729\n",
      "28000/49000 loss: 0.3087491351683916\n",
      "30000/49000 loss: 0.37186424760448805\n",
      "32000/49000 loss: 0.34378493759637285\n",
      "34000/49000 loss: 0.35648225578074255\n",
      "36000/49000 loss: 0.3709280107573992\n",
      "38000/49000 loss: 0.39713145812362527\n",
      "40000/49000 loss: 0.40928027827720026\n",
      "42000/49000 loss: 0.41926969823537685\n",
      "44000/49000 loss: 0.29063071913649297\n",
      "46000/49000 loss: 0.3617197505538841\n",
      "48000/49000 loss: 0.30557377689387044\n",
      "epoch 9: valid acc = 0.869, new learning rate = 0.0003151247048623045\n",
      "2000/49000 loss: 0.34807318718667224\n",
      "4000/49000 loss: 0.3949003690164357\n",
      "6000/49000 loss: 0.32160170940728317\n",
      "8000/49000 loss: 0.37338436799402014\n",
      "10000/49000 loss: 0.41209303133088404\n",
      "12000/49000 loss: 0.2899350048332366\n",
      "14000/49000 loss: 0.37740167985848344\n",
      "16000/49000 loss: 0.3508800421063939\n",
      "18000/49000 loss: 0.3483583140176743\n",
      "20000/49000 loss: 0.33462380008050874\n",
      "22000/49000 loss: 0.30234785653473684\n",
      "24000/49000 loss: 0.2604659698857007\n",
      "26000/49000 loss: 0.3168683812327327\n",
      "28000/49000 loss: 0.37068088991370945\n",
      "30000/49000 loss: 0.3436635924802315\n",
      "32000/49000 loss: 0.3836269917835749\n",
      "34000/49000 loss: 0.30235746256792917\n",
      "36000/49000 loss: 0.33580178638022684\n",
      "38000/49000 loss: 0.3787386511034186\n",
      "40000/49000 loss: 0.3300640541471159\n",
      "42000/49000 loss: 0.2749930945349115\n",
      "44000/49000 loss: 0.36698329682985725\n",
      "46000/49000 loss: 0.2656227396242027\n",
      "48000/49000 loss: 0.2957926858873833\n",
      "epoch 10: valid acc = 0.868, new learning rate = 0.00029936846961918924\n",
      "2000/49000 loss: 0.32723924735388776\n",
      "4000/49000 loss: 0.3068207705858497\n",
      "6000/49000 loss: 0.3574344576231656\n",
      "8000/49000 loss: 0.370628767495086\n",
      "10000/49000 loss: 0.3711795767633022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/49000 loss: 0.2031288035245883\n",
      "14000/49000 loss: 0.28615817978767283\n",
      "16000/49000 loss: 0.32874087039105326\n",
      "18000/49000 loss: 0.36389881554845416\n",
      "20000/49000 loss: 0.37704967157445773\n",
      "22000/49000 loss: 0.3070610535758356\n",
      "24000/49000 loss: 0.3130504655776647\n",
      "26000/49000 loss: 0.29829599083440433\n",
      "28000/49000 loss: 0.30213698533447186\n",
      "30000/49000 loss: 0.3441766594389887\n",
      "32000/49000 loss: 0.3773040852442088\n",
      "34000/49000 loss: 0.29431838258975723\n",
      "36000/49000 loss: 0.32300250281355614\n",
      "38000/49000 loss: 0.37240056862816634\n",
      "40000/49000 loss: 0.33436026256828494\n",
      "42000/49000 loss: 0.41340446678944337\n",
      "44000/49000 loss: 0.2964989037142089\n",
      "46000/49000 loss: 0.376693787013679\n",
      "48000/49000 loss: 0.3382720534598606\n",
      "epoch 11: valid acc = 0.872, new learning rate = 0.00028440004613822977\n",
      "2000/49000 loss: 0.36832482690660606\n",
      "4000/49000 loss: 0.3353491074401536\n",
      "6000/49000 loss: 0.3848660319961423\n",
      "8000/49000 loss: 0.2477532793807408\n",
      "10000/49000 loss: 0.36478737399343614\n",
      "12000/49000 loss: 0.4921335004368602\n",
      "14000/49000 loss: 0.34030137314338843\n",
      "16000/49000 loss: 0.3476762456832232\n",
      "18000/49000 loss: 0.35928640356782215\n",
      "20000/49000 loss: 0.31258360299375465\n",
      "22000/49000 loss: 0.3804471653832825\n",
      "24000/49000 loss: 0.3078933116119152\n",
      "26000/49000 loss: 0.2896972300512936\n",
      "28000/49000 loss: 0.2839895001846415\n",
      "30000/49000 loss: 0.3045326265512193\n",
      "32000/49000 loss: 0.31375713564191904\n",
      "34000/49000 loss: 0.3334468607778163\n",
      "36000/49000 loss: 0.4159345219213222\n",
      "38000/49000 loss: 0.47821585551421397\n",
      "40000/49000 loss: 0.3651118328665528\n",
      "42000/49000 loss: 0.35538952550443725\n",
      "44000/49000 loss: 0.26217839868508\n",
      "46000/49000 loss: 0.3221214685999097\n",
      "48000/49000 loss: 0.35775408387972885\n",
      "epoch 12: valid acc = 0.884, new learning rate = 0.00027018004383131826\n",
      "2000/49000 loss: 0.23859773739564166\n",
      "4000/49000 loss: 0.3514360476013479\n",
      "6000/49000 loss: 0.40077944433646334\n",
      "8000/49000 loss: 0.3314675608682779\n",
      "10000/49000 loss: 0.44408347640018003\n",
      "12000/49000 loss: 0.4048785911323551\n",
      "14000/49000 loss: 0.4045432519187032\n",
      "16000/49000 loss: 0.3438838830857553\n",
      "18000/49000 loss: 0.350785373726319\n",
      "20000/49000 loss: 0.33493821505359533\n",
      "22000/49000 loss: 0.3966521108155719\n",
      "24000/49000 loss: 0.32919735579316345\n",
      "26000/49000 loss: 0.4028912157923391\n",
      "28000/49000 loss: 0.3786370503346985\n",
      "30000/49000 loss: 0.34902119532681086\n",
      "32000/49000 loss: 0.34280037154181947\n",
      "34000/49000 loss: 0.38189816322424\n",
      "36000/49000 loss: 0.3758865233298482\n",
      "38000/49000 loss: 0.2903847348422197\n",
      "40000/49000 loss: 0.36672356546292184\n",
      "42000/49000 loss: 0.36874979357886356\n",
      "44000/49000 loss: 0.34382880213841394\n",
      "46000/49000 loss: 0.3152641775682066\n",
      "48000/49000 loss: 0.2966626979525287\n",
      "epoch 13: valid acc = 0.879, new learning rate = 0.00025667104163975234\n",
      "2000/49000 loss: 0.32838955947349613\n",
      "4000/49000 loss: 0.3106963636647286\n",
      "6000/49000 loss: 0.25188014792913527\n",
      "8000/49000 loss: 0.3591170824977947\n",
      "10000/49000 loss: 0.38366591871918687\n",
      "12000/49000 loss: 0.35768163431568994\n",
      "14000/49000 loss: 0.26873756145774075\n",
      "16000/49000 loss: 0.36317990761244445\n",
      "18000/49000 loss: 0.38697588088835316\n",
      "20000/49000 loss: 0.33394200582561206\n",
      "22000/49000 loss: 0.2994249880728318\n",
      "24000/49000 loss: 0.22511577744239325\n",
      "26000/49000 loss: 0.36166089199407114\n",
      "28000/49000 loss: 0.3662963208843777\n",
      "30000/49000 loss: 0.3316442382226242\n",
      "32000/49000 loss: 0.35006574650970607\n",
      "34000/49000 loss: 0.282263101984506\n",
      "36000/49000 loss: 0.36022492470115586\n",
      "38000/49000 loss: 0.33173498477062446\n",
      "40000/49000 loss: 0.24115751225825258\n",
      "42000/49000 loss: 0.31358781655581597\n",
      "44000/49000 loss: 0.3017156461238004\n",
      "46000/49000 loss: 0.30482909222335947\n",
      "48000/49000 loss: 0.38939836906349873\n",
      "epoch 14: valid acc = 0.878, new learning rate = 0.00024383748955776472\n",
      "2000/49000 loss: 0.31017498404711924\n",
      "4000/49000 loss: 0.295127244442299\n",
      "6000/49000 loss: 0.36251254551617396\n",
      "8000/49000 loss: 0.36937564419443847\n",
      "10000/49000 loss: 0.31762721169490543\n",
      "12000/49000 loss: 0.2850720896912986\n",
      "14000/49000 loss: 0.19691674593155126\n",
      "16000/49000 loss: 0.3865517359124777\n",
      "18000/49000 loss: 0.3142339377634843\n",
      "20000/49000 loss: 0.41212899214110077\n",
      "22000/49000 loss: 0.33533854835188526\n",
      "24000/49000 loss: 0.41028763629424886\n",
      "26000/49000 loss: 0.32239319479527306\n",
      "28000/49000 loss: 0.3617459304735634\n",
      "30000/49000 loss: 0.29839624971881784\n",
      "32000/49000 loss: 0.388618425062044\n",
      "34000/49000 loss: 0.26123702173624974\n",
      "36000/49000 loss: 0.24469712221684425\n",
      "38000/49000 loss: 0.42161051739293576\n",
      "40000/49000 loss: 0.3966264128096396\n",
      "42000/49000 loss: 0.33348596943805753\n",
      "44000/49000 loss: 0.2825042923217958\n",
      "46000/49000 loss: 0.27572592646056177\n",
      "48000/49000 loss: 0.30558087684226265\n",
      "epoch 15: valid acc = 0.885, new learning rate = 0.00023164561507987649\n",
      "2000/49000 loss: 0.3581662117821032\n",
      "4000/49000 loss: 0.2349604970452817\n",
      "6000/49000 loss: 0.2983590787790314\n",
      "8000/49000 loss: 0.2944610431207847\n",
      "10000/49000 loss: 0.3038689467089585\n",
      "12000/49000 loss: 0.3005849536921528\n",
      "14000/49000 loss: 0.3680459539335005\n",
      "16000/49000 loss: 0.3157875218548662\n",
      "18000/49000 loss: 0.5219889766279997\n",
      "20000/49000 loss: 0.3980628739843655\n",
      "22000/49000 loss: 0.332443508568192\n",
      "24000/49000 loss: 0.2960074333605764\n",
      "26000/49000 loss: 0.29614660839085083\n",
      "28000/49000 loss: 0.3399810888265047\n",
      "30000/49000 loss: 0.3960715303777171\n",
      "32000/49000 loss: 0.27570073898280884\n",
      "34000/49000 loss: 0.4077863928706268\n",
      "36000/49000 loss: 0.3256417136879846\n",
      "38000/49000 loss: 0.3068639761981166\n",
      "40000/49000 loss: 0.3384428873797733\n",
      "42000/49000 loss: 0.3447833311358942\n",
      "44000/49000 loss: 0.26871692354775034\n",
      "46000/49000 loss: 0.3328529335577182\n",
      "48000/49000 loss: 0.33917803889100945\n",
      "epoch 16: valid acc = 0.881, new learning rate = 0.00022006333432588265\n",
      "2000/49000 loss: 0.2897509783839623\n",
      "4000/49000 loss: 0.3385550896240894\n",
      "6000/49000 loss: 0.2556356364367838\n",
      "8000/49000 loss: 0.28564193755078776\n",
      "10000/49000 loss: 0.27430684189793686\n",
      "12000/49000 loss: 0.34741245936739246\n",
      "14000/49000 loss: 0.36490423843335856\n",
      "16000/49000 loss: 0.3080277801091051\n",
      "18000/49000 loss: 0.2858470298445662\n",
      "20000/49000 loss: 0.2508326087257278\n",
      "22000/49000 loss: 0.33695653207937587\n",
      "24000/49000 loss: 0.3703729395832324\n",
      "26000/49000 loss: 0.3222194669266232\n",
      "28000/49000 loss: 0.21109834573666517\n",
      "30000/49000 loss: 0.32465546500895565\n",
      "32000/49000 loss: 0.292865465038275\n",
      "34000/49000 loss: 0.4157048890351898\n",
      "36000/49000 loss: 0.322676320797089\n",
      "38000/49000 loss: 0.3300769209167831\n",
      "40000/49000 loss: 0.37968032716356165\n",
      "42000/49000 loss: 0.3544909923228962\n",
      "44000/49000 loss: 0.3655612211411857\n",
      "46000/49000 loss: 0.2883714381310332\n",
      "48000/49000 loss: 0.4043328303077487\n",
      "epoch 17: valid acc = 0.883, new learning rate = 0.00020906016760958852\n",
      "2000/49000 loss: 0.36764593448289357\n",
      "4000/49000 loss: 0.26253818848719884\n",
      "6000/49000 loss: 0.33916921813033946\n",
      "8000/49000 loss: 0.31053061693465994\n",
      "10000/49000 loss: 0.3463500461364476\n",
      "12000/49000 loss: 0.3334458317768773\n",
      "14000/49000 loss: 0.2812333398789711\n",
      "16000/49000 loss: 0.3234272690265663\n",
      "18000/49000 loss: 0.36397152262480653\n",
      "20000/49000 loss: 0.2971960458760267\n",
      "22000/49000 loss: 0.3281438157498971\n",
      "24000/49000 loss: 0.3155247146258676\n",
      "26000/49000 loss: 0.3975886625599054\n",
      "28000/49000 loss: 0.3598426671465763\n",
      "30000/49000 loss: 0.3596130348819784\n",
      "32000/49000 loss: 0.2916783058131792\n",
      "34000/49000 loss: 0.34719371746232824\n",
      "36000/49000 loss: 0.23175960046378594\n",
      "38000/49000 loss: 0.22703225476330577\n",
      "40000/49000 loss: 0.36086245886972873\n",
      "42000/49000 loss: 0.27825663646978155\n",
      "44000/49000 loss: 0.3188509192855077\n",
      "46000/49000 loss: 0.3071166048357024\n",
      "48000/49000 loss: 0.4205505531668675\n",
      "epoch 18: valid acc = 0.887, new learning rate = 0.00019860715922910907\n",
      "2000/49000 loss: 0.3903512376896989\n",
      "4000/49000 loss: 0.2890861695327087\n",
      "6000/49000 loss: 0.31469134668337573\n",
      "8000/49000 loss: 0.30712419844751593\n",
      "10000/49000 loss: 0.33969395029445837\n",
      "12000/49000 loss: 0.24843221283416442\n",
      "14000/49000 loss: 0.33879950730421193\n",
      "16000/49000 loss: 0.27547214137837234\n",
      "18000/49000 loss: 0.2722275306394654\n",
      "20000/49000 loss: 0.31360603389543457\n",
      "22000/49000 loss: 0.33283927356315085\n",
      "24000/49000 loss: 0.31066841367809417\n",
      "26000/49000 loss: 0.33352906016899075\n",
      "28000/49000 loss: 0.31632021709096364\n",
      "30000/49000 loss: 0.3305141240886772\n",
      "32000/49000 loss: 0.4318831222452947\n",
      "34000/49000 loss: 0.2671195608379129\n",
      "36000/49000 loss: 0.2549560413167029\n",
      "38000/49000 loss: 0.3005376837263764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/49000 loss: 0.25595521257992704\n",
      "42000/49000 loss: 0.36401033178292475\n",
      "44000/49000 loss: 0.46011393482537466\n",
      "46000/49000 loss: 0.35123402553485666\n",
      "48000/49000 loss: 0.2805539393644447\n",
      "epoch 19: valid acc = 0.881, new learning rate = 0.0001886768012676536\n",
      "2000/49000 loss: 0.3603226151882917\n",
      "4000/49000 loss: 0.36525110042181763\n",
      "6000/49000 loss: 0.28691765732407803\n",
      "8000/49000 loss: 0.2571945482506538\n",
      "10000/49000 loss: 0.41457560921142894\n",
      "12000/49000 loss: 0.2661647175806735\n",
      "14000/49000 loss: 0.2686701692936063\n",
      "16000/49000 loss: 0.28724195983641126\n",
      "18000/49000 loss: 0.392682004314833\n",
      "20000/49000 loss: 0.25204984505251926\n",
      "22000/49000 loss: 0.2903320547659952\n",
      "24000/49000 loss: 0.24336276232816878\n",
      "26000/49000 loss: 0.30182076710005734\n",
      "28000/49000 loss: 0.3558707367333261\n",
      "30000/49000 loss: 0.41380294547119373\n",
      "32000/49000 loss: 0.35536827599579707\n",
      "34000/49000 loss: 0.2633282664140049\n",
      "36000/49000 loss: 0.2856541684060297\n",
      "38000/49000 loss: 0.3921949489061125\n",
      "40000/49000 loss: 0.3276161684977518\n",
      "42000/49000 loss: 0.28117815448251215\n",
      "44000/49000 loss: 0.25261609438886584\n",
      "46000/49000 loss: 0.2744901053740554\n",
      "48000/49000 loss: 0.3684191772328644\n",
      "epoch 20: valid acc = 0.887, new learning rate = 0.0001792429612042709\n",
      "2000/49000 loss: 0.2927276638588566\n",
      "4000/49000 loss: 0.303163238102423\n",
      "6000/49000 loss: 0.25632129913195295\n",
      "8000/49000 loss: 0.3359130773627595\n",
      "10000/49000 loss: 0.20740656284708842\n",
      "12000/49000 loss: 0.24945653551072103\n",
      "14000/49000 loss: 0.3246575927329584\n",
      "16000/49000 loss: 0.2834444471949225\n",
      "18000/49000 loss: 0.2746702838422214\n",
      "20000/49000 loss: 0.3238741674158434\n",
      "22000/49000 loss: 0.33984530617437636\n",
      "24000/49000 loss: 0.2958475897928775\n",
      "26000/49000 loss: 0.3350106650571477\n",
      "28000/49000 loss: 0.28123739142049725\n",
      "30000/49000 loss: 0.3467076924690607\n",
      "32000/49000 loss: 0.3221337948857638\n",
      "34000/49000 loss: 0.2746795439710932\n",
      "36000/49000 loss: 0.3320379694254551\n",
      "38000/49000 loss: 0.2821460381817\n",
      "40000/49000 loss: 0.333660841847925\n",
      "42000/49000 loss: 0.28855675661090613\n",
      "44000/49000 loss: 0.4029769801291764\n",
      "46000/49000 loss: 0.2696838328769457\n",
      "48000/49000 loss: 0.3478030498139929\n",
      "epoch 21: valid acc = 0.878, new learning rate = 0.00017028081314405735\n",
      "2000/49000 loss: 0.3401637242234143\n",
      "4000/49000 loss: 0.31086764969101655\n",
      "6000/49000 loss: 0.2617694121315685\n",
      "8000/49000 loss: 0.3151855217709948\n",
      "10000/49000 loss: 0.35467868802444563\n",
      "12000/49000 loss: 0.2941206883077953\n",
      "14000/49000 loss: 0.2889416015606905\n",
      "16000/49000 loss: 0.24122442749943593\n",
      "18000/49000 loss: 0.34162901288838643\n",
      "20000/49000 loss: 0.22249532118856574\n",
      "22000/49000 loss: 0.3040619264072485\n",
      "24000/49000 loss: 0.33345978507362806\n",
      "26000/49000 loss: 0.33254667716323394\n",
      "28000/49000 loss: 0.38434819687305805\n",
      "30000/49000 loss: 0.3153228983772106\n",
      "32000/49000 loss: 0.2336855880353851\n",
      "34000/49000 loss: 0.2417838194269535\n",
      "36000/49000 loss: 0.31723164194479353\n",
      "38000/49000 loss: 0.21842939626465285\n",
      "40000/49000 loss: 0.24272302279463084\n",
      "42000/49000 loss: 0.28921515566994355\n",
      "44000/49000 loss: 0.32811308900619285\n",
      "46000/49000 loss: 0.42945719422587725\n",
      "48000/49000 loss: 0.2909548544042573\n",
      "epoch 22: valid acc = 0.88, new learning rate = 0.00016176677248685447\n",
      "2000/49000 loss: 0.25813086142385283\n",
      "4000/49000 loss: 0.2594699318705447\n",
      "6000/49000 loss: 0.32377551803745885\n",
      "8000/49000 loss: 0.3505361990493301\n",
      "10000/49000 loss: 0.3006527902512393\n",
      "12000/49000 loss: 0.2849263456607505\n",
      "14000/49000 loss: 0.3633634417610628\n",
      "16000/49000 loss: 0.21800008220352377\n",
      "18000/49000 loss: 0.2805632554557942\n",
      "20000/49000 loss: 0.3019898642728429\n",
      "22000/49000 loss: 0.3179985961215632\n",
      "24000/49000 loss: 0.3101941105479561\n",
      "26000/49000 loss: 0.3168198557783622\n",
      "28000/49000 loss: 0.25612235436949615\n",
      "30000/49000 loss: 0.40260759954100495\n",
      "32000/49000 loss: 0.31340848169272695\n",
      "34000/49000 loss: 0.29045733324306916\n",
      "36000/49000 loss: 0.32563962971526417\n",
      "38000/49000 loss: 0.3873053612016887\n",
      "40000/49000 loss: 0.2895595320009983\n",
      "42000/49000 loss: 0.3309976679123224\n",
      "44000/49000 loss: 0.3625022104424951\n",
      "46000/49000 loss: 0.30908638671377975\n",
      "48000/49000 loss: 0.2814931363381159\n",
      "epoch 23: valid acc = 0.883, new learning rate = 0.00015367843386251173\n",
      "2000/49000 loss: 0.29859858624025964\n",
      "4000/49000 loss: 0.2763362343307169\n",
      "6000/49000 loss: 0.3926257607015788\n",
      "8000/49000 loss: 0.3264288364825472\n",
      "10000/49000 loss: 0.23573581284105288\n",
      "12000/49000 loss: 0.26227609185133344\n",
      "14000/49000 loss: 0.2879345558355786\n",
      "16000/49000 loss: 0.30462986340980314\n",
      "18000/49000 loss: 0.29300809432956526\n",
      "20000/49000 loss: 0.38266687327240195\n",
      "22000/49000 loss: 0.2624930321447787\n",
      "24000/49000 loss: 0.23255801235911638\n",
      "26000/49000 loss: 0.2790269691416929\n",
      "28000/49000 loss: 0.2563048242119763\n",
      "30000/49000 loss: 0.4315986503440824\n",
      "32000/49000 loss: 0.3438235308346269\n",
      "34000/49000 loss: 0.26693833602008193\n",
      "36000/49000 loss: 0.2681155042909402\n",
      "38000/49000 loss: 0.2976362784260097\n",
      "40000/49000 loss: 0.2731780802714352\n",
      "42000/49000 loss: 0.42546992844553866\n",
      "44000/49000 loss: 0.268459969187414\n",
      "46000/49000 loss: 0.24368887790758328\n",
      "48000/49000 loss: 0.2912223625484279\n",
      "epoch 24: valid acc = 0.886, new learning rate = 0.00014599451216938612\n",
      "2000/49000 loss: 0.3025780248123521\n",
      "4000/49000 loss: 0.28547985633334727\n",
      "6000/49000 loss: 0.2770690162226079\n",
      "8000/49000 loss: 0.2308594031502285\n",
      "10000/49000 loss: 0.2932379088521974\n",
      "12000/49000 loss: 0.3699509529327657\n",
      "14000/49000 loss: 0.324599213414304\n",
      "16000/49000 loss: 0.33131622620367635\n",
      "18000/49000 loss: 0.2961135321016231\n",
      "20000/49000 loss: 0.3527900447132244\n",
      "22000/49000 loss: 0.20690885198968711\n",
      "24000/49000 loss: 0.2928386604063857\n",
      "26000/49000 loss: 0.3017318241792542\n",
      "28000/49000 loss: 0.2548734743662791\n",
      "30000/49000 loss: 0.35459350738679785\n",
      "32000/49000 loss: 0.3641310541363578\n",
      "34000/49000 loss: 0.2605328213167381\n",
      "36000/49000 loss: 0.30050882890820374\n",
      "38000/49000 loss: 0.20010799464802506\n",
      "40000/49000 loss: 0.43588112125461465\n",
      "42000/49000 loss: 0.26780371805630393\n",
      "44000/49000 loss: 0.4050346565892683\n",
      "46000/49000 loss: 0.22838593629541454\n",
      "48000/49000 loss: 0.18194080167629692\n",
      "epoch 25: valid acc = 0.886, new learning rate = 0.00013869478656091682\n",
      "2000/49000 loss: 0.25607695137970904\n",
      "4000/49000 loss: 0.2414244313640089\n",
      "6000/49000 loss: 0.5050610643183973\n",
      "8000/49000 loss: 0.3550848954843159\n",
      "10000/49000 loss: 0.2638374076483662\n",
      "12000/49000 loss: 0.35006444478724635\n",
      "14000/49000 loss: 0.31935404996767414\n",
      "16000/49000 loss: 0.3675589708138332\n",
      "18000/49000 loss: 0.34444862034909834\n",
      "20000/49000 loss: 0.31037310875717633\n",
      "22000/49000 loss: 0.30890401502558107\n",
      "24000/49000 loss: 0.2996822600428607\n",
      "26000/49000 loss: 0.2497440287366565\n",
      "28000/49000 loss: 0.3248989325396332\n",
      "30000/49000 loss: 0.3031641737497595\n",
      "32000/49000 loss: 0.32790989341239357\n",
      "34000/49000 loss: 0.19509194605220778\n",
      "36000/49000 loss: 0.3550738511562824\n",
      "38000/49000 loss: 0.2769492255102401\n",
      "40000/49000 loss: 0.2681994563150471\n",
      "42000/49000 loss: 0.23683175557125996\n",
      "44000/49000 loss: 0.35094651409239075\n",
      "46000/49000 loss: 0.3048100293908499\n",
      "48000/49000 loss: 0.2942931542073847\n",
      "epoch 26: valid acc = 0.883, new learning rate = 0.00013176004723287096\n",
      "2000/49000 loss: 0.2984009111985385\n",
      "4000/49000 loss: 0.2080340396161312\n",
      "6000/49000 loss: 0.2658307069731237\n",
      "8000/49000 loss: 0.2724182237623702\n",
      "10000/49000 loss: 0.254852951050633\n",
      "12000/49000 loss: 0.2863648491443683\n",
      "14000/49000 loss: 0.2508599448318445\n",
      "16000/49000 loss: 0.2468475018793897\n",
      "18000/49000 loss: 0.28029636139877256\n",
      "20000/49000 loss: 0.2885365190285662\n",
      "22000/49000 loss: 0.3130748517534501\n",
      "24000/49000 loss: 0.3049786756179799\n",
      "26000/49000 loss: 0.27887785413447946\n",
      "28000/49000 loss: 0.3015767224166175\n",
      "30000/49000 loss: 0.34518372802426806\n",
      "32000/49000 loss: 0.3559078647010067\n",
      "34000/49000 loss: 0.27241435227457667\n",
      "36000/49000 loss: 0.2681192260634636\n",
      "38000/49000 loss: 0.297667127801861\n",
      "40000/49000 loss: 0.30199913453318344\n",
      "42000/49000 loss: 0.3080701660971275\n",
      "44000/49000 loss: 0.32511115699220317\n",
      "46000/49000 loss: 0.33822493187122604\n",
      "48000/49000 loss: 0.3194862297088265\n",
      "epoch 27: valid acc = 0.884, new learning rate = 0.0001251720448712274\n",
      "2000/49000 loss: 0.32791666289973476\n",
      "4000/49000 loss: 0.381784026523388\n",
      "6000/49000 loss: 0.3044372142417513\n",
      "8000/49000 loss: 0.31234558468970625\n",
      "10000/49000 loss: 0.2913964477816774\n",
      "12000/49000 loss: 0.187613038461265\n",
      "14000/49000 loss: 0.24771975593517928\n",
      "16000/49000 loss: 0.32544067031484747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/49000 loss: 0.24940760150499056\n",
      "20000/49000 loss: 0.26187496577005426\n",
      "22000/49000 loss: 0.3780228014147027\n",
      "24000/49000 loss: 0.26521115927253036\n",
      "26000/49000 loss: 0.2467740437537906\n",
      "28000/49000 loss: 0.27269808179466026\n",
      "30000/49000 loss: 0.29970913315709863\n",
      "32000/49000 loss: 0.3465971162781181\n",
      "34000/49000 loss: 0.2962553862060581\n",
      "36000/49000 loss: 0.2511400662375161\n",
      "38000/49000 loss: 0.27317798816393957\n",
      "40000/49000 loss: 0.2280729230964069\n",
      "42000/49000 loss: 0.32577772868860405\n",
      "44000/49000 loss: 0.4058136358971352\n",
      "46000/49000 loss: 0.26982466790859255\n",
      "48000/49000 loss: 0.28988985521225163\n",
      "epoch 28: valid acc = 0.881, new learning rate = 0.00011891344262766602\n",
      "2000/49000 loss: 0.3226172196685016\n",
      "4000/49000 loss: 0.36830883569351985\n",
      "6000/49000 loss: 0.2819546883998\n",
      "8000/49000 loss: 0.2943255100314558\n",
      "10000/49000 loss: 0.2582594506137161\n",
      "12000/49000 loss: 0.21569959269684222\n",
      "14000/49000 loss: 0.3556125125428565\n",
      "16000/49000 loss: 0.25809437745147784\n",
      "18000/49000 loss: 0.3100005681753747\n",
      "20000/49000 loss: 0.3235083519349446\n",
      "22000/49000 loss: 0.29010595116247495\n",
      "24000/49000 loss: 0.2448200861519791\n",
      "26000/49000 loss: 0.25613102107950886\n",
      "28000/49000 loss: 0.32478607817994865\n",
      "30000/49000 loss: 0.1765446320823278\n",
      "32000/49000 loss: 0.24578493587974862\n",
      "34000/49000 loss: 0.26246609591131786\n",
      "36000/49000 loss: 0.2978984634214792\n",
      "38000/49000 loss: 0.34146114256919136\n",
      "40000/49000 loss: 0.3102902758155413\n",
      "42000/49000 loss: 0.25422759726342353\n",
      "44000/49000 loss: 0.3469161106575431\n",
      "46000/49000 loss: 0.3449190419298253\n",
      "48000/49000 loss: 0.2756002121353609\n",
      "epoch 29: valid acc = 0.887, new learning rate = 0.00011296777049628272\n",
      "2000/49000 loss: 0.32000536974462024\n",
      "4000/49000 loss: 0.2529809565521959\n",
      "6000/49000 loss: 0.24265481122938143\n",
      "8000/49000 loss: 0.3020209563843753\n",
      "10000/49000 loss: 0.3499346140699574\n",
      "12000/49000 loss: 0.22254907751279268\n",
      "14000/49000 loss: 0.2710840166784014\n",
      "16000/49000 loss: 0.3201887430090642\n",
      "18000/49000 loss: 0.2827631325085609\n",
      "20000/49000 loss: 0.38250030876522384\n",
      "22000/49000 loss: 0.29519650427632793\n",
      "24000/49000 loss: 0.31669966965451196\n",
      "26000/49000 loss: 0.27427773608596956\n",
      "28000/49000 loss: 0.3444784053773039\n",
      "30000/49000 loss: 0.32130547955892585\n",
      "32000/49000 loss: 0.2495775564838794\n",
      "34000/49000 loss: 0.29488929692608795\n",
      "36000/49000 loss: 0.22322355051399326\n",
      "38000/49000 loss: 0.156529312841336\n",
      "40000/49000 loss: 0.28743865835561266\n",
      "42000/49000 loss: 0.3040890963819177\n",
      "44000/49000 loss: 0.3101691702088841\n",
      "46000/49000 loss: 0.28894702106260634\n",
      "48000/49000 loss: 0.2773511548298537\n",
      "epoch 30: valid acc = 0.887, new learning rate = 0.00010731938197146858\n",
      "2000/49000 loss: 0.2882305776089215\n",
      "4000/49000 loss: 0.25656165575918144\n",
      "6000/49000 loss: 0.2535638227284394\n",
      "8000/49000 loss: 0.22549271849679015\n",
      "10000/49000 loss: 0.2310338109222756\n",
      "12000/49000 loss: 0.31755843239333853\n",
      "14000/49000 loss: 0.2180868221704356\n",
      "16000/49000 loss: 0.23053679951112732\n",
      "18000/49000 loss: 0.24576166267924182\n",
      "20000/49000 loss: 0.26651948880915194\n",
      "22000/49000 loss: 0.36662853864171974\n",
      "24000/49000 loss: 0.2555066650734698\n",
      "26000/49000 loss: 0.3066811224488929\n",
      "28000/49000 loss: 0.27735884812067024\n",
      "30000/49000 loss: 0.2304678551371913\n",
      "32000/49000 loss: 0.20507714439718908\n",
      "34000/49000 loss: 0.2746344849290487\n",
      "36000/49000 loss: 0.31976201704931606\n",
      "38000/49000 loss: 0.3624304309663595\n",
      "40000/49000 loss: 0.30551326481541335\n",
      "42000/49000 loss: 0.2869655994457915\n",
      "44000/49000 loss: 0.2883043913117937\n",
      "46000/49000 loss: 0.25774311080351753\n",
      "48000/49000 loss: 0.3404901182234076\n",
      "epoch 31: valid acc = 0.888, new learning rate = 0.00010195341287289515\n",
      "2000/49000 loss: 0.28953031168879073\n",
      "4000/49000 loss: 0.2742661120305758\n",
      "6000/49000 loss: 0.26351113690878913\n",
      "8000/49000 loss: 0.2612524926586416\n",
      "10000/49000 loss: 0.39693391105282183\n",
      "12000/49000 loss: 0.21928800788434305\n",
      "14000/49000 loss: 0.34217688951052544\n",
      "16000/49000 loss: 0.3731467638396754\n",
      "18000/49000 loss: 0.345831098843265\n",
      "20000/49000 loss: 0.2864847870343676\n",
      "22000/49000 loss: 0.311846411639282\n",
      "24000/49000 loss: 0.33451228886332907\n",
      "26000/49000 loss: 0.279972045595234\n",
      "28000/49000 loss: 0.2788621696412632\n",
      "30000/49000 loss: 0.2839084790710398\n",
      "32000/49000 loss: 0.3828572193121857\n",
      "34000/49000 loss: 0.2149941634138888\n",
      "36000/49000 loss: 0.2692783898237088\n",
      "38000/49000 loss: 0.28806308985788653\n",
      "40000/49000 loss: 0.27766501108546116\n",
      "42000/49000 loss: 0.35910242406011633\n",
      "44000/49000 loss: 0.26144421990419586\n",
      "46000/49000 loss: 0.22169153122075996\n",
      "48000/49000 loss: 0.2110585914493064\n",
      "epoch 32: valid acc = 0.882, new learning rate = 9.685574222925039e-05\n",
      "2000/49000 loss: 0.3787160208417945\n",
      "4000/49000 loss: 0.33282087632645346\n",
      "6000/49000 loss: 0.17867622060646146\n",
      "8000/49000 loss: 0.2687214126734168\n",
      "10000/49000 loss: 0.19949613150166887\n",
      "12000/49000 loss: 0.33430246933655444\n",
      "14000/49000 loss: 0.24840067784957134\n",
      "16000/49000 loss: 0.44887135312386073\n",
      "18000/49000 loss: 0.28003597367200034\n",
      "20000/49000 loss: 0.30396707152082597\n",
      "22000/49000 loss: 0.2438509875930345\n",
      "24000/49000 loss: 0.2937788143225142\n",
      "26000/49000 loss: 0.3212174728506837\n",
      "28000/49000 loss: 0.28007227448751243\n",
      "30000/49000 loss: 0.33463322760582953\n",
      "32000/49000 loss: 0.3165229761703079\n",
      "34000/49000 loss: 0.26814730868482195\n",
      "36000/49000 loss: 0.2375260275390035\n",
      "38000/49000 loss: 0.32480377348214357\n",
      "40000/49000 loss: 0.2512925790514191\n",
      "42000/49000 loss: 0.30577401369700086\n",
      "44000/49000 loss: 0.1824064913164148\n",
      "46000/49000 loss: 0.2589129668603623\n",
      "48000/49000 loss: 0.2671353451277574\n",
      "epoch 33: valid acc = 0.885, new learning rate = 9.201295511778786e-05\n",
      "2000/49000 loss: 0.2778995127801239\n",
      "4000/49000 loss: 0.22640352545869477\n",
      "6000/49000 loss: 0.3373684184547608\n",
      "8000/49000 loss: 0.22401907163128587\n",
      "10000/49000 loss: 0.2944584647502314\n",
      "12000/49000 loss: 0.27095585614321094\n",
      "14000/49000 loss: 0.295850352348003\n",
      "16000/49000 loss: 0.2618194037906399\n",
      "18000/49000 loss: 0.30778167143964674\n",
      "20000/49000 loss: 0.35649483918168345\n",
      "22000/49000 loss: 0.4476805587923103\n",
      "24000/49000 loss: 0.3188885601453678\n",
      "26000/49000 loss: 0.2773532014939475\n",
      "28000/49000 loss: 0.24147652284115267\n",
      "30000/49000 loss: 0.2800748676715705\n",
      "32000/49000 loss: 0.26972659391794257\n",
      "34000/49000 loss: 0.30512412646143744\n",
      "36000/49000 loss: 0.30691407113716757\n",
      "38000/49000 loss: 0.29227923719125887\n",
      "40000/49000 loss: 0.3133112968986471\n",
      "42000/49000 loss: 0.3502444715991333\n",
      "44000/49000 loss: 0.30880664496118354\n",
      "46000/49000 loss: 0.3195709972144975\n",
      "48000/49000 loss: 0.22548424307288809\n",
      "epoch 34: valid acc = 0.887, new learning rate = 8.741230736189846e-05\n",
      "2000/49000 loss: 0.28834962110487056\n",
      "4000/49000 loss: 0.2586681267164596\n",
      "6000/49000 loss: 0.24297828917266018\n",
      "8000/49000 loss: 0.36430671508978413\n",
      "10000/49000 loss: 0.24123429260955095\n",
      "12000/49000 loss: 0.26281786302862536\n",
      "14000/49000 loss: 0.3303860921103436\n",
      "16000/49000 loss: 0.33668762746737785\n",
      "18000/49000 loss: 0.25275337255218616\n",
      "20000/49000 loss: 0.23631163383781723\n",
      "22000/49000 loss: 0.2725197150842158\n",
      "24000/49000 loss: 0.2413557963443024\n",
      "26000/49000 loss: 0.3144436026643046\n",
      "28000/49000 loss: 0.232881434599512\n",
      "30000/49000 loss: 0.3447528981172479\n",
      "32000/49000 loss: 0.2695193743891487\n",
      "34000/49000 loss: 0.2958031948700277\n",
      "36000/49000 loss: 0.273833501259828\n",
      "38000/49000 loss: 0.25254447908608824\n",
      "40000/49000 loss: 0.34957054767651635\n",
      "42000/49000 loss: 0.29388886715500345\n",
      "44000/49000 loss: 0.3235342606875512\n",
      "46000/49000 loss: 0.2606298548632969\n",
      "48000/49000 loss: 0.2697397397654691\n",
      "epoch 35: valid acc = 0.885, new learning rate = 8.304169199380353e-05\n",
      "2000/49000 loss: 0.28846781636655205\n",
      "4000/49000 loss: 0.33374081912432174\n",
      "6000/49000 loss: 0.2470516296271754\n",
      "8000/49000 loss: 0.24675491311068962\n",
      "10000/49000 loss: 0.26527225723823106\n",
      "12000/49000 loss: 0.3374486662194667\n",
      "14000/49000 loss: 0.25965128174374436\n",
      "16000/49000 loss: 0.28610017929818354\n",
      "18000/49000 loss: 0.3088361200566712\n",
      "20000/49000 loss: 0.2675140010551905\n",
      "22000/49000 loss: 0.3173029394323739\n",
      "24000/49000 loss: 0.2958834258952456\n",
      "26000/49000 loss: 0.2633407956359262\n",
      "28000/49000 loss: 0.25097048608995853\n",
      "30000/49000 loss: 0.3696367193033928\n",
      "32000/49000 loss: 0.3131326454745332\n",
      "34000/49000 loss: 0.21233413636127726\n",
      "36000/49000 loss: 0.3055348055490847\n",
      "38000/49000 loss: 0.24303873085193325\n",
      "40000/49000 loss: 0.23798511235541162\n",
      "42000/49000 loss: 0.26390916648661555\n",
      "44000/49000 loss: 0.2857376692927039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46000/49000 loss: 0.3229627771279191\n",
      "48000/49000 loss: 0.415313181342329\n",
      "epoch 36: valid acc = 0.886, new learning rate = 7.888960739411335e-05\n",
      "2000/49000 loss: 0.3053261526442972\n",
      "4000/49000 loss: 0.20212556575597465\n",
      "6000/49000 loss: 0.29718266849654523\n",
      "8000/49000 loss: 0.35187797930348347\n",
      "10000/49000 loss: 0.35930304381373596\n",
      "12000/49000 loss: 0.2802382099392275\n",
      "14000/49000 loss: 0.3343899560868999\n",
      "16000/49000 loss: 0.21481485436387038\n",
      "18000/49000 loss: 0.3915618234596518\n",
      "20000/49000 loss: 0.18478721467435671\n",
      "22000/49000 loss: 0.3287873883028456\n",
      "24000/49000 loss: 0.28622152367017806\n",
      "26000/49000 loss: 0.3674987932643775\n",
      "28000/49000 loss: 0.21854705860078535\n",
      "30000/49000 loss: 0.24801812053033614\n",
      "32000/49000 loss: 0.2755458497222508\n",
      "34000/49000 loss: 0.29177647751303076\n",
      "36000/49000 loss: 0.28832784413470336\n",
      "38000/49000 loss: 0.3125897480099287\n",
      "40000/49000 loss: 0.3336466783452216\n",
      "42000/49000 loss: 0.26883045284309326\n",
      "44000/49000 loss: 0.3326541213078193\n",
      "46000/49000 loss: 0.21553391462709434\n",
      "48000/49000 loss: 0.26674930034734934\n",
      "epoch 37: valid acc = 0.881, new learning rate = 7.494512702440768e-05\n",
      "2000/49000 loss: 0.37369055334252993\n",
      "4000/49000 loss: 0.25853851758762486\n",
      "6000/49000 loss: 0.24054724469486255\n",
      "8000/49000 loss: 0.345612259216651\n",
      "10000/49000 loss: 0.3135341612729458\n",
      "12000/49000 loss: 0.2762468339657073\n",
      "14000/49000 loss: 0.3050793726459358\n",
      "16000/49000 loss: 0.2386709393038921\n",
      "18000/49000 loss: 0.3307722986534679\n",
      "20000/49000 loss: 0.2430088155124339\n",
      "22000/49000 loss: 0.3438033784652405\n",
      "24000/49000 loss: 0.2542270935728617\n",
      "26000/49000 loss: 0.3197559215035465\n",
      "28000/49000 loss: 0.2576714774371278\n",
      "30000/49000 loss: 0.25296503046590463\n",
      "32000/49000 loss: 0.26901382001302887\n",
      "34000/49000 loss: 0.2652030883160629\n",
      "36000/49000 loss: 0.2879103109448787\n",
      "38000/49000 loss: 0.2772688301938838\n",
      "40000/49000 loss: 0.2507423323202091\n",
      "42000/49000 loss: 0.28085730245130913\n",
      "44000/49000 loss: 0.28750997171414316\n",
      "46000/49000 loss: 0.3830257063872219\n",
      "48000/49000 loss: 0.22118252526974763\n",
      "epoch 38: valid acc = 0.882, new learning rate = 7.119787067318729e-05\n",
      "2000/49000 loss: 0.21864947839558915\n",
      "4000/49000 loss: 0.2459094100804036\n",
      "6000/49000 loss: 0.3306795352672801\n",
      "8000/49000 loss: 0.22347989450618955\n",
      "10000/49000 loss: 0.1914503183653956\n",
      "12000/49000 loss: 0.30311056070149467\n",
      "14000/49000 loss: 0.3330250723671238\n",
      "16000/49000 loss: 0.2693083630737857\n",
      "18000/49000 loss: 0.24022799681139057\n",
      "20000/49000 loss: 0.22376411617725683\n",
      "22000/49000 loss: 0.4310674950604467\n",
      "24000/49000 loss: 0.34247605737972014\n",
      "26000/49000 loss: 0.3410080872425143\n",
      "28000/49000 loss: 0.2868936720180997\n",
      "30000/49000 loss: 0.2745737637395071\n",
      "32000/49000 loss: 0.301044885063365\n",
      "34000/49000 loss: 0.3015113197690252\n",
      "36000/49000 loss: 0.25735400003984565\n",
      "38000/49000 loss: 0.23626014887509847\n",
      "40000/49000 loss: 0.23197181242350914\n",
      "42000/49000 loss: 0.2644113382321232\n",
      "44000/49000 loss: 0.31428754240452\n",
      "46000/49000 loss: 0.3468891654042863\n",
      "48000/49000 loss: 0.21113460762869818\n",
      "epoch 39: valid acc = 0.882, new learning rate = 6.763797713952792e-05\n",
      "2000/49000 loss: 0.3050919286632891\n",
      "4000/49000 loss: 0.25129460581826935\n",
      "6000/49000 loss: 0.38142246392814577\n",
      "8000/49000 loss: 0.3260473123031792\n",
      "10000/49000 loss: 0.263412881204321\n",
      "12000/49000 loss: 0.3052460965051431\n",
      "14000/49000 loss: 0.2933265061932375\n",
      "16000/49000 loss: 0.32670493356966096\n",
      "18000/49000 loss: 0.21437347463638098\n",
      "20000/49000 loss: 0.3084496796480396\n",
      "22000/49000 loss: 0.21198316860666672\n",
      "24000/49000 loss: 0.20495082795904523\n",
      "26000/49000 loss: 0.23800352936974503\n",
      "28000/49000 loss: 0.3345461687746545\n",
      "30000/49000 loss: 0.32191854836995576\n",
      "32000/49000 loss: 0.3264229183640179\n",
      "34000/49000 loss: 0.35958243151268565\n",
      "36000/49000 loss: 0.28136859072553505\n",
      "38000/49000 loss: 0.2789253329337521\n",
      "40000/49000 loss: 0.22387795366658855\n",
      "42000/49000 loss: 0.282770361518239\n",
      "44000/49000 loss: 0.2777605201773553\n",
      "46000/49000 loss: 0.2941814153591392\n",
      "48000/49000 loss: 0.25091661116648906\n",
      "epoch 40: valid acc = 0.886, new learning rate = 6.425607828255152e-05\n",
      "2000/49000 loss: 0.30636160401656165\n",
      "4000/49000 loss: 0.2707226109255315\n",
      "6000/49000 loss: 0.27183302476051874\n",
      "8000/49000 loss: 0.22804430013881358\n",
      "10000/49000 loss: 0.22425886753324384\n",
      "12000/49000 loss: 0.26815453689166063\n",
      "14000/49000 loss: 0.3195632317847993\n",
      "16000/49000 loss: 0.2790859417699277\n",
      "18000/49000 loss: 0.31566349603686195\n",
      "20000/49000 loss: 0.34780864596664596\n",
      "22000/49000 loss: 0.28443912646344366\n",
      "24000/49000 loss: 0.2557033582495158\n",
      "26000/49000 loss: 0.27591585618659364\n",
      "28000/49000 loss: 0.2567778766411032\n",
      "30000/49000 loss: 0.25823647207952233\n",
      "32000/49000 loss: 0.21480314741643375\n",
      "34000/49000 loss: 0.2687097275315171\n",
      "36000/49000 loss: 0.34869562610307225\n",
      "38000/49000 loss: 0.23733351132211405\n",
      "40000/49000 loss: 0.20634408651255512\n",
      "42000/49000 loss: 0.33480887973049633\n",
      "44000/49000 loss: 0.3145138887578839\n",
      "46000/49000 loss: 0.2275160862300507\n",
      "48000/49000 loss: 0.2637298499780855\n",
      "epoch 41: valid acc = 0.886, new learning rate = 6.104327436842394e-05\n",
      "2000/49000 loss: 0.29180473918339295\n",
      "4000/49000 loss: 0.3489257081141884\n",
      "6000/49000 loss: 0.28958013119070125\n",
      "8000/49000 loss: 0.25500247230853174\n",
      "10000/49000 loss: 0.21384281410830225\n",
      "12000/49000 loss: 0.3478284795800315\n",
      "14000/49000 loss: 0.237308731998587\n",
      "16000/49000 loss: 0.2659932714898893\n",
      "18000/49000 loss: 0.35775108499356617\n",
      "20000/49000 loss: 0.21476335967813073\n",
      "22000/49000 loss: 0.25718111634128055\n",
      "24000/49000 loss: 0.22397430993361092\n",
      "26000/49000 loss: 0.26239512146380684\n",
      "28000/49000 loss: 0.22779502804995025\n",
      "30000/49000 loss: 0.27828679767201797\n",
      "32000/49000 loss: 0.2760265121592137\n",
      "34000/49000 loss: 0.28209680813671106\n",
      "36000/49000 loss: 0.21026835295353122\n",
      "38000/49000 loss: 0.274520051769278\n",
      "40000/49000 loss: 0.3298812133071589\n",
      "42000/49000 loss: 0.25963612359419286\n",
      "44000/49000 loss: 0.2801355669398165\n",
      "46000/49000 loss: 0.2384730563936335\n",
      "48000/49000 loss: 0.3169550562014006\n",
      "epoch 42: valid acc = 0.888, new learning rate = 5.799111065000274e-05\n",
      "2000/49000 loss: 0.26391315793436043\n",
      "4000/49000 loss: 0.2537938577215591\n",
      "6000/49000 loss: 0.2701889975307705\n",
      "8000/49000 loss: 0.26099322643526235\n",
      "10000/49000 loss: 0.288576861696665\n",
      "12000/49000 loss: 0.28680043903419405\n",
      "14000/49000 loss: 0.2877417707702363\n",
      "16000/49000 loss: 0.3156039198675322\n",
      "18000/49000 loss: 0.20369405460786666\n",
      "20000/49000 loss: 0.22550852626412443\n",
      "22000/49000 loss: 0.19583562809186814\n",
      "24000/49000 loss: 0.33286488073897413\n",
      "26000/49000 loss: 0.20527952666849725\n",
      "28000/49000 loss: 0.423667662248588\n",
      "30000/49000 loss: 0.3292746372782544\n",
      "32000/49000 loss: 0.24567993673330576\n",
      "34000/49000 loss: 0.2325956526456191\n",
      "36000/49000 loss: 0.26686008055247484\n",
      "38000/49000 loss: 0.31115356202639677\n",
      "40000/49000 loss: 0.3395556586889415\n",
      "42000/49000 loss: 0.3170671450145096\n",
      "44000/49000 loss: 0.3378933199893609\n",
      "46000/49000 loss: 0.24962055354022147\n",
      "48000/49000 loss: 0.40668455370231216\n",
      "epoch 43: valid acc = 0.886, new learning rate = 5.5091555117502596e-05\n",
      "2000/49000 loss: 0.1877259602050858\n",
      "4000/49000 loss: 0.2287455717354237\n",
      "6000/49000 loss: 0.3004690406304033\n",
      "8000/49000 loss: 0.21299211940210686\n",
      "10000/49000 loss: 0.29621648478545715\n",
      "12000/49000 loss: 0.23424977401599437\n",
      "14000/49000 loss: 0.25338680656360607\n",
      "16000/49000 loss: 0.3052920607518983\n",
      "18000/49000 loss: 0.20811006559396128\n",
      "20000/49000 loss: 0.2562969349069935\n",
      "22000/49000 loss: 0.32881029729249667\n",
      "24000/49000 loss: 0.2647288573876966\n",
      "26000/49000 loss: 0.2877256441582271\n",
      "28000/49000 loss: 0.3424425732174845\n",
      "30000/49000 loss: 0.2667012114607669\n",
      "32000/49000 loss: 0.2985887539985031\n",
      "34000/49000 loss: 0.33517578787126756\n",
      "36000/49000 loss: 0.32352248253283555\n",
      "38000/49000 loss: 0.2096154600880591\n",
      "40000/49000 loss: 0.3423513580283868\n",
      "42000/49000 loss: 0.227069178713619\n",
      "44000/49000 loss: 0.35799628055654803\n",
      "46000/49000 loss: 0.260272642545154\n",
      "48000/49000 loss: 0.38266098622405714\n",
      "epoch 44: valid acc = 0.887, new learning rate = 5.2336977361627463e-05\n",
      "2000/49000 loss: 0.2731318488545679\n",
      "4000/49000 loss: 0.31635673063259157\n",
      "6000/49000 loss: 0.3248418554923467\n",
      "8000/49000 loss: 0.15122449361328794\n",
      "10000/49000 loss: 0.2512368773341927\n",
      "12000/49000 loss: 0.30781153224962743\n",
      "14000/49000 loss: 0.3143011296044971\n",
      "16000/49000 loss: 0.24542299844257345\n",
      "18000/49000 loss: 0.20319650265103312\n",
      "20000/49000 loss: 0.28920478132365346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22000/49000 loss: 0.36925210657950674\n",
      "24000/49000 loss: 0.24047461471782805\n",
      "26000/49000 loss: 0.25596146549271176\n",
      "28000/49000 loss: 0.30219508910657733\n",
      "30000/49000 loss: 0.3275870126216501\n",
      "32000/49000 loss: 0.3466822444691633\n",
      "34000/49000 loss: 0.2795980427515727\n",
      "36000/49000 loss: 0.25412272034285355\n",
      "38000/49000 loss: 0.3245172074095834\n",
      "40000/49000 loss: 0.2816542102056645\n",
      "42000/49000 loss: 0.2774592952190595\n",
      "44000/49000 loss: 0.2955273934414634\n",
      "46000/49000 loss: 0.22473586430590062\n",
      "48000/49000 loss: 0.29853809632284645\n",
      "epoch 45: valid acc = 0.886, new learning rate = 4.972012849354609e-05\n",
      "2000/49000 loss: 0.2401934790072746\n",
      "4000/49000 loss: 0.3343867457568936\n",
      "6000/49000 loss: 0.24220944261538313\n",
      "8000/49000 loss: 0.2841613415045401\n",
      "10000/49000 loss: 0.3284310208579281\n",
      "12000/49000 loss: 0.28996753333971986\n",
      "14000/49000 loss: 0.3827240490352692\n",
      "16000/49000 loss: 0.3222839773114729\n",
      "18000/49000 loss: 0.2628150170748059\n",
      "20000/49000 loss: 0.285692444764093\n",
      "22000/49000 loss: 0.3047839698619659\n",
      "24000/49000 loss: 0.40902382097617057\n",
      "26000/49000 loss: 0.24306290354013937\n",
      "28000/49000 loss: 0.25386643612755\n",
      "30000/49000 loss: 0.28862973442563566\n",
      "32000/49000 loss: 0.25298220484725215\n",
      "34000/49000 loss: 0.23475153378386263\n",
      "36000/49000 loss: 0.29526292284918576\n",
      "38000/49000 loss: 0.18425953737785855\n",
      "40000/49000 loss: 0.32196308887033853\n",
      "42000/49000 loss: 0.281203020591731\n",
      "44000/49000 loss: 0.2392551332114666\n",
      "46000/49000 loss: 0.20045537866733296\n",
      "48000/49000 loss: 0.272062590065898\n",
      "epoch 46: valid acc = 0.888, new learning rate = 4.723412206886878e-05\n",
      "2000/49000 loss: 0.36716810149562296\n",
      "4000/49000 loss: 0.2405557794943111\n",
      "6000/49000 loss: 0.24142737872051137\n",
      "8000/49000 loss: 0.23867471211428062\n",
      "10000/49000 loss: 0.2925520109205394\n",
      "12000/49000 loss: 0.2513391707596396\n",
      "14000/49000 loss: 0.24822377833784395\n",
      "16000/49000 loss: 0.2673766205236477\n",
      "18000/49000 loss: 0.30037869322808464\n",
      "20000/49000 loss: 0.28483566088738527\n",
      "22000/49000 loss: 0.27153109152586746\n",
      "24000/49000 loss: 0.2708628953184708\n",
      "26000/49000 loss: 0.295949260649275\n",
      "28000/49000 loss: 0.24540948442972016\n",
      "30000/49000 loss: 0.22328488425082146\n",
      "32000/49000 loss: 0.24279780610410165\n",
      "34000/49000 loss: 0.28926307438797405\n",
      "36000/49000 loss: 0.29166601679288456\n",
      "38000/49000 loss: 0.28878580557539224\n",
      "40000/49000 loss: 0.2873110833683693\n",
      "42000/49000 loss: 0.2814577283783075\n",
      "44000/49000 loss: 0.19401344684253027\n",
      "46000/49000 loss: 0.4112968144265594\n",
      "48000/49000 loss: 0.18811304171342938\n",
      "epoch 47: valid acc = 0.887, new learning rate = 4.487241596542534e-05\n",
      "2000/49000 loss: 0.25043206256968276\n",
      "4000/49000 loss: 0.2603978572182832\n",
      "6000/49000 loss: 0.1703094559912009\n",
      "8000/49000 loss: 0.24832963522320045\n",
      "10000/49000 loss: 0.33407992481604154\n",
      "12000/49000 loss: 0.16604385344262004\n",
      "14000/49000 loss: 0.43495133213925946\n",
      "16000/49000 loss: 0.2729270312797565\n",
      "18000/49000 loss: 0.2435049331395796\n",
      "20000/49000 loss: 0.25569252256837427\n",
      "22000/49000 loss: 0.1812675498324481\n",
      "24000/49000 loss: 0.23876002420034928\n",
      "26000/49000 loss: 0.2584572293086081\n",
      "28000/49000 loss: 0.32651987113210407\n",
      "30000/49000 loss: 0.285377184166088\n",
      "32000/49000 loss: 0.3303874976889032\n",
      "34000/49000 loss: 0.34553059928109087\n",
      "36000/49000 loss: 0.2455101988007835\n",
      "38000/49000 loss: 0.2450692646380069\n",
      "40000/49000 loss: 0.2270756104871731\n",
      "42000/49000 loss: 0.23580250662652727\n",
      "44000/49000 loss: 0.32043459180443895\n",
      "46000/49000 loss: 0.21505070691914904\n",
      "48000/49000 loss: 0.2965370548800561\n",
      "epoch 48: valid acc = 0.885, new learning rate = 4.262879516715407e-05\n",
      "2000/49000 loss: 0.24475373207877232\n",
      "4000/49000 loss: 0.2573570360827042\n",
      "6000/49000 loss: 0.3155670010611947\n",
      "8000/49000 loss: 0.22809089736834698\n",
      "10000/49000 loss: 0.3111472109319959\n",
      "12000/49000 loss: 0.27900733971279007\n",
      "14000/49000 loss: 0.25452178766711553\n",
      "16000/49000 loss: 0.3078079556863545\n",
      "18000/49000 loss: 0.22773671323325131\n",
      "20000/49000 loss: 0.2232550208089147\n",
      "22000/49000 loss: 0.24044359325391473\n",
      "24000/49000 loss: 0.36832975245631006\n",
      "26000/49000 loss: 0.27371575802275594\n",
      "28000/49000 loss: 0.2786228470100497\n",
      "30000/49000 loss: 0.2590964481151567\n",
      "32000/49000 loss: 0.2471856101604755\n",
      "34000/49000 loss: 0.28542287703195746\n",
      "36000/49000 loss: 0.32161305682758873\n",
      "38000/49000 loss: 0.2563945644329489\n",
      "40000/49000 loss: 0.2829636560928971\n",
      "42000/49000 loss: 0.2583391381670754\n",
      "44000/49000 loss: 0.2901490002528853\n",
      "46000/49000 loss: 0.2216900219529267\n",
      "48000/49000 loss: 0.24461543364395374\n",
      "epoch 49: valid acc = 0.886, new learning rate = 4.049735540879637e-05\n",
      "2000/49000 loss: 0.27161201704748\n",
      "4000/49000 loss: 0.2637426226886484\n",
      "6000/49000 loss: 0.37567445817937956\n",
      "8000/49000 loss: 0.377246896205389\n",
      "10000/49000 loss: 0.21640968730460983\n",
      "12000/49000 loss: 0.2735390750366534\n",
      "14000/49000 loss: 0.31462717320523126\n",
      "16000/49000 loss: 0.21908398060436401\n",
      "18000/49000 loss: 0.31081217762352303\n",
      "20000/49000 loss: 0.3126480258864638\n",
      "22000/49000 loss: 0.20537390341473677\n",
      "24000/49000 loss: 0.2378144824220577\n",
      "26000/49000 loss: 0.2360524920177306\n",
      "28000/49000 loss: 0.22187554787279556\n",
      "30000/49000 loss: 0.3135753776485568\n",
      "32000/49000 loss: 0.2620559556619908\n",
      "34000/49000 loss: 0.25198226846618693\n",
      "36000/49000 loss: 0.24597953924616806\n",
      "38000/49000 loss: 0.3347319495995759\n",
      "40000/49000 loss: 0.2883618927852321\n",
      "42000/49000 loss: 0.22741056911803684\n",
      "44000/49000 loss: 0.27583337769898036\n",
      "46000/49000 loss: 0.23919771526337857\n",
      "48000/49000 loss: 0.3616125197153684\n",
      "epoch 50: valid acc = 0.884, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.9029387755102041\n",
      "test acc: 0.884\n",
      "test acc: 0.8724\n",
      "number of batches for training: 245\n",
      "epoch 1: valid acc = 0.738, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.801, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.825, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.846, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.848, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.866, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.868, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.868, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.874, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.871, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.872, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.874, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.876, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.881, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.884, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.883, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.876, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.885, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.888, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.888, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.892, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.888, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.886, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.884, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.887, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.885, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.883, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.89, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.887, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.893, new learning rate = 0.00010731938197146858\n",
      "epoch 31: valid acc = 0.884, new learning rate = 0.00010195341287289515\n",
      "epoch 32: valid acc = 0.89, new learning rate = 9.685574222925039e-05\n",
      "epoch 33: valid acc = 0.891, new learning rate = 9.201295511778786e-05\n",
      "epoch 34: valid acc = 0.89, new learning rate = 8.741230736189846e-05\n",
      "epoch 35: valid acc = 0.889, new learning rate = 8.304169199380353e-05\n",
      "epoch 36: valid acc = 0.893, new learning rate = 7.888960739411335e-05\n",
      "epoch 37: valid acc = 0.892, new learning rate = 7.494512702440768e-05\n",
      "epoch 38: valid acc = 0.892, new learning rate = 7.119787067318729e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 39: valid acc = 0.888, new learning rate = 6.763797713952792e-05\n",
      "epoch 40: valid acc = 0.892, new learning rate = 6.425607828255152e-05\n",
      "epoch 41: valid acc = 0.888, new learning rate = 6.104327436842394e-05\n",
      "epoch 42: valid acc = 0.889, new learning rate = 5.799111065000274e-05\n",
      "epoch 43: valid acc = 0.896, new learning rate = 5.5091555117502596e-05\n",
      "epoch 44: valid acc = 0.893, new learning rate = 5.2336977361627463e-05\n",
      "epoch 45: valid acc = 0.892, new learning rate = 4.972012849354609e-05\n",
      "epoch 46: valid acc = 0.893, new learning rate = 4.723412206886878e-05\n",
      "epoch 47: valid acc = 0.888, new learning rate = 4.487241596542534e-05\n",
      "epoch 48: valid acc = 0.893, new learning rate = 4.262879516715407e-05\n",
      "epoch 49: valid acc = 0.891, new learning rate = 4.049735540879637e-05\n",
      "epoch 50: valid acc = 0.893, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.904530612244898\n",
      "test acc: 0.893\n",
      "test acc: 0.8745\n",
      "number of batches for training: 245\n",
      "2000/49000 loss: 2.9042295557011224\n",
      "4000/49000 loss: 2.9991912689754936\n",
      "6000/49000 loss: 2.576844490347351\n",
      "8000/49000 loss: 2.4253216449613437\n",
      "10000/49000 loss: 2.2336296299294847\n",
      "12000/49000 loss: 2.1850697344249994\n",
      "14000/49000 loss: 1.9874545948761257\n",
      "16000/49000 loss: 1.6907146407666807\n",
      "18000/49000 loss: 1.3512669346994548\n",
      "20000/49000 loss: 1.2973275494152814\n",
      "22000/49000 loss: 1.280595108721196\n",
      "24000/49000 loss: 1.1782690664540199\n",
      "26000/49000 loss: 0.9517560543439011\n",
      "28000/49000 loss: 0.9673181742934693\n",
      "30000/49000 loss: 0.9961227127237382\n",
      "32000/49000 loss: 0.9437725108440835\n",
      "34000/49000 loss: 0.9808653199476423\n",
      "36000/49000 loss: 0.84448509454958\n",
      "38000/49000 loss: 1.0270992502541467\n",
      "40000/49000 loss: 0.809243573139086\n",
      "42000/49000 loss: 0.8064316850260481\n",
      "44000/49000 loss: 0.7686030318576319\n",
      "46000/49000 loss: 0.7329793966228522\n",
      "48000/49000 loss: 0.6701901017763161\n",
      "epoch 1: valid acc = 0.739, new learning rate = 0.000475\n",
      "2000/49000 loss: 0.7175928896686041\n",
      "4000/49000 loss: 0.7047691905039593\n",
      "6000/49000 loss: 0.6773135646626501\n",
      "8000/49000 loss: 0.7118166825107971\n",
      "10000/49000 loss: 0.5991799605751277\n",
      "12000/49000 loss: 0.6027618391972314\n",
      "14000/49000 loss: 0.62957097041607\n",
      "16000/49000 loss: 0.6172914239292608\n",
      "18000/49000 loss: 0.6208861806580582\n",
      "20000/49000 loss: 0.49515380488782024\n",
      "22000/49000 loss: 0.5918852278391457\n",
      "24000/49000 loss: 0.5554486515373431\n",
      "26000/49000 loss: 0.6009641849351769\n",
      "28000/49000 loss: 0.5708000881270954\n",
      "30000/49000 loss: 0.6821701309421822\n",
      "32000/49000 loss: 0.5169227905887237\n",
      "34000/49000 loss: 0.5942137461648024\n",
      "36000/49000 loss: 0.5281071451385065\n",
      "38000/49000 loss: 0.45632403622433776\n",
      "40000/49000 loss: 0.6875371798586088\n",
      "42000/49000 loss: 0.5975196698881489\n",
      "44000/49000 loss: 0.6378737805926883\n",
      "46000/49000 loss: 0.39214282360729136\n",
      "48000/49000 loss: 0.571150541307467\n",
      "epoch 2: valid acc = 0.804, new learning rate = 0.00045125\n",
      "2000/49000 loss: 0.44106298792238907\n",
      "4000/49000 loss: 0.4516674536149717\n",
      "6000/49000 loss: 0.47240936300831193\n",
      "8000/49000 loss: 0.4409773944147334\n",
      "10000/49000 loss: 0.4927662548739461\n",
      "12000/49000 loss: 0.5519009721304127\n",
      "14000/49000 loss: 0.4788480075690737\n",
      "16000/49000 loss: 0.5517903826541316\n",
      "18000/49000 loss: 0.4973274200762833\n",
      "20000/49000 loss: 0.4320993958478225\n",
      "22000/49000 loss: 0.5692870486234679\n",
      "24000/49000 loss: 0.4745990388287858\n",
      "26000/49000 loss: 0.511976753199113\n",
      "28000/49000 loss: 0.45216389227706166\n",
      "30000/49000 loss: 0.5141741135976423\n",
      "32000/49000 loss: 0.46903077172361723\n",
      "34000/49000 loss: 0.3974149155520864\n",
      "36000/49000 loss: 0.4229162171067544\n",
      "38000/49000 loss: 0.47574598571570015\n",
      "40000/49000 loss: 0.48443959609664905\n",
      "42000/49000 loss: 0.44327663087369223\n",
      "44000/49000 loss: 0.37221657244651113\n",
      "46000/49000 loss: 0.5412874080016087\n",
      "48000/49000 loss: 0.43059808823708595\n",
      "epoch 3: valid acc = 0.825, new learning rate = 0.0004286875\n",
      "2000/49000 loss: 0.4796110364440581\n",
      "4000/49000 loss: 0.5020548478315526\n",
      "6000/49000 loss: 0.3585151553218215\n",
      "8000/49000 loss: 0.5091760194428455\n",
      "10000/49000 loss: 0.49561023059523807\n",
      "12000/49000 loss: 0.4503952247162661\n",
      "14000/49000 loss: 0.3822820959979398\n",
      "16000/49000 loss: 0.4558612711983035\n",
      "18000/49000 loss: 0.38837421085441043\n",
      "20000/49000 loss: 0.39636396649524624\n",
      "22000/49000 loss: 0.355073513450936\n",
      "24000/49000 loss: 0.4188231972304401\n",
      "26000/49000 loss: 0.39317763399672606\n",
      "28000/49000 loss: 0.35873034772632\n",
      "30000/49000 loss: 0.36207201906033426\n",
      "32000/49000 loss: 0.4689886434649398\n",
      "34000/49000 loss: 0.4333371854858081\n",
      "36000/49000 loss: 0.461434385472074\n",
      "38000/49000 loss: 0.4775686866327997\n",
      "40000/49000 loss: 0.4763770304756537\n",
      "42000/49000 loss: 0.4655908930706807\n",
      "44000/49000 loss: 0.37355077117870183\n",
      "46000/49000 loss: 0.4950364502924632\n",
      "48000/49000 loss: 0.4027405756860564\n",
      "epoch 4: valid acc = 0.839, new learning rate = 0.00040725312499999993\n",
      "2000/49000 loss: 0.5059654529835625\n",
      "4000/49000 loss: 0.48607187930066226\n",
      "6000/49000 loss: 0.5162207624785614\n",
      "8000/49000 loss: 0.42250181896103484\n",
      "10000/49000 loss: 0.4493478513162766\n",
      "12000/49000 loss: 0.37010222891506434\n",
      "14000/49000 loss: 0.4797002968087497\n",
      "16000/49000 loss: 0.41779605974745704\n",
      "18000/49000 loss: 0.5033495607751816\n",
      "20000/49000 loss: 0.4026352769964508\n",
      "22000/49000 loss: 0.4559721606300106\n",
      "24000/49000 loss: 0.44893332395158025\n",
      "26000/49000 loss: 0.3986248182440534\n",
      "28000/49000 loss: 0.364690421165562\n",
      "30000/49000 loss: 0.3226719794200185\n",
      "32000/49000 loss: 0.4829505459074921\n",
      "34000/49000 loss: 0.47625640086178955\n",
      "36000/49000 loss: 0.3742698406349386\n",
      "38000/49000 loss: 0.47697043675692413\n",
      "40000/49000 loss: 0.37184601476127105\n",
      "42000/49000 loss: 0.4971754891680162\n",
      "44000/49000 loss: 0.40347953328263597\n",
      "46000/49000 loss: 0.423794061133423\n",
      "48000/49000 loss: 0.37162821593542816\n",
      "epoch 5: valid acc = 0.85, new learning rate = 0.0003868904687499999\n",
      "2000/49000 loss: 0.33694851682329924\n",
      "4000/49000 loss: 0.41338501403610006\n",
      "6000/49000 loss: 0.469932366805388\n",
      "8000/49000 loss: 0.4704829368874729\n",
      "10000/49000 loss: 0.4623854047259892\n",
      "12000/49000 loss: 0.41302108774600077\n",
      "14000/49000 loss: 0.41937081636032447\n",
      "16000/49000 loss: 0.4274627397025503\n",
      "18000/49000 loss: 0.3800263567312128\n",
      "20000/49000 loss: 0.3877991770953304\n",
      "22000/49000 loss: 0.44462232353724657\n",
      "24000/49000 loss: 0.49087771562570787\n",
      "26000/49000 loss: 0.3311252840207223\n",
      "28000/49000 loss: 0.36918805145680467\n",
      "30000/49000 loss: 0.3556299873795455\n",
      "32000/49000 loss: 0.4184547753218208\n",
      "34000/49000 loss: 0.3838966395171622\n",
      "36000/49000 loss: 0.3774159048936466\n",
      "38000/49000 loss: 0.36825286659622036\n",
      "40000/49000 loss: 0.291550010127187\n",
      "42000/49000 loss: 0.3885250969743602\n",
      "44000/49000 loss: 0.4907698021921155\n",
      "46000/49000 loss: 0.41608951397772875\n",
      "48000/49000 loss: 0.39059704587450006\n",
      "epoch 6: valid acc = 0.857, new learning rate = 0.0003675459453124999\n",
      "2000/49000 loss: 0.373436932510781\n",
      "4000/49000 loss: 0.3806827660680968\n",
      "6000/49000 loss: 0.44679123834005796\n",
      "8000/49000 loss: 0.3770869890704939\n",
      "10000/49000 loss: 0.40354358865666023\n",
      "12000/49000 loss: 0.39388937547463976\n",
      "14000/49000 loss: 0.46779639776775894\n",
      "16000/49000 loss: 0.4132374211640606\n",
      "18000/49000 loss: 0.3703505745197771\n",
      "20000/49000 loss: 0.4295448214946786\n",
      "22000/49000 loss: 0.3692926863113013\n",
      "24000/49000 loss: 0.39721312082297056\n",
      "26000/49000 loss: 0.3569453570996967\n",
      "28000/49000 loss: 0.33082164627802785\n",
      "30000/49000 loss: 0.36630051571844946\n",
      "32000/49000 loss: 0.34861201550785764\n",
      "34000/49000 loss: 0.3401450325311578\n",
      "36000/49000 loss: 0.38056317441346205\n",
      "38000/49000 loss: 0.40751938106166247\n",
      "40000/49000 loss: 0.4048559994198976\n",
      "42000/49000 loss: 0.313408855664241\n",
      "44000/49000 loss: 0.33461349003629104\n",
      "46000/49000 loss: 0.4746877350922931\n",
      "48000/49000 loss: 0.2648877411001279\n",
      "epoch 7: valid acc = 0.867, new learning rate = 0.00034916864804687486\n",
      "2000/49000 loss: 0.40279139701669014\n",
      "4000/49000 loss: 0.3507374859778992\n",
      "6000/49000 loss: 0.49462273665119943\n",
      "8000/49000 loss: 0.4136815401499858\n",
      "10000/49000 loss: 0.29918199290278835\n",
      "12000/49000 loss: 0.40103823924164256\n",
      "14000/49000 loss: 0.34474458419772125\n",
      "16000/49000 loss: 0.40578518846394357\n",
      "18000/49000 loss: 0.3379553948768162\n",
      "20000/49000 loss: 0.2971569582749173\n",
      "22000/49000 loss: 0.31216106339623473\n",
      "24000/49000 loss: 0.43369891699560703\n",
      "26000/49000 loss: 0.42379986293797867\n",
      "28000/49000 loss: 0.3783263903102986\n",
      "30000/49000 loss: 0.38583230249868855\n",
      "32000/49000 loss: 0.44416903132822255\n",
      "34000/49000 loss: 0.34882418448622293\n",
      "36000/49000 loss: 0.32706730159657993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38000/49000 loss: 0.29162620990116844\n",
      "40000/49000 loss: 0.41197942477573934\n",
      "42000/49000 loss: 0.32477828846557477\n",
      "44000/49000 loss: 0.4251164957366756\n",
      "46000/49000 loss: 0.39267725874526394\n",
      "48000/49000 loss: 0.42167954854893785\n",
      "epoch 8: valid acc = 0.864, new learning rate = 0.0003317102156445311\n",
      "2000/49000 loss: 0.364709552338308\n",
      "4000/49000 loss: 0.37775453864707514\n",
      "6000/49000 loss: 0.32956454469819413\n",
      "8000/49000 loss: 0.41111842527296416\n",
      "10000/49000 loss: 0.4055412098574838\n",
      "12000/49000 loss: 0.4105791173778478\n",
      "14000/49000 loss: 0.3779356610063746\n",
      "16000/49000 loss: 0.3667993319500078\n",
      "18000/49000 loss: 0.4103731550369821\n",
      "20000/49000 loss: 0.3750755131510084\n",
      "22000/49000 loss: 0.2931178532975368\n",
      "24000/49000 loss: 0.27873991550718125\n",
      "26000/49000 loss: 0.4319060882696383\n",
      "28000/49000 loss: 0.485543745108892\n",
      "30000/49000 loss: 0.35436259651404245\n",
      "32000/49000 loss: 0.37403783010519676\n",
      "34000/49000 loss: 0.4031685383922477\n",
      "36000/49000 loss: 0.3149760495223127\n",
      "38000/49000 loss: 0.33072505999638013\n",
      "40000/49000 loss: 0.48403552568045716\n",
      "42000/49000 loss: 0.3046126352371711\n",
      "44000/49000 loss: 0.36564631728838454\n",
      "46000/49000 loss: 0.3592491907524744\n",
      "48000/49000 loss: 0.3100060645819106\n",
      "epoch 9: valid acc = 0.871, new learning rate = 0.0003151247048623045\n",
      "2000/49000 loss: 0.3676851829806421\n",
      "4000/49000 loss: 0.316365077588293\n",
      "6000/49000 loss: 0.4044829480785923\n",
      "8000/49000 loss: 0.2857960972719551\n",
      "10000/49000 loss: 0.30067692377942257\n",
      "12000/49000 loss: 0.35129316181563\n",
      "14000/49000 loss: 0.3827371846486926\n",
      "16000/49000 loss: 0.34794247456180205\n",
      "18000/49000 loss: 0.31700488753650313\n",
      "20000/49000 loss: 0.34619600752035085\n",
      "22000/49000 loss: 0.29028770087305156\n",
      "24000/49000 loss: 0.48585868309181707\n",
      "26000/49000 loss: 0.3631914054437754\n",
      "28000/49000 loss: 0.3282415353488515\n",
      "30000/49000 loss: 0.26666559733643774\n",
      "32000/49000 loss: 0.2911776675136814\n",
      "34000/49000 loss: 0.35703935656344804\n",
      "36000/49000 loss: 0.409329192338883\n",
      "38000/49000 loss: 0.38619844416766125\n",
      "40000/49000 loss: 0.39270222017175543\n",
      "42000/49000 loss: 0.4001038078769162\n",
      "44000/49000 loss: 0.4099360612844499\n",
      "46000/49000 loss: 0.3481685261678326\n",
      "48000/49000 loss: 0.35459068501000957\n",
      "epoch 10: valid acc = 0.877, new learning rate = 0.00029936846961918924\n",
      "2000/49000 loss: 0.3924905285409295\n",
      "4000/49000 loss: 0.302451182641797\n",
      "6000/49000 loss: 0.3497116067154317\n",
      "8000/49000 loss: 0.40074456206092646\n",
      "10000/49000 loss: 0.24006481932334145\n",
      "12000/49000 loss: 0.3159516932377985\n",
      "14000/49000 loss: 0.2901645637851629\n",
      "16000/49000 loss: 0.43503469006538625\n",
      "18000/49000 loss: 0.33638288125965976\n",
      "20000/49000 loss: 0.35047921760391676\n",
      "22000/49000 loss: 0.3434413983915385\n",
      "24000/49000 loss: 0.24310428732281827\n",
      "26000/49000 loss: 0.3191489631572013\n",
      "28000/49000 loss: 0.314252192513059\n",
      "30000/49000 loss: 0.39693177184220757\n",
      "32000/49000 loss: 0.3509017372454499\n",
      "34000/49000 loss: 0.4179396450930775\n",
      "36000/49000 loss: 0.27385897877121407\n",
      "38000/49000 loss: 0.3956380671005544\n",
      "40000/49000 loss: 0.4142377594017178\n",
      "42000/49000 loss: 0.37547188837205925\n",
      "44000/49000 loss: 0.40109343359513405\n",
      "46000/49000 loss: 0.2677652271674329\n",
      "48000/49000 loss: 0.39351463112912843\n",
      "epoch 11: valid acc = 0.879, new learning rate = 0.00028440004613822977\n",
      "2000/49000 loss: 0.3380758087548315\n",
      "4000/49000 loss: 0.30239314080063406\n",
      "6000/49000 loss: 0.2880704319850802\n",
      "8000/49000 loss: 0.36503186761620315\n",
      "10000/49000 loss: 0.34065951464016514\n",
      "12000/49000 loss: 0.42211634628258726\n",
      "14000/49000 loss: 0.3305691731684614\n",
      "16000/49000 loss: 0.32932652601614004\n",
      "18000/49000 loss: 0.300494432040187\n",
      "20000/49000 loss: 0.34418870239298593\n",
      "22000/49000 loss: 0.24198519025176857\n",
      "24000/49000 loss: 0.3209475636988498\n",
      "26000/49000 loss: 0.40007090499001763\n",
      "28000/49000 loss: 0.25968787059238013\n",
      "30000/49000 loss: 0.29546535157798964\n",
      "32000/49000 loss: 0.2959403946876232\n",
      "34000/49000 loss: 0.4543201598081317\n",
      "36000/49000 loss: 0.44986265004521503\n",
      "38000/49000 loss: 0.42804485258802705\n",
      "40000/49000 loss: 0.364979278928904\n",
      "42000/49000 loss: 0.25461449949485304\n",
      "44000/49000 loss: 0.4144746488050376\n",
      "46000/49000 loss: 0.26894223421710656\n",
      "48000/49000 loss: 0.32759186433695303\n",
      "epoch 12: valid acc = 0.885, new learning rate = 0.00027018004383131826\n",
      "2000/49000 loss: 0.36172403796941605\n",
      "4000/49000 loss: 0.3211047434959669\n",
      "6000/49000 loss: 0.3440328142155966\n",
      "8000/49000 loss: 0.2566604997025828\n",
      "10000/49000 loss: 0.3754773520403957\n",
      "12000/49000 loss: 0.38654516152311397\n",
      "14000/49000 loss: 0.2536044053992765\n",
      "16000/49000 loss: 0.3681745481780931\n",
      "18000/49000 loss: 0.3444965096735802\n",
      "20000/49000 loss: 0.37268785968548557\n",
      "22000/49000 loss: 0.30507558884629704\n",
      "24000/49000 loss: 0.33818740770893035\n",
      "26000/49000 loss: 0.38553558856168935\n",
      "28000/49000 loss: 0.3081602790976646\n",
      "30000/49000 loss: 0.46597546958551467\n",
      "32000/49000 loss: 0.3316497485607254\n",
      "34000/49000 loss: 0.4068554338504361\n",
      "36000/49000 loss: 0.31492438724223204\n",
      "38000/49000 loss: 0.32353729356809663\n",
      "40000/49000 loss: 0.3160671179330415\n",
      "42000/49000 loss: 0.3209454415716021\n",
      "44000/49000 loss: 0.33007858252706757\n",
      "46000/49000 loss: 0.36208789078865894\n",
      "48000/49000 loss: 0.29089816425866155\n",
      "epoch 13: valid acc = 0.878, new learning rate = 0.00025667104163975234\n",
      "2000/49000 loss: 0.3134599143837163\n",
      "4000/49000 loss: 0.307116785165796\n",
      "6000/49000 loss: 0.4006034777514424\n",
      "8000/49000 loss: 0.3618578970067364\n",
      "10000/49000 loss: 0.39147038545198265\n",
      "12000/49000 loss: 0.2571923550318757\n",
      "14000/49000 loss: 0.353689665910121\n",
      "16000/49000 loss: 0.3806044130203435\n",
      "18000/49000 loss: 0.42182649148041734\n",
      "20000/49000 loss: 0.38093894597892214\n",
      "22000/49000 loss: 0.3653354274780069\n",
      "24000/49000 loss: 0.3683774918185254\n",
      "26000/49000 loss: 0.4392295362348619\n",
      "28000/49000 loss: 0.3422059100214124\n",
      "30000/49000 loss: 0.31213954220521795\n",
      "32000/49000 loss: 0.32815738191975213\n",
      "34000/49000 loss: 0.40398695719500366\n",
      "36000/49000 loss: 0.37055415897550403\n",
      "38000/49000 loss: 0.42160323867913607\n",
      "40000/49000 loss: 0.37001737113222755\n",
      "42000/49000 loss: 0.36368921484519323\n",
      "44000/49000 loss: 0.2665188882960961\n",
      "46000/49000 loss: 0.3131783169495751\n",
      "48000/49000 loss: 0.32340321494342666\n",
      "epoch 14: valid acc = 0.88, new learning rate = 0.00024383748955776472\n",
      "2000/49000 loss: 0.3404000367267294\n",
      "4000/49000 loss: 0.3447468984496185\n",
      "6000/49000 loss: 0.35292272332852703\n",
      "8000/49000 loss: 0.31188929964004053\n",
      "10000/49000 loss: 0.3053946323400745\n",
      "12000/49000 loss: 0.2846131812177346\n",
      "14000/49000 loss: 0.342342113035898\n",
      "16000/49000 loss: 0.2796937606625957\n",
      "18000/49000 loss: 0.3277003913467656\n",
      "20000/49000 loss: 0.4083345846206327\n",
      "22000/49000 loss: 0.3415339081109384\n",
      "24000/49000 loss: 0.2638640965058023\n",
      "26000/49000 loss: 0.4226728703099484\n",
      "28000/49000 loss: 0.31222422985154374\n",
      "30000/49000 loss: 0.3134846541829234\n",
      "32000/49000 loss: 0.3295450067466955\n",
      "34000/49000 loss: 0.2480447527889862\n",
      "36000/49000 loss: 0.3368014261073236\n",
      "38000/49000 loss: 0.3327712170910323\n",
      "40000/49000 loss: 0.4000768408179326\n",
      "42000/49000 loss: 0.2680696351208857\n",
      "44000/49000 loss: 0.32675546284072315\n",
      "46000/49000 loss: 0.41036383228137885\n",
      "48000/49000 loss: 0.32011089041970925\n",
      "epoch 15: valid acc = 0.879, new learning rate = 0.00023164561507987649\n",
      "2000/49000 loss: 0.32841559117399977\n",
      "4000/49000 loss: 0.33407728726800023\n",
      "6000/49000 loss: 0.42370599915488216\n",
      "8000/49000 loss: 0.27181365862592055\n",
      "10000/49000 loss: 0.2923321566130973\n",
      "12000/49000 loss: 0.3409040409577374\n",
      "14000/49000 loss: 0.32724260949715706\n",
      "16000/49000 loss: 0.31589053313629845\n",
      "18000/49000 loss: 0.3629862658551229\n",
      "20000/49000 loss: 0.37334489170914054\n",
      "22000/49000 loss: 0.24374350697272074\n",
      "24000/49000 loss: 0.37175137391946395\n",
      "26000/49000 loss: 0.3283009137520823\n",
      "28000/49000 loss: 0.29284392444830165\n",
      "30000/49000 loss: 0.284887764793456\n",
      "32000/49000 loss: 0.2804898832027258\n",
      "34000/49000 loss: 0.3164392431946291\n",
      "36000/49000 loss: 0.3456690163008541\n",
      "38000/49000 loss: 0.3661411238254412\n",
      "40000/49000 loss: 0.31481724950508677\n",
      "42000/49000 loss: 0.2533299261875894\n",
      "44000/49000 loss: 0.2835123013113678\n",
      "46000/49000 loss: 0.34115401117444666\n",
      "48000/49000 loss: 0.270586755253581\n",
      "epoch 16: valid acc = 0.883, new learning rate = 0.00022006333432588265\n",
      "2000/49000 loss: 0.33694877327714673\n",
      "4000/49000 loss: 0.2611623422882356\n",
      "6000/49000 loss: 0.35205177212254257\n",
      "8000/49000 loss: 0.3380939424592831\n",
      "10000/49000 loss: 0.2878344227805383\n",
      "12000/49000 loss: 0.3821854375110924\n",
      "14000/49000 loss: 0.3802710615780618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000/49000 loss: 0.30963802341581836\n",
      "18000/49000 loss: 0.3080269176073181\n",
      "20000/49000 loss: 0.3232381570741646\n",
      "22000/49000 loss: 0.38714671342140444\n",
      "24000/49000 loss: 0.3039823489216965\n",
      "26000/49000 loss: 0.42145658768494293\n",
      "28000/49000 loss: 0.26305567897150134\n",
      "30000/49000 loss: 0.37312813423201596\n",
      "32000/49000 loss: 0.36855989922973725\n",
      "34000/49000 loss: 0.3572090171282648\n",
      "36000/49000 loss: 0.32105276178386133\n",
      "38000/49000 loss: 0.3783308159532609\n",
      "40000/49000 loss: 0.3714568273037204\n",
      "42000/49000 loss: 0.331216851844624\n",
      "44000/49000 loss: 0.357066731584671\n",
      "46000/49000 loss: 0.3542503663699984\n",
      "48000/49000 loss: 0.38672738400179263\n",
      "epoch 17: valid acc = 0.887, new learning rate = 0.00020906016760958852\n",
      "2000/49000 loss: 0.22958996128674208\n",
      "4000/49000 loss: 0.26668662752500666\n",
      "6000/49000 loss: 0.3237258958492173\n",
      "8000/49000 loss: 0.28441475897831986\n",
      "10000/49000 loss: 0.3276086133801589\n",
      "12000/49000 loss: 0.3906519780852813\n",
      "14000/49000 loss: 0.3655099897835998\n",
      "16000/49000 loss: 0.2578933702093646\n",
      "18000/49000 loss: 0.31540347692511017\n",
      "20000/49000 loss: 0.2961633540241341\n",
      "22000/49000 loss: 0.3212008850416345\n",
      "24000/49000 loss: 0.34856244546018744\n",
      "26000/49000 loss: 0.34761332442264714\n",
      "28000/49000 loss: 0.2096306122820409\n",
      "30000/49000 loss: 0.3385345732864195\n",
      "32000/49000 loss: 0.38430270584477744\n",
      "34000/49000 loss: 0.31672979473298274\n",
      "36000/49000 loss: 0.31693061301886677\n",
      "38000/49000 loss: 0.30534921288149036\n",
      "40000/49000 loss: 0.3698544161036905\n",
      "42000/49000 loss: 0.3348919251040969\n",
      "44000/49000 loss: 0.31914569615352134\n",
      "46000/49000 loss: 0.3159278549720229\n",
      "48000/49000 loss: 0.3128950334256362\n",
      "epoch 18: valid acc = 0.884, new learning rate = 0.00019860715922910907\n",
      "2000/49000 loss: 0.2705761880336868\n",
      "4000/49000 loss: 0.4015359107589093\n",
      "6000/49000 loss: 0.24541156718399307\n",
      "8000/49000 loss: 0.34501646180532003\n",
      "10000/49000 loss: 0.35062128086578065\n",
      "12000/49000 loss: 0.31118395637400115\n",
      "14000/49000 loss: 0.34502860166283295\n",
      "16000/49000 loss: 0.3179533690818363\n",
      "18000/49000 loss: 0.28192601333363393\n",
      "20000/49000 loss: 0.299915883073683\n",
      "22000/49000 loss: 0.3565726950780742\n",
      "24000/49000 loss: 0.3278241123106385\n",
      "26000/49000 loss: 0.28252485525682536\n",
      "28000/49000 loss: 0.3102292464308935\n",
      "30000/49000 loss: 0.23293080150244141\n",
      "32000/49000 loss: 0.23670345268207832\n",
      "34000/49000 loss: 0.3529693897018798\n",
      "36000/49000 loss: 0.3001048719937289\n",
      "38000/49000 loss: 0.3314292931930046\n",
      "40000/49000 loss: 0.26168336307588086\n",
      "42000/49000 loss: 0.37337077498588084\n",
      "44000/49000 loss: 0.39972242878529674\n",
      "46000/49000 loss: 0.28162810759827506\n",
      "48000/49000 loss: 0.3510135470579151\n",
      "epoch 19: valid acc = 0.879, new learning rate = 0.0001886768012676536\n",
      "2000/49000 loss: 0.26790155550329925\n",
      "4000/49000 loss: 0.27530756251065586\n",
      "6000/49000 loss: 0.3215804923083246\n",
      "8000/49000 loss: 0.3314772373493091\n",
      "10000/49000 loss: 0.37287482027028135\n",
      "12000/49000 loss: 0.3566468926984822\n",
      "14000/49000 loss: 0.23881702399238772\n",
      "16000/49000 loss: 0.26181047476733926\n",
      "18000/49000 loss: 0.2266247692877115\n",
      "20000/49000 loss: 0.2946204397277816\n",
      "22000/49000 loss: 0.3605535599875887\n",
      "24000/49000 loss: 0.3143575218981736\n",
      "26000/49000 loss: 0.38902831619681816\n",
      "28000/49000 loss: 0.320541603390429\n",
      "30000/49000 loss: 0.365400693514615\n",
      "32000/49000 loss: 0.27974171822152977\n",
      "34000/49000 loss: 0.33152067467060603\n",
      "36000/49000 loss: 0.29265208562655914\n",
      "38000/49000 loss: 0.30626713205920486\n",
      "40000/49000 loss: 0.3026527425843209\n",
      "42000/49000 loss: 0.3112338024783954\n",
      "44000/49000 loss: 0.3184603755348658\n",
      "46000/49000 loss: 0.3389853202462099\n",
      "48000/49000 loss: 0.31285244541892643\n",
      "epoch 20: valid acc = 0.884, new learning rate = 0.0001792429612042709\n",
      "2000/49000 loss: 0.23651317792021734\n",
      "4000/49000 loss: 0.27427987374269885\n",
      "6000/49000 loss: 0.3071918045553515\n",
      "8000/49000 loss: 0.19939704538974923\n",
      "10000/49000 loss: 0.32793309488574307\n",
      "12000/49000 loss: 0.3510875686016276\n",
      "14000/49000 loss: 0.3183853376836362\n",
      "16000/49000 loss: 0.26800456957099345\n",
      "18000/49000 loss: 0.3118808121705932\n",
      "20000/49000 loss: 0.3511658739563042\n",
      "22000/49000 loss: 0.32195128346479995\n",
      "24000/49000 loss: 0.368773910112497\n",
      "26000/49000 loss: 0.3406041692880258\n",
      "28000/49000 loss: 0.306273538366318\n",
      "30000/49000 loss: 0.30717533263148655\n",
      "32000/49000 loss: 0.27768431324023474\n",
      "34000/49000 loss: 0.2899331658772899\n",
      "36000/49000 loss: 0.3350253094528794\n",
      "38000/49000 loss: 0.28119611008032874\n",
      "40000/49000 loss: 0.2664423712825046\n",
      "42000/49000 loss: 0.2594786035132481\n",
      "44000/49000 loss: 0.29906558264082583\n",
      "46000/49000 loss: 0.3145547390100926\n",
      "48000/49000 loss: 0.19881647949622214\n",
      "epoch 21: valid acc = 0.885, new learning rate = 0.00017028081314405735\n",
      "2000/49000 loss: 0.2517306117435147\n",
      "4000/49000 loss: 0.34076239509669726\n",
      "6000/49000 loss: 0.4060003031335162\n",
      "8000/49000 loss: 0.3042366397620548\n",
      "10000/49000 loss: 0.36468313718253614\n",
      "12000/49000 loss: 0.3482516970913287\n",
      "14000/49000 loss: 0.2834588291202194\n",
      "16000/49000 loss: 0.39416103318742446\n",
      "18000/49000 loss: 0.33349776606005505\n",
      "20000/49000 loss: 0.3241466500950352\n",
      "22000/49000 loss: 0.32059894115352894\n",
      "24000/49000 loss: 0.3137691506720116\n",
      "26000/49000 loss: 0.33618019274261257\n",
      "28000/49000 loss: 0.27398419257814394\n",
      "30000/49000 loss: 0.3027917879909188\n",
      "32000/49000 loss: 0.2474918626753244\n",
      "34000/49000 loss: 0.21798969223538703\n",
      "36000/49000 loss: 0.30890471896623595\n",
      "38000/49000 loss: 0.24804212532699774\n",
      "40000/49000 loss: 0.3389355249884476\n",
      "42000/49000 loss: 0.23383564158892356\n",
      "44000/49000 loss: 0.3493485807908375\n",
      "46000/49000 loss: 0.2939360140441558\n",
      "48000/49000 loss: 0.35288644595741564\n",
      "epoch 22: valid acc = 0.89, new learning rate = 0.00016176677248685447\n",
      "2000/49000 loss: 0.23399065967127755\n",
      "4000/49000 loss: 0.39059483422154784\n",
      "6000/49000 loss: 0.24588027265670923\n",
      "8000/49000 loss: 0.2833779654942797\n",
      "10000/49000 loss: 0.3791696755367464\n",
      "12000/49000 loss: 0.36083536243111985\n",
      "14000/49000 loss: 0.2875676007698114\n",
      "16000/49000 loss: 0.2727192088052507\n",
      "18000/49000 loss: 0.3188080620599599\n",
      "20000/49000 loss: 0.27111247240068526\n",
      "22000/49000 loss: 0.29697145606415426\n",
      "24000/49000 loss: 0.36510877380491713\n",
      "26000/49000 loss: 0.29008111461163694\n",
      "28000/49000 loss: 0.3383330541549339\n",
      "30000/49000 loss: 0.3043412085924755\n",
      "32000/49000 loss: 0.30021736950530037\n",
      "34000/49000 loss: 0.301597239424973\n",
      "36000/49000 loss: 0.27284256583857647\n",
      "38000/49000 loss: 0.3247454900888455\n",
      "40000/49000 loss: 0.2860058908158559\n",
      "42000/49000 loss: 0.3407108347002773\n",
      "44000/49000 loss: 0.23674991244690524\n",
      "46000/49000 loss: 0.34864727752425906\n",
      "48000/49000 loss: 0.29734619932512907\n",
      "epoch 23: valid acc = 0.886, new learning rate = 0.00015367843386251173\n",
      "2000/49000 loss: 0.3115110364607194\n",
      "4000/49000 loss: 0.2544032039173077\n",
      "6000/49000 loss: 0.2607009064765534\n",
      "8000/49000 loss: 0.3450296820051373\n",
      "10000/49000 loss: 0.3261927028782374\n",
      "12000/49000 loss: 0.3122214995886326\n",
      "14000/49000 loss: 0.36676363877718193\n",
      "16000/49000 loss: 0.3279267489653623\n",
      "18000/49000 loss: 0.2503977547313875\n",
      "20000/49000 loss: 0.2722576284755703\n",
      "22000/49000 loss: 0.2955698416984496\n",
      "24000/49000 loss: 0.2803144833481634\n",
      "26000/49000 loss: 0.35857222108077913\n",
      "28000/49000 loss: 0.3910417756908002\n",
      "30000/49000 loss: 0.3813894327416704\n",
      "32000/49000 loss: 0.29907545231207805\n",
      "34000/49000 loss: 0.23398704559070568\n",
      "36000/49000 loss: 0.23242008770145875\n",
      "38000/49000 loss: 0.25932457262015085\n",
      "40000/49000 loss: 0.2882753295551587\n",
      "42000/49000 loss: 0.29107226958270754\n",
      "44000/49000 loss: 0.30442325062238046\n",
      "46000/49000 loss: 0.3253473205281221\n",
      "48000/49000 loss: 0.25892614904611505\n",
      "epoch 24: valid acc = 0.888, new learning rate = 0.00014599451216938612\n",
      "2000/49000 loss: 0.24627380753132516\n",
      "4000/49000 loss: 0.21163787799556608\n",
      "6000/49000 loss: 0.3204305424838811\n",
      "8000/49000 loss: 0.3030592792337357\n",
      "10000/49000 loss: 0.3489863880826165\n",
      "12000/49000 loss: 0.34527441481723875\n",
      "14000/49000 loss: 0.3222531203109159\n",
      "16000/49000 loss: 0.31523005452379843\n",
      "18000/49000 loss: 0.33376880718003954\n",
      "20000/49000 loss: 0.34034253278112797\n",
      "22000/49000 loss: 0.3222429839730198\n",
      "24000/49000 loss: 0.24189539719114556\n",
      "26000/49000 loss: 0.26123422124804985\n",
      "28000/49000 loss: 0.2677913796284613\n",
      "30000/49000 loss: 0.3367189072393372\n",
      "32000/49000 loss: 0.2712769167368556\n",
      "34000/49000 loss: 0.24606832648941765\n",
      "36000/49000 loss: 0.29956414502267587\n",
      "38000/49000 loss: 0.29054077469165346\n",
      "40000/49000 loss: 0.35111979266985993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/49000 loss: 0.23121880423143473\n",
      "44000/49000 loss: 0.2563873435834974\n",
      "46000/49000 loss: 0.2997600074904103\n",
      "48000/49000 loss: 0.22561577304698954\n",
      "epoch 25: valid acc = 0.891, new learning rate = 0.00013869478656091682\n",
      "2000/49000 loss: 0.2906260611319822\n",
      "4000/49000 loss: 0.33715027158847594\n",
      "6000/49000 loss: 0.3050610924209143\n",
      "8000/49000 loss: 0.34006310356041025\n",
      "10000/49000 loss: 0.3288089362346281\n",
      "12000/49000 loss: 0.25838539676044864\n",
      "14000/49000 loss: 0.31884633078514435\n",
      "16000/49000 loss: 0.31412509536755084\n",
      "18000/49000 loss: 0.2721176403877851\n",
      "20000/49000 loss: 0.3767665353567325\n",
      "22000/49000 loss: 0.34427107313476496\n",
      "24000/49000 loss: 0.28073110566464315\n",
      "26000/49000 loss: 0.27990466864933516\n",
      "28000/49000 loss: 0.2877855727130904\n",
      "30000/49000 loss: 0.3444114089613487\n",
      "32000/49000 loss: 0.3375443203823646\n",
      "34000/49000 loss: 0.25993065441823343\n",
      "36000/49000 loss: 0.3526211257934835\n",
      "38000/49000 loss: 0.3636179580980032\n",
      "40000/49000 loss: 0.2825507935616451\n",
      "42000/49000 loss: 0.2824428118204482\n",
      "44000/49000 loss: 0.34334638619380997\n",
      "46000/49000 loss: 0.2202445464533279\n",
      "48000/49000 loss: 0.26197188627083934\n",
      "epoch 26: valid acc = 0.886, new learning rate = 0.00013176004723287096\n",
      "2000/49000 loss: 0.2510715062065683\n",
      "4000/49000 loss: 0.22845944438254934\n",
      "6000/49000 loss: 0.2124886440488335\n",
      "8000/49000 loss: 0.3059861410987823\n",
      "10000/49000 loss: 0.38683495636548093\n",
      "12000/49000 loss: 0.3087955527369469\n",
      "14000/49000 loss: 0.26096661883863176\n",
      "16000/49000 loss: 0.3247114163576497\n",
      "18000/49000 loss: 0.18340643205160045\n",
      "20000/49000 loss: 0.21815255472892653\n",
      "22000/49000 loss: 0.3843114567266289\n",
      "24000/49000 loss: 0.31263944537823213\n",
      "26000/49000 loss: 0.3688104690822282\n",
      "28000/49000 loss: 0.3143989777839594\n",
      "30000/49000 loss: 0.3436100643113823\n",
      "32000/49000 loss: 0.3124662427598365\n",
      "34000/49000 loss: 0.2692593443877418\n",
      "36000/49000 loss: 0.2631250078359686\n",
      "38000/49000 loss: 0.41426850430166046\n",
      "40000/49000 loss: 0.30652139070855117\n",
      "42000/49000 loss: 0.3247201942762875\n",
      "44000/49000 loss: 0.35389915915899045\n",
      "46000/49000 loss: 0.26944364047543495\n",
      "48000/49000 loss: 0.3097071735025811\n",
      "epoch 27: valid acc = 0.885, new learning rate = 0.0001251720448712274\n",
      "2000/49000 loss: 0.2985638765374787\n",
      "4000/49000 loss: 0.31179580130920626\n",
      "6000/49000 loss: 0.3141290258898363\n",
      "8000/49000 loss: 0.3391499824243563\n",
      "10000/49000 loss: 0.2589062921444262\n",
      "12000/49000 loss: 0.285383499078507\n",
      "14000/49000 loss: 0.29100883639721414\n",
      "16000/49000 loss: 0.2799798198525781\n",
      "18000/49000 loss: 0.3009717584470779\n",
      "20000/49000 loss: 0.20336959843527538\n",
      "22000/49000 loss: 0.22819272402212545\n",
      "24000/49000 loss: 0.22019210863307287\n",
      "26000/49000 loss: 0.32875146778839653\n",
      "28000/49000 loss: 0.31928931474464917\n",
      "30000/49000 loss: 0.253341469991619\n",
      "32000/49000 loss: 0.3345085687475811\n",
      "34000/49000 loss: 0.24519874465825806\n",
      "36000/49000 loss: 0.356607144840245\n",
      "38000/49000 loss: 0.24906352812925128\n",
      "40000/49000 loss: 0.2816907053463762\n",
      "42000/49000 loss: 0.25549992243196906\n",
      "44000/49000 loss: 0.27567025540198425\n",
      "46000/49000 loss: 0.2986552721699897\n",
      "48000/49000 loss: 0.29680298882339207\n",
      "epoch 28: valid acc = 0.885, new learning rate = 0.00011891344262766602\n",
      "2000/49000 loss: 0.19250354643595358\n",
      "4000/49000 loss: 0.3583045176077879\n",
      "6000/49000 loss: 0.2725211255125458\n",
      "8000/49000 loss: 0.23217189372102265\n",
      "10000/49000 loss: 0.2651665862641062\n",
      "12000/49000 loss: 0.2775142992995785\n",
      "14000/49000 loss: 0.26836286918642377\n",
      "16000/49000 loss: 0.3348218309646004\n",
      "18000/49000 loss: 0.27157848523272116\n",
      "20000/49000 loss: 0.29419503411371084\n",
      "22000/49000 loss: 0.29947679308752645\n",
      "24000/49000 loss: 0.2914234119106339\n",
      "26000/49000 loss: 0.2480628804464339\n",
      "28000/49000 loss: 0.3246975727393929\n",
      "30000/49000 loss: 0.28249955136294214\n",
      "32000/49000 loss: 0.3285899474824723\n",
      "34000/49000 loss: 0.307521957667388\n",
      "36000/49000 loss: 0.264325775838075\n",
      "38000/49000 loss: 0.3585590044376265\n",
      "40000/49000 loss: 0.3742638720583479\n",
      "42000/49000 loss: 0.3459113015456087\n",
      "44000/49000 loss: 0.3522592368909718\n",
      "46000/49000 loss: 0.32422385933233405\n",
      "48000/49000 loss: 0.4427524021053276\n",
      "epoch 29: valid acc = 0.889, new learning rate = 0.00011296777049628272\n",
      "2000/49000 loss: 0.27522520555387975\n",
      "4000/49000 loss: 0.36078782207907484\n",
      "6000/49000 loss: 0.3010485501581117\n",
      "8000/49000 loss: 0.2789950195724073\n",
      "10000/49000 loss: 0.29314472706842387\n",
      "12000/49000 loss: 0.29265342330411903\n",
      "14000/49000 loss: 0.3382523411667375\n",
      "16000/49000 loss: 0.3453540030218651\n",
      "18000/49000 loss: 0.26878876738694596\n",
      "20000/49000 loss: 0.32744185996051284\n",
      "22000/49000 loss: 0.25289440544017716\n",
      "24000/49000 loss: 0.2881883220596995\n",
      "26000/49000 loss: 0.26394031622298453\n",
      "28000/49000 loss: 0.2521499429828205\n",
      "30000/49000 loss: 0.2259866859997239\n",
      "32000/49000 loss: 0.2878172747324984\n",
      "34000/49000 loss: 0.2359888306446279\n",
      "36000/49000 loss: 0.308280699996528\n",
      "38000/49000 loss: 0.2898213699235187\n",
      "40000/49000 loss: 0.2655145532166746\n",
      "42000/49000 loss: 0.25726147116571485\n",
      "44000/49000 loss: 0.2991585258015028\n",
      "46000/49000 loss: 0.21696789757583884\n",
      "48000/49000 loss: 0.32755256146598216\n",
      "epoch 30: valid acc = 0.888, new learning rate = 0.00010731938197146858\n",
      "2000/49000 loss: 0.23647332688715797\n",
      "4000/49000 loss: 0.2905894810724533\n",
      "6000/49000 loss: 0.2132386943419112\n",
      "8000/49000 loss: 0.3464253035428787\n",
      "10000/49000 loss: 0.3216292831438817\n",
      "12000/49000 loss: 0.2697639397809462\n",
      "14000/49000 loss: 0.3054575984393565\n",
      "16000/49000 loss: 0.26223045747796353\n",
      "18000/49000 loss: 0.4539375539151362\n",
      "20000/49000 loss: 0.2769678504370027\n",
      "22000/49000 loss: 0.275414370633519\n",
      "24000/49000 loss: 0.30886338745030234\n",
      "26000/49000 loss: 0.33891968638990644\n",
      "28000/49000 loss: 0.31810475021604795\n",
      "30000/49000 loss: 0.29031122795407105\n",
      "32000/49000 loss: 0.24864640262128218\n",
      "34000/49000 loss: 0.3065197409719483\n",
      "36000/49000 loss: 0.36490053220061164\n",
      "38000/49000 loss: 0.2802227693264902\n",
      "40000/49000 loss: 0.2676204400116955\n",
      "42000/49000 loss: 0.23591422594199996\n",
      "44000/49000 loss: 0.2530307855734071\n",
      "46000/49000 loss: 0.2354691503241599\n",
      "48000/49000 loss: 0.35069101421804505\n",
      "epoch 31: valid acc = 0.889, new learning rate = 0.00010195341287289515\n",
      "2000/49000 loss: 0.29189649050142996\n",
      "4000/49000 loss: 0.2355950359220216\n",
      "6000/49000 loss: 0.27958141637214834\n",
      "8000/49000 loss: 0.3867409897979172\n",
      "10000/49000 loss: 0.2691835565794497\n",
      "12000/49000 loss: 0.27996019861551996\n",
      "14000/49000 loss: 0.3412324952183545\n",
      "16000/49000 loss: 0.2902313279464343\n",
      "18000/49000 loss: 0.2994641628239347\n",
      "20000/49000 loss: 0.317671910262529\n",
      "22000/49000 loss: 0.3371635111386844\n",
      "24000/49000 loss: 0.301862686779705\n",
      "26000/49000 loss: 0.33923291112808274\n",
      "28000/49000 loss: 0.28055597861121223\n",
      "30000/49000 loss: 0.22984827257205773\n",
      "32000/49000 loss: 0.2802518954083327\n",
      "34000/49000 loss: 0.4105507995063944\n",
      "36000/49000 loss: 0.29932286147651094\n",
      "38000/49000 loss: 0.3001119866684513\n",
      "40000/49000 loss: 0.36505681219600566\n",
      "42000/49000 loss: 0.2822690403223955\n",
      "44000/49000 loss: 0.32846491588908944\n",
      "46000/49000 loss: 0.28705974447867527\n",
      "48000/49000 loss: 0.24492737929142572\n",
      "epoch 32: valid acc = 0.883, new learning rate = 9.685574222925039e-05\n",
      "2000/49000 loss: 0.2552753045661837\n",
      "4000/49000 loss: 0.32014950236557255\n",
      "6000/49000 loss: 0.26324371695342086\n",
      "8000/49000 loss: 0.323807305295484\n",
      "10000/49000 loss: 0.23611866258147082\n",
      "12000/49000 loss: 0.28974964660941843\n",
      "14000/49000 loss: 0.28960150281827346\n",
      "16000/49000 loss: 0.3868829349632828\n",
      "18000/49000 loss: 0.3181553890440138\n",
      "20000/49000 loss: 0.22118273682391762\n",
      "22000/49000 loss: 0.388874173066465\n",
      "24000/49000 loss: 0.26878270407871757\n",
      "26000/49000 loss: 0.29676826069498635\n",
      "28000/49000 loss: 0.2774375842246559\n",
      "30000/49000 loss: 0.4270030015423769\n",
      "32000/49000 loss: 0.2866888671484234\n",
      "34000/49000 loss: 0.2558265246794543\n",
      "36000/49000 loss: 0.32379542410511103\n",
      "38000/49000 loss: 0.2842959758087293\n",
      "40000/49000 loss: 0.2614257996916767\n",
      "42000/49000 loss: 0.23899575432408074\n",
      "44000/49000 loss: 0.34214975665648056\n",
      "46000/49000 loss: 0.39520733776437633\n",
      "48000/49000 loss: 0.330680911443498\n",
      "epoch 33: valid acc = 0.885, new learning rate = 9.201295511778786e-05\n",
      "2000/49000 loss: 0.27984768806448884\n",
      "4000/49000 loss: 0.2532874055931866\n",
      "6000/49000 loss: 0.30217045433522277\n",
      "8000/49000 loss: 0.34895270271676465\n",
      "10000/49000 loss: 0.3082234830694028\n",
      "12000/49000 loss: 0.2412132469074422\n",
      "14000/49000 loss: 0.2808365526624755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000/49000 loss: 0.22793323056928783\n",
      "18000/49000 loss: 0.2371241525790253\n",
      "20000/49000 loss: 0.20425548719483913\n",
      "22000/49000 loss: 0.2880567089802719\n",
      "24000/49000 loss: 0.32187207104996624\n",
      "26000/49000 loss: 0.304705935607533\n",
      "28000/49000 loss: 0.3184223615250638\n",
      "30000/49000 loss: 0.28444089055964705\n",
      "32000/49000 loss: 0.2457212892575334\n",
      "34000/49000 loss: 0.27310291838994366\n",
      "36000/49000 loss: 0.32974112035158404\n",
      "38000/49000 loss: 0.3284310093334116\n",
      "40000/49000 loss: 0.2704460790425576\n",
      "42000/49000 loss: 0.2562583915274631\n",
      "44000/49000 loss: 0.3859165261462404\n",
      "46000/49000 loss: 0.31387034068215447\n",
      "48000/49000 loss: 0.22803293549745782\n",
      "epoch 34: valid acc = 0.892, new learning rate = 8.741230736189846e-05\n",
      "2000/49000 loss: 0.3243189288187399\n",
      "4000/49000 loss: 0.2934863310291777\n",
      "6000/49000 loss: 0.3226306043160468\n",
      "8000/49000 loss: 0.25279939613182956\n",
      "10000/49000 loss: 0.3075602629713314\n",
      "12000/49000 loss: 0.25986340867656793\n",
      "14000/49000 loss: 0.2282409050714142\n",
      "16000/49000 loss: 0.311825169106507\n",
      "18000/49000 loss: 0.25901619465503206\n",
      "20000/49000 loss: 0.2879825595287971\n",
      "22000/49000 loss: 0.31739916015844943\n",
      "24000/49000 loss: 0.21038275587291408\n",
      "26000/49000 loss: 0.2972780628684877\n",
      "28000/49000 loss: 0.24966310403889458\n",
      "30000/49000 loss: 0.27816783048917954\n",
      "32000/49000 loss: 0.255937798692834\n",
      "34000/49000 loss: 0.25070018457577525\n",
      "36000/49000 loss: 0.2368307532899071\n",
      "38000/49000 loss: 0.2625422855370817\n",
      "40000/49000 loss: 0.4049103346354799\n",
      "42000/49000 loss: 0.2667103379645589\n",
      "44000/49000 loss: 0.29817284914204845\n",
      "46000/49000 loss: 0.2842574091811555\n",
      "48000/49000 loss: 0.33236963874688796\n",
      "epoch 35: valid acc = 0.892, new learning rate = 8.304169199380353e-05\n",
      "2000/49000 loss: 0.3121947948231133\n",
      "4000/49000 loss: 0.17409024796317657\n",
      "6000/49000 loss: 0.2167727888251232\n",
      "8000/49000 loss: 0.1618500274425819\n",
      "10000/49000 loss: 0.2692405622577304\n",
      "12000/49000 loss: 0.2800460276027702\n",
      "14000/49000 loss: 0.33296028388296756\n",
      "16000/49000 loss: 0.2281621435291559\n",
      "18000/49000 loss: 0.34379642358168827\n",
      "20000/49000 loss: 0.3404434011545149\n",
      "22000/49000 loss: 0.289588147766072\n",
      "24000/49000 loss: 0.2950613391400617\n",
      "26000/49000 loss: 0.29589860139079577\n",
      "28000/49000 loss: 0.29315628344803957\n",
      "30000/49000 loss: 0.27124402931279906\n",
      "32000/49000 loss: 0.2960936392240228\n",
      "34000/49000 loss: 0.26429675059168417\n",
      "36000/49000 loss: 0.3180961164154701\n",
      "38000/49000 loss: 0.3058501998709297\n",
      "40000/49000 loss: 0.2673405188438089\n",
      "42000/49000 loss: 0.23889940194756404\n",
      "44000/49000 loss: 0.32427136951331115\n",
      "46000/49000 loss: 0.33289223047719635\n",
      "48000/49000 loss: 0.26519169818917787\n",
      "epoch 36: valid acc = 0.887, new learning rate = 7.888960739411335e-05\n",
      "2000/49000 loss: 0.2682461096765637\n",
      "4000/49000 loss: 0.31578341305324314\n",
      "6000/49000 loss: 0.35697399478955655\n",
      "8000/49000 loss: 0.328492566089582\n",
      "10000/49000 loss: 0.24083434443767482\n",
      "12000/49000 loss: 0.31487423516179947\n",
      "14000/49000 loss: 0.3877793955257765\n",
      "16000/49000 loss: 0.22866387612295555\n",
      "18000/49000 loss: 0.2988307090938699\n",
      "20000/49000 loss: 0.2574582594274549\n",
      "22000/49000 loss: 0.28055965800106963\n",
      "24000/49000 loss: 0.3718026606664906\n",
      "26000/49000 loss: 0.22682374771096395\n",
      "28000/49000 loss: 0.2869599298887063\n",
      "30000/49000 loss: 0.2810463971913407\n",
      "32000/49000 loss: 0.27581148664978905\n",
      "34000/49000 loss: 0.2937115400276736\n",
      "36000/49000 loss: 0.319788925108979\n",
      "38000/49000 loss: 0.3779470685512439\n",
      "40000/49000 loss: 0.36416127343404464\n",
      "42000/49000 loss: 0.34846264832928336\n",
      "44000/49000 loss: 0.171532165785941\n",
      "46000/49000 loss: 0.3184313735284478\n",
      "48000/49000 loss: 0.3470431593213246\n",
      "epoch 37: valid acc = 0.885, new learning rate = 7.494512702440768e-05\n",
      "2000/49000 loss: 0.2379462232805335\n",
      "4000/49000 loss: 0.3140551704396953\n",
      "6000/49000 loss: 0.3108047772974571\n",
      "8000/49000 loss: 0.2668470287255618\n",
      "10000/49000 loss: 0.24457876554744198\n",
      "12000/49000 loss: 0.2891988687392649\n",
      "14000/49000 loss: 0.31926654011115946\n",
      "16000/49000 loss: 0.3055755023713357\n",
      "18000/49000 loss: 0.3363284885806348\n",
      "20000/49000 loss: 0.28346125917796144\n",
      "22000/49000 loss: 0.3016416083397233\n",
      "24000/49000 loss: 0.3001600357117235\n",
      "26000/49000 loss: 0.2719026093659392\n",
      "28000/49000 loss: 0.4013640610366557\n",
      "30000/49000 loss: 0.22428259962070016\n",
      "32000/49000 loss: 0.3266168553123177\n",
      "34000/49000 loss: 0.3037307376826929\n",
      "36000/49000 loss: 0.22331878800274493\n",
      "38000/49000 loss: 0.2934611935460803\n",
      "40000/49000 loss: 0.2795931362859429\n",
      "42000/49000 loss: 0.3104660535537741\n",
      "44000/49000 loss: 0.3600491378964073\n",
      "46000/49000 loss: 0.26509446409746174\n",
      "48000/49000 loss: 0.3182013140990806\n",
      "epoch 38: valid acc = 0.884, new learning rate = 7.119787067318729e-05\n",
      "2000/49000 loss: 0.26361119983385195\n",
      "4000/49000 loss: 0.25509381575629236\n",
      "6000/49000 loss: 0.2675058032672335\n",
      "8000/49000 loss: 0.3791724817564485\n",
      "10000/49000 loss: 0.2751162575134826\n",
      "12000/49000 loss: 0.2365462914244389\n",
      "14000/49000 loss: 0.27495836618360836\n",
      "16000/49000 loss: 0.2040033122619729\n",
      "18000/49000 loss: 0.37880636704637616\n",
      "20000/49000 loss: 0.22923461627374914\n",
      "22000/49000 loss: 0.33630234031635964\n",
      "24000/49000 loss: 0.3220931345476212\n",
      "26000/49000 loss: 0.23824189422453432\n",
      "28000/49000 loss: 0.3176289834301107\n",
      "30000/49000 loss: 0.25779774585019866\n",
      "32000/49000 loss: 0.40224577689684754\n",
      "34000/49000 loss: 0.3028712516385204\n",
      "36000/49000 loss: 0.332069081370087\n",
      "38000/49000 loss: 0.31151035731235466\n",
      "40000/49000 loss: 0.2921784272941262\n",
      "42000/49000 loss: 0.2733507149473588\n",
      "44000/49000 loss: 0.31305463526530297\n",
      "46000/49000 loss: 0.2618797237729616\n",
      "48000/49000 loss: 0.285882335049483\n",
      "epoch 39: valid acc = 0.887, new learning rate = 6.763797713952792e-05\n",
      "2000/49000 loss: 0.26478665437739146\n",
      "4000/49000 loss: 0.2435982843588713\n",
      "6000/49000 loss: 0.29646703951131054\n",
      "8000/49000 loss: 0.27773513935971045\n",
      "10000/49000 loss: 0.20041664126088043\n",
      "12000/49000 loss: 0.4144320070690459\n",
      "14000/49000 loss: 0.2962689246673333\n",
      "16000/49000 loss: 0.2534253462440689\n",
      "18000/49000 loss: 0.3281805623030405\n",
      "20000/49000 loss: 0.23834796820063198\n",
      "22000/49000 loss: 0.2755134539374668\n",
      "24000/49000 loss: 0.36491813439700693\n",
      "26000/49000 loss: 0.245472798397\n",
      "28000/49000 loss: 0.22358362714760127\n",
      "30000/49000 loss: 0.34629718792571784\n",
      "32000/49000 loss: 0.3383436438060508\n",
      "34000/49000 loss: 0.33537513584023315\n",
      "36000/49000 loss: 0.2375494824108335\n",
      "38000/49000 loss: 0.27187934320512075\n",
      "40000/49000 loss: 0.34821195196669275\n",
      "42000/49000 loss: 0.22599752903396322\n",
      "44000/49000 loss: 0.24723127530783967\n",
      "46000/49000 loss: 0.3010198591297343\n",
      "48000/49000 loss: 0.2740472731231581\n",
      "epoch 40: valid acc = 0.886, new learning rate = 6.425607828255152e-05\n",
      "2000/49000 loss: 0.3131206169626136\n",
      "4000/49000 loss: 0.2625170303871321\n",
      "6000/49000 loss: 0.30119647616585926\n",
      "8000/49000 loss: 0.2963059309623697\n",
      "10000/49000 loss: 0.2539035195704476\n",
      "12000/49000 loss: 0.28950659492375586\n",
      "14000/49000 loss: 0.3727526174082446\n",
      "16000/49000 loss: 0.2663709783274662\n",
      "18000/49000 loss: 0.22072620377370805\n",
      "20000/49000 loss: 0.23722327552569342\n",
      "22000/49000 loss: 0.24020033189768322\n",
      "24000/49000 loss: 0.20517889678328535\n",
      "26000/49000 loss: 0.25260461212502305\n",
      "28000/49000 loss: 0.3550175191580995\n",
      "30000/49000 loss: 0.3085623386489072\n",
      "32000/49000 loss: 0.2921855645642642\n",
      "34000/49000 loss: 0.2570620446039645\n",
      "36000/49000 loss: 0.281728902506657\n",
      "38000/49000 loss: 0.3318684425072314\n",
      "40000/49000 loss: 0.29458969242407373\n",
      "42000/49000 loss: 0.26748295870212024\n",
      "44000/49000 loss: 0.3985701190856695\n",
      "46000/49000 loss: 0.3023749320005576\n",
      "48000/49000 loss: 0.3501056609549702\n",
      "epoch 41: valid acc = 0.887, new learning rate = 6.104327436842394e-05\n",
      "2000/49000 loss: 0.29367725157852415\n",
      "4000/49000 loss: 0.30711999295054243\n",
      "6000/49000 loss: 0.33620196962190413\n",
      "8000/49000 loss: 0.2736262795953134\n",
      "10000/49000 loss: 0.34269391048659026\n",
      "12000/49000 loss: 0.21818142988199304\n",
      "14000/49000 loss: 0.29133687347830195\n",
      "16000/49000 loss: 0.2756132553527878\n",
      "18000/49000 loss: 0.28211579662935987\n",
      "20000/49000 loss: 0.31640068773714153\n",
      "22000/49000 loss: 0.3738395813497425\n",
      "24000/49000 loss: 0.307780802397505\n",
      "26000/49000 loss: 0.2944138312971893\n",
      "28000/49000 loss: 0.2654977429953704\n",
      "30000/49000 loss: 0.24710689044173376\n",
      "32000/49000 loss: 0.22139169750702706\n",
      "34000/49000 loss: 0.2994809739711011\n",
      "36000/49000 loss: 0.22019144009503766\n",
      "38000/49000 loss: 0.22345996545610772\n",
      "40000/49000 loss: 0.23900407716070354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/49000 loss: 0.29677728202427633\n",
      "44000/49000 loss: 0.2526253724295309\n",
      "46000/49000 loss: 0.33126252108015575\n",
      "48000/49000 loss: 0.23654003532187096\n",
      "epoch 42: valid acc = 0.887, new learning rate = 5.799111065000274e-05\n",
      "2000/49000 loss: 0.2469396125816554\n",
      "4000/49000 loss: 0.26314425554650356\n",
      "6000/49000 loss: 0.25802167186016844\n",
      "8000/49000 loss: 0.30241479515221975\n",
      "10000/49000 loss: 0.24776175133661787\n",
      "12000/49000 loss: 0.31432746951757606\n",
      "14000/49000 loss: 0.21692443394062308\n",
      "16000/49000 loss: 0.25186380080174287\n",
      "18000/49000 loss: 0.3270834097723018\n",
      "20000/49000 loss: 0.2992792904852973\n",
      "22000/49000 loss: 0.25706117987513766\n",
      "24000/49000 loss: 0.29187711208286465\n",
      "26000/49000 loss: 0.24985364978018018\n",
      "28000/49000 loss: 0.25980119065460977\n",
      "30000/49000 loss: 0.3195249311605232\n",
      "32000/49000 loss: 0.25508593299945953\n",
      "34000/49000 loss: 0.27139482764760126\n",
      "36000/49000 loss: 0.25188390080299944\n",
      "38000/49000 loss: 0.34680223903262125\n",
      "40000/49000 loss: 0.2820768308733336\n",
      "42000/49000 loss: 0.25935792958088316\n",
      "44000/49000 loss: 0.2852148629668615\n",
      "46000/49000 loss: 0.21495172707698654\n",
      "48000/49000 loss: 0.29875122211936755\n",
      "epoch 43: valid acc = 0.886, new learning rate = 5.5091555117502596e-05\n",
      "2000/49000 loss: 0.28912622915881714\n",
      "4000/49000 loss: 0.3091256964604418\n",
      "6000/49000 loss: 0.2711365234370933\n",
      "8000/49000 loss: 0.2795752237251239\n",
      "10000/49000 loss: 0.2731622323157547\n",
      "12000/49000 loss: 0.3049128545953147\n",
      "14000/49000 loss: 0.3502569869714489\n",
      "16000/49000 loss: 0.26440436124633504\n",
      "18000/49000 loss: 0.25704584161924304\n",
      "20000/49000 loss: 0.26811970305149024\n",
      "22000/49000 loss: 0.2977155673820807\n",
      "24000/49000 loss: 0.20895726702382147\n",
      "26000/49000 loss: 0.2584936483694734\n",
      "28000/49000 loss: 0.26292402070029924\n",
      "30000/49000 loss: 0.28811062973543006\n",
      "32000/49000 loss: 0.2525806086479919\n",
      "34000/49000 loss: 0.2204926282326548\n",
      "36000/49000 loss: 0.38772229530085045\n",
      "38000/49000 loss: 0.33735900561058557\n",
      "40000/49000 loss: 0.3154624485254649\n",
      "42000/49000 loss: 0.3398712285179972\n",
      "44000/49000 loss: 0.28513950342095035\n",
      "46000/49000 loss: 0.3072714406834422\n",
      "48000/49000 loss: 0.2892432006710539\n",
      "epoch 44: valid acc = 0.885, new learning rate = 5.2336977361627463e-05\n",
      "2000/49000 loss: 0.23834312421287593\n",
      "4000/49000 loss: 0.2807872303504199\n",
      "6000/49000 loss: 0.30546895101798915\n",
      "8000/49000 loss: 0.24007621799232876\n",
      "10000/49000 loss: 0.23829997446774737\n",
      "12000/49000 loss: 0.25772295385067356\n",
      "14000/49000 loss: 0.23661351169110623\n",
      "16000/49000 loss: 0.25436039770036795\n",
      "18000/49000 loss: 0.31055855503214325\n",
      "20000/49000 loss: 0.31369643891454196\n",
      "22000/49000 loss: 0.235447421744807\n",
      "24000/49000 loss: 0.30046093259276807\n",
      "26000/49000 loss: 0.22448182158305463\n",
      "28000/49000 loss: 0.2630821831256304\n",
      "30000/49000 loss: 0.3048654549335602\n",
      "32000/49000 loss: 0.3299869562470879\n",
      "34000/49000 loss: 0.24252398325821453\n",
      "36000/49000 loss: 0.1951162992080875\n",
      "38000/49000 loss: 0.25694822043869064\n",
      "40000/49000 loss: 0.2407716085795253\n",
      "42000/49000 loss: 0.24614315970546902\n",
      "44000/49000 loss: 0.36636825974659193\n",
      "46000/49000 loss: 0.33096682416138673\n",
      "48000/49000 loss: 0.2804475732420903\n",
      "epoch 45: valid acc = 0.886, new learning rate = 4.972012849354609e-05\n",
      "2000/49000 loss: 0.2912962856821532\n",
      "4000/49000 loss: 0.20417045253016627\n",
      "6000/49000 loss: 0.26695428750464806\n",
      "8000/49000 loss: 0.2767545747804012\n",
      "10000/49000 loss: 0.3095359479393846\n",
      "12000/49000 loss: 0.2847618348543539\n",
      "14000/49000 loss: 0.2479349838978819\n",
      "16000/49000 loss: 0.2512400458971873\n",
      "18000/49000 loss: 0.33401924246046216\n",
      "20000/49000 loss: 0.2501756837599357\n",
      "22000/49000 loss: 0.2048119046965551\n",
      "24000/49000 loss: 0.30667827510482515\n",
      "26000/49000 loss: 0.24672319819097469\n",
      "28000/49000 loss: 0.3166809206596353\n",
      "30000/49000 loss: 0.27543148411581214\n",
      "32000/49000 loss: 0.2288207976767132\n",
      "34000/49000 loss: 0.2999565409946289\n",
      "36000/49000 loss: 0.25035107389092043\n",
      "38000/49000 loss: 0.22233327349479454\n",
      "40000/49000 loss: 0.2674337638267048\n",
      "42000/49000 loss: 0.2463745637948606\n",
      "44000/49000 loss: 0.30025636981265036\n",
      "46000/49000 loss: 0.24103065492284068\n",
      "48000/49000 loss: 0.3287122007654547\n",
      "epoch 46: valid acc = 0.889, new learning rate = 4.723412206886878e-05\n",
      "2000/49000 loss: 0.28328522955444835\n",
      "4000/49000 loss: 0.3629911409795764\n",
      "6000/49000 loss: 0.2519125546432375\n",
      "8000/49000 loss: 0.22190162003217306\n",
      "10000/49000 loss: 0.2829717877108628\n",
      "12000/49000 loss: 0.19065011896119027\n",
      "14000/49000 loss: 0.28459997398794457\n",
      "16000/49000 loss: 0.2942902887411552\n",
      "18000/49000 loss: 0.256030540237231\n",
      "20000/49000 loss: 0.33372659523854686\n",
      "22000/49000 loss: 0.30314388480795584\n",
      "24000/49000 loss: 0.29946940448143894\n",
      "26000/49000 loss: 0.3471837435181129\n",
      "28000/49000 loss: 0.29983938221899303\n",
      "30000/49000 loss: 0.2278690526207352\n",
      "32000/49000 loss: 0.2481251246672599\n",
      "34000/49000 loss: 0.21258090150524522\n",
      "36000/49000 loss: 0.27091407315773414\n",
      "38000/49000 loss: 0.308192614256238\n",
      "40000/49000 loss: 0.2829261392312756\n",
      "42000/49000 loss: 0.23957187697610294\n",
      "44000/49000 loss: 0.17063096197443664\n",
      "46000/49000 loss: 0.2655470147291435\n",
      "48000/49000 loss: 0.2200336224955841\n",
      "epoch 47: valid acc = 0.892, new learning rate = 4.487241596542534e-05\n",
      "2000/49000 loss: 0.25895485151167885\n",
      "4000/49000 loss: 0.24675221211664106\n",
      "6000/49000 loss: 0.21345690453266214\n",
      "8000/49000 loss: 0.32226024016763866\n",
      "10000/49000 loss: 0.2379656845285537\n",
      "12000/49000 loss: 0.3314825863837615\n",
      "14000/49000 loss: 0.22989231717296066\n",
      "16000/49000 loss: 0.3047262697404449\n",
      "18000/49000 loss: 0.26182023873512145\n",
      "20000/49000 loss: 0.2739396909962223\n",
      "22000/49000 loss: 0.37808602039943906\n",
      "24000/49000 loss: 0.3444020977955003\n",
      "26000/49000 loss: 0.25956641825405535\n",
      "28000/49000 loss: 0.2751564961141663\n",
      "30000/49000 loss: 0.2653955083588554\n",
      "32000/49000 loss: 0.36849950062895404\n",
      "34000/49000 loss: 0.2667523545910496\n",
      "36000/49000 loss: 0.23054297254213005\n",
      "38000/49000 loss: 0.3600431054793753\n",
      "40000/49000 loss: 0.37095091539070274\n",
      "42000/49000 loss: 0.25374494805476866\n",
      "44000/49000 loss: 0.36842279893758334\n",
      "46000/49000 loss: 0.3116715976278763\n",
      "48000/49000 loss: 0.29253499804755984\n",
      "epoch 48: valid acc = 0.888, new learning rate = 4.262879516715407e-05\n",
      "2000/49000 loss: 0.2733597032188575\n",
      "4000/49000 loss: 0.24131777881797964\n",
      "6000/49000 loss: 0.352110079888655\n",
      "8000/49000 loss: 0.3084873980395369\n",
      "10000/49000 loss: 0.24354459921848853\n",
      "12000/49000 loss: 0.3494286155849743\n",
      "14000/49000 loss: 0.3160873339038388\n",
      "16000/49000 loss: 0.3154942902612188\n",
      "18000/49000 loss: 0.30080409727358887\n",
      "20000/49000 loss: 0.33985363268637603\n",
      "22000/49000 loss: 0.29016402929330626\n",
      "24000/49000 loss: 0.3581222411196772\n",
      "26000/49000 loss: 0.2906377613949864\n",
      "28000/49000 loss: 0.18659439830178942\n",
      "30000/49000 loss: 0.2676225708133451\n",
      "32000/49000 loss: 0.24909014859036668\n",
      "34000/49000 loss: 0.34930430125401835\n",
      "36000/49000 loss: 0.2526581813032368\n",
      "38000/49000 loss: 0.3068649574184058\n",
      "40000/49000 loss: 0.22182175818590869\n",
      "42000/49000 loss: 0.26853277900632516\n",
      "44000/49000 loss: 0.20055553579111998\n",
      "46000/49000 loss: 0.25265890082423414\n",
      "48000/49000 loss: 0.3253829058727321\n",
      "epoch 49: valid acc = 0.888, new learning rate = 4.049735540879637e-05\n",
      "2000/49000 loss: 0.36932707884741234\n",
      "4000/49000 loss: 0.3074329634133163\n",
      "6000/49000 loss: 0.2761917101249369\n",
      "8000/49000 loss: 0.29203662085537463\n",
      "10000/49000 loss: 0.3327255878429866\n",
      "12000/49000 loss: 0.3024496169334712\n",
      "14000/49000 loss: 0.3517762998521407\n",
      "16000/49000 loss: 0.3222649045410466\n",
      "18000/49000 loss: 0.2714526062169234\n",
      "20000/49000 loss: 0.2544854568997553\n",
      "22000/49000 loss: 0.2574257938651368\n",
      "24000/49000 loss: 0.3097207456822404\n",
      "26000/49000 loss: 0.2624156253307127\n",
      "28000/49000 loss: 0.27636050243051213\n",
      "30000/49000 loss: 0.2810544135456716\n",
      "32000/49000 loss: 0.37482842942078193\n",
      "34000/49000 loss: 0.2425329230757299\n",
      "36000/49000 loss: 0.2543106155588108\n",
      "38000/49000 loss: 0.21239441059454925\n",
      "40000/49000 loss: 0.28466278417166774\n",
      "42000/49000 loss: 0.2927188879004168\n",
      "44000/49000 loss: 0.23973859568759676\n",
      "46000/49000 loss: 0.2548206867181078\n",
      "48000/49000 loss: 0.3030663364208249\n",
      "epoch 50: valid acc = 0.892, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.9042857142857142\n",
      "test acc: 0.892\n",
      "test acc: 0.8724\n",
      "number of batches for training: 245\n",
      "epoch 1: valid acc = 0.734, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.81, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.824, new learning rate = 0.0004286875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4: valid acc = 0.835, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.851, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.858, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.861, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.87, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.871, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.874, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.878, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.881, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.881, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.887, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.884, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.879, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.885, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.883, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.884, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.883, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.884, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.885, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.892, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.888, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.894, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.883, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.884, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.887, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.889, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.888, new learning rate = 0.00010731938197146858\n",
      "epoch 31: valid acc = 0.886, new learning rate = 0.00010195341287289515\n",
      "epoch 32: valid acc = 0.884, new learning rate = 9.685574222925039e-05\n",
      "epoch 33: valid acc = 0.886, new learning rate = 9.201295511778786e-05\n",
      "epoch 34: valid acc = 0.885, new learning rate = 8.741230736189846e-05\n",
      "epoch 35: valid acc = 0.884, new learning rate = 8.304169199380353e-05\n",
      "epoch 36: valid acc = 0.888, new learning rate = 7.888960739411335e-05\n",
      "epoch 37: valid acc = 0.882, new learning rate = 7.494512702440768e-05\n",
      "epoch 38: valid acc = 0.89, new learning rate = 7.119787067318729e-05\n",
      "epoch 39: valid acc = 0.885, new learning rate = 6.763797713952792e-05\n",
      "epoch 40: valid acc = 0.888, new learning rate = 6.425607828255152e-05\n",
      "epoch 41: valid acc = 0.887, new learning rate = 6.104327436842394e-05\n",
      "epoch 42: valid acc = 0.888, new learning rate = 5.799111065000274e-05\n",
      "epoch 43: valid acc = 0.887, new learning rate = 5.5091555117502596e-05\n",
      "epoch 44: valid acc = 0.887, new learning rate = 5.2336977361627463e-05\n",
      "epoch 45: valid acc = 0.883, new learning rate = 4.972012849354609e-05\n",
      "epoch 46: valid acc = 0.886, new learning rate = 4.723412206886878e-05\n",
      "epoch 47: valid acc = 0.883, new learning rate = 4.487241596542534e-05\n",
      "epoch 48: valid acc = 0.881, new learning rate = 4.262879516715407e-05\n",
      "epoch 49: valid acc = 0.884, new learning rate = 4.049735540879637e-05\n",
      "epoch 50: valid acc = 0.886, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.906\n",
      "test acc: 0.886\n",
      "test acc: 0.8734\n",
      "number of batches for training: 245\n",
      "2000/49000 loss: 2.847223861673091\n",
      "4000/49000 loss: 2.730819925917433\n",
      "6000/49000 loss: 2.9770770677412837\n",
      "8000/49000 loss: 2.348440594047432\n",
      "10000/49000 loss: 2.207514146297929\n",
      "12000/49000 loss: 2.224729183560698\n",
      "14000/49000 loss: 1.8788419527111302\n",
      "16000/49000 loss: 1.6158220077385623\n",
      "18000/49000 loss: 1.425127172055743\n",
      "20000/49000 loss: 1.2798512752483706\n",
      "22000/49000 loss: 1.3269562224966482\n",
      "24000/49000 loss: 1.0053845282613394\n",
      "26000/49000 loss: 1.1213207704633188\n",
      "28000/49000 loss: 1.1378086724591854\n",
      "30000/49000 loss: 1.1016778122512718\n",
      "32000/49000 loss: 0.9551932858170389\n",
      "34000/49000 loss: 0.9962601659295534\n",
      "36000/49000 loss: 0.970269123163669\n",
      "38000/49000 loss: 0.7978978869958057\n",
      "40000/49000 loss: 0.9132798737640809\n",
      "42000/49000 loss: 0.8378631558755438\n",
      "44000/49000 loss: 0.7095557432059348\n",
      "46000/49000 loss: 0.7206081900295214\n",
      "48000/49000 loss: 0.7309382487322831\n",
      "epoch 1: valid acc = 0.751, new learning rate = 0.000475\n",
      "2000/49000 loss: 0.7171444563254501\n",
      "4000/49000 loss: 0.8092757827620324\n",
      "6000/49000 loss: 0.6763267968800263\n",
      "8000/49000 loss: 0.6004943564953028\n",
      "10000/49000 loss: 0.698566684249711\n",
      "12000/49000 loss: 0.5881569398508268\n",
      "14000/49000 loss: 0.5925680217675661\n",
      "16000/49000 loss: 0.7966245323148957\n",
      "18000/49000 loss: 0.53708783136793\n",
      "20000/49000 loss: 0.6244786133977092\n",
      "22000/49000 loss: 0.49775632910064316\n",
      "24000/49000 loss: 0.44962487095054454\n",
      "26000/49000 loss: 0.5638276407682311\n",
      "28000/49000 loss: 0.6174586040240416\n",
      "30000/49000 loss: 0.5241051861033019\n",
      "32000/49000 loss: 0.53434241583785\n",
      "34000/49000 loss: 0.65074022048843\n",
      "36000/49000 loss: 0.5763797657986005\n",
      "38000/49000 loss: 0.5492961727439739\n",
      "40000/49000 loss: 0.5762209908775877\n",
      "42000/49000 loss: 0.5631202535397712\n",
      "44000/49000 loss: 0.42325580009005886\n",
      "46000/49000 loss: 0.4891674562198132\n",
      "48000/49000 loss: 0.5090713250691996\n",
      "epoch 2: valid acc = 0.793, new learning rate = 0.00045125\n",
      "2000/49000 loss: 0.46636625070791166\n",
      "4000/49000 loss: 0.4254285931204559\n",
      "6000/49000 loss: 0.46758816249087176\n",
      "8000/49000 loss: 0.5278678341230151\n",
      "10000/49000 loss: 0.4978592979914268\n",
      "12000/49000 loss: 0.5830595413958658\n",
      "14000/49000 loss: 0.476264188521325\n",
      "16000/49000 loss: 0.5541886662721303\n",
      "18000/49000 loss: 0.5216959793062604\n",
      "20000/49000 loss: 0.4107675950760415\n",
      "22000/49000 loss: 0.42851736503508125\n",
      "24000/49000 loss: 0.45559082900297265\n",
      "26000/49000 loss: 0.4551218638418239\n",
      "28000/49000 loss: 0.41941759692187064\n",
      "30000/49000 loss: 0.46002930074227405\n",
      "32000/49000 loss: 0.5012315209875758\n",
      "34000/49000 loss: 0.5484107658806013\n",
      "36000/49000 loss: 0.37223935363668686\n",
      "38000/49000 loss: 0.4442494707397691\n",
      "40000/49000 loss: 0.4568816973202176\n",
      "42000/49000 loss: 0.406047247234895\n",
      "44000/49000 loss: 0.48341257841349644\n",
      "46000/49000 loss: 0.45660830540943487\n",
      "48000/49000 loss: 0.5280492362848923\n",
      "epoch 3: valid acc = 0.828, new learning rate = 0.0004286875\n",
      "2000/49000 loss: 0.4503229902509183\n",
      "4000/49000 loss: 0.4743481146987548\n",
      "6000/49000 loss: 0.5063070678297155\n",
      "8000/49000 loss: 0.4389170885097698\n",
      "10000/49000 loss: 0.45238441522382544\n",
      "12000/49000 loss: 0.43715676467372433\n",
      "14000/49000 loss: 0.500566778520829\n",
      "16000/49000 loss: 0.4631483059802499\n",
      "18000/49000 loss: 0.4561904198024356\n",
      "20000/49000 loss: 0.35209555602172576\n",
      "22000/49000 loss: 0.5759970106273474\n",
      "24000/49000 loss: 0.41224765086037723\n",
      "26000/49000 loss: 0.4432504132707666\n",
      "28000/49000 loss: 0.3716499347571861\n",
      "30000/49000 loss: 0.493578019512824\n",
      "32000/49000 loss: 0.5343917159639155\n",
      "34000/49000 loss: 0.37144028717441807\n",
      "36000/49000 loss: 0.38709064996937287\n",
      "38000/49000 loss: 0.33220817553795745\n",
      "40000/49000 loss: 0.48413880603518045\n",
      "42000/49000 loss: 0.4253422153543109\n",
      "44000/49000 loss: 0.4603680818115412\n",
      "46000/49000 loss: 0.3761824383358347\n",
      "48000/49000 loss: 0.3835115644070432\n",
      "epoch 4: valid acc = 0.832, new learning rate = 0.00040725312499999993\n",
      "2000/49000 loss: 0.4289458889551015\n",
      "4000/49000 loss: 0.42745458081433696\n",
      "6000/49000 loss: 0.44642616097675814\n",
      "8000/49000 loss: 0.34723505616070055\n",
      "10000/49000 loss: 0.36044326581063535\n",
      "12000/49000 loss: 0.5720478243730954\n",
      "14000/49000 loss: 0.45816555541196885\n",
      "16000/49000 loss: 0.38519574286679226\n",
      "18000/49000 loss: 0.4265431189322562\n",
      "20000/49000 loss: 0.3737489611793038\n",
      "22000/49000 loss: 0.4002657903722466\n",
      "24000/49000 loss: 0.383464969254395\n",
      "26000/49000 loss: 0.38348282394517025\n",
      "28000/49000 loss: 0.37298940433042826\n",
      "30000/49000 loss: 0.36417632802534183\n",
      "32000/49000 loss: 0.39191556510213954\n",
      "34000/49000 loss: 0.35221029742954896\n",
      "36000/49000 loss: 0.36480896096329424\n",
      "38000/49000 loss: 0.34046829023643377\n",
      "40000/49000 loss: 0.47885652594972966\n",
      "42000/49000 loss: 0.4132594874924964\n",
      "44000/49000 loss: 0.46952104564265096\n",
      "46000/49000 loss: 0.40011758249866225\n",
      "48000/49000 loss: 0.423168074995251\n",
      "epoch 5: valid acc = 0.85, new learning rate = 0.0003868904687499999\n",
      "2000/49000 loss: 0.3988356526592148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/49000 loss: 0.3932158023454556\n",
      "6000/49000 loss: 0.4401125637307748\n",
      "8000/49000 loss: 0.32686030105836544\n",
      "10000/49000 loss: 0.41634328194195636\n",
      "12000/49000 loss: 0.475134072557328\n",
      "14000/49000 loss: 0.3768758598359923\n",
      "16000/49000 loss: 0.4412077760721675\n",
      "18000/49000 loss: 0.3754127406273629\n",
      "20000/49000 loss: 0.41690599971517006\n",
      "22000/49000 loss: 0.3956706900131626\n",
      "24000/49000 loss: 0.3573497107449701\n",
      "26000/49000 loss: 0.4766405227797831\n",
      "28000/49000 loss: 0.36086384477275324\n",
      "30000/49000 loss: 0.49551489542949895\n",
      "32000/49000 loss: 0.3232724985506777\n",
      "34000/49000 loss: 0.3997420059301333\n",
      "36000/49000 loss: 0.42240436109473084\n",
      "38000/49000 loss: 0.4222981486132569\n",
      "40000/49000 loss: 0.348074232677939\n",
      "42000/49000 loss: 0.4666171057525898\n",
      "44000/49000 loss: 0.3453520508998588\n",
      "46000/49000 loss: 0.48726011664317254\n",
      "48000/49000 loss: 0.36328543445714373\n",
      "epoch 6: valid acc = 0.853, new learning rate = 0.0003675459453124999\n",
      "2000/49000 loss: 0.45604572305747415\n",
      "4000/49000 loss: 0.36476052308703794\n",
      "6000/49000 loss: 0.3480838242098337\n",
      "8000/49000 loss: 0.4252119233780117\n",
      "10000/49000 loss: 0.30204567228180296\n",
      "12000/49000 loss: 0.39410879201980775\n",
      "14000/49000 loss: 0.3490875598492366\n",
      "16000/49000 loss: 0.3860949052638475\n",
      "18000/49000 loss: 0.37858967535599397\n",
      "20000/49000 loss: 0.379595416634816\n",
      "22000/49000 loss: 0.45517422946131525\n",
      "24000/49000 loss: 0.30157465752517465\n",
      "26000/49000 loss: 0.4001546501763612\n",
      "28000/49000 loss: 0.44958391839412265\n",
      "30000/49000 loss: 0.35694555696663743\n",
      "32000/49000 loss: 0.4650006226171778\n",
      "34000/49000 loss: 0.3395052869203271\n",
      "36000/49000 loss: 0.3770472773093105\n",
      "38000/49000 loss: 0.49238835524353236\n",
      "40000/49000 loss: 0.39437728007575756\n",
      "42000/49000 loss: 0.37948964960727266\n",
      "44000/49000 loss: 0.37837578600179794\n",
      "46000/49000 loss: 0.34068304332975713\n",
      "48000/49000 loss: 0.3583536292062178\n",
      "epoch 7: valid acc = 0.862, new learning rate = 0.00034916864804687486\n",
      "2000/49000 loss: 0.3783756607174878\n",
      "4000/49000 loss: 0.3343281435045726\n",
      "6000/49000 loss: 0.36798185951285844\n",
      "8000/49000 loss: 0.39734791864481217\n",
      "10000/49000 loss: 0.3544095767012596\n",
      "12000/49000 loss: 0.43775942223824965\n",
      "14000/49000 loss: 0.3817902023751455\n",
      "16000/49000 loss: 0.3693696458651561\n",
      "18000/49000 loss: 0.4427219603258037\n",
      "20000/49000 loss: 0.38356027856501196\n",
      "22000/49000 loss: 0.33091006710284687\n",
      "24000/49000 loss: 0.38830803542250425\n",
      "26000/49000 loss: 0.4109913217000241\n",
      "28000/49000 loss: 0.34045593395955964\n",
      "30000/49000 loss: 0.3056531622741622\n",
      "32000/49000 loss: 0.3710566421397117\n",
      "34000/49000 loss: 0.34800804883771524\n",
      "36000/49000 loss: 0.32712153616331446\n",
      "38000/49000 loss: 0.33973776179876863\n",
      "40000/49000 loss: 0.38210836554100647\n",
      "42000/49000 loss: 0.34744945804478083\n",
      "44000/49000 loss: 0.4236243383319972\n",
      "46000/49000 loss: 0.45246479051035976\n",
      "48000/49000 loss: 0.3726750611718589\n",
      "epoch 8: valid acc = 0.866, new learning rate = 0.0003317102156445311\n",
      "2000/49000 loss: 0.4215070549520719\n",
      "4000/49000 loss: 0.4333101521590471\n",
      "6000/49000 loss: 0.3297808237917044\n",
      "8000/49000 loss: 0.3124987139485509\n",
      "10000/49000 loss: 0.29015497777096705\n",
      "12000/49000 loss: 0.3637342004226523\n",
      "14000/49000 loss: 0.28182797468338155\n",
      "16000/49000 loss: 0.38871712657985635\n",
      "18000/49000 loss: 0.31287114106484254\n",
      "20000/49000 loss: 0.36039402669015075\n",
      "22000/49000 loss: 0.3345457979883521\n",
      "24000/49000 loss: 0.28395911356943043\n",
      "26000/49000 loss: 0.36777957164263037\n",
      "28000/49000 loss: 0.3865807444654784\n",
      "30000/49000 loss: 0.39537273175156784\n",
      "32000/49000 loss: 0.3013364237269783\n",
      "34000/49000 loss: 0.41137652182915385\n",
      "36000/49000 loss: 0.29739634700785683\n",
      "38000/49000 loss: 0.40576690507602486\n",
      "40000/49000 loss: 0.3906751083129976\n",
      "42000/49000 loss: 0.37081572774865446\n",
      "44000/49000 loss: 0.32624558481093574\n",
      "46000/49000 loss: 0.3468104176778863\n",
      "48000/49000 loss: 0.38702908473804104\n",
      "epoch 9: valid acc = 0.875, new learning rate = 0.0003151247048623045\n",
      "2000/49000 loss: 0.33878591820021253\n",
      "4000/49000 loss: 0.24793872038603532\n",
      "6000/49000 loss: 0.3929970081434541\n",
      "8000/49000 loss: 0.3648456998006094\n",
      "10000/49000 loss: 0.40599056455241217\n",
      "12000/49000 loss: 0.382137759245174\n",
      "14000/49000 loss: 0.38599789060549106\n",
      "16000/49000 loss: 0.3680079487205584\n",
      "18000/49000 loss: 0.4104439978186212\n",
      "20000/49000 loss: 0.31566150530692694\n",
      "22000/49000 loss: 0.36211090786089817\n",
      "24000/49000 loss: 0.3238184997735318\n",
      "26000/49000 loss: 0.3264404354023802\n",
      "28000/49000 loss: 0.3390181875794875\n",
      "30000/49000 loss: 0.357152461128039\n",
      "32000/49000 loss: 0.30519256020657426\n",
      "34000/49000 loss: 0.3855326877201326\n",
      "36000/49000 loss: 0.29223850273167656\n",
      "38000/49000 loss: 0.4066680123551559\n",
      "40000/49000 loss: 0.3766289721282329\n",
      "42000/49000 loss: 0.40623886916395224\n",
      "44000/49000 loss: 0.3404857841628538\n",
      "46000/49000 loss: 0.3583330083348473\n",
      "48000/49000 loss: 0.3392629955573185\n",
      "epoch 10: valid acc = 0.871, new learning rate = 0.00029936846961918924\n",
      "2000/49000 loss: 0.2330065631148304\n",
      "4000/49000 loss: 0.3636384337894958\n",
      "6000/49000 loss: 0.25577568185488286\n",
      "8000/49000 loss: 0.36996989364869426\n",
      "10000/49000 loss: 0.2968197505588925\n",
      "12000/49000 loss: 0.3743517644830568\n",
      "14000/49000 loss: 0.26025587933660277\n",
      "16000/49000 loss: 0.32909537706940056\n",
      "18000/49000 loss: 0.3809207305373663\n",
      "20000/49000 loss: 0.29531891342588845\n",
      "22000/49000 loss: 0.3026502020258063\n",
      "24000/49000 loss: 0.310442117170368\n",
      "26000/49000 loss: 0.43121094094600315\n",
      "28000/49000 loss: 0.32318273138325043\n",
      "30000/49000 loss: 0.3170259836174521\n",
      "32000/49000 loss: 0.36033052018923406\n",
      "34000/49000 loss: 0.3590210712674631\n",
      "36000/49000 loss: 0.3804383994974186\n",
      "38000/49000 loss: 0.3249106359764328\n",
      "40000/49000 loss: 0.20071352790116592\n",
      "42000/49000 loss: 0.3092335729096517\n",
      "44000/49000 loss: 0.31946260991595515\n",
      "46000/49000 loss: 0.45987450616112624\n",
      "48000/49000 loss: 0.43662297664820116\n",
      "epoch 11: valid acc = 0.879, new learning rate = 0.00028440004613822977\n",
      "2000/49000 loss: 0.2792229274352948\n",
      "4000/49000 loss: 0.3056206497730205\n",
      "6000/49000 loss: 0.39400011496561205\n",
      "8000/49000 loss: 0.3613606038033154\n",
      "10000/49000 loss: 0.3443528154836408\n",
      "12000/49000 loss: 0.3453905156088652\n",
      "14000/49000 loss: 0.332793236176766\n",
      "16000/49000 loss: 0.34629466597918424\n",
      "18000/49000 loss: 0.4064028778205132\n",
      "20000/49000 loss: 0.3785038080941989\n",
      "22000/49000 loss: 0.2571860043737237\n",
      "24000/49000 loss: 0.2895899600618568\n",
      "26000/49000 loss: 0.2811311822036221\n",
      "28000/49000 loss: 0.28583150518624545\n",
      "30000/49000 loss: 0.31119948906776146\n",
      "32000/49000 loss: 0.36397940152330355\n",
      "34000/49000 loss: 0.2903983081595507\n",
      "36000/49000 loss: 0.37623936210471154\n",
      "38000/49000 loss: 0.39023355949978983\n",
      "40000/49000 loss: 0.28630872543693875\n",
      "42000/49000 loss: 0.2850912429461113\n",
      "44000/49000 loss: 0.3249015388348342\n",
      "46000/49000 loss: 0.36632372490454074\n",
      "48000/49000 loss: 0.24810180109592922\n",
      "epoch 12: valid acc = 0.874, new learning rate = 0.00027018004383131826\n",
      "2000/49000 loss: 0.24160198988766482\n",
      "4000/49000 loss: 0.3552965766957371\n",
      "6000/49000 loss: 0.3765544320573729\n",
      "8000/49000 loss: 0.2970465747109994\n",
      "10000/49000 loss: 0.37234631857385103\n",
      "12000/49000 loss: 0.32692867914653323\n",
      "14000/49000 loss: 0.38385391815370373\n",
      "16000/49000 loss: 0.2861911694461211\n",
      "18000/49000 loss: 0.4116329786745342\n",
      "20000/49000 loss: 0.33557544097293185\n",
      "22000/49000 loss: 0.3732575224976081\n",
      "24000/49000 loss: 0.2643156983537227\n",
      "26000/49000 loss: 0.4432050814198557\n",
      "28000/49000 loss: 0.38867687238891696\n",
      "30000/49000 loss: 0.279957458224104\n",
      "32000/49000 loss: 0.3160107501109968\n",
      "34000/49000 loss: 0.353640566650292\n",
      "36000/49000 loss: 0.34545396491136343\n",
      "38000/49000 loss: 0.31398227462071887\n",
      "40000/49000 loss: 0.32595791461834095\n",
      "42000/49000 loss: 0.3751795665677997\n",
      "44000/49000 loss: 0.3461978062146764\n",
      "46000/49000 loss: 0.27532644313674276\n",
      "48000/49000 loss: 0.3800527549487835\n",
      "epoch 13: valid acc = 0.887, new learning rate = 0.00025667104163975234\n",
      "2000/49000 loss: 0.38325574566992016\n",
      "4000/49000 loss: 0.30515548954443\n",
      "6000/49000 loss: 0.3807599902701715\n",
      "8000/49000 loss: 0.3121286240770332\n",
      "10000/49000 loss: 0.3668228268433357\n",
      "12000/49000 loss: 0.3300656552330236\n",
      "14000/49000 loss: 0.34222088712551063\n",
      "16000/49000 loss: 0.3179092127035413\n",
      "18000/49000 loss: 0.35516690137233337\n",
      "20000/49000 loss: 0.2526052789430452\n",
      "22000/49000 loss: 0.3598012737569841\n",
      "24000/49000 loss: 0.3625428950977856\n",
      "26000/49000 loss: 0.3741319992372282\n",
      "28000/49000 loss: 0.3879986416800253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000/49000 loss: 0.4433525843273291\n",
      "32000/49000 loss: 0.4003124556371967\n",
      "34000/49000 loss: 0.40845843687841893\n",
      "36000/49000 loss: 0.37314042991897206\n",
      "38000/49000 loss: 0.42125421622516596\n",
      "40000/49000 loss: 0.28039839289359375\n",
      "42000/49000 loss: 0.2918722798119492\n",
      "44000/49000 loss: 0.384590623547088\n",
      "46000/49000 loss: 0.37485615649946924\n",
      "48000/49000 loss: 0.34747636492034734\n",
      "epoch 14: valid acc = 0.883, new learning rate = 0.00024383748955776472\n",
      "2000/49000 loss: 0.3042951856056862\n",
      "4000/49000 loss: 0.241718114327061\n",
      "6000/49000 loss: 0.2732255321436368\n",
      "8000/49000 loss: 0.31589151441518354\n",
      "10000/49000 loss: 0.2884170451511826\n",
      "12000/49000 loss: 0.27863623188238656\n",
      "14000/49000 loss: 0.3617293815044251\n",
      "16000/49000 loss: 0.3823539204796885\n",
      "18000/49000 loss: 0.30638673620207824\n",
      "20000/49000 loss: 0.3721809743291066\n",
      "22000/49000 loss: 0.33226577666090984\n",
      "24000/49000 loss: 0.32826579094070474\n",
      "26000/49000 loss: 0.38866428358938404\n",
      "28000/49000 loss: 0.3466206089446707\n",
      "30000/49000 loss: 0.29756809766755027\n",
      "32000/49000 loss: 0.337828357430472\n",
      "34000/49000 loss: 0.33492575757729703\n",
      "36000/49000 loss: 0.31828436367017054\n",
      "38000/49000 loss: 0.304697348662776\n",
      "40000/49000 loss: 0.30644728855136816\n",
      "42000/49000 loss: 0.3299570477512838\n",
      "44000/49000 loss: 0.4172924464046984\n",
      "46000/49000 loss: 0.33804141449549324\n",
      "48000/49000 loss: 0.32353867485291593\n",
      "epoch 15: valid acc = 0.879, new learning rate = 0.00023164561507987649\n",
      "2000/49000 loss: 0.42322016619783887\n",
      "4000/49000 loss: 0.3017342128650141\n",
      "6000/49000 loss: 0.27632469815138117\n",
      "8000/49000 loss: 0.37057527455946365\n",
      "10000/49000 loss: 0.33480108437431616\n",
      "12000/49000 loss: 0.24377373721150816\n",
      "14000/49000 loss: 0.32514016140508634\n",
      "16000/49000 loss: 0.3435804817702758\n",
      "18000/49000 loss: 0.34711416799629136\n",
      "20000/49000 loss: 0.4386342613577689\n",
      "22000/49000 loss: 0.27269886936226867\n",
      "24000/49000 loss: 0.3416712914744344\n",
      "26000/49000 loss: 0.32550783499903224\n",
      "28000/49000 loss: 0.29047145408915126\n",
      "30000/49000 loss: 0.2603651855701464\n",
      "32000/49000 loss: 0.3592772507688996\n",
      "34000/49000 loss: 0.4351244174492356\n",
      "36000/49000 loss: 0.3156252868498927\n",
      "38000/49000 loss: 0.27235283318700165\n",
      "40000/49000 loss: 0.2638132546634872\n",
      "42000/49000 loss: 0.27718237949638397\n",
      "44000/49000 loss: 0.34707883044273286\n",
      "46000/49000 loss: 0.3517481329688041\n",
      "48000/49000 loss: 0.391315109212483\n",
      "epoch 16: valid acc = 0.876, new learning rate = 0.00022006333432588265\n",
      "2000/49000 loss: 0.34481419561741894\n",
      "4000/49000 loss: 0.2762683217065993\n",
      "6000/49000 loss: 0.2964455655290267\n",
      "8000/49000 loss: 0.35479315084455726\n",
      "10000/49000 loss: 0.32257879810883083\n",
      "12000/49000 loss: 0.32328968850716805\n",
      "14000/49000 loss: 0.24091568182998077\n",
      "16000/49000 loss: 0.26392633187502296\n",
      "18000/49000 loss: 0.3083961932700044\n",
      "20000/49000 loss: 0.29998569195538893\n",
      "22000/49000 loss: 0.3280801351841519\n",
      "24000/49000 loss: 0.2562326197851141\n",
      "26000/49000 loss: 0.28605084989245316\n",
      "28000/49000 loss: 0.3391363686399141\n",
      "30000/49000 loss: 0.2649462070166018\n",
      "32000/49000 loss: 0.38094285487284174\n",
      "34000/49000 loss: 0.43225054268097296\n",
      "36000/49000 loss: 0.250679832925084\n",
      "38000/49000 loss: 0.3747768651895603\n",
      "40000/49000 loss: 0.22818416227222468\n",
      "42000/49000 loss: 0.3268809593337264\n",
      "44000/49000 loss: 0.3278109788402254\n",
      "46000/49000 loss: 0.28993949133193925\n",
      "48000/49000 loss: 0.3606032007482832\n",
      "epoch 17: valid acc = 0.883, new learning rate = 0.00020906016760958852\n",
      "2000/49000 loss: 0.24346740649467222\n",
      "4000/49000 loss: 0.2783233952545532\n",
      "6000/49000 loss: 0.2859731240331686\n",
      "8000/49000 loss: 0.3198164505727996\n",
      "10000/49000 loss: 0.2546780421978654\n",
      "12000/49000 loss: 0.34141662262701045\n",
      "14000/49000 loss: 0.38172289720433833\n",
      "16000/49000 loss: 0.3402312344091902\n",
      "18000/49000 loss: 0.2808180465165218\n",
      "20000/49000 loss: 0.26873203983645555\n",
      "22000/49000 loss: 0.3039095718523759\n",
      "24000/49000 loss: 0.3340474213831992\n",
      "26000/49000 loss: 0.22756271754401308\n",
      "28000/49000 loss: 0.2889630646957607\n",
      "30000/49000 loss: 0.3776701286401981\n",
      "32000/49000 loss: 0.2340039018158358\n",
      "34000/49000 loss: 0.3280928973816554\n",
      "36000/49000 loss: 0.3552359018747928\n",
      "38000/49000 loss: 0.37677107692162093\n",
      "40000/49000 loss: 0.3693556154487566\n",
      "42000/49000 loss: 0.2776982517122173\n",
      "44000/49000 loss: 0.36911467532115566\n",
      "46000/49000 loss: 0.31537312050034\n",
      "48000/49000 loss: 0.30460143096684733\n",
      "epoch 18: valid acc = 0.891, new learning rate = 0.00019860715922910907\n",
      "2000/49000 loss: 0.27602041628122836\n",
      "4000/49000 loss: 0.29918903391124957\n",
      "6000/49000 loss: 0.35246639944548197\n",
      "8000/49000 loss: 0.2949236994997143\n",
      "10000/49000 loss: 0.36201316802725914\n",
      "12000/49000 loss: 0.3042259809065964\n",
      "14000/49000 loss: 0.31529493675982706\n",
      "16000/49000 loss: 0.36292317593112944\n",
      "18000/49000 loss: 0.2959020958394012\n",
      "20000/49000 loss: 0.3027559799840842\n",
      "22000/49000 loss: 0.24019677401609918\n",
      "24000/49000 loss: 0.3395614241746136\n",
      "26000/49000 loss: 0.3354620695719419\n",
      "28000/49000 loss: 0.2841377662348056\n",
      "30000/49000 loss: 0.39745848139754325\n",
      "32000/49000 loss: 0.29126175686405137\n",
      "34000/49000 loss: 0.20622305584155504\n",
      "36000/49000 loss: 0.27759250794237145\n",
      "38000/49000 loss: 0.3332948398114388\n",
      "40000/49000 loss: 0.32989927737719915\n",
      "42000/49000 loss: 0.294165987334256\n",
      "44000/49000 loss: 0.3672098766118149\n",
      "46000/49000 loss: 0.2892408256369224\n",
      "48000/49000 loss: 0.4178690755459045\n",
      "epoch 19: valid acc = 0.882, new learning rate = 0.0001886768012676536\n",
      "2000/49000 loss: 0.3448827703993163\n",
      "4000/49000 loss: 0.2964770149040114\n",
      "6000/49000 loss: 0.3020534661697308\n",
      "8000/49000 loss: 0.2650602771134379\n",
      "10000/49000 loss: 0.3123248493539882\n",
      "12000/49000 loss: 0.35618990489032887\n",
      "14000/49000 loss: 0.3013851619054999\n",
      "16000/49000 loss: 0.302104306548433\n",
      "18000/49000 loss: 0.28928350028345107\n",
      "20000/49000 loss: 0.28378137261726843\n",
      "22000/49000 loss: 0.33075025545534015\n",
      "24000/49000 loss: 0.3979825236843592\n",
      "26000/49000 loss: 0.3254025013267113\n",
      "28000/49000 loss: 0.3191877893407278\n",
      "30000/49000 loss: 0.2775095048248386\n",
      "32000/49000 loss: 0.3940631845215505\n",
      "34000/49000 loss: 0.2552397076218442\n",
      "36000/49000 loss: 0.30259384486685575\n",
      "38000/49000 loss: 0.37551949947459484\n",
      "40000/49000 loss: 0.28500428211017154\n",
      "42000/49000 loss: 0.24845601131240816\n",
      "44000/49000 loss: 0.26439747159409244\n",
      "46000/49000 loss: 0.32722095112319305\n",
      "48000/49000 loss: 0.3981598067195346\n",
      "epoch 20: valid acc = 0.891, new learning rate = 0.0001792429612042709\n",
      "2000/49000 loss: 0.35992051628629795\n",
      "4000/49000 loss: 0.35838536015650735\n",
      "6000/49000 loss: 0.15988606786438392\n",
      "8000/49000 loss: 0.3680097656999026\n",
      "10000/49000 loss: 0.344316761658445\n",
      "12000/49000 loss: 0.3332736250435995\n",
      "14000/49000 loss: 0.42246366231270427\n",
      "16000/49000 loss: 0.3787543384452008\n",
      "18000/49000 loss: 0.3630212497637024\n",
      "20000/49000 loss: 0.2495789526448266\n",
      "22000/49000 loss: 0.36040121000212033\n",
      "24000/49000 loss: 0.31183835794742454\n",
      "26000/49000 loss: 0.3219218066858027\n",
      "28000/49000 loss: 0.22439426629320344\n",
      "30000/49000 loss: 0.326448200026844\n",
      "32000/49000 loss: 0.24102402782099155\n",
      "34000/49000 loss: 0.3295136103486912\n",
      "36000/49000 loss: 0.27773959117638763\n",
      "38000/49000 loss: 0.28743417964254225\n",
      "40000/49000 loss: 0.3420016486906732\n",
      "42000/49000 loss: 0.28236502397463714\n",
      "44000/49000 loss: 0.4087925240205441\n",
      "46000/49000 loss: 0.3820635719408298\n",
      "48000/49000 loss: 0.3575496385958369\n",
      "epoch 21: valid acc = 0.886, new learning rate = 0.00017028081314405735\n",
      "2000/49000 loss: 0.3275100658031481\n",
      "4000/49000 loss: 0.2714439522860854\n",
      "6000/49000 loss: 0.2548334555245129\n",
      "8000/49000 loss: 0.2670877916533655\n",
      "10000/49000 loss: 0.30878418583965134\n",
      "12000/49000 loss: 0.414336895424307\n",
      "14000/49000 loss: 0.30517485744358824\n",
      "16000/49000 loss: 0.29307549806521943\n",
      "18000/49000 loss: 0.34788311066304534\n",
      "20000/49000 loss: 0.4115148390526031\n",
      "22000/49000 loss: 0.23636986546410452\n",
      "24000/49000 loss: 0.3690102548156082\n",
      "26000/49000 loss: 0.3318369021330273\n",
      "28000/49000 loss: 0.2700386505714533\n",
      "30000/49000 loss: 0.3287002990441482\n",
      "32000/49000 loss: 0.27088747542062985\n",
      "34000/49000 loss: 0.23986741152254643\n",
      "36000/49000 loss: 0.3703466899727068\n",
      "38000/49000 loss: 0.31767333563578665\n",
      "40000/49000 loss: 0.4003901907142831\n",
      "42000/49000 loss: 0.3430355594726691\n",
      "44000/49000 loss: 0.22699992167039626\n",
      "46000/49000 loss: 0.30236635915411303\n",
      "48000/49000 loss: 0.26503517755763367\n",
      "epoch 22: valid acc = 0.881, new learning rate = 0.00016176677248685447\n",
      "2000/49000 loss: 0.29494372600870766\n",
      "4000/49000 loss: 0.32178917813007774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/49000 loss: 0.36015550486481446\n",
      "8000/49000 loss: 0.35371053104297184\n",
      "10000/49000 loss: 0.2842651604940543\n",
      "12000/49000 loss: 0.24974066858975427\n",
      "14000/49000 loss: 0.29890239983481026\n",
      "16000/49000 loss: 0.327539854188956\n",
      "18000/49000 loss: 0.2088659842810905\n",
      "20000/49000 loss: 0.22479133178905003\n",
      "22000/49000 loss: 0.31488760099649377\n",
      "24000/49000 loss: 0.3615816689640005\n",
      "26000/49000 loss: 0.40451718560671723\n",
      "28000/49000 loss: 0.33038439906307326\n",
      "30000/49000 loss: 0.3116030998003876\n",
      "32000/49000 loss: 0.27858640734931805\n",
      "34000/49000 loss: 0.33750199241729373\n",
      "36000/49000 loss: 0.39049683282452996\n",
      "38000/49000 loss: 0.40534511775310417\n",
      "40000/49000 loss: 0.28919630782764444\n",
      "42000/49000 loss: 0.37315819112203996\n",
      "44000/49000 loss: 0.25438435578674296\n",
      "46000/49000 loss: 0.3009336731407677\n",
      "48000/49000 loss: 0.2756649143036194\n",
      "epoch 23: valid acc = 0.881, new learning rate = 0.00015367843386251173\n",
      "2000/49000 loss: 0.250792917443212\n",
      "4000/49000 loss: 0.4121378421945628\n",
      "6000/49000 loss: 0.42015320947939533\n",
      "8000/49000 loss: 0.3607202427440949\n",
      "10000/49000 loss: 0.37059356065517135\n",
      "12000/49000 loss: 0.2646785764814516\n",
      "14000/49000 loss: 0.2827962244987895\n",
      "16000/49000 loss: 0.28647247603009396\n",
      "18000/49000 loss: 0.34648954218092204\n",
      "20000/49000 loss: 0.3271344183492396\n",
      "22000/49000 loss: 0.3114997513158769\n",
      "24000/49000 loss: 0.26188990022482606\n",
      "26000/49000 loss: 0.32372088792565173\n",
      "28000/49000 loss: 0.30012433240248776\n",
      "30000/49000 loss: 0.3117624719234044\n",
      "32000/49000 loss: 0.34272136729721586\n",
      "34000/49000 loss: 0.25793853748291123\n",
      "36000/49000 loss: 0.2608593971921152\n",
      "38000/49000 loss: 0.3477866434874165\n",
      "40000/49000 loss: 0.319829667712442\n",
      "42000/49000 loss: 0.30972671406612357\n",
      "44000/49000 loss: 0.3372908399223597\n",
      "46000/49000 loss: 0.3278433286199603\n",
      "48000/49000 loss: 0.23610179065123557\n",
      "epoch 24: valid acc = 0.888, new learning rate = 0.00014599451216938612\n",
      "2000/49000 loss: 0.2688434710155271\n",
      "4000/49000 loss: 0.2905695874595078\n",
      "6000/49000 loss: 0.37839001462634986\n",
      "8000/49000 loss: 0.3112339692094694\n",
      "10000/49000 loss: 0.2573784041928301\n",
      "12000/49000 loss: 0.3339633797444618\n",
      "14000/49000 loss: 0.2843279936690109\n",
      "16000/49000 loss: 0.3917452903139435\n",
      "18000/49000 loss: 0.26846764236296866\n",
      "20000/49000 loss: 0.25561846677287936\n",
      "22000/49000 loss: 0.31577744276580527\n",
      "24000/49000 loss: 0.2046775250565821\n",
      "26000/49000 loss: 0.2782142657816612\n",
      "28000/49000 loss: 0.21063134623152815\n",
      "30000/49000 loss: 0.30474458912810504\n",
      "32000/49000 loss: 0.20688741456218546\n",
      "34000/49000 loss: 0.2941738883945605\n",
      "36000/49000 loss: 0.3931962261118734\n",
      "38000/49000 loss: 0.3092740340750889\n",
      "40000/49000 loss: 0.2645984244153897\n",
      "42000/49000 loss: 0.2676176293078596\n",
      "44000/49000 loss: 0.22731522278004268\n",
      "46000/49000 loss: 0.3855395367616372\n",
      "48000/49000 loss: 0.2505370228698401\n",
      "epoch 25: valid acc = 0.884, new learning rate = 0.00013869478656091682\n",
      "2000/49000 loss: 0.34578149097031813\n",
      "4000/49000 loss: 0.2966306335460864\n",
      "6000/49000 loss: 0.26993009308431754\n",
      "8000/49000 loss: 0.25886963991258044\n",
      "10000/49000 loss: 0.3438677843051663\n",
      "12000/49000 loss: 0.3110093042584302\n",
      "14000/49000 loss: 0.31222229665076134\n",
      "16000/49000 loss: 0.31820297370779577\n",
      "18000/49000 loss: 0.22398438950685842\n",
      "20000/49000 loss: 0.20653601924868836\n",
      "22000/49000 loss: 0.3084025303615235\n",
      "24000/49000 loss: 0.3157878038786286\n",
      "26000/49000 loss: 0.28778933556537273\n",
      "28000/49000 loss: 0.2807789370632231\n",
      "30000/49000 loss: 0.28577818627327156\n",
      "32000/49000 loss: 0.23245555106702553\n",
      "34000/49000 loss: 0.24293653403812918\n",
      "36000/49000 loss: 0.3557719320461702\n",
      "38000/49000 loss: 0.29884498429699197\n",
      "40000/49000 loss: 0.24838998950455943\n",
      "42000/49000 loss: 0.29824489028505274\n",
      "44000/49000 loss: 0.35314153542077414\n",
      "46000/49000 loss: 0.24493628298518227\n",
      "48000/49000 loss: 0.36057192070177524\n",
      "epoch 26: valid acc = 0.886, new learning rate = 0.00013176004723287096\n",
      "2000/49000 loss: 0.2533615627465771\n",
      "4000/49000 loss: 0.30506171749263694\n",
      "6000/49000 loss: 0.3856023593398803\n",
      "8000/49000 loss: 0.26447829832463676\n",
      "10000/49000 loss: 0.3533287006005684\n",
      "12000/49000 loss: 0.23732646881554556\n",
      "14000/49000 loss: 0.3683842784950371\n",
      "16000/49000 loss: 0.24189956252252015\n",
      "18000/49000 loss: 0.2917193133744091\n",
      "20000/49000 loss: 0.4332063682725499\n",
      "22000/49000 loss: 0.37333295502793157\n",
      "24000/49000 loss: 0.2364903848017244\n",
      "26000/49000 loss: 0.33980154840514193\n",
      "28000/49000 loss: 0.3242370171732511\n",
      "30000/49000 loss: 0.2422577033340321\n",
      "32000/49000 loss: 0.2868092684976091\n",
      "34000/49000 loss: 0.34533489661886124\n",
      "36000/49000 loss: 0.24852847752426302\n",
      "38000/49000 loss: 0.3615360356042285\n",
      "40000/49000 loss: 0.2356326743248699\n",
      "42000/49000 loss: 0.3911105637189588\n",
      "44000/49000 loss: 0.3892247309689014\n",
      "46000/49000 loss: 0.31901980187086454\n",
      "48000/49000 loss: 0.3297373415358941\n",
      "epoch 27: valid acc = 0.892, new learning rate = 0.0001251720448712274\n",
      "2000/49000 loss: 0.2279819613863991\n",
      "4000/49000 loss: 0.27353971260627896\n",
      "6000/49000 loss: 0.3108504130857191\n",
      "8000/49000 loss: 0.2439857673544021\n",
      "10000/49000 loss: 0.2884450439758082\n",
      "12000/49000 loss: 0.33022247226307594\n",
      "14000/49000 loss: 0.27312844442688455\n",
      "16000/49000 loss: 0.34258334996065576\n",
      "18000/49000 loss: 0.34951185563940107\n",
      "20000/49000 loss: 0.26475773837146155\n",
      "22000/49000 loss: 0.3393127738133229\n",
      "24000/49000 loss: 0.2938091464619765\n",
      "26000/49000 loss: 0.3128175039460243\n",
      "28000/49000 loss: 0.2571680950047281\n",
      "30000/49000 loss: 0.2921452870039993\n",
      "32000/49000 loss: 0.25611645152055434\n",
      "34000/49000 loss: 0.3573512897768752\n",
      "36000/49000 loss: 0.35714492620118143\n",
      "38000/49000 loss: 0.3040954762708221\n",
      "40000/49000 loss: 0.35177047142756673\n",
      "42000/49000 loss: 0.3240698579780234\n",
      "44000/49000 loss: 0.22938704108017297\n",
      "46000/49000 loss: 0.22143739395467327\n",
      "48000/49000 loss: 0.27891658425450694\n",
      "epoch 28: valid acc = 0.888, new learning rate = 0.00011891344262766602\n",
      "2000/49000 loss: 0.3083203928768759\n",
      "4000/49000 loss: 0.3006461859036211\n",
      "6000/49000 loss: 0.31510707696846957\n",
      "8000/49000 loss: 0.20973470965418983\n",
      "10000/49000 loss: 0.2838356980610356\n",
      "12000/49000 loss: 0.224749324576636\n",
      "14000/49000 loss: 0.27022473975698796\n",
      "16000/49000 loss: 0.3492401384240252\n",
      "18000/49000 loss: 0.19205510544167703\n",
      "20000/49000 loss: 0.306107539544723\n",
      "22000/49000 loss: 0.3219736826270238\n",
      "24000/49000 loss: 0.2680161211227092\n",
      "26000/49000 loss: 0.2489360574161633\n",
      "28000/49000 loss: 0.2958620958463746\n",
      "30000/49000 loss: 0.40625659151199683\n",
      "32000/49000 loss: 0.2984335191069408\n",
      "34000/49000 loss: 0.2955456565718698\n",
      "36000/49000 loss: 0.3300170535925785\n",
      "38000/49000 loss: 0.23890080687197437\n",
      "40000/49000 loss: 0.3488555420993081\n",
      "42000/49000 loss: 0.2864413636196869\n",
      "44000/49000 loss: 0.2972007134177781\n",
      "46000/49000 loss: 0.25697618424573104\n",
      "48000/49000 loss: 0.2723402987602456\n",
      "epoch 29: valid acc = 0.882, new learning rate = 0.00011296777049628272\n",
      "2000/49000 loss: 0.24624522207933214\n",
      "4000/49000 loss: 0.2748027176838221\n",
      "6000/49000 loss: 0.2447218468758146\n",
      "8000/49000 loss: 0.3449267350728918\n",
      "10000/49000 loss: 0.30048768414893207\n",
      "12000/49000 loss: 0.24235310392657758\n",
      "14000/49000 loss: 0.25941383496776577\n",
      "16000/49000 loss: 0.2218460412605556\n",
      "18000/49000 loss: 0.3053681844274733\n",
      "20000/49000 loss: 0.22724518609173297\n",
      "22000/49000 loss: 0.2874936081467184\n",
      "24000/49000 loss: 0.3344314083587759\n",
      "26000/49000 loss: 0.26283071583878564\n",
      "28000/49000 loss: 0.25459588771954356\n",
      "30000/49000 loss: 0.32956731575737563\n",
      "32000/49000 loss: 0.3236445999510249\n",
      "34000/49000 loss: 0.30907213612353396\n",
      "36000/49000 loss: 0.24705083933383373\n",
      "38000/49000 loss: 0.2851321781935669\n",
      "40000/49000 loss: 0.23464735085818683\n",
      "42000/49000 loss: 0.29991146253820383\n",
      "44000/49000 loss: 0.31642429760694596\n",
      "46000/49000 loss: 0.2397681090739773\n",
      "48000/49000 loss: 0.3611041406656177\n",
      "epoch 30: valid acc = 0.891, new learning rate = 0.00010731938197146858\n",
      "2000/49000 loss: 0.3305537682942544\n",
      "4000/49000 loss: 0.41113327734145716\n",
      "6000/49000 loss: 0.21702528573329863\n",
      "8000/49000 loss: 0.3118487675863677\n",
      "10000/49000 loss: 0.2533049279580737\n",
      "12000/49000 loss: 0.28513471051533235\n",
      "14000/49000 loss: 0.31756028158025035\n",
      "16000/49000 loss: 0.31862034079358664\n",
      "18000/49000 loss: 0.22867210817495115\n",
      "20000/49000 loss: 0.29658461843627376\n",
      "22000/49000 loss: 0.30337524510913844\n",
      "24000/49000 loss: 0.3651307772796024\n",
      "26000/49000 loss: 0.302455652517579\n",
      "28000/49000 loss: 0.30713365954303995\n",
      "30000/49000 loss: 0.38587273312008785\n",
      "32000/49000 loss: 0.28056713854604803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34000/49000 loss: 0.2635331401814056\n",
      "36000/49000 loss: 0.19967539346507696\n",
      "38000/49000 loss: 0.2791474587301455\n",
      "40000/49000 loss: 0.27576465832182007\n",
      "42000/49000 loss: 0.29613930660288956\n",
      "44000/49000 loss: 0.25559187117096976\n",
      "46000/49000 loss: 0.24444615696743116\n",
      "48000/49000 loss: 0.28140227176167065\n",
      "epoch 31: valid acc = 0.889, new learning rate = 0.00010195341287289515\n",
      "2000/49000 loss: 0.28907598007362195\n",
      "4000/49000 loss: 0.3329677181724432\n",
      "6000/49000 loss: 0.288917658404171\n",
      "8000/49000 loss: 0.25585720491252045\n",
      "10000/49000 loss: 0.30902359476947866\n",
      "12000/49000 loss: 0.2450700569006449\n",
      "14000/49000 loss: 0.32866974466590765\n",
      "16000/49000 loss: 0.2326244402447443\n",
      "18000/49000 loss: 0.23703225809891454\n",
      "20000/49000 loss: 0.20581899340715287\n",
      "22000/49000 loss: 0.22097210762783137\n",
      "24000/49000 loss: 0.225743706545206\n",
      "26000/49000 loss: 0.23326787442688357\n",
      "28000/49000 loss: 0.3122713275922223\n",
      "30000/49000 loss: 0.2938187418218026\n",
      "32000/49000 loss: 0.2909124996655121\n",
      "34000/49000 loss: 0.30158488702719977\n",
      "36000/49000 loss: 0.2514893842838994\n",
      "38000/49000 loss: 0.24398927969021283\n",
      "40000/49000 loss: 0.3158467077834545\n",
      "42000/49000 loss: 0.31874438586647724\n",
      "44000/49000 loss: 0.2698156918649433\n",
      "46000/49000 loss: 0.28994104333962895\n",
      "48000/49000 loss: 0.29684769813869233\n",
      "epoch 32: valid acc = 0.882, new learning rate = 9.685574222925039e-05\n",
      "2000/49000 loss: 0.2636839254297164\n",
      "4000/49000 loss: 0.2684755604146513\n",
      "6000/49000 loss: 0.2888313230485603\n",
      "8000/49000 loss: 0.25656426695780826\n",
      "10000/49000 loss: 0.31901283686980003\n",
      "12000/49000 loss: 0.31795527257775386\n",
      "14000/49000 loss: 0.25790567899622185\n",
      "16000/49000 loss: 0.2602036169801181\n",
      "18000/49000 loss: 0.4105217951500797\n",
      "20000/49000 loss: 0.2869402728162892\n",
      "22000/49000 loss: 0.3126736042465916\n",
      "24000/49000 loss: 0.18198366948336983\n",
      "26000/49000 loss: 0.24298520045789274\n",
      "28000/49000 loss: 0.3499893782677829\n",
      "30000/49000 loss: 0.2325384610766618\n",
      "32000/49000 loss: 0.3133922777663711\n",
      "34000/49000 loss: 0.3563373789046243\n",
      "36000/49000 loss: 0.2162253088591974\n",
      "38000/49000 loss: 0.2597537299895523\n",
      "40000/49000 loss: 0.3246207485689671\n",
      "42000/49000 loss: 0.291126656611833\n",
      "44000/49000 loss: 0.24442582438601307\n",
      "46000/49000 loss: 0.2113108638995603\n",
      "48000/49000 loss: 0.3689259024758923\n",
      "epoch 33: valid acc = 0.884, new learning rate = 9.201295511778786e-05\n",
      "2000/49000 loss: 0.2014811802370161\n",
      "4000/49000 loss: 0.321374201115159\n",
      "6000/49000 loss: 0.2446154708266357\n",
      "8000/49000 loss: 0.37502257308639314\n",
      "10000/49000 loss: 0.23345181128594605\n",
      "12000/49000 loss: 0.28150287098468896\n",
      "14000/49000 loss: 0.33202217488269814\n",
      "16000/49000 loss: 0.3177483932534859\n",
      "18000/49000 loss: 0.2384177496643896\n",
      "20000/49000 loss: 0.24012267744832444\n",
      "22000/49000 loss: 0.30496904762980553\n",
      "24000/49000 loss: 0.23187452296391736\n",
      "26000/49000 loss: 0.30597695700029587\n",
      "28000/49000 loss: 0.3832161468290624\n",
      "30000/49000 loss: 0.30855505708509356\n",
      "32000/49000 loss: 0.2593880079642215\n",
      "34000/49000 loss: 0.3242233831391635\n",
      "36000/49000 loss: 0.22338656381926172\n",
      "38000/49000 loss: 0.24982704796707667\n",
      "40000/49000 loss: 0.24003552409512668\n",
      "42000/49000 loss: 0.32625972008942167\n",
      "44000/49000 loss: 0.28748694642666567\n",
      "46000/49000 loss: 0.2587786095071111\n",
      "48000/49000 loss: 0.27563869137700997\n",
      "epoch 34: valid acc = 0.884, new learning rate = 8.741230736189846e-05\n",
      "2000/49000 loss: 0.3811903648278454\n",
      "4000/49000 loss: 0.24163363437295945\n",
      "6000/49000 loss: 0.24169038317148178\n",
      "8000/49000 loss: 0.24208180858762007\n",
      "10000/49000 loss: 0.27432375722105645\n",
      "12000/49000 loss: 0.33007489833045045\n",
      "14000/49000 loss: 0.2518477823487113\n",
      "16000/49000 loss: 0.25919266511113587\n",
      "18000/49000 loss: 0.26732113860040924\n",
      "20000/49000 loss: 0.23056879620874704\n",
      "22000/49000 loss: 0.31971739387725256\n",
      "24000/49000 loss: 0.277937055516695\n",
      "26000/49000 loss: 0.21554205929276885\n",
      "28000/49000 loss: 0.3983119303739956\n",
      "30000/49000 loss: 0.29269284717717087\n",
      "32000/49000 loss: 0.2720838428153371\n",
      "34000/49000 loss: 0.3414471818285425\n",
      "36000/49000 loss: 0.3317154977088974\n",
      "38000/49000 loss: 0.29792160314754557\n",
      "40000/49000 loss: 0.29767311857835\n",
      "42000/49000 loss: 0.2866808461957875\n",
      "44000/49000 loss: 0.31297174303487146\n",
      "46000/49000 loss: 0.3603106470970211\n",
      "48000/49000 loss: 0.3156990053269871\n",
      "epoch 35: valid acc = 0.885, new learning rate = 8.304169199380353e-05\n",
      "2000/49000 loss: 0.23157996435241293\n",
      "4000/49000 loss: 0.32109599191694915\n",
      "6000/49000 loss: 0.3029934507247712\n",
      "8000/49000 loss: 0.32216619718178857\n",
      "10000/49000 loss: 0.34148366408714537\n",
      "12000/49000 loss: 0.2595618224086935\n",
      "14000/49000 loss: 0.3172959483648189\n",
      "16000/49000 loss: 0.28854626841639247\n",
      "18000/49000 loss: 0.30946848695694174\n",
      "20000/49000 loss: 0.2058233700273693\n",
      "22000/49000 loss: 0.25642984963813287\n",
      "24000/49000 loss: 0.20254475734957067\n",
      "26000/49000 loss: 0.2957945752936196\n",
      "28000/49000 loss: 0.311273222454207\n",
      "30000/49000 loss: 0.2690553935871346\n",
      "32000/49000 loss: 0.2597566669192745\n",
      "34000/49000 loss: 0.262403976422331\n",
      "36000/49000 loss: 0.21187019407379565\n",
      "38000/49000 loss: 0.1950646872703981\n",
      "40000/49000 loss: 0.23213011076210432\n",
      "42000/49000 loss: 0.2626254150157329\n",
      "44000/49000 loss: 0.2755112390321843\n",
      "46000/49000 loss: 0.2889050088781153\n",
      "48000/49000 loss: 0.2966263412431875\n",
      "epoch 36: valid acc = 0.887, new learning rate = 7.888960739411335e-05\n",
      "2000/49000 loss: 0.3222469148809913\n",
      "4000/49000 loss: 0.34082314206522485\n",
      "6000/49000 loss: 0.2136551992088636\n",
      "8000/49000 loss: 0.31090041437834653\n",
      "10000/49000 loss: 0.2930021671434262\n",
      "12000/49000 loss: 0.2468989063277155\n",
      "14000/49000 loss: 0.20464844985369018\n",
      "16000/49000 loss: 0.2744364837800487\n",
      "18000/49000 loss: 0.23671593426505683\n",
      "20000/49000 loss: 0.2608360925623739\n",
      "22000/49000 loss: 0.266700903080791\n",
      "24000/49000 loss: 0.29755574343844327\n",
      "26000/49000 loss: 0.31872513023016785\n",
      "28000/49000 loss: 0.2566105921627149\n",
      "30000/49000 loss: 0.30019677600767375\n",
      "32000/49000 loss: 0.26364663801139204\n",
      "34000/49000 loss: 0.2730022249307701\n",
      "36000/49000 loss: 0.21663013806660505\n",
      "38000/49000 loss: 0.31863263427536037\n",
      "40000/49000 loss: 0.27262932770006254\n",
      "42000/49000 loss: 0.2048880274202698\n",
      "44000/49000 loss: 0.35877348358259414\n",
      "46000/49000 loss: 0.22063752537508152\n",
      "48000/49000 loss: 0.23997439113083932\n",
      "epoch 37: valid acc = 0.883, new learning rate = 7.494512702440768e-05\n",
      "2000/49000 loss: 0.2574875971270303\n",
      "4000/49000 loss: 0.32921159700149033\n",
      "6000/49000 loss: 0.2686224766433467\n",
      "8000/49000 loss: 0.3054997209879996\n",
      "10000/49000 loss: 0.3059507619253685\n",
      "12000/49000 loss: 0.26383336396251517\n",
      "14000/49000 loss: 0.3011016365495645\n",
      "16000/49000 loss: 0.30682573339750485\n",
      "18000/49000 loss: 0.3253207438834699\n",
      "20000/49000 loss: 0.3279328209634571\n",
      "22000/49000 loss: 0.22895236588728676\n",
      "24000/49000 loss: 0.3007537393419664\n",
      "26000/49000 loss: 0.2883179294798284\n",
      "28000/49000 loss: 0.2734529142957826\n",
      "30000/49000 loss: 0.2638506260062462\n",
      "32000/49000 loss: 0.27771762049809207\n",
      "34000/49000 loss: 0.257000774660395\n",
      "36000/49000 loss: 0.24011663841379802\n",
      "38000/49000 loss: 0.34715364696292317\n",
      "40000/49000 loss: 0.3198738418070876\n",
      "42000/49000 loss: 0.24125847300596256\n",
      "44000/49000 loss: 0.24600529232935292\n",
      "46000/49000 loss: 0.26815416373885675\n",
      "48000/49000 loss: 0.2811627919185428\n",
      "epoch 38: valid acc = 0.886, new learning rate = 7.119787067318729e-05\n",
      "2000/49000 loss: 0.24039313298564965\n",
      "4000/49000 loss: 0.3357063282560823\n",
      "6000/49000 loss: 0.333963735986296\n",
      "8000/49000 loss: 0.3474350534124493\n",
      "10000/49000 loss: 0.23806243685615003\n",
      "12000/49000 loss: 0.23732399060715542\n",
      "14000/49000 loss: 0.21608825349790334\n",
      "16000/49000 loss: 0.23625931731362404\n",
      "18000/49000 loss: 0.20828090072062905\n",
      "20000/49000 loss: 0.26591132023800595\n",
      "22000/49000 loss: 0.28015156085455617\n",
      "24000/49000 loss: 0.2746070485490127\n",
      "26000/49000 loss: 0.2943328773975781\n",
      "28000/49000 loss: 0.22711882561770472\n",
      "30000/49000 loss: 0.21175643863884194\n",
      "32000/49000 loss: 0.2980682655227126\n",
      "34000/49000 loss: 0.44119766187795045\n",
      "36000/49000 loss: 0.3149493309244005\n",
      "38000/49000 loss: 0.17673168098617256\n",
      "40000/49000 loss: 0.24173425145869562\n",
      "42000/49000 loss: 0.22759453655435283\n",
      "44000/49000 loss: 0.274094805509706\n",
      "46000/49000 loss: 0.33579285577186213\n",
      "48000/49000 loss: 0.25855656464124277\n",
      "epoch 39: valid acc = 0.888, new learning rate = 6.763797713952792e-05\n",
      "2000/49000 loss: 0.28711979071214927\n",
      "4000/49000 loss: 0.3255795565447203\n",
      "6000/49000 loss: 0.2710328683005002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/49000 loss: 0.3462653086645003\n",
      "10000/49000 loss: 0.3031979782596058\n",
      "12000/49000 loss: 0.29843830560123963\n",
      "14000/49000 loss: 0.3075527924714881\n",
      "16000/49000 loss: 0.2505510252713702\n",
      "18000/49000 loss: 0.2718183890418372\n",
      "20000/49000 loss: 0.23679712718533996\n",
      "22000/49000 loss: 0.2792229248583379\n",
      "24000/49000 loss: 0.2662594953193342\n",
      "26000/49000 loss: 0.31650958945112107\n",
      "28000/49000 loss: 0.3376121230843336\n",
      "30000/49000 loss: 0.3184859379893514\n",
      "32000/49000 loss: 0.2453864615964593\n",
      "34000/49000 loss: 0.27941130927956787\n",
      "36000/49000 loss: 0.2814080185711579\n",
      "38000/49000 loss: 0.2519700877617287\n",
      "40000/49000 loss: 0.24645898224382082\n",
      "42000/49000 loss: 0.2150168485401265\n",
      "44000/49000 loss: 0.2214543246327956\n",
      "46000/49000 loss: 0.3035135091656683\n",
      "48000/49000 loss: 0.29395991304059466\n",
      "epoch 40: valid acc = 0.881, new learning rate = 6.425607828255152e-05\n",
      "2000/49000 loss: 0.2488637591175909\n",
      "4000/49000 loss: 0.25777481077086756\n",
      "6000/49000 loss: 0.22740977994242567\n",
      "8000/49000 loss: 0.33377190351964625\n",
      "10000/49000 loss: 0.24628510853186056\n",
      "12000/49000 loss: 0.23913236301959095\n",
      "14000/49000 loss: 0.27355771819778407\n",
      "16000/49000 loss: 0.24910122045139668\n",
      "18000/49000 loss: 0.27773588252074793\n",
      "20000/49000 loss: 0.32193869964748917\n",
      "22000/49000 loss: 0.30869397573103957\n",
      "24000/49000 loss: 0.4036200113732185\n",
      "26000/49000 loss: 0.30641050174128537\n",
      "28000/49000 loss: 0.3804455715481667\n",
      "30000/49000 loss: 0.3409606452459681\n",
      "32000/49000 loss: 0.2745168283410521\n",
      "34000/49000 loss: 0.28697292940058533\n",
      "36000/49000 loss: 0.2852187993786171\n",
      "38000/49000 loss: 0.22848067198154742\n",
      "40000/49000 loss: 0.2016776618440662\n",
      "42000/49000 loss: 0.2875951968222788\n",
      "44000/49000 loss: 0.2275934927366764\n",
      "46000/49000 loss: 0.25468841546160054\n",
      "48000/49000 loss: 0.3053590884225752\n",
      "epoch 41: valid acc = 0.885, new learning rate = 6.104327436842394e-05\n",
      "2000/49000 loss: 0.2671599480074177\n",
      "4000/49000 loss: 0.32960861171822653\n",
      "6000/49000 loss: 0.26769009067680255\n",
      "8000/49000 loss: 0.2929189799350089\n",
      "10000/49000 loss: 0.23439263925844064\n",
      "12000/49000 loss: 0.31967186827715594\n",
      "14000/49000 loss: 0.29034569358470685\n",
      "16000/49000 loss: 0.3244992094042967\n",
      "18000/49000 loss: 0.2970919534001379\n",
      "20000/49000 loss: 0.24307671545856818\n",
      "22000/49000 loss: 0.26310899797357323\n",
      "24000/49000 loss: 0.31456069830839467\n",
      "26000/49000 loss: 0.22669405193057743\n",
      "28000/49000 loss: 0.2814746960440753\n",
      "30000/49000 loss: 0.27512514461745596\n",
      "32000/49000 loss: 0.2815243399932922\n",
      "34000/49000 loss: 0.23330628132760953\n",
      "36000/49000 loss: 0.1935154380267287\n",
      "38000/49000 loss: 0.23407289902259862\n",
      "40000/49000 loss: 0.2271833292166672\n",
      "42000/49000 loss: 0.2622330431284534\n",
      "44000/49000 loss: 0.22899639237400826\n",
      "46000/49000 loss: 0.24843813970521117\n",
      "48000/49000 loss: 0.28536396134351594\n",
      "epoch 42: valid acc = 0.884, new learning rate = 5.799111065000274e-05\n",
      "2000/49000 loss: 0.31597489401282414\n",
      "4000/49000 loss: 0.294079207248083\n",
      "6000/49000 loss: 0.24479003780079628\n",
      "8000/49000 loss: 0.35841924154470856\n",
      "10000/49000 loss: 0.2889964400749095\n",
      "12000/49000 loss: 0.2790919231340489\n",
      "14000/49000 loss: 0.3188144870363193\n",
      "16000/49000 loss: 0.29314790087529635\n",
      "18000/49000 loss: 0.24168891944077894\n",
      "20000/49000 loss: 0.25313701089046375\n",
      "22000/49000 loss: 0.25853997396934847\n",
      "24000/49000 loss: 0.24365903470545955\n",
      "26000/49000 loss: 0.22191781642988082\n",
      "28000/49000 loss: 0.2760699428716356\n",
      "30000/49000 loss: 0.25743687564534584\n",
      "32000/49000 loss: 0.2227352067902803\n",
      "34000/49000 loss: 0.25664247916574756\n",
      "36000/49000 loss: 0.22424680115520101\n",
      "38000/49000 loss: 0.2764319167802098\n",
      "40000/49000 loss: 0.24252664252488618\n",
      "42000/49000 loss: 0.3108625002562008\n",
      "44000/49000 loss: 0.24516792094918763\n",
      "46000/49000 loss: 0.27645743806313583\n",
      "48000/49000 loss: 0.3166679465097157\n",
      "epoch 43: valid acc = 0.884, new learning rate = 5.5091555117502596e-05\n",
      "2000/49000 loss: 0.2061502998054649\n",
      "4000/49000 loss: 0.18339821184105104\n",
      "6000/49000 loss: 0.263778738673277\n",
      "8000/49000 loss: 0.3106367953765913\n",
      "10000/49000 loss: 0.282799723175856\n",
      "12000/49000 loss: 0.3619845843719324\n",
      "14000/49000 loss: 0.3377962497555402\n",
      "16000/49000 loss: 0.33173084566482847\n",
      "18000/49000 loss: 0.24522005089792878\n",
      "20000/49000 loss: 0.22872920361715673\n",
      "22000/49000 loss: 0.3031332490453086\n",
      "24000/49000 loss: 0.2641214885825675\n",
      "26000/49000 loss: 0.2738608924283808\n",
      "28000/49000 loss: 0.23938998050097565\n",
      "30000/49000 loss: 0.35991563692103123\n",
      "32000/49000 loss: 0.3505066192508287\n",
      "34000/49000 loss: 0.35057407034028487\n",
      "36000/49000 loss: 0.24224135355680132\n",
      "38000/49000 loss: 0.16244434833766094\n",
      "40000/49000 loss: 0.37599402209778365\n",
      "42000/49000 loss: 0.2525782473469263\n",
      "44000/49000 loss: 0.34164642458977784\n",
      "46000/49000 loss: 0.27300328282083386\n",
      "48000/49000 loss: 0.2690963110880862\n",
      "epoch 44: valid acc = 0.886, new learning rate = 5.2336977361627463e-05\n",
      "2000/49000 loss: 0.24739943136035414\n",
      "4000/49000 loss: 0.25964845151830235\n",
      "6000/49000 loss: 0.24091253964673537\n",
      "8000/49000 loss: 0.2614867894798343\n",
      "10000/49000 loss: 0.37536540095787735\n",
      "12000/49000 loss: 0.3832271143960732\n",
      "14000/49000 loss: 0.20549003964578338\n",
      "16000/49000 loss: 0.26667545384250496\n",
      "18000/49000 loss: 0.3035009347732936\n",
      "20000/49000 loss: 0.3000512707246076\n",
      "22000/49000 loss: 0.289372390690013\n",
      "24000/49000 loss: 0.3202732830502305\n",
      "26000/49000 loss: 0.3179650556912099\n",
      "28000/49000 loss: 0.24691959839442315\n",
      "30000/49000 loss: 0.30032675208440773\n",
      "32000/49000 loss: 0.3808276906758363\n",
      "34000/49000 loss: 0.2384633922811693\n",
      "36000/49000 loss: 0.23843198736579824\n",
      "38000/49000 loss: 0.2786271179286651\n",
      "40000/49000 loss: 0.2745335808593021\n",
      "42000/49000 loss: 0.23567886176953642\n",
      "44000/49000 loss: 0.2754702811923606\n",
      "46000/49000 loss: 0.3330805155929046\n",
      "48000/49000 loss: 0.2703495189149945\n",
      "epoch 45: valid acc = 0.884, new learning rate = 4.972012849354609e-05\n",
      "2000/49000 loss: 0.3437751935549265\n",
      "4000/49000 loss: 0.3125047546158345\n",
      "6000/49000 loss: 0.26178699885543094\n",
      "8000/49000 loss: 0.21915047783991717\n",
      "10000/49000 loss: 0.27603601465299943\n",
      "12000/49000 loss: 0.2977344982762297\n",
      "14000/49000 loss: 0.3309398900322609\n",
      "16000/49000 loss: 0.30509539873944586\n",
      "18000/49000 loss: 0.24555474082484235\n",
      "20000/49000 loss: 0.23776665512613335\n",
      "22000/49000 loss: 0.2488011292038968\n",
      "24000/49000 loss: 0.31738952171381835\n",
      "26000/49000 loss: 0.30478122948957953\n",
      "28000/49000 loss: 0.2347878993824057\n",
      "30000/49000 loss: 0.24082182517323392\n",
      "32000/49000 loss: 0.2310915431731282\n",
      "34000/49000 loss: 0.20662031725076888\n",
      "36000/49000 loss: 0.3239802482689712\n",
      "38000/49000 loss: 0.31198249817414214\n",
      "40000/49000 loss: 0.220143064310653\n",
      "42000/49000 loss: 0.27935803451451147\n",
      "44000/49000 loss: 0.30835023299830494\n",
      "46000/49000 loss: 0.3189471686281713\n",
      "48000/49000 loss: 0.26004441713530396\n",
      "epoch 46: valid acc = 0.89, new learning rate = 4.723412206886878e-05\n",
      "2000/49000 loss: 0.20642213290380834\n",
      "4000/49000 loss: 0.22549042513423267\n",
      "6000/49000 loss: 0.29361315436790947\n",
      "8000/49000 loss: 0.28739468348255565\n",
      "10000/49000 loss: 0.28209897578220794\n",
      "12000/49000 loss: 0.29949323912352294\n",
      "14000/49000 loss: 0.29172035484583586\n",
      "16000/49000 loss: 0.27710431845666894\n",
      "18000/49000 loss: 0.26286137278088195\n",
      "20000/49000 loss: 0.23501484798092798\n",
      "22000/49000 loss: 0.28047940877789634\n",
      "24000/49000 loss: 0.32225195051685157\n",
      "26000/49000 loss: 0.34143891044967417\n",
      "28000/49000 loss: 0.27619276458335046\n",
      "30000/49000 loss: 0.26535894576054136\n",
      "32000/49000 loss: 0.264226804289007\n",
      "34000/49000 loss: 0.30302211193862044\n",
      "36000/49000 loss: 0.279709557091199\n",
      "38000/49000 loss: 0.25644515480873736\n",
      "40000/49000 loss: 0.3636282715789461\n",
      "42000/49000 loss: 0.3759970672484393\n",
      "44000/49000 loss: 0.41176446199855365\n",
      "46000/49000 loss: 0.3147893681066601\n",
      "48000/49000 loss: 0.25228526601732537\n",
      "epoch 47: valid acc = 0.881, new learning rate = 4.487241596542534e-05\n",
      "2000/49000 loss: 0.22015070060376613\n",
      "4000/49000 loss: 0.2794852135739477\n",
      "6000/49000 loss: 0.23609196115230935\n",
      "8000/49000 loss: 0.24588347543075656\n",
      "10000/49000 loss: 0.2939913941629434\n",
      "12000/49000 loss: 0.27127260192993013\n",
      "14000/49000 loss: 0.19740019278608056\n",
      "16000/49000 loss: 0.21294099607162506\n",
      "18000/49000 loss: 0.29748697418677483\n",
      "20000/49000 loss: 0.23900990985068762\n",
      "22000/49000 loss: 0.2865439538774908\n",
      "24000/49000 loss: 0.2686054219313041\n",
      "26000/49000 loss: 0.28507313749872937\n",
      "28000/49000 loss: 0.3195696155454035\n",
      "30000/49000 loss: 0.3394381813423956\n",
      "32000/49000 loss: 0.2595747377678689\n",
      "34000/49000 loss: 0.3015571378882824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36000/49000 loss: 0.3004344112013565\n",
      "38000/49000 loss: 0.28566801066357855\n",
      "40000/49000 loss: 0.20604474926308147\n",
      "42000/49000 loss: 0.2664291686722306\n",
      "44000/49000 loss: 0.3390026633238\n",
      "46000/49000 loss: 0.327754647816286\n",
      "48000/49000 loss: 0.3216739265970483\n",
      "epoch 48: valid acc = 0.888, new learning rate = 4.262879516715407e-05\n",
      "2000/49000 loss: 0.2633585681482076\n",
      "4000/49000 loss: 0.2991885709461862\n",
      "6000/49000 loss: 0.3427120920002547\n",
      "8000/49000 loss: 0.33076047008724846\n",
      "10000/49000 loss: 0.30523754771560907\n",
      "12000/49000 loss: 0.31389086809807454\n",
      "14000/49000 loss: 0.2555356266855062\n",
      "16000/49000 loss: 0.3758151400031693\n",
      "18000/49000 loss: 0.2677767253469297\n",
      "20000/49000 loss: 0.1787174236999929\n",
      "22000/49000 loss: 0.28520944936239145\n",
      "24000/49000 loss: 0.21313679054324947\n",
      "26000/49000 loss: 0.2814167802671829\n",
      "28000/49000 loss: 0.29122762702709337\n",
      "30000/49000 loss: 0.309389996411752\n",
      "32000/49000 loss: 0.26441336224728096\n",
      "34000/49000 loss: 0.2846917657468962\n",
      "36000/49000 loss: 0.2720008154286746\n",
      "38000/49000 loss: 0.312432945077771\n",
      "40000/49000 loss: 0.24450966855596137\n",
      "42000/49000 loss: 0.35186191762514757\n",
      "44000/49000 loss: 0.24912178692827064\n",
      "46000/49000 loss: 0.28007224851422463\n",
      "48000/49000 loss: 0.22997949190839514\n",
      "epoch 49: valid acc = 0.883, new learning rate = 4.049735540879637e-05\n",
      "2000/49000 loss: 0.23760640941097264\n",
      "4000/49000 loss: 0.29859349831114046\n",
      "6000/49000 loss: 0.2372633288615663\n",
      "8000/49000 loss: 0.30334115250079585\n",
      "10000/49000 loss: 0.3138656994693759\n",
      "12000/49000 loss: 0.26588920471946503\n",
      "14000/49000 loss: 0.34810686571896887\n",
      "16000/49000 loss: 0.14725485261664398\n",
      "18000/49000 loss: 0.3168477401671325\n",
      "20000/49000 loss: 0.24617023217168188\n",
      "22000/49000 loss: 0.3611222865216434\n",
      "24000/49000 loss: 0.325082439769665\n",
      "26000/49000 loss: 0.2419266218966233\n",
      "28000/49000 loss: 0.21011454013165604\n",
      "30000/49000 loss: 0.3268427877388984\n",
      "32000/49000 loss: 0.24994438981326497\n",
      "34000/49000 loss: 0.24593796956430425\n",
      "36000/49000 loss: 0.20413754499491807\n",
      "38000/49000 loss: 0.25928777576667383\n",
      "40000/49000 loss: 0.25693217069655605\n",
      "42000/49000 loss: 0.23238893561291935\n",
      "44000/49000 loss: 0.2548516894158144\n",
      "46000/49000 loss: 0.3043999438855232\n",
      "48000/49000 loss: 0.19899567409184107\n",
      "epoch 50: valid acc = 0.89, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.9043877551020408\n",
      "test acc: 0.89\n",
      "test acc: 0.8735\n",
      "number of batches for training: 245\n",
      "epoch 1: valid acc = 0.739, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.812, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.822, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.847, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.849, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.86, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.857, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.862, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.872, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.871, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.872, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.879, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.874, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.881, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.882, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.878, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.885, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.885, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.89, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.883, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.885, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.891, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.885, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.879, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.886, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.889, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.89, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.89, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.889, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.891, new learning rate = 0.00010731938197146858\n",
      "epoch 31: valid acc = 0.887, new learning rate = 0.00010195341287289515\n",
      "epoch 32: valid acc = 0.89, new learning rate = 9.685574222925039e-05\n",
      "epoch 33: valid acc = 0.89, new learning rate = 9.201295511778786e-05\n",
      "epoch 34: valid acc = 0.89, new learning rate = 8.741230736189846e-05\n",
      "epoch 35: valid acc = 0.891, new learning rate = 8.304169199380353e-05\n",
      "epoch 36: valid acc = 0.888, new learning rate = 7.888960739411335e-05\n",
      "epoch 37: valid acc = 0.887, new learning rate = 7.494512702440768e-05\n",
      "epoch 38: valid acc = 0.892, new learning rate = 7.119787067318729e-05\n",
      "epoch 39: valid acc = 0.887, new learning rate = 6.763797713952792e-05\n",
      "epoch 40: valid acc = 0.89, new learning rate = 6.425607828255152e-05\n",
      "epoch 41: valid acc = 0.891, new learning rate = 6.104327436842394e-05\n",
      "epoch 42: valid acc = 0.889, new learning rate = 5.799111065000274e-05\n",
      "epoch 43: valid acc = 0.89, new learning rate = 5.5091555117502596e-05\n",
      "epoch 44: valid acc = 0.89, new learning rate = 5.2336977361627463e-05\n",
      "epoch 45: valid acc = 0.888, new learning rate = 4.972012849354609e-05\n",
      "epoch 46: valid acc = 0.89, new learning rate = 4.723412206886878e-05\n",
      "epoch 47: valid acc = 0.889, new learning rate = 4.487241596542534e-05\n",
      "epoch 48: valid acc = 0.888, new learning rate = 4.262879516715407e-05\n",
      "epoch 49: valid acc = 0.887, new learning rate = 4.049735540879637e-05\n",
      "epoch 50: valid acc = 0.889, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.9048571428571428\n",
      "test acc: 0.889\n",
      "test acc: 0.8725\n",
      "number of batches for training: 81\n",
      "6000/49000 loss: 3.0106156490043037\n",
      "12000/49000 loss: 2.7186397331191965\n",
      "18000/49000 loss: 2.4640124617244767\n",
      "24000/49000 loss: 2.3501554018671853\n",
      "30000/49000 loss: 2.2075167328512597\n",
      "36000/49000 loss: 2.0386802404166984\n",
      "42000/49000 loss: 1.9906155232481277\n",
      "48000/49000 loss: 1.6952950462859382\n",
      "epoch 1: valid acc = 0.514, new learning rate = 0.000475\n",
      "6000/49000 loss: 1.2933419829953607\n",
      "12000/49000 loss: 1.3245369958109252\n",
      "18000/49000 loss: 1.1996327586675841\n",
      "24000/49000 loss: 1.1017291421606914\n",
      "30000/49000 loss: 1.066361936957491\n",
      "36000/49000 loss: 1.1243544211092336\n",
      "42000/49000 loss: 0.9778207537729802\n",
      "48000/49000 loss: 0.995294272930215\n",
      "epoch 2: valid acc = 0.648, new learning rate = 0.00045125\n",
      "6000/49000 loss: 0.9751855018674791\n",
      "12000/49000 loss: 0.9245999729216736\n",
      "18000/49000 loss: 0.9559259986835069\n",
      "24000/49000 loss: 0.8679154615210817\n",
      "30000/49000 loss: 0.8347961295965548\n",
      "36000/49000 loss: 0.8004331674577453\n",
      "42000/49000 loss: 0.8210095952531438\n",
      "48000/49000 loss: 0.7397659599895209\n",
      "epoch 3: valid acc = 0.727, new learning rate = 0.0004286875\n",
      "6000/49000 loss: 0.701989273334258\n",
      "12000/49000 loss: 0.7566272225737672\n",
      "18000/49000 loss: 0.6181283936401062\n",
      "24000/49000 loss: 0.7221309273175689\n",
      "30000/49000 loss: 0.7076099880289067\n",
      "36000/49000 loss: 0.6776952760066918\n",
      "42000/49000 loss: 0.5647521326356448\n",
      "48000/49000 loss: 0.6791713418146863\n",
      "epoch 4: valid acc = 0.744, new learning rate = 0.00040725312499999993\n",
      "6000/49000 loss: 0.6260414109966621\n",
      "12000/49000 loss: 0.5753549741115271\n",
      "18000/49000 loss: 0.6327445806249319\n",
      "24000/49000 loss: 0.6165720973514213\n",
      "30000/49000 loss: 0.6058849059799016\n",
      "36000/49000 loss: 0.6367553924521419\n",
      "42000/49000 loss: 0.6258703775369283\n",
      "48000/49000 loss: 0.5918957610147889\n",
      "epoch 5: valid acc = 0.781, new learning rate = 0.0003868904687499999\n",
      "6000/49000 loss: 0.5971048556845865\n",
      "12000/49000 loss: 0.5475569581487449\n",
      "18000/49000 loss: 0.6039001152372505\n",
      "24000/49000 loss: 0.6053544805244134\n",
      "30000/49000 loss: 0.5747522268042056\n",
      "36000/49000 loss: 0.6022158957881547\n",
      "42000/49000 loss: 0.5650025594443079\n",
      "48000/49000 loss: 0.5303395948861526\n",
      "epoch 6: valid acc = 0.797, new learning rate = 0.0003675459453124999\n",
      "6000/49000 loss: 0.4828099597663978\n",
      "12000/49000 loss: 0.5159542207693651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/49000 loss: 0.5472010975569715\n",
      "24000/49000 loss: 0.5376506032425106\n",
      "30000/49000 loss: 0.520321266378469\n",
      "36000/49000 loss: 0.5254569336398258\n",
      "42000/49000 loss: 0.5435594055348477\n",
      "48000/49000 loss: 0.5478982497408281\n",
      "epoch 7: valid acc = 0.799, new learning rate = 0.00034916864804687486\n",
      "6000/49000 loss: 0.5140030781847931\n",
      "12000/49000 loss: 0.5292447982780144\n",
      "18000/49000 loss: 0.49360019120109394\n",
      "24000/49000 loss: 0.5230840354721334\n",
      "30000/49000 loss: 0.5066144922522307\n",
      "36000/49000 loss: 0.5168289204467357\n",
      "42000/49000 loss: 0.4827211118574587\n",
      "48000/49000 loss: 0.525174504569935\n",
      "epoch 8: valid acc = 0.813, new learning rate = 0.0003317102156445311\n",
      "6000/49000 loss: 0.4964572348108807\n",
      "12000/49000 loss: 0.5267104729618024\n",
      "18000/49000 loss: 0.4893909032134756\n",
      "24000/49000 loss: 0.47038313305250357\n",
      "30000/49000 loss: 0.4630236949819229\n",
      "36000/49000 loss: 0.51802049498385\n",
      "42000/49000 loss: 0.48317762380209034\n",
      "48000/49000 loss: 0.4869167229954026\n",
      "epoch 9: valid acc = 0.826, new learning rate = 0.0003151247048623045\n",
      "6000/49000 loss: 0.4731782614972428\n",
      "12000/49000 loss: 0.4361226318633362\n",
      "18000/49000 loss: 0.44085135752955673\n",
      "24000/49000 loss: 0.5306876061189497\n",
      "30000/49000 loss: 0.4643290681497154\n",
      "36000/49000 loss: 0.47103642411048036\n",
      "42000/49000 loss: 0.4420172274701121\n",
      "48000/49000 loss: 0.47598505562612575\n",
      "epoch 10: valid acc = 0.831, new learning rate = 0.00029936846961918924\n",
      "6000/49000 loss: 0.43897040237391377\n",
      "12000/49000 loss: 0.46584800990627745\n",
      "18000/49000 loss: 0.48968242758342145\n",
      "24000/49000 loss: 0.46706309169207094\n",
      "30000/49000 loss: 0.4733400651308191\n",
      "36000/49000 loss: 0.46422974799428957\n",
      "42000/49000 loss: 0.47565089354784834\n",
      "48000/49000 loss: 0.49582605983143674\n",
      "epoch 11: valid acc = 0.83, new learning rate = 0.00028440004613822977\n",
      "6000/49000 loss: 0.5193152642613746\n",
      "12000/49000 loss: 0.46674768840917663\n",
      "18000/49000 loss: 0.470989930297405\n",
      "24000/49000 loss: 0.5197072784177048\n",
      "30000/49000 loss: 0.393312280301521\n",
      "36000/49000 loss: 0.46176487703141106\n",
      "42000/49000 loss: 0.41917049367442066\n",
      "48000/49000 loss: 0.44106230458580037\n",
      "epoch 12: valid acc = 0.835, new learning rate = 0.00027018004383131826\n",
      "6000/49000 loss: 0.4510223249014212\n",
      "12000/49000 loss: 0.40802222501526025\n",
      "18000/49000 loss: 0.3991422906420716\n",
      "24000/49000 loss: 0.4364657872170446\n",
      "30000/49000 loss: 0.5016820200111188\n",
      "36000/49000 loss: 0.41444311316884846\n",
      "42000/49000 loss: 0.44407180601057683\n",
      "48000/49000 loss: 0.381676021133137\n",
      "epoch 13: valid acc = 0.839, new learning rate = 0.00025667104163975234\n",
      "6000/49000 loss: 0.48410670249166904\n",
      "12000/49000 loss: 0.5322869062990342\n",
      "18000/49000 loss: 0.43099632183924785\n",
      "24000/49000 loss: 0.4458296794681224\n",
      "30000/49000 loss: 0.5232996619173578\n",
      "36000/49000 loss: 0.4548344445550289\n",
      "42000/49000 loss: 0.4860646763167272\n",
      "48000/49000 loss: 0.4470561854112274\n",
      "epoch 14: valid acc = 0.841, new learning rate = 0.00024383748955776472\n",
      "6000/49000 loss: 0.44319598263750476\n",
      "12000/49000 loss: 0.40903135891809256\n",
      "18000/49000 loss: 0.3853743486853749\n",
      "24000/49000 loss: 0.4021719420214003\n",
      "30000/49000 loss: 0.4372838077224013\n",
      "36000/49000 loss: 0.48122404881355263\n",
      "42000/49000 loss: 0.410884839860846\n",
      "48000/49000 loss: 0.403223740754971\n",
      "epoch 15: valid acc = 0.841, new learning rate = 0.00023164561507987649\n",
      "6000/49000 loss: 0.4154468001618124\n",
      "12000/49000 loss: 0.4120016030566865\n",
      "18000/49000 loss: 0.381696695274833\n",
      "24000/49000 loss: 0.40714975520751157\n",
      "30000/49000 loss: 0.4106772504277278\n",
      "36000/49000 loss: 0.4279409015624525\n",
      "42000/49000 loss: 0.44073326385071976\n",
      "48000/49000 loss: 0.3937646637177173\n",
      "epoch 16: valid acc = 0.842, new learning rate = 0.00022006333432588265\n",
      "6000/49000 loss: 0.36524542259023457\n",
      "12000/49000 loss: 0.4394913109812527\n",
      "18000/49000 loss: 0.4402922541355162\n",
      "24000/49000 loss: 0.432231100043421\n",
      "30000/49000 loss: 0.47900917594436215\n",
      "36000/49000 loss: 0.392497002267227\n",
      "42000/49000 loss: 0.48603331234519537\n",
      "48000/49000 loss: 0.42385262427668213\n",
      "epoch 17: valid acc = 0.849, new learning rate = 0.00020906016760958852\n",
      "6000/49000 loss: 0.42856607646398814\n",
      "12000/49000 loss: 0.43174682442248885\n",
      "18000/49000 loss: 0.4207041068802194\n",
      "24000/49000 loss: 0.39572587177887597\n",
      "30000/49000 loss: 0.4236993723614041\n",
      "36000/49000 loss: 0.39713622835405077\n",
      "42000/49000 loss: 0.42967490353973886\n",
      "48000/49000 loss: 0.41954755342645444\n",
      "epoch 18: valid acc = 0.847, new learning rate = 0.00019860715922910907\n",
      "6000/49000 loss: 0.43533432005552153\n",
      "12000/49000 loss: 0.3900953815156305\n",
      "18000/49000 loss: 0.42232512758591745\n",
      "24000/49000 loss: 0.4225773314431536\n",
      "30000/49000 loss: 0.3914785716117137\n",
      "36000/49000 loss: 0.3718982199025999\n",
      "42000/49000 loss: 0.39919124004712014\n",
      "48000/49000 loss: 0.4906096284616331\n",
      "epoch 19: valid acc = 0.848, new learning rate = 0.0001886768012676536\n",
      "6000/49000 loss: 0.36971993475255877\n",
      "12000/49000 loss: 0.39787390304421605\n",
      "18000/49000 loss: 0.47896977295498916\n",
      "24000/49000 loss: 0.3889841350885938\n",
      "30000/49000 loss: 0.39255346337299507\n",
      "36000/49000 loss: 0.4782292883948878\n",
      "42000/49000 loss: 0.3784525292636277\n",
      "48000/49000 loss: 0.4190190844440166\n",
      "epoch 20: valid acc = 0.853, new learning rate = 0.0001792429612042709\n",
      "6000/49000 loss: 0.44079167780888073\n",
      "12000/49000 loss: 0.4040526068693183\n",
      "18000/49000 loss: 0.4276949736116686\n",
      "24000/49000 loss: 0.4164608611512297\n",
      "30000/49000 loss: 0.4040303451023419\n",
      "36000/49000 loss: 0.38582193683656135\n",
      "42000/49000 loss: 0.4196632246129684\n",
      "48000/49000 loss: 0.33232962077292455\n",
      "epoch 21: valid acc = 0.859, new learning rate = 0.00017028081314405735\n",
      "6000/49000 loss: 0.44116995782153046\n",
      "12000/49000 loss: 0.4111810585979895\n",
      "18000/49000 loss: 0.3974680183729497\n",
      "24000/49000 loss: 0.4416680843112127\n",
      "30000/49000 loss: 0.3610685856521703\n",
      "36000/49000 loss: 0.3725602364700168\n",
      "42000/49000 loss: 0.36084562824733285\n",
      "48000/49000 loss: 0.38207276262018064\n",
      "epoch 22: valid acc = 0.859, new learning rate = 0.00016176677248685447\n",
      "6000/49000 loss: 0.4729806098240752\n",
      "12000/49000 loss: 0.4314950230837584\n",
      "18000/49000 loss: 0.4284357011661003\n",
      "24000/49000 loss: 0.4298228099832445\n",
      "30000/49000 loss: 0.4008256399329603\n",
      "36000/49000 loss: 0.3662008103062097\n",
      "42000/49000 loss: 0.4334308977290362\n",
      "48000/49000 loss: 0.4189539246096674\n",
      "epoch 23: valid acc = 0.858, new learning rate = 0.00015367843386251173\n",
      "6000/49000 loss: 0.40756262950074323\n",
      "12000/49000 loss: 0.39119142676248303\n",
      "18000/49000 loss: 0.4135442162620894\n",
      "24000/49000 loss: 0.39586018885248897\n",
      "30000/49000 loss: 0.43034921515965935\n",
      "36000/49000 loss: 0.3983293792940357\n",
      "42000/49000 loss: 0.3629010282163941\n",
      "48000/49000 loss: 0.43510481667388945\n",
      "epoch 24: valid acc = 0.858, new learning rate = 0.00014599451216938612\n",
      "6000/49000 loss: 0.3840534962682742\n",
      "12000/49000 loss: 0.41085224745413135\n",
      "18000/49000 loss: 0.38085252335809805\n",
      "24000/49000 loss: 0.4376509949163729\n",
      "30000/49000 loss: 0.40406555622304363\n",
      "36000/49000 loss: 0.3758884063695552\n",
      "42000/49000 loss: 0.4047102694901935\n",
      "48000/49000 loss: 0.42384968252686733\n",
      "epoch 25: valid acc = 0.861, new learning rate = 0.00013869478656091682\n",
      "6000/49000 loss: 0.40644398637125156\n",
      "12000/49000 loss: 0.37023008002901286\n",
      "18000/49000 loss: 0.3948441888220189\n",
      "24000/49000 loss: 0.36582564486101593\n",
      "30000/49000 loss: 0.36652089073047384\n",
      "36000/49000 loss: 0.37893017835141607\n",
      "42000/49000 loss: 0.41480391129315103\n",
      "48000/49000 loss: 0.40848973455896237\n",
      "epoch 26: valid acc = 0.86, new learning rate = 0.00013176004723287096\n",
      "6000/49000 loss: 0.3816150809296425\n",
      "12000/49000 loss: 0.39110816611999843\n",
      "18000/49000 loss: 0.43980040745566445\n",
      "24000/49000 loss: 0.3544951436596689\n",
      "30000/49000 loss: 0.3677970933376329\n",
      "36000/49000 loss: 0.36837103807286475\n",
      "42000/49000 loss: 0.311263328830472\n",
      "48000/49000 loss: 0.36872214161941447\n",
      "epoch 27: valid acc = 0.862, new learning rate = 0.0001251720448712274\n",
      "6000/49000 loss: 0.38544124174472494\n",
      "12000/49000 loss: 0.4040826187031141\n",
      "18000/49000 loss: 0.38156489088256235\n",
      "24000/49000 loss: 0.39934202342442765\n",
      "30000/49000 loss: 0.3944340512512225\n",
      "36000/49000 loss: 0.4144439307945923\n",
      "42000/49000 loss: 0.41506199802033955\n",
      "48000/49000 loss: 0.4252812817429384\n",
      "epoch 28: valid acc = 0.864, new learning rate = 0.00011891344262766602\n",
      "6000/49000 loss: 0.4037539024280224\n",
      "12000/49000 loss: 0.38715429607617424\n",
      "18000/49000 loss: 0.3586429964408988\n",
      "24000/49000 loss: 0.40239322473874656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000/49000 loss: 0.41919084216256497\n",
      "36000/49000 loss: 0.36328965525377144\n",
      "42000/49000 loss: 0.39744114885437903\n",
      "48000/49000 loss: 0.42396675969260894\n",
      "epoch 29: valid acc = 0.867, new learning rate = 0.00011296777049628272\n",
      "6000/49000 loss: 0.43084238617523724\n",
      "12000/49000 loss: 0.4155577460925178\n",
      "18000/49000 loss: 0.389084910215525\n",
      "24000/49000 loss: 0.41296382234900175\n",
      "30000/49000 loss: 0.39066011860234845\n",
      "36000/49000 loss: 0.39593915103187133\n",
      "42000/49000 loss: 0.41398635740576417\n",
      "48000/49000 loss: 0.3666046148741738\n",
      "epoch 30: valid acc = 0.863, new learning rate = 0.00010731938197146858\n",
      "6000/49000 loss: 0.3753617357089759\n",
      "12000/49000 loss: 0.4474630688648801\n",
      "18000/49000 loss: 0.40662859948420926\n",
      "24000/49000 loss: 0.4168587659635373\n",
      "30000/49000 loss: 0.33666987053149583\n",
      "36000/49000 loss: 0.37854625273257037\n",
      "42000/49000 loss: 0.38734043866449713\n",
      "48000/49000 loss: 0.4246279028550785\n",
      "epoch 31: valid acc = 0.864, new learning rate = 0.00010195341287289515\n",
      "6000/49000 loss: 0.38811047673005683\n",
      "12000/49000 loss: 0.4073816601464547\n",
      "18000/49000 loss: 0.458081283374254\n",
      "24000/49000 loss: 0.3745602949794207\n",
      "30000/49000 loss: 0.4023070288645312\n",
      "36000/49000 loss: 0.3755960804392553\n",
      "42000/49000 loss: 0.43762314741349756\n",
      "48000/49000 loss: 0.37237324018159607\n",
      "epoch 32: valid acc = 0.868, new learning rate = 9.685574222925039e-05\n",
      "6000/49000 loss: 0.3630300760371778\n",
      "12000/49000 loss: 0.40279673604031097\n",
      "18000/49000 loss: 0.34878966620871693\n",
      "24000/49000 loss: 0.3754318895880187\n",
      "30000/49000 loss: 0.42088759575829804\n",
      "36000/49000 loss: 0.329930630384611\n",
      "42000/49000 loss: 0.3735699061730238\n",
      "48000/49000 loss: 0.3354180281874424\n",
      "epoch 33: valid acc = 0.864, new learning rate = 9.201295511778786e-05\n",
      "6000/49000 loss: 0.38034385469405\n",
      "12000/49000 loss: 0.3982211028403946\n",
      "18000/49000 loss: 0.39269495836768875\n",
      "24000/49000 loss: 0.4008116649961503\n",
      "30000/49000 loss: 0.35980146599813817\n",
      "36000/49000 loss: 0.38645693320032387\n",
      "42000/49000 loss: 0.3697765294689104\n",
      "48000/49000 loss: 0.34739556243346875\n",
      "epoch 34: valid acc = 0.862, new learning rate = 8.741230736189846e-05\n",
      "6000/49000 loss: 0.4253248079287713\n",
      "12000/49000 loss: 0.38528280596731224\n",
      "18000/49000 loss: 0.35540962959782413\n",
      "24000/49000 loss: 0.3759764774278112\n",
      "30000/49000 loss: 0.3509975940067659\n",
      "36000/49000 loss: 0.38475209307654246\n",
      "42000/49000 loss: 0.39010625147552075\n",
      "48000/49000 loss: 0.37689785692751393\n",
      "epoch 35: valid acc = 0.864, new learning rate = 8.304169199380353e-05\n",
      "6000/49000 loss: 0.4134477907026222\n",
      "12000/49000 loss: 0.33538935526657476\n",
      "18000/49000 loss: 0.31365186260864863\n",
      "24000/49000 loss: 0.4117912078976335\n",
      "30000/49000 loss: 0.34559627008646127\n",
      "36000/49000 loss: 0.38046271829338796\n",
      "42000/49000 loss: 0.3457500174986978\n",
      "48000/49000 loss: 0.37882111177848543\n",
      "epoch 36: valid acc = 0.863, new learning rate = 7.888960739411335e-05\n",
      "6000/49000 loss: 0.44239137589734356\n",
      "12000/49000 loss: 0.33647156819676466\n",
      "18000/49000 loss: 0.3817654456768963\n",
      "24000/49000 loss: 0.40157824132673997\n",
      "30000/49000 loss: 0.3633134508214691\n",
      "36000/49000 loss: 0.3713974051030974\n",
      "42000/49000 loss: 0.3920416446572677\n",
      "48000/49000 loss: 0.412697260271636\n",
      "epoch 37: valid acc = 0.864, new learning rate = 7.494512702440768e-05\n",
      "6000/49000 loss: 0.3796690114082926\n",
      "12000/49000 loss: 0.36494668705511535\n",
      "18000/49000 loss: 0.38384117370838855\n",
      "24000/49000 loss: 0.37582819279459073\n",
      "30000/49000 loss: 0.3940820462722798\n",
      "36000/49000 loss: 0.3955428943607957\n",
      "42000/49000 loss: 0.3373428911665111\n",
      "48000/49000 loss: 0.35633110566473886\n",
      "epoch 38: valid acc = 0.865, new learning rate = 7.119787067318729e-05\n",
      "6000/49000 loss: 0.3975036510974352\n",
      "12000/49000 loss: 0.3125010461674735\n",
      "18000/49000 loss: 0.3958880246808635\n",
      "24000/49000 loss: 0.4176885285959156\n",
      "30000/49000 loss: 0.3691765128807184\n",
      "36000/49000 loss: 0.426677819610332\n",
      "42000/49000 loss: 0.36196610377417765\n",
      "48000/49000 loss: 0.3869421417418248\n",
      "epoch 39: valid acc = 0.864, new learning rate = 6.763797713952792e-05\n",
      "6000/49000 loss: 0.37960411154318735\n",
      "12000/49000 loss: 0.37859211170241036\n",
      "18000/49000 loss: 0.3922040253811633\n",
      "24000/49000 loss: 0.40032327818099345\n",
      "30000/49000 loss: 0.3935781084607437\n",
      "36000/49000 loss: 0.3343326389595244\n",
      "42000/49000 loss: 0.36972225771705025\n",
      "48000/49000 loss: 0.41746150283263633\n",
      "epoch 40: valid acc = 0.866, new learning rate = 6.425607828255152e-05\n",
      "6000/49000 loss: 0.3152979655634397\n",
      "12000/49000 loss: 0.45166293527171175\n",
      "18000/49000 loss: 0.35564838325051656\n",
      "24000/49000 loss: 0.33300935530380316\n",
      "30000/49000 loss: 0.41290570092581846\n",
      "36000/49000 loss: 0.28960726383656193\n",
      "42000/49000 loss: 0.39090393817892105\n",
      "48000/49000 loss: 0.30064075865593615\n",
      "epoch 41: valid acc = 0.866, new learning rate = 6.104327436842394e-05\n",
      "6000/49000 loss: 0.3590002635378409\n",
      "12000/49000 loss: 0.4070116282280699\n",
      "18000/49000 loss: 0.39872579012697384\n",
      "24000/49000 loss: 0.33917965147472345\n",
      "30000/49000 loss: 0.3497425302024056\n",
      "36000/49000 loss: 0.3743808235375513\n",
      "42000/49000 loss: 0.35258091418972703\n",
      "48000/49000 loss: 0.4287113212459203\n",
      "epoch 42: valid acc = 0.865, new learning rate = 5.799111065000274e-05\n",
      "6000/49000 loss: 0.4144187029657274\n",
      "12000/49000 loss: 0.35629158179858283\n",
      "18000/49000 loss: 0.41763601524534294\n",
      "24000/49000 loss: 0.35931975086411333\n",
      "30000/49000 loss: 0.41573336193539046\n",
      "36000/49000 loss: 0.43241333037955243\n",
      "42000/49000 loss: 0.3586500089359367\n",
      "48000/49000 loss: 0.35057889162665185\n",
      "epoch 43: valid acc = 0.867, new learning rate = 5.5091555117502596e-05\n",
      "6000/49000 loss: 0.35956594941240944\n",
      "12000/49000 loss: 0.3515265887073751\n",
      "18000/49000 loss: 0.3445008709861391\n",
      "24000/49000 loss: 0.34285895434995534\n",
      "30000/49000 loss: 0.40721853544067144\n",
      "36000/49000 loss: 0.48153995962998786\n",
      "42000/49000 loss: 0.3552486575775763\n",
      "48000/49000 loss: 0.4130634297546817\n",
      "epoch 44: valid acc = 0.867, new learning rate = 5.2336977361627463e-05\n",
      "6000/49000 loss: 0.31067590633971237\n",
      "12000/49000 loss: 0.37529418278191373\n",
      "18000/49000 loss: 0.36669287887027135\n",
      "24000/49000 loss: 0.3495434241672641\n",
      "30000/49000 loss: 0.3767823304469536\n",
      "36000/49000 loss: 0.38955840669832104\n",
      "42000/49000 loss: 0.3851623749022445\n",
      "48000/49000 loss: 0.3883367139013411\n",
      "epoch 45: valid acc = 0.868, new learning rate = 4.972012849354609e-05\n",
      "6000/49000 loss: 0.33793540050216814\n",
      "12000/49000 loss: 0.3695263026335255\n",
      "18000/49000 loss: 0.31606296262970085\n",
      "24000/49000 loss: 0.33007653819614763\n",
      "30000/49000 loss: 0.34448640753103615\n",
      "36000/49000 loss: 0.341500736751518\n",
      "42000/49000 loss: 0.3601648450654759\n",
      "48000/49000 loss: 0.37006404912941093\n",
      "epoch 46: valid acc = 0.867, new learning rate = 4.723412206886878e-05\n",
      "6000/49000 loss: 0.35698015186473\n",
      "12000/49000 loss: 0.40556200494463424\n",
      "18000/49000 loss: 0.3826731671656719\n",
      "24000/49000 loss: 0.3759394378860214\n",
      "30000/49000 loss: 0.3329512974227917\n",
      "36000/49000 loss: 0.3202644175339175\n",
      "42000/49000 loss: 0.38979497794215423\n",
      "48000/49000 loss: 0.39634458707330544\n",
      "epoch 47: valid acc = 0.867, new learning rate = 4.487241596542534e-05\n",
      "6000/49000 loss: 0.36933151337191716\n",
      "12000/49000 loss: 0.3569133277635328\n",
      "18000/49000 loss: 0.39092245831966593\n",
      "24000/49000 loss: 0.37161668375130413\n",
      "30000/49000 loss: 0.3907805761832362\n",
      "36000/49000 loss: 0.3557064332756492\n",
      "42000/49000 loss: 0.36001899654139297\n",
      "48000/49000 loss: 0.35645416518693734\n",
      "epoch 48: valid acc = 0.867, new learning rate = 4.262879516715407e-05\n",
      "6000/49000 loss: 0.39564932261941577\n",
      "12000/49000 loss: 0.3709769044053305\n",
      "18000/49000 loss: 0.4318707993039801\n",
      "24000/49000 loss: 0.3740317475785419\n",
      "30000/49000 loss: 0.3224572705503037\n",
      "36000/49000 loss: 0.3634775234562569\n",
      "42000/49000 loss: 0.35322183340601315\n",
      "48000/49000 loss: 0.3828742677971729\n",
      "epoch 49: valid acc = 0.867, new learning rate = 4.049735540879637e-05\n",
      "6000/49000 loss: 0.4058695063533677\n",
      "12000/49000 loss: 0.3937280216670897\n",
      "18000/49000 loss: 0.3951502605348117\n",
      "24000/49000 loss: 0.3719160100513876\n",
      "30000/49000 loss: 0.4196883250883186\n",
      "36000/49000 loss: 0.3389344317031789\n",
      "42000/49000 loss: 0.4037591571063789\n",
      "48000/49000 loss: 0.37345039678791786\n",
      "epoch 50: valid acc = 0.868, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.8681836734693877\n",
      "test acc: 0.868\n",
      "test acc: 0.8468\n",
      "number of batches for training: 81\n",
      "epoch 1: valid acc = 0.508, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.671, new learning rate = 0.00045125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3: valid acc = 0.74, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.756, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.79, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.797, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.811, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.826, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.822, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.826, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.832, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.83, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.83, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.842, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.84, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.84, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.848, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.849, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.851, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.848, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.85, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.853, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.852, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.854, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.856, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.853, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.857, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.855, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.856, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.856, new learning rate = 0.00010731938197146858\n",
      "epoch 31: valid acc = 0.857, new learning rate = 0.00010195341287289515\n",
      "epoch 32: valid acc = 0.859, new learning rate = 9.685574222925039e-05\n",
      "epoch 33: valid acc = 0.858, new learning rate = 9.201295511778786e-05\n",
      "epoch 34: valid acc = 0.857, new learning rate = 8.741230736189846e-05\n",
      "epoch 35: valid acc = 0.86, new learning rate = 8.304169199380353e-05\n",
      "epoch 36: valid acc = 0.86, new learning rate = 7.888960739411335e-05\n",
      "epoch 37: valid acc = 0.857, new learning rate = 7.494512702440768e-05\n",
      "epoch 38: valid acc = 0.863, new learning rate = 7.119787067318729e-05\n",
      "epoch 39: valid acc = 0.863, new learning rate = 6.763797713952792e-05\n",
      "epoch 40: valid acc = 0.862, new learning rate = 6.425607828255152e-05\n",
      "epoch 41: valid acc = 0.864, new learning rate = 6.104327436842394e-05\n",
      "epoch 42: valid acc = 0.863, new learning rate = 5.799111065000274e-05\n",
      "epoch 43: valid acc = 0.862, new learning rate = 5.5091555117502596e-05\n",
      "epoch 44: valid acc = 0.862, new learning rate = 5.2336977361627463e-05\n",
      "epoch 45: valid acc = 0.861, new learning rate = 4.972012849354609e-05\n",
      "epoch 46: valid acc = 0.863, new learning rate = 4.723412206886878e-05\n",
      "epoch 47: valid acc = 0.862, new learning rate = 4.487241596542534e-05\n",
      "epoch 48: valid acc = 0.861, new learning rate = 4.262879516715407e-05\n",
      "epoch 49: valid acc = 0.865, new learning rate = 4.049735540879637e-05\n",
      "epoch 50: valid acc = 0.865, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.8688571428571429\n",
      "test acc: 0.865\n",
      "test acc: 0.8481\n",
      "number of batches for training: 81\n",
      "6000/49000 loss: 2.6613844653141254\n",
      "12000/49000 loss: 2.736529378994653\n",
      "18000/49000 loss: 2.7597932372018774\n",
      "24000/49000 loss: 2.350984668557201\n",
      "30000/49000 loss: 2.2888335923949668\n",
      "36000/49000 loss: 1.9585651228495349\n",
      "42000/49000 loss: 1.9295554142384248\n",
      "48000/49000 loss: 1.6816966143756953\n",
      "epoch 1: valid acc = 0.519, new learning rate = 0.000475\n",
      "6000/49000 loss: 1.4091703863119842\n",
      "12000/49000 loss: 1.3254315262210625\n",
      "18000/49000 loss: 1.2453901662235374\n",
      "24000/49000 loss: 1.2292673100672558\n",
      "30000/49000 loss: 1.161351905320677\n",
      "36000/49000 loss: 1.0663302940536852\n",
      "42000/49000 loss: 1.0491338138711621\n",
      "48000/49000 loss: 1.0636287019974051\n",
      "epoch 2: valid acc = 0.672, new learning rate = 0.00045125\n",
      "6000/49000 loss: 0.9826549510094659\n",
      "12000/49000 loss: 0.9458637190484404\n",
      "18000/49000 loss: 0.8897226602764002\n",
      "24000/49000 loss: 0.9328306282642036\n",
      "30000/49000 loss: 0.8020918304071942\n",
      "36000/49000 loss: 0.832111984016623\n",
      "42000/49000 loss: 0.8534580702861704\n",
      "48000/49000 loss: 0.7558390794068739\n",
      "epoch 3: valid acc = 0.74, new learning rate = 0.0004286875\n",
      "6000/49000 loss: 0.6957898684307704\n",
      "12000/49000 loss: 0.684406195237199\n",
      "18000/49000 loss: 0.7461810338013768\n",
      "24000/49000 loss: 0.6775108164500924\n",
      "30000/49000 loss: 0.6464259742822709\n",
      "36000/49000 loss: 0.7153372275193047\n",
      "42000/49000 loss: 0.658587008584097\n",
      "48000/49000 loss: 0.6296548118611086\n",
      "epoch 4: valid acc = 0.761, new learning rate = 0.00040725312499999993\n",
      "6000/49000 loss: 0.6439747138759598\n",
      "12000/49000 loss: 0.6205448512079809\n",
      "18000/49000 loss: 0.6475174138832894\n",
      "24000/49000 loss: 0.553300920016109\n",
      "30000/49000 loss: 0.6254298774619516\n",
      "36000/49000 loss: 0.5857674617909441\n",
      "42000/49000 loss: 0.5853629193932822\n",
      "48000/49000 loss: 0.615423018340044\n",
      "epoch 5: valid acc = 0.784, new learning rate = 0.0003868904687499999\n",
      "6000/49000 loss: 0.5811319302710287\n",
      "12000/49000 loss: 0.4963717398087468\n",
      "18000/49000 loss: 0.5716345168573439\n",
      "24000/49000 loss: 0.5150533209352485\n",
      "30000/49000 loss: 0.5534428160935437\n",
      "36000/49000 loss: 0.5846402917418355\n",
      "42000/49000 loss: 0.5761867988355246\n",
      "48000/49000 loss: 0.5091117061613621\n",
      "epoch 6: valid acc = 0.808, new learning rate = 0.0003675459453124999\n",
      "6000/49000 loss: 0.5616895699293446\n",
      "12000/49000 loss: 0.5207890474598421\n",
      "18000/49000 loss: 0.5161938913073008\n",
      "24000/49000 loss: 0.5717425619187847\n",
      "30000/49000 loss: 0.5853195070872464\n",
      "36000/49000 loss: 0.49576234043864825\n",
      "42000/49000 loss: 0.525820415497599\n",
      "48000/49000 loss: 0.4967006567534446\n",
      "epoch 7: valid acc = 0.808, new learning rate = 0.00034916864804687486\n",
      "6000/49000 loss: 0.5224874286512887\n",
      "12000/49000 loss: 0.5160700197841098\n",
      "18000/49000 loss: 0.5343776709713077\n",
      "24000/49000 loss: 0.5169292388611674\n",
      "30000/49000 loss: 0.4794797769277486\n",
      "36000/49000 loss: 0.4888827575016074\n",
      "42000/49000 loss: 0.43871977359242803\n",
      "48000/49000 loss: 0.513055876268072\n",
      "epoch 8: valid acc = 0.811, new learning rate = 0.0003317102156445311\n",
      "6000/49000 loss: 0.4325430253763813\n",
      "12000/49000 loss: 0.4512482307110228\n",
      "18000/49000 loss: 0.4570414414745937\n",
      "24000/49000 loss: 0.47403067412096733\n",
      "30000/49000 loss: 0.4815367478701912\n",
      "36000/49000 loss: 0.5069497581378615\n",
      "42000/49000 loss: 0.4860546472613326\n",
      "48000/49000 loss: 0.5156605776211586\n",
      "epoch 9: valid acc = 0.823, new learning rate = 0.0003151247048623045\n",
      "6000/49000 loss: 0.46546775105051175\n",
      "12000/49000 loss: 0.4955777691076859\n",
      "18000/49000 loss: 0.4609806342924022\n",
      "24000/49000 loss: 0.5265549804497991\n",
      "30000/49000 loss: 0.4523114323995973\n",
      "36000/49000 loss: 0.4982350508314875\n",
      "42000/49000 loss: 0.4823564738675283\n",
      "48000/49000 loss: 0.454866061109467\n",
      "epoch 10: valid acc = 0.819, new learning rate = 0.00029936846961918924\n",
      "6000/49000 loss: 0.4266942830673351\n",
      "12000/49000 loss: 0.4395250512396894\n",
      "18000/49000 loss: 0.46121200386901584\n",
      "24000/49000 loss: 0.4375579495622173\n",
      "30000/49000 loss: 0.4561619542685151\n",
      "36000/49000 loss: 0.45193468293934075\n",
      "42000/49000 loss: 0.530217466524628\n",
      "48000/49000 loss: 0.4393893624996874\n",
      "epoch 11: valid acc = 0.828, new learning rate = 0.00028440004613822977\n",
      "6000/49000 loss: 0.39712732707000886\n",
      "12000/49000 loss: 0.4563517175692269\n",
      "18000/49000 loss: 0.4935652089291903\n",
      "24000/49000 loss: 0.402658455032386\n",
      "30000/49000 loss: 0.4594190111259096\n",
      "36000/49000 loss: 0.4740170428420479\n",
      "42000/49000 loss: 0.4642768568342705\n",
      "48000/49000 loss: 0.457860875649535\n",
      "epoch 12: valid acc = 0.835, new learning rate = 0.00027018004383131826\n",
      "6000/49000 loss: 0.44880543068325324\n",
      "12000/49000 loss: 0.47415560361096787\n",
      "18000/49000 loss: 0.4171578554433903\n",
      "24000/49000 loss: 0.44952442215429717\n",
      "30000/49000 loss: 0.5027375355768693\n",
      "36000/49000 loss: 0.4880966980783569\n",
      "42000/49000 loss: 0.39557775615999835\n",
      "48000/49000 loss: 0.44369584154622393\n",
      "epoch 13: valid acc = 0.833, new learning rate = 0.00025667104163975234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/49000 loss: 0.4112606592807319\n",
      "12000/49000 loss: 0.43755643188585436\n",
      "18000/49000 loss: 0.42338415871001783\n",
      "24000/49000 loss: 0.5100635309026009\n",
      "30000/49000 loss: 0.402289761626042\n",
      "36000/49000 loss: 0.40216541854146826\n",
      "42000/49000 loss: 0.3876607994078575\n",
      "48000/49000 loss: 0.42072938970180623\n",
      "epoch 14: valid acc = 0.84, new learning rate = 0.00024383748955776472\n",
      "6000/49000 loss: 0.47973130180535745\n",
      "12000/49000 loss: 0.41929118843110397\n",
      "18000/49000 loss: 0.515422767165492\n",
      "24000/49000 loss: 0.49565399487820433\n",
      "30000/49000 loss: 0.4049393500585099\n",
      "36000/49000 loss: 0.40952377421985325\n",
      "42000/49000 loss: 0.415024643255489\n",
      "48000/49000 loss: 0.49559016856755767\n",
      "epoch 15: valid acc = 0.84, new learning rate = 0.00023164561507987649\n",
      "6000/49000 loss: 0.4463081847451186\n",
      "12000/49000 loss: 0.4317546223037966\n",
      "18000/49000 loss: 0.3590230877646374\n",
      "24000/49000 loss: 0.4058819215393799\n",
      "30000/49000 loss: 0.4014564851457569\n",
      "36000/49000 loss: 0.44386929841752804\n",
      "42000/49000 loss: 0.4794348863435281\n",
      "48000/49000 loss: 0.41632754650407494\n",
      "epoch 16: valid acc = 0.845, new learning rate = 0.00022006333432588265\n",
      "6000/49000 loss: 0.42111706376593905\n",
      "12000/49000 loss: 0.452235142644951\n",
      "18000/49000 loss: 0.3747834216690625\n",
      "24000/49000 loss: 0.4033437620462898\n",
      "30000/49000 loss: 0.39121059661166524\n",
      "36000/49000 loss: 0.42430433273953044\n",
      "42000/49000 loss: 0.4029029306340909\n",
      "48000/49000 loss: 0.40364075583912906\n",
      "epoch 17: valid acc = 0.851, new learning rate = 0.00020906016760958852\n",
      "6000/49000 loss: 0.40868132069788204\n",
      "12000/49000 loss: 0.4169589853926607\n",
      "18000/49000 loss: 0.4230861632914633\n",
      "24000/49000 loss: 0.4447138254488889\n",
      "30000/49000 loss: 0.4201920617435571\n",
      "36000/49000 loss: 0.4342138534257509\n",
      "42000/49000 loss: 0.43180521661190835\n",
      "48000/49000 loss: 0.3917623473308178\n",
      "epoch 18: valid acc = 0.855, new learning rate = 0.00019860715922910907\n",
      "6000/49000 loss: 0.42956306073744466\n",
      "12000/49000 loss: 0.40798581562963804\n",
      "18000/49000 loss: 0.3500217775600055\n",
      "24000/49000 loss: 0.42877163792047635\n",
      "30000/49000 loss: 0.40145789262811205\n",
      "36000/49000 loss: 0.46534911198135204\n",
      "42000/49000 loss: 0.4071541451738024\n",
      "48000/49000 loss: 0.40499296210294816\n",
      "epoch 19: valid acc = 0.854, new learning rate = 0.0001886768012676536\n",
      "6000/49000 loss: 0.40434600471488397\n",
      "12000/49000 loss: 0.369289773435624\n",
      "18000/49000 loss: 0.4178240953627811\n",
      "24000/49000 loss: 0.3764749066953438\n",
      "30000/49000 loss: 0.35239751304209316\n",
      "36000/49000 loss: 0.3693047812617467\n",
      "42000/49000 loss: 0.3955239114718452\n",
      "48000/49000 loss: 0.36981120706455184\n",
      "epoch 20: valid acc = 0.856, new learning rate = 0.0001792429612042709\n",
      "6000/49000 loss: 0.402407349561464\n",
      "12000/49000 loss: 0.41361440888484813\n",
      "18000/49000 loss: 0.3992274689069795\n",
      "24000/49000 loss: 0.4044828422310311\n",
      "30000/49000 loss: 0.4349241399517116\n",
      "36000/49000 loss: 0.40761879168103493\n",
      "42000/49000 loss: 0.39984821497211515\n",
      "48000/49000 loss: 0.4127409544401206\n",
      "epoch 21: valid acc = 0.855, new learning rate = 0.00017028081314405735\n",
      "6000/49000 loss: 0.35797442923606415\n",
      "12000/49000 loss: 0.3801672733841586\n",
      "18000/49000 loss: 0.41583521646113375\n",
      "24000/49000 loss: 0.40704782167284864\n",
      "30000/49000 loss: 0.3889176710654144\n",
      "36000/49000 loss: 0.43547547306277035\n",
      "42000/49000 loss: 0.42161286270118636\n",
      "48000/49000 loss: 0.35595826753369253\n",
      "epoch 22: valid acc = 0.854, new learning rate = 0.00016176677248685447\n",
      "6000/49000 loss: 0.3690384505785895\n",
      "12000/49000 loss: 0.3658509908185639\n",
      "18000/49000 loss: 0.45567089854039805\n",
      "24000/49000 loss: 0.367074494797333\n",
      "30000/49000 loss: 0.32762940191809814\n",
      "36000/49000 loss: 0.3397732694466614\n",
      "42000/49000 loss: 0.4036498870284623\n",
      "48000/49000 loss: 0.4060037823205394\n",
      "epoch 23: valid acc = 0.855, new learning rate = 0.00015367843386251173\n",
      "6000/49000 loss: 0.47838153954616713\n",
      "12000/49000 loss: 0.33739913486838186\n",
      "18000/49000 loss: 0.47894203922531386\n",
      "24000/49000 loss: 0.3987406155796447\n",
      "30000/49000 loss: 0.3629328186432003\n",
      "36000/49000 loss: 0.3782474658173922\n",
      "42000/49000 loss: 0.40057222045073443\n",
      "48000/49000 loss: 0.42308413878185963\n",
      "epoch 24: valid acc = 0.856, new learning rate = 0.00014599451216938612\n",
      "6000/49000 loss: 0.42763954278335103\n",
      "12000/49000 loss: 0.42693967498218216\n",
      "18000/49000 loss: 0.4051991613962891\n",
      "24000/49000 loss: 0.374921918389008\n",
      "30000/49000 loss: 0.4254328317532344\n",
      "36000/49000 loss: 0.4174776956427913\n",
      "42000/49000 loss: 0.3604260511192837\n",
      "48000/49000 loss: 0.3982274720272137\n",
      "epoch 25: valid acc = 0.856, new learning rate = 0.00013869478656091682\n",
      "6000/49000 loss: 0.4239152306404828\n",
      "12000/49000 loss: 0.37911213958459\n",
      "18000/49000 loss: 0.4028324273172709\n",
      "24000/49000 loss: 0.34317886643578305\n",
      "30000/49000 loss: 0.37682519251365904\n",
      "36000/49000 loss: 0.3964532953427186\n",
      "42000/49000 loss: 0.37981385147100327\n",
      "48000/49000 loss: 0.3999870157347888\n",
      "epoch 26: valid acc = 0.855, new learning rate = 0.00013176004723287096\n",
      "6000/49000 loss: 0.3574999865229658\n",
      "12000/49000 loss: 0.40408781167477037\n",
      "18000/49000 loss: 0.3943706581141588\n",
      "24000/49000 loss: 0.4406669710858018\n",
      "30000/49000 loss: 0.3640069972132933\n",
      "36000/49000 loss: 0.39642708546674865\n",
      "42000/49000 loss: 0.40987343961725137\n",
      "48000/49000 loss: 0.3718026697316977\n",
      "epoch 27: valid acc = 0.858, new learning rate = 0.0001251720448712274\n",
      "6000/49000 loss: 0.402956375759988\n",
      "12000/49000 loss: 0.4689746435005984\n",
      "18000/49000 loss: 0.40311905205526305\n",
      "24000/49000 loss: 0.3891394765718822\n",
      "30000/49000 loss: 0.34190044215013227\n",
      "36000/49000 loss: 0.4174477152436531\n",
      "42000/49000 loss: 0.4075178755511225\n",
      "48000/49000 loss: 0.40665804536414896\n",
      "epoch 28: valid acc = 0.861, new learning rate = 0.00011891344262766602\n",
      "6000/49000 loss: 0.41431339437241044\n",
      "12000/49000 loss: 0.416503748673309\n",
      "18000/49000 loss: 0.3413014548587916\n",
      "24000/49000 loss: 0.36582988889490364\n",
      "30000/49000 loss: 0.37852704179177393\n",
      "36000/49000 loss: 0.39466946455439617\n",
      "42000/49000 loss: 0.3278879081956603\n",
      "48000/49000 loss: 0.3722065218547904\n",
      "epoch 29: valid acc = 0.859, new learning rate = 0.00011296777049628272\n",
      "6000/49000 loss: 0.37757579745863745\n",
      "12000/49000 loss: 0.39475952733413244\n",
      "18000/49000 loss: 0.3658496687059922\n",
      "24000/49000 loss: 0.3798820679894165\n",
      "30000/49000 loss: 0.4053021771775349\n",
      "36000/49000 loss: 0.3552672571390888\n",
      "42000/49000 loss: 0.40538044187510197\n",
      "48000/49000 loss: 0.3702462423380627\n",
      "epoch 30: valid acc = 0.859, new learning rate = 0.00010731938197146858\n",
      "6000/49000 loss: 0.3906788067131335\n",
      "12000/49000 loss: 0.46894318483590025\n",
      "18000/49000 loss: 0.3369827868422711\n",
      "24000/49000 loss: 0.3929194680005514\n",
      "30000/49000 loss: 0.4054149860784707\n",
      "36000/49000 loss: 0.40134033365513044\n",
      "42000/49000 loss: 0.3532884667971875\n",
      "48000/49000 loss: 0.37990561485241614\n",
      "epoch 31: valid acc = 0.859, new learning rate = 0.00010195341287289515\n",
      "6000/49000 loss: 0.42411044232639933\n",
      "12000/49000 loss: 0.4135421710031749\n",
      "18000/49000 loss: 0.3209754606740894\n",
      "24000/49000 loss: 0.3610259365701927\n",
      "30000/49000 loss: 0.36923634015061757\n",
      "36000/49000 loss: 0.36092338365897925\n",
      "42000/49000 loss: 0.34425740477369526\n",
      "48000/49000 loss: 0.425808459908102\n",
      "epoch 32: valid acc = 0.86, new learning rate = 9.685574222925039e-05\n",
      "6000/49000 loss: 0.36412765075320225\n",
      "12000/49000 loss: 0.37695222746239304\n",
      "18000/49000 loss: 0.34612584749738595\n",
      "24000/49000 loss: 0.4533038716413148\n",
      "30000/49000 loss: 0.3746524092500849\n",
      "36000/49000 loss: 0.37290190111717225\n",
      "42000/49000 loss: 0.3807011755179658\n",
      "48000/49000 loss: 0.39879888375369105\n",
      "epoch 33: valid acc = 0.859, new learning rate = 9.201295511778786e-05\n",
      "6000/49000 loss: 0.45265444216325124\n",
      "12000/49000 loss: 0.3892722563590423\n",
      "18000/49000 loss: 0.34531139364145114\n",
      "24000/49000 loss: 0.4202193957849257\n",
      "30000/49000 loss: 0.4250303492115956\n",
      "36000/49000 loss: 0.40190255322275087\n",
      "42000/49000 loss: 0.38041791222564164\n",
      "48000/49000 loss: 0.33068713075934536\n",
      "epoch 34: valid acc = 0.859, new learning rate = 8.741230736189846e-05\n",
      "6000/49000 loss: 0.38905080631644806\n",
      "12000/49000 loss: 0.37604857655612584\n",
      "18000/49000 loss: 0.2839347193196735\n",
      "24000/49000 loss: 0.3692615500119599\n",
      "30000/49000 loss: 0.3777216296992047\n",
      "36000/49000 loss: 0.34020808160749816\n",
      "42000/49000 loss: 0.35107291855722905\n",
      "48000/49000 loss: 0.3952427161933504\n",
      "epoch 35: valid acc = 0.857, new learning rate = 8.304169199380353e-05\n",
      "6000/49000 loss: 0.39586082843620524\n",
      "12000/49000 loss: 0.3450328820706253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/49000 loss: 0.39370664552521806\n",
      "24000/49000 loss: 0.42135547270148416\n",
      "30000/49000 loss: 0.3838372778076274\n",
      "36000/49000 loss: 0.3647723034390949\n",
      "42000/49000 loss: 0.36310876054710184\n",
      "48000/49000 loss: 0.41418751097450135\n",
      "epoch 36: valid acc = 0.859, new learning rate = 7.888960739411335e-05\n",
      "6000/49000 loss: 0.319110846195898\n",
      "12000/49000 loss: 0.3056724730813218\n",
      "18000/49000 loss: 0.31938990130153494\n",
      "24000/49000 loss: 0.4030127612792088\n",
      "30000/49000 loss: 0.3545461683650926\n",
      "36000/49000 loss: 0.37193527229631257\n",
      "42000/49000 loss: 0.3755158399901761\n",
      "48000/49000 loss: 0.39802746579448883\n",
      "epoch 37: valid acc = 0.86, new learning rate = 7.494512702440768e-05\n",
      "6000/49000 loss: 0.38611402549630375\n",
      "12000/49000 loss: 0.35825381138485696\n",
      "18000/49000 loss: 0.35530197420255644\n",
      "24000/49000 loss: 0.38711826360862006\n",
      "30000/49000 loss: 0.35626320837856507\n",
      "36000/49000 loss: 0.343826138938438\n",
      "42000/49000 loss: 0.4065232296993966\n",
      "48000/49000 loss: 0.37071229488903673\n",
      "epoch 38: valid acc = 0.861, new learning rate = 7.119787067318729e-05\n",
      "6000/49000 loss: 0.45140535978190693\n",
      "12000/49000 loss: 0.3492626345826289\n",
      "18000/49000 loss: 0.3653298464394493\n",
      "24000/49000 loss: 0.36818140158613516\n",
      "30000/49000 loss: 0.3173805939290187\n",
      "36000/49000 loss: 0.39317352795058885\n",
      "42000/49000 loss: 0.4142310798469338\n",
      "48000/49000 loss: 0.36470272813797827\n",
      "epoch 39: valid acc = 0.862, new learning rate = 6.763797713952792e-05\n",
      "6000/49000 loss: 0.35724495553678587\n",
      "12000/49000 loss: 0.34831155504412536\n",
      "18000/49000 loss: 0.39136363757022413\n",
      "24000/49000 loss: 0.33282704636653\n",
      "30000/49000 loss: 0.330502802180166\n",
      "36000/49000 loss: 0.3647032191333445\n",
      "42000/49000 loss: 0.347334546647596\n",
      "48000/49000 loss: 0.4205477368995032\n",
      "epoch 40: valid acc = 0.862, new learning rate = 6.425607828255152e-05\n",
      "6000/49000 loss: 0.37943257762663274\n",
      "12000/49000 loss: 0.4015588008794228\n",
      "18000/49000 loss: 0.4198425536809485\n",
      "24000/49000 loss: 0.3854152318877414\n",
      "30000/49000 loss: 0.37714493298758417\n",
      "36000/49000 loss: 0.34616890044101467\n",
      "42000/49000 loss: 0.3336918243752169\n",
      "48000/49000 loss: 0.40330631177046145\n",
      "epoch 41: valid acc = 0.86, new learning rate = 6.104327436842394e-05\n",
      "6000/49000 loss: 0.3876432730751727\n",
      "12000/49000 loss: 0.37280725379114094\n",
      "18000/49000 loss: 0.36748173436017884\n",
      "24000/49000 loss: 0.33000984391125043\n",
      "30000/49000 loss: 0.37480118633853754\n",
      "36000/49000 loss: 0.39147297316326696\n",
      "42000/49000 loss: 0.3577493209049701\n",
      "48000/49000 loss: 0.40851053674874604\n",
      "epoch 42: valid acc = 0.86, new learning rate = 5.799111065000274e-05\n",
      "6000/49000 loss: 0.3819040569189135\n",
      "12000/49000 loss: 0.37026238539528666\n",
      "18000/49000 loss: 0.38382974514050994\n",
      "24000/49000 loss: 0.37492844207791604\n",
      "30000/49000 loss: 0.3629556226255422\n",
      "36000/49000 loss: 0.40035545617229185\n",
      "42000/49000 loss: 0.38802885346621036\n",
      "48000/49000 loss: 0.3573472499283589\n",
      "epoch 43: valid acc = 0.861, new learning rate = 5.5091555117502596e-05\n",
      "6000/49000 loss: 0.4212081396558668\n",
      "12000/49000 loss: 0.3609591591734855\n",
      "18000/49000 loss: 0.40690658950100883\n",
      "24000/49000 loss: 0.3420918123044826\n",
      "30000/49000 loss: 0.34933566710975505\n",
      "36000/49000 loss: 0.36473791959571894\n",
      "42000/49000 loss: 0.32084453044963135\n",
      "48000/49000 loss: 0.3756352695892562\n",
      "epoch 44: valid acc = 0.862, new learning rate = 5.2336977361627463e-05\n",
      "6000/49000 loss: 0.3777503532518016\n",
      "12000/49000 loss: 0.34966492148234063\n",
      "18000/49000 loss: 0.3595718203034626\n",
      "24000/49000 loss: 0.41167717602857884\n",
      "30000/49000 loss: 0.4416224042493563\n",
      "36000/49000 loss: 0.34276860673808845\n",
      "42000/49000 loss: 0.4170666192918079\n",
      "48000/49000 loss: 0.3269371578347958\n",
      "epoch 45: valid acc = 0.862, new learning rate = 4.972012849354609e-05\n",
      "6000/49000 loss: 0.3414264682758252\n",
      "12000/49000 loss: 0.42573494487318475\n",
      "18000/49000 loss: 0.4363304011412306\n",
      "24000/49000 loss: 0.4243678503073459\n",
      "30000/49000 loss: 0.3799501778946046\n",
      "36000/49000 loss: 0.3469423722662362\n",
      "42000/49000 loss: 0.3453394283663337\n",
      "48000/49000 loss: 0.39815974922118935\n",
      "epoch 46: valid acc = 0.861, new learning rate = 4.723412206886878e-05\n",
      "6000/49000 loss: 0.3609444400972656\n",
      "12000/49000 loss: 0.38107339848127053\n",
      "18000/49000 loss: 0.4203818849080485\n",
      "24000/49000 loss: 0.3894881524419381\n",
      "30000/49000 loss: 0.39669754278106645\n",
      "36000/49000 loss: 0.3690077220595095\n",
      "42000/49000 loss: 0.3475158523975729\n",
      "48000/49000 loss: 0.32739990220176746\n",
      "epoch 47: valid acc = 0.861, new learning rate = 4.487241596542534e-05\n",
      "6000/49000 loss: 0.37188640349449453\n",
      "12000/49000 loss: 0.3613314769105837\n",
      "18000/49000 loss: 0.3595637342701403\n",
      "24000/49000 loss: 0.3200673365293562\n",
      "30000/49000 loss: 0.38022429681811737\n",
      "36000/49000 loss: 0.30083942614077125\n",
      "42000/49000 loss: 0.36449109098339116\n",
      "48000/49000 loss: 0.343068410427857\n",
      "epoch 48: valid acc = 0.862, new learning rate = 4.262879516715407e-05\n",
      "6000/49000 loss: 0.4207875696107268\n",
      "12000/49000 loss: 0.3854956617115626\n",
      "18000/49000 loss: 0.3399793646872046\n",
      "24000/49000 loss: 0.3338222544886632\n",
      "30000/49000 loss: 0.380717462891916\n",
      "36000/49000 loss: 0.4325654032289248\n",
      "42000/49000 loss: 0.32495761127660083\n",
      "48000/49000 loss: 0.3627218606700123\n",
      "epoch 49: valid acc = 0.863, new learning rate = 4.049735540879637e-05\n",
      "6000/49000 loss: 0.3899369912324836\n",
      "12000/49000 loss: 0.399084632921363\n",
      "18000/49000 loss: 0.3771024041552293\n",
      "24000/49000 loss: 0.38987573049072155\n",
      "30000/49000 loss: 0.3434321804819113\n",
      "36000/49000 loss: 0.38001870282943545\n",
      "42000/49000 loss: 0.4087670837556703\n",
      "48000/49000 loss: 0.3728262862012551\n",
      "epoch 50: valid acc = 0.86, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.8688367346938776\n",
      "test acc: 0.86\n",
      "test acc: 0.8472\n",
      "number of batches for training: 81\n",
      "epoch 1: valid acc = 0.511, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.651, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.74, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.765, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.786, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.799, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.805, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.817, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.822, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.824, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.826, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.83, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.835, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.835, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.842, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.846, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.847, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.845, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.847, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.852, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.856, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.854, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.856, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.856, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.855, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.854, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.856, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.859, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.858, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.858, new learning rate = 0.00010731938197146858\n",
      "epoch 31: valid acc = 0.859, new learning rate = 0.00010195341287289515\n",
      "epoch 32: valid acc = 0.863, new learning rate = 9.685574222925039e-05\n",
      "epoch 33: valid acc = 0.862, new learning rate = 9.201295511778786e-05\n",
      "epoch 34: valid acc = 0.861, new learning rate = 8.741230736189846e-05\n",
      "epoch 35: valid acc = 0.862, new learning rate = 8.304169199380353e-05\n",
      "epoch 36: valid acc = 0.861, new learning rate = 7.888960739411335e-05\n",
      "epoch 37: valid acc = 0.864, new learning rate = 7.494512702440768e-05\n",
      "epoch 38: valid acc = 0.865, new learning rate = 7.119787067318729e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 39: valid acc = 0.863, new learning rate = 6.763797713952792e-05\n",
      "epoch 40: valid acc = 0.864, new learning rate = 6.425607828255152e-05\n",
      "epoch 41: valid acc = 0.863, new learning rate = 6.104327436842394e-05\n",
      "epoch 42: valid acc = 0.862, new learning rate = 5.799111065000274e-05\n",
      "epoch 43: valid acc = 0.863, new learning rate = 5.5091555117502596e-05\n",
      "epoch 44: valid acc = 0.861, new learning rate = 5.2336977361627463e-05\n",
      "epoch 45: valid acc = 0.865, new learning rate = 4.972012849354609e-05\n",
      "epoch 46: valid acc = 0.863, new learning rate = 4.723412206886878e-05\n",
      "epoch 47: valid acc = 0.864, new learning rate = 4.487241596542534e-05\n",
      "epoch 48: valid acc = 0.865, new learning rate = 4.262879516715407e-05\n",
      "epoch 49: valid acc = 0.864, new learning rate = 4.049735540879637e-05\n",
      "epoch 50: valid acc = 0.865, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.8685306122448979\n",
      "test acc: 0.865\n",
      "test acc: 0.8476\n",
      "number of batches for training: 81\n",
      "6000/49000 loss: 2.9129174734891543\n",
      "12000/49000 loss: 2.6655944840214847\n",
      "18000/49000 loss: 2.4814857101475836\n",
      "24000/49000 loss: 2.3869027644574845\n",
      "30000/49000 loss: 2.1440117418651194\n",
      "36000/49000 loss: 2.184467892332512\n",
      "42000/49000 loss: 1.9201600871046358\n",
      "48000/49000 loss: 1.604975850762162\n",
      "epoch 1: valid acc = 0.516, new learning rate = 0.000475\n",
      "6000/49000 loss: 1.3298485298470766\n",
      "12000/49000 loss: 1.264860863424784\n",
      "18000/49000 loss: 1.1478548138315392\n",
      "24000/49000 loss: 1.1226544829142944\n",
      "30000/49000 loss: 1.275253669416932\n",
      "36000/49000 loss: 1.0392669674652408\n",
      "42000/49000 loss: 0.9710880278636602\n",
      "48000/49000 loss: 0.9833319239701993\n",
      "epoch 2: valid acc = 0.669, new learning rate = 0.00045125\n",
      "6000/49000 loss: 0.9304812409685599\n",
      "12000/49000 loss: 0.8702475745759121\n",
      "18000/49000 loss: 0.8308986664068282\n",
      "24000/49000 loss: 0.8493240556784346\n",
      "30000/49000 loss: 0.8348394291938689\n",
      "36000/49000 loss: 0.8378542901656801\n",
      "42000/49000 loss: 0.7974178753813214\n",
      "48000/49000 loss: 0.7777600108370593\n",
      "epoch 3: valid acc = 0.742, new learning rate = 0.0004286875\n",
      "6000/49000 loss: 0.7431128375732263\n",
      "12000/49000 loss: 0.7311265588498759\n",
      "18000/49000 loss: 0.67940626249821\n",
      "24000/49000 loss: 0.7730483240807455\n",
      "30000/49000 loss: 0.6823119937188475\n",
      "36000/49000 loss: 0.6791745601707938\n",
      "42000/49000 loss: 0.6192071226265445\n",
      "48000/49000 loss: 0.620737418438001\n",
      "epoch 4: valid acc = 0.766, new learning rate = 0.00040725312499999993\n",
      "6000/49000 loss: 0.6405678040526392\n",
      "12000/49000 loss: 0.6084646091722673\n",
      "18000/49000 loss: 0.6137465469616721\n",
      "24000/49000 loss: 0.5592413256210027\n",
      "30000/49000 loss: 0.5885532546092482\n",
      "36000/49000 loss: 0.5879138410112926\n",
      "42000/49000 loss: 0.5689720919592964\n",
      "48000/49000 loss: 0.5572898124723605\n",
      "epoch 5: valid acc = 0.786, new learning rate = 0.0003868904687499999\n",
      "6000/49000 loss: 0.6023734400921635\n",
      "12000/49000 loss: 0.5721402313998395\n",
      "18000/49000 loss: 0.5438710466403897\n",
      "24000/49000 loss: 0.5695942714599505\n",
      "30000/49000 loss: 0.555671987405128\n",
      "36000/49000 loss: 0.588677250407773\n",
      "42000/49000 loss: 0.5023935765508011\n",
      "48000/49000 loss: 0.4979439563698563\n",
      "epoch 6: valid acc = 0.793, new learning rate = 0.0003675459453124999\n",
      "6000/49000 loss: 0.6454933219691555\n",
      "12000/49000 loss: 0.5718051452622354\n",
      "18000/49000 loss: 0.5193218756479752\n",
      "24000/49000 loss: 0.4941441751975537\n",
      "30000/49000 loss: 0.45506783479971996\n",
      "36000/49000 loss: 0.5272955078391978\n",
      "42000/49000 loss: 0.5754403143867522\n",
      "48000/49000 loss: 0.5151989895137664\n",
      "epoch 7: valid acc = 0.813, new learning rate = 0.00034916864804687486\n",
      "6000/49000 loss: 0.47746768748818946\n",
      "12000/49000 loss: 0.47137064173957394\n",
      "18000/49000 loss: 0.532633601916472\n",
      "24000/49000 loss: 0.4978242139162168\n",
      "30000/49000 loss: 0.4889443933153001\n",
      "36000/49000 loss: 0.5335105775895196\n",
      "42000/49000 loss: 0.5755851216181443\n",
      "48000/49000 loss: 0.5076159427043763\n",
      "epoch 8: valid acc = 0.814, new learning rate = 0.0003317102156445311\n",
      "6000/49000 loss: 0.4908184118299432\n",
      "12000/49000 loss: 0.5186823500002987\n",
      "18000/49000 loss: 0.44910460639121685\n",
      "24000/49000 loss: 0.4665446794061836\n",
      "30000/49000 loss: 0.5087458308140915\n",
      "36000/49000 loss: 0.49371161257022367\n",
      "42000/49000 loss: 0.4733794260127318\n",
      "48000/49000 loss: 0.4430428934455898\n",
      "epoch 9: valid acc = 0.819, new learning rate = 0.0003151247048623045\n",
      "6000/49000 loss: 0.5108199517966795\n",
      "12000/49000 loss: 0.5293925670225637\n",
      "18000/49000 loss: 0.5179855500504222\n",
      "24000/49000 loss: 0.49624150183753446\n",
      "30000/49000 loss: 0.4583607887379226\n",
      "36000/49000 loss: 0.5112306099394932\n",
      "42000/49000 loss: 0.4878829749424146\n",
      "48000/49000 loss: 0.4417718702231984\n",
      "epoch 10: valid acc = 0.827, new learning rate = 0.00029936846961918924\n",
      "6000/49000 loss: 0.41830254408310785\n",
      "12000/49000 loss: 0.5020026481561239\n",
      "18000/49000 loss: 0.45612068906548703\n",
      "24000/49000 loss: 0.4005571121207498\n",
      "30000/49000 loss: 0.45747217124643763\n",
      "36000/49000 loss: 0.47849455078937236\n",
      "42000/49000 loss: 0.46923082128011556\n",
      "48000/49000 loss: 0.48702047191069203\n",
      "epoch 11: valid acc = 0.831, new learning rate = 0.00028440004613822977\n",
      "6000/49000 loss: 0.42361553851639316\n",
      "12000/49000 loss: 0.4369899653198366\n",
      "18000/49000 loss: 0.44044745676836744\n",
      "24000/49000 loss: 0.47807726338834156\n",
      "30000/49000 loss: 0.4254258757328034\n",
      "36000/49000 loss: 0.4896148862989911\n",
      "42000/49000 loss: 0.44798290638812643\n",
      "48000/49000 loss: 0.49134450251702466\n",
      "epoch 12: valid acc = 0.835, new learning rate = 0.00027018004383131826\n",
      "6000/49000 loss: 0.45012025573104697\n",
      "12000/49000 loss: 0.4418206415549997\n",
      "18000/49000 loss: 0.4079578745656832\n",
      "24000/49000 loss: 0.38764432581789776\n",
      "30000/49000 loss: 0.44523041030499916\n",
      "36000/49000 loss: 0.40448678747682904\n",
      "42000/49000 loss: 0.4380393546205914\n",
      "48000/49000 loss: 0.4856841573096602\n",
      "epoch 13: valid acc = 0.832, new learning rate = 0.00025667104163975234\n",
      "6000/49000 loss: 0.4094235807395037\n",
      "12000/49000 loss: 0.4270436049092595\n",
      "18000/49000 loss: 0.4490625644879086\n",
      "24000/49000 loss: 0.4413001190744705\n",
      "30000/49000 loss: 0.5125232403675737\n",
      "36000/49000 loss: 0.4528268956396354\n",
      "42000/49000 loss: 0.48061629733442535\n",
      "48000/49000 loss: 0.3965969744533301\n",
      "epoch 14: valid acc = 0.837, new learning rate = 0.00024383748955776472\n",
      "6000/49000 loss: 0.47541887650234665\n",
      "12000/49000 loss: 0.4679265911511544\n",
      "18000/49000 loss: 0.5082195202474349\n",
      "24000/49000 loss: 0.4453187894204505\n",
      "30000/49000 loss: 0.4003967214793728\n",
      "36000/49000 loss: 0.4364751061387835\n",
      "42000/49000 loss: 0.41781419420407784\n",
      "48000/49000 loss: 0.4256965561655605\n",
      "epoch 15: valid acc = 0.835, new learning rate = 0.00023164561507987649\n",
      "6000/49000 loss: 0.3974922770469076\n",
      "12000/49000 loss: 0.5016016531855931\n",
      "18000/49000 loss: 0.401524453715823\n",
      "24000/49000 loss: 0.46881964753879324\n",
      "30000/49000 loss: 0.3709460861209535\n",
      "36000/49000 loss: 0.4171024647141607\n",
      "42000/49000 loss: 0.43821124559161556\n",
      "48000/49000 loss: 0.46563876606847737\n",
      "epoch 16: valid acc = 0.841, new learning rate = 0.00022006333432588265\n",
      "6000/49000 loss: 0.4229239908504053\n",
      "12000/49000 loss: 0.45360878673842414\n",
      "18000/49000 loss: 0.4054171326071494\n",
      "24000/49000 loss: 0.40419173688261695\n",
      "30000/49000 loss: 0.4240489645962495\n",
      "36000/49000 loss: 0.4165702250898094\n",
      "42000/49000 loss: 0.4484254417956554\n",
      "48000/49000 loss: 0.4563159011461486\n",
      "epoch 17: valid acc = 0.841, new learning rate = 0.00020906016760958852\n",
      "6000/49000 loss: 0.41319273373403737\n",
      "12000/49000 loss: 0.3907137287537001\n",
      "18000/49000 loss: 0.3899610753333277\n",
      "24000/49000 loss: 0.4010454570213952\n",
      "30000/49000 loss: 0.39810474020382197\n",
      "36000/49000 loss: 0.3933502971976458\n",
      "42000/49000 loss: 0.475296598432433\n",
      "48000/49000 loss: 0.38690779404031084\n",
      "epoch 18: valid acc = 0.844, new learning rate = 0.00019860715922910907\n",
      "6000/49000 loss: 0.3945808965854317\n",
      "12000/49000 loss: 0.38415489155004223\n",
      "18000/49000 loss: 0.41627649991042553\n",
      "24000/49000 loss: 0.42789584700866107\n",
      "30000/49000 loss: 0.39521411516144705\n",
      "36000/49000 loss: 0.3773600015651718\n",
      "42000/49000 loss: 0.3891871418303549\n",
      "48000/49000 loss: 0.3561403979147994\n",
      "epoch 19: valid acc = 0.846, new learning rate = 0.0001886768012676536\n",
      "6000/49000 loss: 0.3597264400001392\n",
      "12000/49000 loss: 0.3591682330532894\n",
      "18000/49000 loss: 0.4149858005496858\n",
      "24000/49000 loss: 0.40598245153437684\n",
      "30000/49000 loss: 0.4025036286693474\n",
      "36000/49000 loss: 0.40475626486626\n",
      "42000/49000 loss: 0.444432817368745\n",
      "48000/49000 loss: 0.4115582820296813\n",
      "epoch 20: valid acc = 0.849, new learning rate = 0.0001792429612042709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/49000 loss: 0.3805893743301358\n",
      "12000/49000 loss: 0.40150699841010257\n",
      "18000/49000 loss: 0.4602907110277744\n",
      "24000/49000 loss: 0.3992216373928453\n",
      "30000/49000 loss: 0.41791229200298374\n",
      "36000/49000 loss: 0.39423242403609576\n",
      "42000/49000 loss: 0.3840028417323559\n",
      "48000/49000 loss: 0.4331073850836107\n",
      "epoch 21: valid acc = 0.851, new learning rate = 0.00017028081314405735\n",
      "6000/49000 loss: 0.4255297149910177\n",
      "12000/49000 loss: 0.3665963301882658\n",
      "18000/49000 loss: 0.39178013899015296\n",
      "24000/49000 loss: 0.42755988536073797\n",
      "30000/49000 loss: 0.45977828477395566\n",
      "36000/49000 loss: 0.404927635747622\n",
      "42000/49000 loss: 0.395299303043356\n",
      "48000/49000 loss: 0.3788916144010661\n",
      "epoch 22: valid acc = 0.85, new learning rate = 0.00016176677248685447\n",
      "6000/49000 loss: 0.4533936446081687\n",
      "12000/49000 loss: 0.39979419735640975\n",
      "18000/49000 loss: 0.4306584002974154\n",
      "24000/49000 loss: 0.43196342939949817\n",
      "30000/49000 loss: 0.3719748519307853\n",
      "36000/49000 loss: 0.40218605620114006\n",
      "42000/49000 loss: 0.41489093190389303\n",
      "48000/49000 loss: 0.40673344120895966\n",
      "epoch 23: valid acc = 0.855, new learning rate = 0.00015367843386251173\n",
      "6000/49000 loss: 0.37554210688381884\n",
      "12000/49000 loss: 0.4577225198401304\n",
      "18000/49000 loss: 0.38473774355963475\n",
      "24000/49000 loss: 0.427058353962486\n",
      "30000/49000 loss: 0.38433533401153114\n",
      "36000/49000 loss: 0.3979429724563696\n",
      "42000/49000 loss: 0.43213690149083933\n",
      "48000/49000 loss: 0.3910079962630607\n",
      "epoch 24: valid acc = 0.854, new learning rate = 0.00014599451216938612\n",
      "6000/49000 loss: 0.44918810958912514\n",
      "12000/49000 loss: 0.3758077537022801\n",
      "18000/49000 loss: 0.4270442133909052\n",
      "24000/49000 loss: 0.3559700584414235\n",
      "30000/49000 loss: 0.4475693729857321\n",
      "36000/49000 loss: 0.36345367647806276\n",
      "42000/49000 loss: 0.39858094532304233\n",
      "48000/49000 loss: 0.3648021402153047\n",
      "epoch 25: valid acc = 0.856, new learning rate = 0.00013869478656091682\n",
      "6000/49000 loss: 0.38816487475084377\n",
      "12000/49000 loss: 0.37515090699325465\n",
      "18000/49000 loss: 0.42718926704179666\n",
      "24000/49000 loss: 0.37228591167497777\n",
      "30000/49000 loss: 0.4195014050140583\n",
      "36000/49000 loss: 0.4039219180222116\n",
      "42000/49000 loss: 0.3653565993459069\n",
      "48000/49000 loss: 0.4027707909774117\n",
      "epoch 26: valid acc = 0.857, new learning rate = 0.00013176004723287096\n",
      "6000/49000 loss: 0.42715454097794947\n",
      "12000/49000 loss: 0.4053721761155111\n",
      "18000/49000 loss: 0.38228875964106585\n",
      "24000/49000 loss: 0.39217809681048754\n",
      "30000/49000 loss: 0.3984208356519329\n",
      "36000/49000 loss: 0.4114746960619479\n",
      "42000/49000 loss: 0.4442049673955278\n",
      "48000/49000 loss: 0.37827729448991176\n",
      "epoch 27: valid acc = 0.856, new learning rate = 0.0001251720448712274\n",
      "6000/49000 loss: 0.4262657934926186\n",
      "12000/49000 loss: 0.37622206643748957\n",
      "18000/49000 loss: 0.36011064863880743\n",
      "24000/49000 loss: 0.34751992891102446\n",
      "30000/49000 loss: 0.410763550965343\n",
      "36000/49000 loss: 0.43400350451360825\n",
      "42000/49000 loss: 0.4192342018245719\n",
      "48000/49000 loss: 0.38633506395706146\n",
      "epoch 28: valid acc = 0.856, new learning rate = 0.00011891344262766602\n",
      "6000/49000 loss: 0.41482745585988146\n",
      "12000/49000 loss: 0.41049775706529573\n",
      "18000/49000 loss: 0.3894791213109499\n",
      "24000/49000 loss: 0.3757746088477828\n",
      "30000/49000 loss: 0.4129212284523732\n",
      "36000/49000 loss: 0.42928612914545256\n",
      "42000/49000 loss: 0.3773456948462771\n",
      "48000/49000 loss: 0.3678121989800432\n",
      "epoch 29: valid acc = 0.853, new learning rate = 0.00011296777049628272\n",
      "6000/49000 loss: 0.40252697890141587\n",
      "12000/49000 loss: 0.3929800008554109\n",
      "18000/49000 loss: 0.33482465900128766\n",
      "24000/49000 loss: 0.3793234972907338\n",
      "30000/49000 loss: 0.3902356823671935\n",
      "36000/49000 loss: 0.4290009411392258\n",
      "42000/49000 loss: 0.3800087092561124\n",
      "48000/49000 loss: 0.3964249334096727\n",
      "epoch 30: valid acc = 0.858, new learning rate = 0.00010731938197146858\n",
      "6000/49000 loss: 0.38957028008856226\n",
      "12000/49000 loss: 0.3824531123030558\n",
      "18000/49000 loss: 0.3977507184001396\n",
      "24000/49000 loss: 0.439325752783008\n",
      "30000/49000 loss: 0.32949443910124243\n",
      "36000/49000 loss: 0.4026840561653459\n",
      "42000/49000 loss: 0.3616126849976174\n",
      "48000/49000 loss: 0.3897529925289477\n",
      "epoch 31: valid acc = 0.859, new learning rate = 0.00010195341287289515\n",
      "6000/49000 loss: 0.3921457100673009\n",
      "12000/49000 loss: 0.4055558255133736\n",
      "18000/49000 loss: 0.4054404679630424\n",
      "24000/49000 loss: 0.4211689356248967\n",
      "30000/49000 loss: 0.362544709652202\n",
      "36000/49000 loss: 0.37761019537482243\n",
      "42000/49000 loss: 0.3876301594082562\n",
      "48000/49000 loss: 0.40530919098756885\n",
      "epoch 32: valid acc = 0.86, new learning rate = 9.685574222925039e-05\n",
      "6000/49000 loss: 0.4279436935565502\n",
      "12000/49000 loss: 0.42363910517317654\n",
      "18000/49000 loss: 0.3656770376772823\n",
      "24000/49000 loss: 0.3631587354975403\n",
      "30000/49000 loss: 0.32594259596790065\n",
      "36000/49000 loss: 0.3785524033486407\n",
      "42000/49000 loss: 0.3460858706474928\n",
      "48000/49000 loss: 0.4022431036019952\n",
      "epoch 33: valid acc = 0.862, new learning rate = 9.201295511778786e-05\n",
      "6000/49000 loss: 0.42370164803758587\n",
      "12000/49000 loss: 0.4071453577187229\n",
      "18000/49000 loss: 0.4197405209439296\n",
      "24000/49000 loss: 0.3876401760359642\n",
      "30000/49000 loss: 0.36433696425084194\n",
      "36000/49000 loss: 0.4266938345692627\n",
      "42000/49000 loss: 0.36626126634977235\n",
      "48000/49000 loss: 0.4343250344473279\n",
      "epoch 34: valid acc = 0.858, new learning rate = 8.741230736189846e-05\n",
      "6000/49000 loss: 0.3817973052646343\n",
      "12000/49000 loss: 0.3827237590361258\n",
      "18000/49000 loss: 0.3737456843394432\n",
      "24000/49000 loss: 0.3926840996936126\n",
      "30000/49000 loss: 0.39218503634958196\n",
      "36000/49000 loss: 0.3739411233447247\n",
      "42000/49000 loss: 0.41627155276470745\n",
      "48000/49000 loss: 0.36998613399376584\n",
      "epoch 35: valid acc = 0.86, new learning rate = 8.304169199380353e-05\n",
      "6000/49000 loss: 0.3520720730714875\n",
      "12000/49000 loss: 0.39493657996544795\n",
      "18000/49000 loss: 0.39223633365822175\n",
      "24000/49000 loss: 0.33910405740258287\n",
      "30000/49000 loss: 0.3903635171038523\n",
      "36000/49000 loss: 0.3474621790866493\n",
      "42000/49000 loss: 0.3774322253458053\n",
      "48000/49000 loss: 0.4047358818758763\n",
      "epoch 36: valid acc = 0.858, new learning rate = 7.888960739411335e-05\n",
      "6000/49000 loss: 0.41200481129695155\n",
      "12000/49000 loss: 0.3546456908187038\n",
      "18000/49000 loss: 0.3806332223801125\n",
      "24000/49000 loss: 0.38263894324856323\n",
      "30000/49000 loss: 0.4108575579374518\n",
      "36000/49000 loss: 0.36444066003324344\n",
      "42000/49000 loss: 0.35629570551540835\n",
      "48000/49000 loss: 0.38770676721486896\n",
      "epoch 37: valid acc = 0.86, new learning rate = 7.494512702440768e-05\n",
      "6000/49000 loss: 0.4098971586189709\n",
      "12000/49000 loss: 0.39386713972501314\n",
      "18000/49000 loss: 0.3486761367157429\n",
      "24000/49000 loss: 0.4297430125899064\n",
      "30000/49000 loss: 0.32855615819658324\n",
      "36000/49000 loss: 0.34023362738874524\n",
      "42000/49000 loss: 0.36542155426752104\n",
      "48000/49000 loss: 0.34846325593022737\n",
      "epoch 38: valid acc = 0.861, new learning rate = 7.119787067318729e-05\n",
      "6000/49000 loss: 0.3934524330268567\n",
      "12000/49000 loss: 0.4007680542511361\n",
      "18000/49000 loss: 0.3396833953665163\n",
      "24000/49000 loss: 0.41331585429425444\n",
      "30000/49000 loss: 0.3607993521532464\n",
      "36000/49000 loss: 0.4620081344605091\n",
      "42000/49000 loss: 0.35912808697771426\n",
      "48000/49000 loss: 0.39633668041567766\n",
      "epoch 39: valid acc = 0.863, new learning rate = 6.763797713952792e-05\n",
      "6000/49000 loss: 0.32107683951452387\n",
      "12000/49000 loss: 0.3911819659904903\n",
      "18000/49000 loss: 0.3560770456718499\n",
      "24000/49000 loss: 0.3949403668994718\n",
      "30000/49000 loss: 0.37450056220523503\n",
      "36000/49000 loss: 0.33822488392283045\n",
      "42000/49000 loss: 0.3519973881581681\n",
      "48000/49000 loss: 0.3812847957254347\n",
      "epoch 40: valid acc = 0.863, new learning rate = 6.425607828255152e-05\n",
      "6000/49000 loss: 0.33356377572430174\n",
      "12000/49000 loss: 0.3930473564939509\n",
      "18000/49000 loss: 0.38847939658890884\n",
      "24000/49000 loss: 0.37893245456432034\n",
      "30000/49000 loss: 0.39345116504370675\n",
      "36000/49000 loss: 0.40564038279977693\n",
      "42000/49000 loss: 0.40154879342200805\n",
      "48000/49000 loss: 0.3318171533647966\n",
      "epoch 41: valid acc = 0.861, new learning rate = 6.104327436842394e-05\n",
      "6000/49000 loss: 0.419908449942664\n",
      "12000/49000 loss: 0.3801183580172209\n",
      "18000/49000 loss: 0.3331663707846054\n",
      "24000/49000 loss: 0.341938929513016\n",
      "30000/49000 loss: 0.3477244294733287\n",
      "36000/49000 loss: 0.36303015554222595\n",
      "42000/49000 loss: 0.38633927702704096\n",
      "48000/49000 loss: 0.4159890517417147\n",
      "epoch 42: valid acc = 0.86, new learning rate = 5.799111065000274e-05\n",
      "6000/49000 loss: 0.37389523148733844\n",
      "12000/49000 loss: 0.3722358279427328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/49000 loss: 0.4155046797737885\n",
      "24000/49000 loss: 0.3414322310229225\n",
      "30000/49000 loss: 0.3828998780135801\n",
      "36000/49000 loss: 0.43477187893709685\n",
      "42000/49000 loss: 0.3870793559964588\n",
      "48000/49000 loss: 0.3444036886644566\n",
      "epoch 43: valid acc = 0.863, new learning rate = 5.5091555117502596e-05\n",
      "6000/49000 loss: 0.3489475552447099\n",
      "12000/49000 loss: 0.3242904559297253\n",
      "18000/49000 loss: 0.37000076019079337\n",
      "24000/49000 loss: 0.34893909830795666\n",
      "30000/49000 loss: 0.36635691298859613\n",
      "36000/49000 loss: 0.3684256418063869\n",
      "42000/49000 loss: 0.3886649651418555\n",
      "48000/49000 loss: 0.3305357969433581\n",
      "epoch 44: valid acc = 0.861, new learning rate = 5.2336977361627463e-05\n",
      "6000/49000 loss: 0.35948269544179684\n",
      "12000/49000 loss: 0.41076629900026235\n",
      "18000/49000 loss: 0.4175498148239877\n",
      "24000/49000 loss: 0.4240048397748076\n",
      "30000/49000 loss: 0.3909384257230498\n",
      "36000/49000 loss: 0.4000653641815124\n",
      "42000/49000 loss: 0.36025343344909233\n",
      "48000/49000 loss: 0.3273441113970075\n",
      "epoch 45: valid acc = 0.863, new learning rate = 4.972012849354609e-05\n",
      "6000/49000 loss: 0.4233662898287305\n",
      "12000/49000 loss: 0.34920960987894495\n",
      "18000/49000 loss: 0.3810133990007494\n",
      "24000/49000 loss: 0.39977747651938844\n",
      "30000/49000 loss: 0.3950027147426331\n",
      "36000/49000 loss: 0.376129105493329\n",
      "42000/49000 loss: 0.37208343751904516\n",
      "48000/49000 loss: 0.33936186868419393\n",
      "epoch 46: valid acc = 0.863, new learning rate = 4.723412206886878e-05\n",
      "6000/49000 loss: 0.37338105712931813\n",
      "12000/49000 loss: 0.3570463927814076\n",
      "18000/49000 loss: 0.3795786979630209\n",
      "24000/49000 loss: 0.38731553076234326\n",
      "30000/49000 loss: 0.34974547922514393\n",
      "36000/49000 loss: 0.3630942341345952\n",
      "42000/49000 loss: 0.36678864128659233\n",
      "48000/49000 loss: 0.36192603374241056\n",
      "epoch 47: valid acc = 0.863, new learning rate = 4.487241596542534e-05\n",
      "6000/49000 loss: 0.3605106367526172\n",
      "12000/49000 loss: 0.4705083610178983\n",
      "18000/49000 loss: 0.31340404402285693\n",
      "24000/49000 loss: 0.3699482878269979\n",
      "30000/49000 loss: 0.378273462911214\n",
      "36000/49000 loss: 0.360464155588431\n",
      "42000/49000 loss: 0.3611862822049902\n",
      "48000/49000 loss: 0.34562136838802726\n",
      "epoch 48: valid acc = 0.863, new learning rate = 4.262879516715407e-05\n",
      "6000/49000 loss: 0.38225351429272547\n",
      "12000/49000 loss: 0.3745180555500542\n",
      "18000/49000 loss: 0.36444921202389613\n",
      "24000/49000 loss: 0.36971398235138814\n",
      "30000/49000 loss: 0.3652746108211731\n",
      "36000/49000 loss: 0.37941051067170517\n",
      "42000/49000 loss: 0.3505259832958869\n",
      "48000/49000 loss: 0.37171917223017026\n",
      "epoch 49: valid acc = 0.861, new learning rate = 4.049735540879637e-05\n",
      "6000/49000 loss: 0.3939801255847481\n",
      "12000/49000 loss: 0.36232093289776296\n",
      "18000/49000 loss: 0.341194456864332\n",
      "24000/49000 loss: 0.39286759200354526\n",
      "30000/49000 loss: 0.34730528903332497\n",
      "36000/49000 loss: 0.37378765391077845\n",
      "42000/49000 loss: 0.40925824330944116\n",
      "48000/49000 loss: 0.3647932017489174\n",
      "epoch 50: valid acc = 0.865, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.8684081632653061\n",
      "test acc: 0.865\n",
      "test acc: 0.8473\n",
      "number of batches for training: 81\n",
      "epoch 1: valid acc = 0.516, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.667, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.74, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.76, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.78, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.795, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.801, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.817, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.819, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.829, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.834, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.835, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.834, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.841, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.842, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.842, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.846, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.85, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.848, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.851, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.854, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.851, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.855, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.856, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.86, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.861, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.859, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.86, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.865, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.861, new learning rate = 0.00010731938197146858\n",
      "epoch 31: valid acc = 0.861, new learning rate = 0.00010195341287289515\n",
      "epoch 32: valid acc = 0.86, new learning rate = 9.685574222925039e-05\n",
      "epoch 33: valid acc = 0.861, new learning rate = 9.201295511778786e-05\n",
      "epoch 34: valid acc = 0.864, new learning rate = 8.741230736189846e-05\n",
      "epoch 35: valid acc = 0.861, new learning rate = 8.304169199380353e-05\n",
      "epoch 36: valid acc = 0.866, new learning rate = 7.888960739411335e-05\n",
      "epoch 37: valid acc = 0.861, new learning rate = 7.494512702440768e-05\n",
      "epoch 38: valid acc = 0.862, new learning rate = 7.119787067318729e-05\n",
      "epoch 39: valid acc = 0.861, new learning rate = 6.763797713952792e-05\n",
      "epoch 40: valid acc = 0.863, new learning rate = 6.425607828255152e-05\n",
      "epoch 41: valid acc = 0.864, new learning rate = 6.104327436842394e-05\n",
      "epoch 42: valid acc = 0.863, new learning rate = 5.799111065000274e-05\n",
      "epoch 43: valid acc = 0.861, new learning rate = 5.5091555117502596e-05\n",
      "epoch 44: valid acc = 0.865, new learning rate = 5.2336977361627463e-05\n",
      "epoch 45: valid acc = 0.861, new learning rate = 4.972012849354609e-05\n",
      "epoch 46: valid acc = 0.865, new learning rate = 4.723412206886878e-05\n",
      "epoch 47: valid acc = 0.866, new learning rate = 4.487241596542534e-05\n",
      "epoch 48: valid acc = 0.864, new learning rate = 4.262879516715407e-05\n",
      "epoch 49: valid acc = 0.864, new learning rate = 4.049735540879637e-05\n",
      "epoch 50: valid acc = 0.863, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.867734693877551\n",
      "test acc: 0.863\n",
      "test acc: 0.8464\n",
      "number of batches for training: 49\n",
      "10000/49000 loss: 2.8879117423685283\n",
      "20000/49000 loss: 2.7944458044985514\n",
      "30000/49000 loss: 2.50639316462355\n",
      "40000/49000 loss: 2.4408494311218782\n",
      "epoch 1: valid acc = 0.357, new learning rate = 0.000475\n",
      "10000/49000 loss: 2.123127318071528\n",
      "20000/49000 loss: 2.028874474408885\n",
      "30000/49000 loss: 1.6130305157725051\n",
      "40000/49000 loss: 1.458013542622592\n",
      "epoch 2: valid acc = 0.551, new learning rate = 0.00045125\n",
      "10000/49000 loss: 1.2438854538074418\n",
      "20000/49000 loss: 1.2041186380603996\n",
      "30000/49000 loss: 1.1812880641359635\n",
      "40000/49000 loss: 1.1021662163036299\n",
      "epoch 3: valid acc = 0.627, new learning rate = 0.0004286875\n",
      "10000/49000 loss: 0.9738511542933675\n",
      "20000/49000 loss: 1.042727462411474\n",
      "30000/49000 loss: 0.9968139425629509\n",
      "40000/49000 loss: 0.9385477976198342\n",
      "epoch 4: valid acc = 0.7, new learning rate = 0.00040725312499999993\n",
      "10000/49000 loss: 0.8139938014182243\n",
      "20000/49000 loss: 0.8286731989940436\n",
      "30000/49000 loss: 0.8574607761639093\n",
      "40000/49000 loss: 0.7896589203020739\n",
      "epoch 5: valid acc = 0.727, new learning rate = 0.0003868904687499999\n",
      "10000/49000 loss: 0.7533496509218567\n",
      "20000/49000 loss: 0.7727955317886058\n",
      "30000/49000 loss: 0.7600434338824035\n",
      "40000/49000 loss: 0.7175876991226189\n",
      "epoch 6: valid acc = 0.747, new learning rate = 0.0003675459453124999\n",
      "10000/49000 loss: 0.7014409875541987\n",
      "20000/49000 loss: 0.6611668175947729\n",
      "30000/49000 loss: 0.65273825310756\n",
      "40000/49000 loss: 0.6496628331465398\n",
      "epoch 7: valid acc = 0.761, new learning rate = 0.00034916864804687486\n",
      "10000/49000 loss: 0.6021488585124025\n",
      "20000/49000 loss: 0.6208496398291153\n",
      "30000/49000 loss: 0.6497395504838007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/49000 loss: 0.6004727432041085\n",
      "epoch 8: valid acc = 0.765, new learning rate = 0.0003317102156445311\n",
      "10000/49000 loss: 0.6091838756743349\n",
      "20000/49000 loss: 0.6148436873257814\n",
      "30000/49000 loss: 0.6184881543302281\n",
      "40000/49000 loss: 0.5754668013036253\n",
      "epoch 9: valid acc = 0.779, new learning rate = 0.0003151247048623045\n",
      "10000/49000 loss: 0.6114924013827879\n",
      "20000/49000 loss: 0.5498820761818937\n",
      "30000/49000 loss: 0.5486766949938561\n",
      "40000/49000 loss: 0.5844489188462335\n",
      "epoch 10: valid acc = 0.782, new learning rate = 0.00029936846961918924\n",
      "10000/49000 loss: 0.5558979810353426\n",
      "20000/49000 loss: 0.556275406749835\n",
      "30000/49000 loss: 0.581708599469187\n",
      "40000/49000 loss: 0.5982835036544465\n",
      "epoch 11: valid acc = 0.794, new learning rate = 0.00028440004613822977\n",
      "10000/49000 loss: 0.549153597686515\n",
      "20000/49000 loss: 0.5529855429587364\n",
      "30000/49000 loss: 0.5326258846249846\n",
      "40000/49000 loss: 0.5479718934089086\n",
      "epoch 12: valid acc = 0.798, new learning rate = 0.00027018004383131826\n",
      "10000/49000 loss: 0.5237845727470969\n",
      "20000/49000 loss: 0.5152950636281132\n",
      "30000/49000 loss: 0.5396056345832924\n",
      "40000/49000 loss: 0.5438703115409803\n",
      "epoch 13: valid acc = 0.805, new learning rate = 0.00025667104163975234\n",
      "10000/49000 loss: 0.4997919659510357\n",
      "20000/49000 loss: 0.5423200761065705\n",
      "30000/49000 loss: 0.5438455384239516\n",
      "40000/49000 loss: 0.540053369656068\n",
      "epoch 14: valid acc = 0.806, new learning rate = 0.00024383748955776472\n",
      "10000/49000 loss: 0.4952118517164883\n",
      "20000/49000 loss: 0.5540534033874223\n",
      "30000/49000 loss: 0.5377648886647195\n",
      "40000/49000 loss: 0.5339314988408635\n",
      "epoch 15: valid acc = 0.808, new learning rate = 0.00023164561507987649\n",
      "10000/49000 loss: 0.5045319291230188\n",
      "20000/49000 loss: 0.5183580473645669\n",
      "30000/49000 loss: 0.48671931549568725\n",
      "40000/49000 loss: 0.5056224027708728\n",
      "epoch 16: valid acc = 0.815, new learning rate = 0.00022006333432588265\n",
      "10000/49000 loss: 0.4558144366067001\n",
      "20000/49000 loss: 0.5226447727269591\n",
      "30000/49000 loss: 0.47229434312541546\n",
      "40000/49000 loss: 0.5242800113664523\n",
      "epoch 17: valid acc = 0.813, new learning rate = 0.00020906016760958852\n",
      "10000/49000 loss: 0.47177146439623824\n",
      "20000/49000 loss: 0.4845778708783598\n",
      "30000/49000 loss: 0.5302059682978802\n",
      "40000/49000 loss: 0.4654785487101631\n",
      "epoch 18: valid acc = 0.817, new learning rate = 0.00019860715922910907\n",
      "10000/49000 loss: 0.502374525557524\n",
      "20000/49000 loss: 0.46309045911620084\n",
      "30000/49000 loss: 0.5022035299998854\n",
      "40000/49000 loss: 0.4909522113208842\n",
      "epoch 19: valid acc = 0.818, new learning rate = 0.0001886768012676536\n",
      "10000/49000 loss: 0.45318856153376363\n",
      "20000/49000 loss: 0.4467282477236189\n",
      "30000/49000 loss: 0.5368998955234852\n",
      "40000/49000 loss: 0.5105142532355504\n",
      "epoch 20: valid acc = 0.817, new learning rate = 0.0001792429612042709\n",
      "10000/49000 loss: 0.5026961791000621\n",
      "20000/49000 loss: 0.5124279997091954\n",
      "30000/49000 loss: 0.47082449700234097\n",
      "40000/49000 loss: 0.48952802650503957\n",
      "epoch 21: valid acc = 0.822, new learning rate = 0.00017028081314405735\n",
      "10000/49000 loss: 0.4571549685916646\n",
      "20000/49000 loss: 0.44316836093961626\n",
      "30000/49000 loss: 0.46565529149703305\n",
      "40000/49000 loss: 0.4694591536686343\n",
      "epoch 22: valid acc = 0.822, new learning rate = 0.00016176677248685447\n",
      "10000/49000 loss: 0.42989675657136894\n",
      "20000/49000 loss: 0.49629831176992817\n",
      "30000/49000 loss: 0.4598537373794056\n",
      "40000/49000 loss: 0.4304638255592205\n",
      "epoch 23: valid acc = 0.826, new learning rate = 0.00015367843386251173\n",
      "10000/49000 loss: 0.4427312990216816\n",
      "20000/49000 loss: 0.46158192014878047\n",
      "30000/49000 loss: 0.4845227602506973\n",
      "40000/49000 loss: 0.46705380698293425\n",
      "epoch 24: valid acc = 0.826, new learning rate = 0.00014599451216938612\n",
      "10000/49000 loss: 0.5154699324573591\n",
      "20000/49000 loss: 0.4619494286091408\n",
      "30000/49000 loss: 0.44668830195904663\n",
      "40000/49000 loss: 0.5419089620306119\n",
      "epoch 25: valid acc = 0.826, new learning rate = 0.00013869478656091682\n",
      "10000/49000 loss: 0.4718017047603298\n",
      "20000/49000 loss: 0.4601365366956238\n",
      "30000/49000 loss: 0.44538148305510505\n",
      "40000/49000 loss: 0.528926270707165\n",
      "epoch 26: valid acc = 0.827, new learning rate = 0.00013176004723287096\n",
      "10000/49000 loss: 0.45063870466520045\n",
      "20000/49000 loss: 0.45516592661465816\n",
      "30000/49000 loss: 0.4238328218149631\n",
      "40000/49000 loss: 0.4567228880582404\n",
      "epoch 27: valid acc = 0.833, new learning rate = 0.0001251720448712274\n",
      "10000/49000 loss: 0.47696322570807104\n",
      "20000/49000 loss: 0.446088647305794\n",
      "30000/49000 loss: 0.4873246646970183\n",
      "40000/49000 loss: 0.42947174317142284\n",
      "epoch 28: valid acc = 0.83, new learning rate = 0.00011891344262766602\n",
      "10000/49000 loss: 0.49064429383926195\n",
      "20000/49000 loss: 0.4321499843655394\n",
      "30000/49000 loss: 0.4522984356380215\n",
      "40000/49000 loss: 0.4499344209180529\n",
      "epoch 29: valid acc = 0.831, new learning rate = 0.00011296777049628272\n",
      "10000/49000 loss: 0.44824293964467177\n",
      "20000/49000 loss: 0.48192965702085033\n",
      "30000/49000 loss: 0.41914740212453033\n",
      "40000/49000 loss: 0.420899828275788\n",
      "epoch 30: valid acc = 0.833, new learning rate = 0.00010731938197146858\n",
      "10000/49000 loss: 0.45117264688138237\n",
      "20000/49000 loss: 0.4314774206299759\n",
      "30000/49000 loss: 0.44376835431497047\n",
      "40000/49000 loss: 0.4443940379209427\n",
      "epoch 31: valid acc = 0.832, new learning rate = 0.00010195341287289515\n",
      "10000/49000 loss: 0.4454336184979036\n",
      "20000/49000 loss: 0.42945772089134265\n",
      "30000/49000 loss: 0.3993065564947389\n",
      "40000/49000 loss: 0.5112069454874372\n",
      "epoch 32: valid acc = 0.833, new learning rate = 9.685574222925039e-05\n",
      "10000/49000 loss: 0.44743090945971603\n",
      "20000/49000 loss: 0.46132108369277824\n",
      "30000/49000 loss: 0.4388081496045809\n",
      "40000/49000 loss: 0.4230960791089634\n",
      "epoch 33: valid acc = 0.833, new learning rate = 9.201295511778786e-05\n",
      "10000/49000 loss: 0.439737479248757\n",
      "20000/49000 loss: 0.44118720071075523\n",
      "30000/49000 loss: 0.41431504263760544\n",
      "40000/49000 loss: 0.41206729040754\n",
      "epoch 34: valid acc = 0.831, new learning rate = 8.741230736189846e-05\n",
      "10000/49000 loss: 0.4559719614122669\n",
      "20000/49000 loss: 0.49139682222478864\n",
      "30000/49000 loss: 0.3997881059820099\n",
      "40000/49000 loss: 0.4433584878023123\n",
      "epoch 35: valid acc = 0.835, new learning rate = 8.304169199380353e-05\n",
      "10000/49000 loss: 0.41417364299592674\n",
      "20000/49000 loss: 0.4572597301053049\n",
      "30000/49000 loss: 0.40466229821431876\n",
      "40000/49000 loss: 0.428360334282754\n",
      "epoch 36: valid acc = 0.836, new learning rate = 7.888960739411335e-05\n",
      "10000/49000 loss: 0.4056795721599212\n",
      "20000/49000 loss: 0.4711953039107611\n",
      "30000/49000 loss: 0.43323136558951386\n",
      "40000/49000 loss: 0.44335157753039933\n",
      "epoch 37: valid acc = 0.834, new learning rate = 7.494512702440768e-05\n",
      "10000/49000 loss: 0.41391777426690474\n",
      "20000/49000 loss: 0.46010973387341925\n",
      "30000/49000 loss: 0.45286784259099355\n",
      "40000/49000 loss: 0.41147084415497515\n",
      "epoch 38: valid acc = 0.833, new learning rate = 7.119787067318729e-05\n",
      "10000/49000 loss: 0.4174450863096141\n",
      "20000/49000 loss: 0.4604513204657055\n",
      "30000/49000 loss: 0.387972411895326\n",
      "40000/49000 loss: 0.40961800985241714\n",
      "epoch 39: valid acc = 0.835, new learning rate = 6.763797713952792e-05\n",
      "10000/49000 loss: 0.49446250717267726\n",
      "20000/49000 loss: 0.4681693116375488\n",
      "30000/49000 loss: 0.4172804637952671\n",
      "40000/49000 loss: 0.4490921075198151\n",
      "epoch 40: valid acc = 0.838, new learning rate = 6.425607828255152e-05\n",
      "10000/49000 loss: 0.41286750066898387\n",
      "20000/49000 loss: 0.4269358209668597\n",
      "30000/49000 loss: 0.4224952028194673\n",
      "40000/49000 loss: 0.44088594502037415\n",
      "epoch 41: valid acc = 0.838, new learning rate = 6.104327436842394e-05\n",
      "10000/49000 loss: 0.41492173397348736\n",
      "20000/49000 loss: 0.40783536948744903\n",
      "30000/49000 loss: 0.4275634525868275\n",
      "40000/49000 loss: 0.4281042075960349\n",
      "epoch 42: valid acc = 0.842, new learning rate = 5.799111065000274e-05\n",
      "10000/49000 loss: 0.4253025348384513\n",
      "20000/49000 loss: 0.44881692853771005\n",
      "30000/49000 loss: 0.4447995172719804\n",
      "40000/49000 loss: 0.45212325602589565\n",
      "epoch 43: valid acc = 0.843, new learning rate = 5.5091555117502596e-05\n",
      "10000/49000 loss: 0.43884318800964306\n",
      "20000/49000 loss: 0.45868913371004155\n",
      "30000/49000 loss: 0.40785968907245496\n",
      "40000/49000 loss: 0.412310743610013\n",
      "epoch 44: valid acc = 0.842, new learning rate = 5.2336977361627463e-05\n",
      "10000/49000 loss: 0.4131893911154297\n",
      "20000/49000 loss: 0.4192726227463489\n",
      "30000/49000 loss: 0.4342022655657861\n",
      "40000/49000 loss: 0.4215367127587459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 45: valid acc = 0.845, new learning rate = 4.972012849354609e-05\n",
      "10000/49000 loss: 0.4481582056318959\n",
      "20000/49000 loss: 0.4207900354536043\n",
      "30000/49000 loss: 0.4258488874174424\n",
      "40000/49000 loss: 0.44132846027608125\n",
      "epoch 46: valid acc = 0.842, new learning rate = 4.723412206886878e-05\n",
      "10000/49000 loss: 0.4666476408921109\n",
      "20000/49000 loss: 0.4440459205947712\n",
      "30000/49000 loss: 0.39028588533677083\n",
      "40000/49000 loss: 0.4666896644923765\n",
      "epoch 47: valid acc = 0.842, new learning rate = 4.487241596542534e-05\n",
      "10000/49000 loss: 0.4258333361090065\n",
      "20000/49000 loss: 0.44162495660853074\n",
      "30000/49000 loss: 0.4067505643825077\n",
      "40000/49000 loss: 0.4049347878070258\n",
      "epoch 48: valid acc = 0.846, new learning rate = 4.262879516715407e-05\n",
      "10000/49000 loss: 0.42948723810226075\n",
      "20000/49000 loss: 0.4387815257462298\n",
      "30000/49000 loss: 0.47447350754996237\n",
      "40000/49000 loss: 0.411518502990736\n",
      "epoch 49: valid acc = 0.845, new learning rate = 4.049735540879637e-05\n",
      "10000/49000 loss: 0.43518244584191074\n",
      "20000/49000 loss: 0.430765393775346\n",
      "30000/49000 loss: 0.40344495001093966\n",
      "40000/49000 loss: 0.43591020991326446\n",
      "epoch 50: valid acc = 0.846, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.8499795918367347\n",
      "test acc: 0.846\n",
      "test acc: 0.8345\n",
      "number of batches for training: 49\n",
      "epoch 1: valid acc = 0.418, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.519, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.651, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.71, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.725, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.74, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.762, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.775, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.784, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.789, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.796, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.801, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.808, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.813, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.814, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.808, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.818, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.819, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.823, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.828, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.826, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.829, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.83, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.834, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.832, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.836, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.835, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.838, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.838, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.84, new learning rate = 0.00010731938197146858\n",
      "epoch 31: valid acc = 0.838, new learning rate = 0.00010195341287289515\n",
      "epoch 32: valid acc = 0.834, new learning rate = 9.685574222925039e-05\n",
      "epoch 33: valid acc = 0.839, new learning rate = 9.201295511778786e-05\n",
      "epoch 34: valid acc = 0.842, new learning rate = 8.741230736189846e-05\n",
      "epoch 35: valid acc = 0.842, new learning rate = 8.304169199380353e-05\n",
      "epoch 36: valid acc = 0.845, new learning rate = 7.888960739411335e-05\n",
      "epoch 37: valid acc = 0.843, new learning rate = 7.494512702440768e-05\n",
      "epoch 38: valid acc = 0.842, new learning rate = 7.119787067318729e-05\n",
      "epoch 39: valid acc = 0.842, new learning rate = 6.763797713952792e-05\n",
      "epoch 40: valid acc = 0.844, new learning rate = 6.425607828255152e-05\n",
      "epoch 41: valid acc = 0.843, new learning rate = 6.104327436842394e-05\n",
      "epoch 42: valid acc = 0.844, new learning rate = 5.799111065000274e-05\n",
      "epoch 43: valid acc = 0.845, new learning rate = 5.5091555117502596e-05\n",
      "epoch 44: valid acc = 0.844, new learning rate = 5.2336977361627463e-05\n",
      "epoch 45: valid acc = 0.847, new learning rate = 4.972012849354609e-05\n",
      "epoch 46: valid acc = 0.849, new learning rate = 4.723412206886878e-05\n",
      "epoch 47: valid acc = 0.849, new learning rate = 4.487241596542534e-05\n",
      "epoch 48: valid acc = 0.849, new learning rate = 4.262879516715407e-05\n",
      "epoch 49: valid acc = 0.849, new learning rate = 4.049735540879637e-05\n",
      "epoch 50: valid acc = 0.844, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.850326530612245\n",
      "test acc: 0.844\n",
      "test acc: 0.8353\n",
      "number of batches for training: 49\n",
      "10000/49000 loss: 2.612727480313656\n",
      "20000/49000 loss: 2.672989298432725\n",
      "30000/49000 loss: 2.5548358909443403\n",
      "40000/49000 loss: 2.4504932157621138\n",
      "epoch 1: valid acc = 0.379, new learning rate = 0.000475\n",
      "10000/49000 loss: 2.062169750243857\n",
      "20000/49000 loss: 1.981951624564009\n",
      "30000/49000 loss: 1.6270186972757992\n",
      "40000/49000 loss: 1.3778746511828361\n",
      "epoch 2: valid acc = 0.514, new learning rate = 0.00045125\n",
      "10000/49000 loss: 1.2525705868292085\n",
      "20000/49000 loss: 1.1475855892278701\n",
      "30000/49000 loss: 1.1129308870833325\n",
      "40000/49000 loss: 1.1283660559292608\n",
      "epoch 3: valid acc = 0.646, new learning rate = 0.0004286875\n",
      "10000/49000 loss: 1.0879376932028342\n",
      "20000/49000 loss: 1.0000514642326233\n",
      "30000/49000 loss: 0.9986351500891514\n",
      "40000/49000 loss: 0.9659170283239463\n",
      "epoch 4: valid acc = 0.701, new learning rate = 0.00040725312499999993\n",
      "10000/49000 loss: 0.8696713623053194\n",
      "20000/49000 loss: 0.866130156119586\n",
      "30000/49000 loss: 0.8038247553960791\n",
      "40000/49000 loss: 0.7687892249315434\n",
      "epoch 5: valid acc = 0.734, new learning rate = 0.0003868904687499999\n",
      "10000/49000 loss: 0.7512577251680916\n",
      "20000/49000 loss: 0.7875327309859461\n",
      "30000/49000 loss: 0.6823215718954476\n",
      "40000/49000 loss: 0.7237061124375427\n",
      "epoch 6: valid acc = 0.742, new learning rate = 0.0003675459453124999\n",
      "10000/49000 loss: 0.7202759689380446\n",
      "20000/49000 loss: 0.6691125750190318\n",
      "30000/49000 loss: 0.6492476861678808\n",
      "40000/49000 loss: 0.6912263204284809\n",
      "epoch 7: valid acc = 0.757, new learning rate = 0.00034916864804687486\n",
      "10000/49000 loss: 0.6433376996988671\n",
      "20000/49000 loss: 0.6233341608244357\n",
      "30000/49000 loss: 0.639878055848985\n",
      "40000/49000 loss: 0.5946474133872832\n",
      "epoch 8: valid acc = 0.771, new learning rate = 0.0003317102156445311\n",
      "10000/49000 loss: 0.610209989388178\n",
      "20000/49000 loss: 0.6244400608916371\n",
      "30000/49000 loss: 0.5726245211255155\n",
      "40000/49000 loss: 0.6380794792075192\n",
      "epoch 9: valid acc = 0.788, new learning rate = 0.0003151247048623045\n",
      "10000/49000 loss: 0.6278622757004548\n",
      "20000/49000 loss: 0.5640919495160237\n",
      "30000/49000 loss: 0.5432443759004391\n",
      "40000/49000 loss: 0.5809066577018752\n",
      "epoch 10: valid acc = 0.79, new learning rate = 0.00029936846961918924\n",
      "10000/49000 loss: 0.5702069659995702\n",
      "20000/49000 loss: 0.5848439112945841\n",
      "30000/49000 loss: 0.5560387504370238\n",
      "40000/49000 loss: 0.5841941092630575\n",
      "epoch 11: valid acc = 0.792, new learning rate = 0.00028440004613822977\n",
      "10000/49000 loss: 0.5457520146106682\n",
      "20000/49000 loss: 0.549819826687907\n",
      "30000/49000 loss: 0.5440319344474666\n",
      "40000/49000 loss: 0.533649508731899\n",
      "epoch 12: valid acc = 0.804, new learning rate = 0.00027018004383131826\n",
      "10000/49000 loss: 0.5579200164336103\n",
      "20000/49000 loss: 0.5409077458365623\n",
      "30000/49000 loss: 0.5118967761472314\n",
      "40000/49000 loss: 0.5172749561929897\n",
      "epoch 13: valid acc = 0.811, new learning rate = 0.00025667104163975234\n",
      "10000/49000 loss: 0.5120771408568102\n",
      "20000/49000 loss: 0.5175996515811621\n",
      "30000/49000 loss: 0.5111043144261098\n",
      "40000/49000 loss: 0.5033610460515088\n",
      "epoch 14: valid acc = 0.808, new learning rate = 0.00024383748955776472\n",
      "10000/49000 loss: 0.49161068091314025\n",
      "20000/49000 loss: 0.48716870225725417\n",
      "30000/49000 loss: 0.546588328532286\n",
      "40000/49000 loss: 0.5120357881431828\n",
      "epoch 15: valid acc = 0.813, new learning rate = 0.00023164561507987649\n",
      "10000/49000 loss: 0.46743757067439473\n",
      "20000/49000 loss: 0.5376166912996787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000/49000 loss: 0.5000372105412315\n",
      "40000/49000 loss: 0.505070410979865\n",
      "epoch 16: valid acc = 0.82, new learning rate = 0.00022006333432588265\n",
      "10000/49000 loss: 0.4735332892426258\n",
      "20000/49000 loss: 0.464061313847018\n",
      "30000/49000 loss: 0.5093101881622185\n",
      "40000/49000 loss: 0.4581089631204389\n",
      "epoch 17: valid acc = 0.821, new learning rate = 0.00020906016760958852\n",
      "10000/49000 loss: 0.4944493069076756\n",
      "20000/49000 loss: 0.45950256231240516\n",
      "30000/49000 loss: 0.5030345554706217\n",
      "40000/49000 loss: 0.47790850974359556\n",
      "epoch 18: valid acc = 0.824, new learning rate = 0.00019860715922910907\n",
      "10000/49000 loss: 0.47265707514958255\n",
      "20000/49000 loss: 0.49114288174095544\n",
      "30000/49000 loss: 0.46897037270402747\n",
      "40000/49000 loss: 0.4530732798321364\n",
      "epoch 19: valid acc = 0.823, new learning rate = 0.0001886768012676536\n",
      "10000/49000 loss: 0.5105589258027569\n",
      "20000/49000 loss: 0.4739663098023759\n",
      "30000/49000 loss: 0.4683866887579458\n",
      "40000/49000 loss: 0.48051441746333756\n",
      "epoch 20: valid acc = 0.826, new learning rate = 0.0001792429612042709\n",
      "10000/49000 loss: 0.434358677962847\n",
      "20000/49000 loss: 0.4547601916371762\n",
      "30000/49000 loss: 0.472836263056764\n",
      "40000/49000 loss: 0.45715107988418735\n",
      "epoch 21: valid acc = 0.827, new learning rate = 0.00017028081314405735\n",
      "10000/49000 loss: 0.44354484606104155\n",
      "20000/49000 loss: 0.4640195189397167\n",
      "30000/49000 loss: 0.49654201420972266\n",
      "40000/49000 loss: 0.4576764114649898\n",
      "epoch 22: valid acc = 0.83, new learning rate = 0.00016176677248685447\n",
      "10000/49000 loss: 0.4643326481196753\n",
      "20000/49000 loss: 0.46796833273433136\n",
      "30000/49000 loss: 0.4217602562667559\n",
      "40000/49000 loss: 0.4627619636150644\n",
      "epoch 23: valid acc = 0.828, new learning rate = 0.00015367843386251173\n",
      "10000/49000 loss: 0.44701208347919275\n",
      "20000/49000 loss: 0.450282687914333\n",
      "30000/49000 loss: 0.454737414342467\n",
      "40000/49000 loss: 0.47706453460287995\n",
      "epoch 24: valid acc = 0.828, new learning rate = 0.00014599451216938612\n",
      "10000/49000 loss: 0.4766052164454939\n",
      "20000/49000 loss: 0.4701415400127614\n",
      "30000/49000 loss: 0.4601318280301866\n",
      "40000/49000 loss: 0.4525027726533164\n",
      "epoch 25: valid acc = 0.835, new learning rate = 0.00013869478656091682\n",
      "10000/49000 loss: 0.43536217474208266\n",
      "20000/49000 loss: 0.456110515369148\n",
      "30000/49000 loss: 0.41041342523791047\n",
      "40000/49000 loss: 0.41565129751614616\n",
      "epoch 26: valid acc = 0.834, new learning rate = 0.00013176004723287096\n",
      "10000/49000 loss: 0.4887193215363878\n",
      "20000/49000 loss: 0.4266704124062961\n",
      "30000/49000 loss: 0.47265664298890364\n",
      "40000/49000 loss: 0.4344434907167484\n",
      "epoch 27: valid acc = 0.834, new learning rate = 0.0001251720448712274\n",
      "10000/49000 loss: 0.39912103896575607\n",
      "20000/49000 loss: 0.4480404727986176\n",
      "30000/49000 loss: 0.47748989518132967\n",
      "40000/49000 loss: 0.4664793218244413\n",
      "epoch 28: valid acc = 0.837, new learning rate = 0.00011891344262766602\n",
      "10000/49000 loss: 0.47928464207070604\n",
      "20000/49000 loss: 0.4316811906834314\n",
      "30000/49000 loss: 0.47031749536063794\n",
      "40000/49000 loss: 0.5011461530239516\n",
      "epoch 29: valid acc = 0.838, new learning rate = 0.00011296777049628272\n",
      "10000/49000 loss: 0.4714182251945436\n",
      "20000/49000 loss: 0.4746791738201788\n",
      "30000/49000 loss: 0.45688314788971607\n",
      "40000/49000 loss: 0.42177715473970195\n",
      "epoch 30: valid acc = 0.838, new learning rate = 0.00010731938197146858\n",
      "10000/49000 loss: 0.4483369500640395\n",
      "20000/49000 loss: 0.48263777642129285\n",
      "30000/49000 loss: 0.4122073901219379\n",
      "40000/49000 loss: 0.4402157406326675\n",
      "epoch 31: valid acc = 0.843, new learning rate = 0.00010195341287289515\n",
      "10000/49000 loss: 0.4174208074317441\n",
      "20000/49000 loss: 0.4265723220065443\n",
      "30000/49000 loss: 0.44930008914795566\n",
      "40000/49000 loss: 0.4659539145786652\n",
      "epoch 32: valid acc = 0.845, new learning rate = 9.685574222925039e-05\n",
      "10000/49000 loss: 0.40814267956501715\n",
      "20000/49000 loss: 0.43472626928766994\n",
      "30000/49000 loss: 0.4300425848743662\n",
      "40000/49000 loss: 0.447605467391231\n",
      "epoch 33: valid acc = 0.841, new learning rate = 9.201295511778786e-05\n",
      "10000/49000 loss: 0.44329225608934186\n",
      "20000/49000 loss: 0.4315055353930117\n",
      "30000/49000 loss: 0.44895304840078326\n",
      "40000/49000 loss: 0.4279206752094142\n",
      "epoch 34: valid acc = 0.842, new learning rate = 8.741230736189846e-05\n",
      "10000/49000 loss: 0.447978751954696\n",
      "20000/49000 loss: 0.4509860547147045\n",
      "30000/49000 loss: 0.436011072473417\n",
      "40000/49000 loss: 0.45473675897558635\n",
      "epoch 35: valid acc = 0.838, new learning rate = 8.304169199380353e-05\n",
      "10000/49000 loss: 0.46596227917522587\n",
      "20000/49000 loss: 0.44919516274119586\n",
      "30000/49000 loss: 0.44702559600307723\n",
      "40000/49000 loss: 0.4124796392521167\n",
      "epoch 36: valid acc = 0.841, new learning rate = 7.888960739411335e-05\n",
      "10000/49000 loss: 0.4427824507486619\n",
      "20000/49000 loss: 0.411034180546308\n",
      "30000/49000 loss: 0.4375929032250095\n",
      "40000/49000 loss: 0.4203122164768517\n",
      "epoch 37: valid acc = 0.84, new learning rate = 7.494512702440768e-05\n",
      "10000/49000 loss: 0.432962942578092\n",
      "20000/49000 loss: 0.48100453138939586\n",
      "30000/49000 loss: 0.4256219122742827\n",
      "40000/49000 loss: 0.4211127447377347\n",
      "epoch 38: valid acc = 0.84, new learning rate = 7.119787067318729e-05\n",
      "10000/49000 loss: 0.42018195285106585\n",
      "20000/49000 loss: 0.3917870274795702\n",
      "30000/49000 loss: 0.41650555786639554\n",
      "40000/49000 loss: 0.4560341721490857\n",
      "epoch 39: valid acc = 0.843, new learning rate = 6.763797713952792e-05\n",
      "10000/49000 loss: 0.44543931791762065\n",
      "20000/49000 loss: 0.4267375758429319\n",
      "30000/49000 loss: 0.43093330257498835\n",
      "40000/49000 loss: 0.4216828002987438\n",
      "epoch 40: valid acc = 0.85, new learning rate = 6.425607828255152e-05\n",
      "10000/49000 loss: 0.4878308517490219\n",
      "20000/49000 loss: 0.4067307860513935\n",
      "30000/49000 loss: 0.3712166151914199\n",
      "40000/49000 loss: 0.41731894539742115\n",
      "epoch 41: valid acc = 0.844, new learning rate = 6.104327436842394e-05\n",
      "10000/49000 loss: 0.4229764428605136\n",
      "20000/49000 loss: 0.4346165750496766\n",
      "30000/49000 loss: 0.496585155271217\n",
      "40000/49000 loss: 0.44566165910015454\n",
      "epoch 42: valid acc = 0.846, new learning rate = 5.799111065000274e-05\n",
      "10000/49000 loss: 0.4520577554543969\n",
      "20000/49000 loss: 0.4159286377145946\n",
      "30000/49000 loss: 0.4156605248194281\n",
      "40000/49000 loss: 0.4028922912153666\n",
      "epoch 43: valid acc = 0.843, new learning rate = 5.5091555117502596e-05\n",
      "10000/49000 loss: 0.4042101551004385\n",
      "20000/49000 loss: 0.41169410395827216\n",
      "30000/49000 loss: 0.40873961534031394\n",
      "40000/49000 loss: 0.45472901076092326\n",
      "epoch 44: valid acc = 0.847, new learning rate = 5.2336977361627463e-05\n",
      "10000/49000 loss: 0.4440415848576023\n",
      "20000/49000 loss: 0.4209009859534613\n",
      "30000/49000 loss: 0.42539362206786396\n",
      "40000/49000 loss: 0.4163395147068635\n",
      "epoch 45: valid acc = 0.847, new learning rate = 4.972012849354609e-05\n",
      "10000/49000 loss: 0.47155372861851025\n",
      "20000/49000 loss: 0.4436006955696467\n",
      "30000/49000 loss: 0.45160928205822376\n",
      "40000/49000 loss: 0.4361421914573845\n",
      "epoch 46: valid acc = 0.844, new learning rate = 4.723412206886878e-05\n",
      "10000/49000 loss: 0.4190858040726648\n",
      "20000/49000 loss: 0.43962424003304335\n",
      "30000/49000 loss: 0.44111235350995154\n",
      "40000/49000 loss: 0.43662020209988345\n",
      "epoch 47: valid acc = 0.844, new learning rate = 4.487241596542534e-05\n",
      "10000/49000 loss: 0.4096601392622553\n",
      "20000/49000 loss: 0.4298924994838201\n",
      "30000/49000 loss: 0.4203530964293579\n",
      "40000/49000 loss: 0.39951562554679054\n",
      "epoch 48: valid acc = 0.844, new learning rate = 4.262879516715407e-05\n",
      "10000/49000 loss: 0.4388227271341452\n",
      "20000/49000 loss: 0.46184060780799097\n",
      "30000/49000 loss: 0.4280226789487517\n",
      "40000/49000 loss: 0.4273704514442316\n",
      "epoch 49: valid acc = 0.844, new learning rate = 4.049735540879637e-05\n",
      "10000/49000 loss: 0.4197471577308262\n",
      "20000/49000 loss: 0.394341934022135\n",
      "30000/49000 loss: 0.39678042136393754\n",
      "40000/49000 loss: 0.47179775056863366\n",
      "epoch 50: valid acc = 0.845, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.8508775510204082\n",
      "test acc: 0.845\n",
      "test acc: 0.8347\n",
      "number of batches for training: 49\n",
      "epoch 1: valid acc = 0.412, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.505, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.639, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.69, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.727, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.75, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.751, new learning rate = 0.00034916864804687486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8: valid acc = 0.775, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.791, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.793, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.803, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.803, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.808, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.818, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.812, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.819, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.82, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.822, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.824, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.829, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.827, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.83, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.831, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.835, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.833, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.832, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.835, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.835, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.832, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.837, new learning rate = 0.00010731938197146858\n",
      "epoch 31: valid acc = 0.836, new learning rate = 0.00010195341287289515\n",
      "epoch 32: valid acc = 0.836, new learning rate = 9.685574222925039e-05\n",
      "epoch 33: valid acc = 0.837, new learning rate = 9.201295511778786e-05\n",
      "epoch 34: valid acc = 0.837, new learning rate = 8.741230736189846e-05\n",
      "epoch 35: valid acc = 0.835, new learning rate = 8.304169199380353e-05\n",
      "epoch 36: valid acc = 0.839, new learning rate = 7.888960739411335e-05\n",
      "epoch 37: valid acc = 0.841, new learning rate = 7.494512702440768e-05\n",
      "epoch 38: valid acc = 0.841, new learning rate = 7.119787067318729e-05\n",
      "epoch 39: valid acc = 0.843, new learning rate = 6.763797713952792e-05\n",
      "epoch 40: valid acc = 0.841, new learning rate = 6.425607828255152e-05\n",
      "epoch 41: valid acc = 0.839, new learning rate = 6.104327436842394e-05\n",
      "epoch 42: valid acc = 0.841, new learning rate = 5.799111065000274e-05\n",
      "epoch 43: valid acc = 0.839, new learning rate = 5.5091555117502596e-05\n",
      "epoch 44: valid acc = 0.841, new learning rate = 5.2336977361627463e-05\n",
      "epoch 45: valid acc = 0.84, new learning rate = 4.972012849354609e-05\n",
      "epoch 46: valid acc = 0.842, new learning rate = 4.723412206886878e-05\n",
      "epoch 47: valid acc = 0.84, new learning rate = 4.487241596542534e-05\n",
      "epoch 48: valid acc = 0.842, new learning rate = 4.262879516715407e-05\n",
      "epoch 49: valid acc = 0.842, new learning rate = 4.049735540879637e-05\n",
      "epoch 50: valid acc = 0.843, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.8513061224489796\n",
      "test acc: 0.843\n",
      "test acc: 0.8352\n",
      "number of batches for training: 49\n",
      "10000/49000 loss: 2.873773667049834\n",
      "20000/49000 loss: 2.6183202669839205\n",
      "30000/49000 loss: 2.5237158843502687\n",
      "40000/49000 loss: 2.303797166877687\n",
      "epoch 1: valid acc = 0.35, new learning rate = 0.000475\n",
      "10000/49000 loss: 2.120965397607231\n",
      "20000/49000 loss: 1.9712774610265809\n",
      "30000/49000 loss: 1.6307415241894954\n",
      "40000/49000 loss: 1.4656886967807254\n",
      "epoch 2: valid acc = 0.552, new learning rate = 0.00045125\n",
      "10000/49000 loss: 1.2616403598455197\n",
      "20000/49000 loss: 1.269970161752796\n",
      "30000/49000 loss: 1.078732585530053\n",
      "40000/49000 loss: 1.107626151373095\n",
      "epoch 3: valid acc = 0.627, new learning rate = 0.0004286875\n",
      "10000/49000 loss: 1.0920390991938984\n",
      "20000/49000 loss: 0.9659104034691325\n",
      "30000/49000 loss: 0.9369635087365075\n",
      "40000/49000 loss: 0.9388415704252508\n",
      "epoch 4: valid acc = 0.692, new learning rate = 0.00040725312499999993\n",
      "10000/49000 loss: 0.905644922723435\n",
      "20000/49000 loss: 0.8843548564520274\n",
      "30000/49000 loss: 0.8114164045559176\n",
      "40000/49000 loss: 0.8186219169678053\n",
      "epoch 5: valid acc = 0.736, new learning rate = 0.0003868904687499999\n",
      "10000/49000 loss: 0.7931975308310358\n",
      "20000/49000 loss: 0.7911564974703599\n",
      "30000/49000 loss: 0.7631003852378871\n",
      "40000/49000 loss: 0.6903952706424612\n",
      "epoch 6: valid acc = 0.751, new learning rate = 0.0003675459453124999\n",
      "10000/49000 loss: 0.6751764202273932\n",
      "20000/49000 loss: 0.703556207313974\n",
      "30000/49000 loss: 0.6212313251732974\n",
      "40000/49000 loss: 0.706002824719687\n",
      "epoch 7: valid acc = 0.758, new learning rate = 0.00034916864804687486\n",
      "10000/49000 loss: 0.6605667371747548\n",
      "20000/49000 loss: 0.5978312668015906\n",
      "30000/49000 loss: 0.5926017860129078\n",
      "40000/49000 loss: 0.6596241810719041\n",
      "epoch 8: valid acc = 0.769, new learning rate = 0.0003317102156445311\n",
      "10000/49000 loss: 0.5539866197596959\n",
      "20000/49000 loss: 0.6103774139136252\n",
      "30000/49000 loss: 0.6143283058146138\n",
      "40000/49000 loss: 0.6077058309859308\n",
      "epoch 9: valid acc = 0.782, new learning rate = 0.0003151247048623045\n",
      "10000/49000 loss: 0.5741822019807753\n",
      "20000/49000 loss: 0.5666000388573902\n",
      "30000/49000 loss: 0.5121341736233481\n",
      "40000/49000 loss: 0.5454718640828524\n",
      "epoch 10: valid acc = 0.797, new learning rate = 0.00029936846961918924\n",
      "10000/49000 loss: 0.529175952755223\n",
      "20000/49000 loss: 0.5321605778642398\n",
      "30000/49000 loss: 0.5575911281645968\n",
      "40000/49000 loss: 0.5641969380570719\n",
      "epoch 11: valid acc = 0.801, new learning rate = 0.00028440004613822977\n",
      "10000/49000 loss: 0.5496273973583357\n",
      "20000/49000 loss: 0.4794432383513469\n",
      "30000/49000 loss: 0.5255930573773742\n",
      "40000/49000 loss: 0.5906738706811513\n",
      "epoch 12: valid acc = 0.804, new learning rate = 0.00027018004383131826\n",
      "10000/49000 loss: 0.5095012228812303\n",
      "20000/49000 loss: 0.524451983243682\n",
      "30000/49000 loss: 0.515235463036479\n",
      "40000/49000 loss: 0.5394547373303665\n",
      "epoch 13: valid acc = 0.811, new learning rate = 0.00025667104163975234\n",
      "10000/49000 loss: 0.5317538876491974\n",
      "20000/49000 loss: 0.6090064363223001\n",
      "30000/49000 loss: 0.4971873886072589\n",
      "40000/49000 loss: 0.453169738278383\n",
      "epoch 14: valid acc = 0.817, new learning rate = 0.00024383748955776472\n",
      "10000/49000 loss: 0.4590940858974714\n",
      "20000/49000 loss: 0.5241257448065526\n",
      "30000/49000 loss: 0.5178213773831339\n",
      "40000/49000 loss: 0.5023585440001264\n",
      "epoch 15: valid acc = 0.815, new learning rate = 0.00023164561507987649\n",
      "10000/49000 loss: 0.4957442666505682\n",
      "20000/49000 loss: 0.4782888447131241\n",
      "30000/49000 loss: 0.4965613037981909\n",
      "40000/49000 loss: 0.55624115880946\n",
      "epoch 16: valid acc = 0.816, new learning rate = 0.00022006333432588265\n",
      "10000/49000 loss: 0.46944325045879354\n",
      "20000/49000 loss: 0.474868801202855\n",
      "30000/49000 loss: 0.48509002441587185\n",
      "40000/49000 loss: 0.5021184870410567\n",
      "epoch 17: valid acc = 0.82, new learning rate = 0.00020906016760958852\n",
      "10000/49000 loss: 0.4967819761001208\n",
      "20000/49000 loss: 0.4708563950516986\n",
      "30000/49000 loss: 0.4618115980913378\n",
      "40000/49000 loss: 0.4477376098288441\n",
      "epoch 18: valid acc = 0.825, new learning rate = 0.00019860715922910907\n",
      "10000/49000 loss: 0.515018001768988\n",
      "20000/49000 loss: 0.4574257833889223\n",
      "30000/49000 loss: 0.4568335619939481\n",
      "40000/49000 loss: 0.458839102765381\n",
      "epoch 19: valid acc = 0.828, new learning rate = 0.0001886768012676536\n",
      "10000/49000 loss: 0.4540858063235365\n",
      "20000/49000 loss: 0.49796341415488643\n",
      "30000/49000 loss: 0.447055597487422\n",
      "40000/49000 loss: 0.4896971092426316\n",
      "epoch 20: valid acc = 0.825, new learning rate = 0.0001792429612042709\n",
      "10000/49000 loss: 0.4396523502584416\n",
      "20000/49000 loss: 0.47812500052602813\n",
      "30000/49000 loss: 0.493842282089341\n",
      "40000/49000 loss: 0.4905074632454232\n",
      "epoch 21: valid acc = 0.828, new learning rate = 0.00017028081314405735\n",
      "10000/49000 loss: 0.41039491869414935\n",
      "20000/49000 loss: 0.47012610743631233\n",
      "30000/49000 loss: 0.4480804897853166\n",
      "40000/49000 loss: 0.4667908397691391\n",
      "epoch 22: valid acc = 0.828, new learning rate = 0.00016176677248685447\n",
      "10000/49000 loss: 0.47176491364978573\n",
      "20000/49000 loss: 0.4227637970045657\n",
      "30000/49000 loss: 0.4096443093420431\n",
      "40000/49000 loss: 0.493349000682787\n",
      "epoch 23: valid acc = 0.828, new learning rate = 0.00015367843386251173\n",
      "10000/49000 loss: 0.46570776280778314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/49000 loss: 0.4596590955917065\n",
      "30000/49000 loss: 0.45933257663160754\n",
      "40000/49000 loss: 0.44181071761060164\n",
      "epoch 24: valid acc = 0.826, new learning rate = 0.00014599451216938612\n",
      "10000/49000 loss: 0.45841083076503397\n",
      "20000/49000 loss: 0.4264599411087349\n",
      "30000/49000 loss: 0.4795115773336284\n",
      "40000/49000 loss: 0.4730900300848102\n",
      "epoch 25: valid acc = 0.829, new learning rate = 0.00013869478656091682\n",
      "10000/49000 loss: 0.46225644455219356\n",
      "20000/49000 loss: 0.4241423643882656\n",
      "30000/49000 loss: 0.4103824746437441\n",
      "40000/49000 loss: 0.5058324144430041\n",
      "epoch 26: valid acc = 0.833, new learning rate = 0.00013176004723287096\n",
      "10000/49000 loss: 0.47046277073831266\n",
      "20000/49000 loss: 0.4280966330857058\n",
      "30000/49000 loss: 0.44328168943532037\n",
      "40000/49000 loss: 0.4298625285598205\n",
      "epoch 27: valid acc = 0.832, new learning rate = 0.0001251720448712274\n",
      "10000/49000 loss: 0.4339409828300766\n",
      "20000/49000 loss: 0.45828035979083076\n",
      "30000/49000 loss: 0.4240614819889135\n",
      "40000/49000 loss: 0.4511262845863575\n",
      "epoch 28: valid acc = 0.835, new learning rate = 0.00011891344262766602\n",
      "10000/49000 loss: 0.42183953630492044\n",
      "20000/49000 loss: 0.4371130803772141\n",
      "30000/49000 loss: 0.4494712669530395\n",
      "40000/49000 loss: 0.42848856339728963\n",
      "epoch 29: valid acc = 0.834, new learning rate = 0.00011296777049628272\n",
      "10000/49000 loss: 0.464659743452587\n",
      "20000/49000 loss: 0.43945899393181387\n",
      "30000/49000 loss: 0.4813269435038908\n",
      "40000/49000 loss: 0.4387637580636459\n",
      "epoch 30: valid acc = 0.832, new learning rate = 0.00010731938197146858\n",
      "10000/49000 loss: 0.4164128285072774\n",
      "20000/49000 loss: 0.4514710208017879\n",
      "30000/49000 loss: 0.44966472528926704\n",
      "40000/49000 loss: 0.42191795071524496\n",
      "epoch 31: valid acc = 0.834, new learning rate = 0.00010195341287289515\n",
      "10000/49000 loss: 0.45306265113090477\n",
      "20000/49000 loss: 0.49013564057528675\n",
      "30000/49000 loss: 0.4136241963955249\n",
      "40000/49000 loss: 0.43382217608166995\n",
      "epoch 32: valid acc = 0.836, new learning rate = 9.685574222925039e-05\n",
      "10000/49000 loss: 0.4449678236524103\n",
      "20000/49000 loss: 0.520655049394185\n",
      "30000/49000 loss: 0.44225742045231414\n",
      "40000/49000 loss: 0.43145660483463394\n",
      "epoch 33: valid acc = 0.836, new learning rate = 9.201295511778786e-05\n",
      "10000/49000 loss: 0.39583297562212383\n",
      "20000/49000 loss: 0.4351344667624529\n",
      "30000/49000 loss: 0.3998183689845386\n",
      "40000/49000 loss: 0.46292248768348465\n",
      "epoch 34: valid acc = 0.839, new learning rate = 8.741230736189846e-05\n",
      "10000/49000 loss: 0.46575235863385395\n",
      "20000/49000 loss: 0.3709048799258258\n",
      "30000/49000 loss: 0.4294113469737212\n",
      "40000/49000 loss: 0.45139601884822556\n",
      "epoch 35: valid acc = 0.838, new learning rate = 8.304169199380353e-05\n",
      "10000/49000 loss: 0.45546990165902485\n",
      "20000/49000 loss: 0.41410311983834924\n",
      "30000/49000 loss: 0.42814901581668924\n",
      "40000/49000 loss: 0.46046396266629297\n",
      "epoch 36: valid acc = 0.841, new learning rate = 7.888960739411335e-05\n",
      "10000/49000 loss: 0.4474851382181518\n",
      "20000/49000 loss: 0.44057044359924\n",
      "30000/49000 loss: 0.4618024577376345\n",
      "40000/49000 loss: 0.42424636636136015\n",
      "epoch 37: valid acc = 0.841, new learning rate = 7.494512702440768e-05\n",
      "10000/49000 loss: 0.41909326945591907\n",
      "20000/49000 loss: 0.46565196279684984\n",
      "30000/49000 loss: 0.44023091597068453\n",
      "40000/49000 loss: 0.42525687936546575\n",
      "epoch 38: valid acc = 0.841, new learning rate = 7.119787067318729e-05\n",
      "10000/49000 loss: 0.39174357164038925\n",
      "20000/49000 loss: 0.3758304827602525\n",
      "30000/49000 loss: 0.4043291773361212\n",
      "40000/49000 loss: 0.4513447200891053\n",
      "epoch 39: valid acc = 0.839, new learning rate = 6.763797713952792e-05\n",
      "10000/49000 loss: 0.46326218730399643\n",
      "20000/49000 loss: 0.4418324687823823\n",
      "30000/49000 loss: 0.40981943730557074\n",
      "40000/49000 loss: 0.46736115037416087\n",
      "epoch 40: valid acc = 0.842, new learning rate = 6.425607828255152e-05\n",
      "10000/49000 loss: 0.4149450536857416\n",
      "20000/49000 loss: 0.42490842251251176\n",
      "30000/49000 loss: 0.44243218054393757\n",
      "40000/49000 loss: 0.4604008702178025\n",
      "epoch 41: valid acc = 0.844, new learning rate = 6.104327436842394e-05\n",
      "10000/49000 loss: 0.42725513090354605\n",
      "20000/49000 loss: 0.37085212241062826\n",
      "30000/49000 loss: 0.4214759031670856\n",
      "40000/49000 loss: 0.45555399303893357\n",
      "epoch 42: valid acc = 0.847, new learning rate = 5.799111065000274e-05\n",
      "10000/49000 loss: 0.38762526808314535\n",
      "20000/49000 loss: 0.41761743989206035\n",
      "30000/49000 loss: 0.4303716162546039\n",
      "40000/49000 loss: 0.45403591513909985\n",
      "epoch 43: valid acc = 0.845, new learning rate = 5.5091555117502596e-05\n",
      "10000/49000 loss: 0.4319108910716933\n",
      "20000/49000 loss: 0.40557032169802754\n",
      "30000/49000 loss: 0.43457473683969533\n",
      "40000/49000 loss: 0.43894453255018867\n",
      "epoch 44: valid acc = 0.844, new learning rate = 5.2336977361627463e-05\n",
      "10000/49000 loss: 0.45580150093373883\n",
      "20000/49000 loss: 0.44090924287000743\n",
      "30000/49000 loss: 0.4164288899194272\n",
      "40000/49000 loss: 0.4246393457460416\n",
      "epoch 45: valid acc = 0.847, new learning rate = 4.972012849354609e-05\n",
      "10000/49000 loss: 0.42469584958460627\n",
      "20000/49000 loss: 0.40348551302592034\n",
      "30000/49000 loss: 0.42810462177636904\n",
      "40000/49000 loss: 0.445809065221122\n",
      "epoch 46: valid acc = 0.847, new learning rate = 4.723412206886878e-05\n",
      "10000/49000 loss: 0.40105102231669915\n",
      "20000/49000 loss: 0.48897124824028476\n",
      "30000/49000 loss: 0.3700858269934116\n",
      "40000/49000 loss: 0.3996292216733337\n",
      "epoch 47: valid acc = 0.844, new learning rate = 4.487241596542534e-05\n",
      "10000/49000 loss: 0.41913373816715815\n",
      "20000/49000 loss: 0.44828772857245597\n",
      "30000/49000 loss: 0.4249852228548327\n",
      "40000/49000 loss: 0.4348152962243409\n",
      "epoch 48: valid acc = 0.845, new learning rate = 4.262879516715407e-05\n",
      "10000/49000 loss: 0.4280478895443931\n",
      "20000/49000 loss: 0.4254089175315052\n",
      "30000/49000 loss: 0.4568315829706859\n",
      "40000/49000 loss: 0.4251474677017474\n",
      "epoch 49: valid acc = 0.845, new learning rate = 4.049735540879637e-05\n",
      "10000/49000 loss: 0.42622207109986515\n",
      "20000/49000 loss: 0.4315473907992465\n",
      "30000/49000 loss: 0.44136933433411235\n",
      "40000/49000 loss: 0.4471834279548906\n",
      "epoch 50: valid acc = 0.849, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.8505714285714285\n",
      "test acc: 0.849\n",
      "test acc: 0.8353\n",
      "number of batches for training: 49\n",
      "epoch 1: valid acc = 0.387, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.526, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.647, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.701, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.728, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.747, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.756, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.77, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.785, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.777, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.797, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.802, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.805, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.815, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.817, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.815, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.817, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.82, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.82, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.826, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.829, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.829, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.83, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.83, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.831, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.831, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.829, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.834, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.833, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.832, new learning rate = 0.00010731938197146858\n",
      "epoch 31: valid acc = 0.835, new learning rate = 0.00010195341287289515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 32: valid acc = 0.836, new learning rate = 9.685574222925039e-05\n",
      "epoch 33: valid acc = 0.835, new learning rate = 9.201295511778786e-05\n",
      "epoch 34: valid acc = 0.834, new learning rate = 8.741230736189846e-05\n",
      "epoch 35: valid acc = 0.835, new learning rate = 8.304169199380353e-05\n",
      "epoch 36: valid acc = 0.838, new learning rate = 7.888960739411335e-05\n",
      "epoch 37: valid acc = 0.837, new learning rate = 7.494512702440768e-05\n",
      "epoch 38: valid acc = 0.839, new learning rate = 7.119787067318729e-05\n",
      "epoch 39: valid acc = 0.84, new learning rate = 6.763797713952792e-05\n",
      "epoch 40: valid acc = 0.84, new learning rate = 6.425607828255152e-05\n",
      "epoch 41: valid acc = 0.839, new learning rate = 6.104327436842394e-05\n",
      "epoch 42: valid acc = 0.84, new learning rate = 5.799111065000274e-05\n",
      "epoch 43: valid acc = 0.84, new learning rate = 5.5091555117502596e-05\n",
      "epoch 44: valid acc = 0.839, new learning rate = 5.2336977361627463e-05\n",
      "epoch 45: valid acc = 0.84, new learning rate = 4.972012849354609e-05\n",
      "epoch 46: valid acc = 0.84, new learning rate = 4.723412206886878e-05\n",
      "epoch 47: valid acc = 0.844, new learning rate = 4.487241596542534e-05\n",
      "epoch 48: valid acc = 0.843, new learning rate = 4.262879516715407e-05\n",
      "epoch 49: valid acc = 0.844, new learning rate = 4.049735540879637e-05\n",
      "epoch 50: valid acc = 0.844, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.8510612244897959\n",
      "test acc: 0.844\n",
      "test acc: 0.8352\n",
      "number of batches for training: 245\n",
      "2000/49000 loss: 3.5715200204638005\n",
      "4000/49000 loss: 3.5365468943596885\n",
      "6000/49000 loss: 3.008123989927719\n",
      "8000/49000 loss: 2.506497005732356\n",
      "10000/49000 loss: 2.120400428138999\n",
      "12000/49000 loss: 2.2311845750278985\n",
      "14000/49000 loss: 1.9314187833381564\n",
      "16000/49000 loss: 1.5297511956338414\n",
      "18000/49000 loss: 1.3756582550799694\n",
      "20000/49000 loss: 1.2982329864759636\n",
      "22000/49000 loss: 1.2254811401033203\n",
      "24000/49000 loss: 1.1044929474621408\n",
      "26000/49000 loss: 0.964209270030858\n",
      "28000/49000 loss: 1.0850995966279258\n",
      "30000/49000 loss: 1.0342744116968174\n",
      "32000/49000 loss: 0.8652516253536281\n",
      "34000/49000 loss: 1.0769694614174967\n",
      "36000/49000 loss: 0.9450731350801554\n",
      "38000/49000 loss: 0.8771402267726317\n",
      "40000/49000 loss: 0.8302971926989356\n",
      "42000/49000 loss: 0.9139690933064841\n",
      "44000/49000 loss: 0.8332255185500232\n",
      "46000/49000 loss: 0.7717132186349344\n",
      "48000/49000 loss: 0.8501648462648879\n",
      "epoch 1: valid acc = 0.748, new learning rate = 0.000475\n",
      "2000/49000 loss: 0.7104302210909673\n",
      "4000/49000 loss: 0.6784938869074796\n",
      "6000/49000 loss: 0.6396682164662979\n",
      "8000/49000 loss: 0.6482081126204563\n",
      "10000/49000 loss: 0.736422384489441\n",
      "12000/49000 loss: 0.6944298402864527\n",
      "14000/49000 loss: 0.6244954006112284\n",
      "16000/49000 loss: 0.6748461754410152\n",
      "18000/49000 loss: 0.7691234663775546\n",
      "20000/49000 loss: 0.667567338620919\n",
      "22000/49000 loss: 0.7139095694468931\n",
      "24000/49000 loss: 0.6671853318231219\n",
      "26000/49000 loss: 0.6238918923829466\n",
      "28000/49000 loss: 0.6297287483426706\n",
      "30000/49000 loss: 0.6634139390337729\n",
      "32000/49000 loss: 0.5402156187293848\n",
      "34000/49000 loss: 0.6206837972293837\n",
      "36000/49000 loss: 0.5578934453574059\n",
      "38000/49000 loss: 0.3856182915080701\n",
      "40000/49000 loss: 0.5270418783707408\n",
      "42000/49000 loss: 0.43346664713827565\n",
      "44000/49000 loss: 0.543368937539177\n",
      "46000/49000 loss: 0.5223560875513875\n",
      "48000/49000 loss: 0.6118582041403354\n",
      "epoch 2: valid acc = 0.801, new learning rate = 0.00045125\n",
      "2000/49000 loss: 0.4932401774031765\n",
      "4000/49000 loss: 0.5333156061088935\n",
      "6000/49000 loss: 0.5840957788509229\n",
      "8000/49000 loss: 0.5449229241022884\n",
      "10000/49000 loss: 0.5230816286404444\n",
      "12000/49000 loss: 0.43191929638640775\n",
      "14000/49000 loss: 0.5378664201489665\n",
      "16000/49000 loss: 0.5438122452833482\n",
      "18000/49000 loss: 0.5859468655512747\n",
      "20000/49000 loss: 0.4322006120866987\n",
      "22000/49000 loss: 0.5899275071110271\n",
      "24000/49000 loss: 0.4913683749366641\n",
      "26000/49000 loss: 0.4698230596848193\n",
      "28000/49000 loss: 0.4268022677671132\n",
      "30000/49000 loss: 0.522144590186237\n",
      "32000/49000 loss: 0.42095936225884145\n",
      "34000/49000 loss: 0.482598350511979\n",
      "36000/49000 loss: 0.40137054378263887\n",
      "38000/49000 loss: 0.4256188696129425\n",
      "40000/49000 loss: 0.4219352113768013\n",
      "42000/49000 loss: 0.4912214089053287\n",
      "44000/49000 loss: 0.5809690685222251\n",
      "46000/49000 loss: 0.6141369742858191\n",
      "48000/49000 loss: 0.4179461748195171\n",
      "epoch 3: valid acc = 0.833, new learning rate = 0.0004286875\n",
      "2000/49000 loss: 0.44268982332415535\n",
      "4000/49000 loss: 0.45023213772395854\n",
      "6000/49000 loss: 0.4057541845866423\n",
      "8000/49000 loss: 0.39171573862427345\n",
      "10000/49000 loss: 0.4907482000892279\n",
      "12000/49000 loss: 0.38516056603126597\n",
      "14000/49000 loss: 0.4672942898343423\n",
      "16000/49000 loss: 0.4139714462981535\n",
      "18000/49000 loss: 0.4965664584041071\n",
      "20000/49000 loss: 0.40814267389178494\n",
      "22000/49000 loss: 0.40790620264865945\n",
      "24000/49000 loss: 0.45246842013359667\n",
      "26000/49000 loss: 0.4892366562484325\n",
      "28000/49000 loss: 0.43589681143556785\n",
      "30000/49000 loss: 0.44744743705648593\n",
      "32000/49000 loss: 0.46100959894006976\n",
      "34000/49000 loss: 0.4236659016598267\n",
      "36000/49000 loss: 0.4159584521968605\n",
      "38000/49000 loss: 0.4146303165682231\n",
      "40000/49000 loss: 0.35648981305969824\n",
      "42000/49000 loss: 0.3340580575091929\n",
      "44000/49000 loss: 0.5294048174406276\n",
      "46000/49000 loss: 0.419467326648615\n",
      "48000/49000 loss: 0.4347445689343733\n",
      "epoch 4: valid acc = 0.834, new learning rate = 0.00040725312499999993\n",
      "2000/49000 loss: 0.383690419500209\n",
      "4000/49000 loss: 0.4249260383914435\n",
      "6000/49000 loss: 0.5349535848758802\n",
      "8000/49000 loss: 0.4018139520579849\n",
      "10000/49000 loss: 0.3206781455648727\n",
      "12000/49000 loss: 0.35867877552176397\n",
      "14000/49000 loss: 0.40754312847517754\n",
      "16000/49000 loss: 0.3713044394630216\n",
      "18000/49000 loss: 0.4628617517899348\n",
      "20000/49000 loss: 0.545066206310071\n",
      "22000/49000 loss: 0.41364912752389715\n",
      "24000/49000 loss: 0.46679192079243453\n",
      "26000/49000 loss: 0.438660111058653\n",
      "28000/49000 loss: 0.4797315917134304\n",
      "30000/49000 loss: 0.34809202391842703\n",
      "32000/49000 loss: 0.3206654559586058\n",
      "34000/49000 loss: 0.44259376883773693\n",
      "36000/49000 loss: 0.3954823575969284\n",
      "38000/49000 loss: 0.4612976856679583\n",
      "40000/49000 loss: 0.38074697626022314\n",
      "42000/49000 loss: 0.34360084166779353\n",
      "44000/49000 loss: 0.46084577320232245\n",
      "46000/49000 loss: 0.4733646511015335\n",
      "48000/49000 loss: 0.2903182252981028\n",
      "epoch 5: valid acc = 0.854, new learning rate = 0.0003868904687499999\n",
      "2000/49000 loss: 0.38662799120733654\n",
      "4000/49000 loss: 0.44921972518017794\n",
      "6000/49000 loss: 0.4277618118240757\n",
      "8000/49000 loss: 0.36859372363674986\n",
      "10000/49000 loss: 0.35041382604080107\n",
      "12000/49000 loss: 0.3731465947089029\n",
      "14000/49000 loss: 0.434865036766115\n",
      "16000/49000 loss: 0.4864308603368074\n",
      "18000/49000 loss: 0.3752430147732344\n",
      "20000/49000 loss: 0.38504839513504807\n",
      "22000/49000 loss: 0.3407129796997654\n",
      "24000/49000 loss: 0.45946021543050175\n",
      "26000/49000 loss: 0.4522548377467337\n",
      "28000/49000 loss: 0.444139025826812\n",
      "30000/49000 loss: 0.39577157012223846\n",
      "32000/49000 loss: 0.3608535597352367\n",
      "34000/49000 loss: 0.4053397792320656\n",
      "36000/49000 loss: 0.4002335281293656\n",
      "38000/49000 loss: 0.40278209118371483\n",
      "40000/49000 loss: 0.4822053984394551\n",
      "42000/49000 loss: 0.4612200353156419\n",
      "44000/49000 loss: 0.46629655994453023\n",
      "46000/49000 loss: 0.48961587505760734\n",
      "48000/49000 loss: 0.4974089716477944\n",
      "epoch 6: valid acc = 0.863, new learning rate = 0.0003675459453124999\n",
      "2000/49000 loss: 0.3151430458643919\n",
      "4000/49000 loss: 0.4097424976663388\n",
      "6000/49000 loss: 0.36858006241794583\n",
      "8000/49000 loss: 0.3957285257592912\n",
      "10000/49000 loss: 0.3917707585122428\n",
      "12000/49000 loss: 0.39288044752813306\n",
      "14000/49000 loss: 0.47726248486484996\n",
      "16000/49000 loss: 0.3482396784476556\n",
      "18000/49000 loss: 0.3666745989115591\n",
      "20000/49000 loss: 0.443116621927024\n",
      "22000/49000 loss: 0.40487266860934223\n",
      "24000/49000 loss: 0.35233833111426305\n",
      "26000/49000 loss: 0.3802113742076327\n",
      "28000/49000 loss: 0.4649642460143358\n",
      "30000/49000 loss: 0.3687581015945511\n",
      "32000/49000 loss: 0.3501630267992591\n",
      "34000/49000 loss: 0.37136756570855445\n",
      "36000/49000 loss: 0.3863366642991264\n",
      "38000/49000 loss: 0.3908545487230984\n",
      "40000/49000 loss: 0.4040967769118389\n",
      "42000/49000 loss: 0.43555965951598574\n",
      "44000/49000 loss: 0.3484768241603608\n",
      "46000/49000 loss: 0.3943597054756829\n",
      "48000/49000 loss: 0.3357303471152153\n",
      "epoch 7: valid acc = 0.869, new learning rate = 0.00034916864804687486\n",
      "2000/49000 loss: 0.3429243853250668\n",
      "4000/49000 loss: 0.302549529683591\n",
      "6000/49000 loss: 0.3642463255873994\n",
      "8000/49000 loss: 0.4120405762742007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/49000 loss: 0.39132922893342814\n",
      "12000/49000 loss: 0.34978686911320384\n",
      "14000/49000 loss: 0.3611280777871451\n",
      "16000/49000 loss: 0.37277658350079973\n",
      "18000/49000 loss: 0.31096502171401075\n",
      "20000/49000 loss: 0.3179045737456203\n",
      "22000/49000 loss: 0.29473082197430633\n",
      "24000/49000 loss: 0.28207824227860984\n",
      "26000/49000 loss: 0.2887650704186255\n",
      "28000/49000 loss: 0.4719998925067598\n",
      "30000/49000 loss: 0.3458316297303592\n",
      "32000/49000 loss: 0.40011293524721775\n",
      "34000/49000 loss: 0.35827903102072467\n",
      "36000/49000 loss: 0.3333828748506279\n",
      "38000/49000 loss: 0.35405562203755486\n",
      "40000/49000 loss: 0.31803671185659615\n",
      "42000/49000 loss: 0.5322658107636321\n",
      "44000/49000 loss: 0.3204600282376588\n",
      "46000/49000 loss: 0.3591455877191857\n",
      "48000/49000 loss: 0.3502336361635494\n",
      "epoch 8: valid acc = 0.868, new learning rate = 0.0003317102156445311\n",
      "2000/49000 loss: 0.3290279432316845\n",
      "4000/49000 loss: 0.4796941917658327\n",
      "6000/49000 loss: 0.3527096169501515\n",
      "8000/49000 loss: 0.3185694974684926\n",
      "10000/49000 loss: 0.3603239782238215\n",
      "12000/49000 loss: 0.3814547228335069\n",
      "14000/49000 loss: 0.35329405671257985\n",
      "16000/49000 loss: 0.37672042693702884\n",
      "18000/49000 loss: 0.3921157451142016\n",
      "20000/49000 loss: 0.28766987747838363\n",
      "22000/49000 loss: 0.35498668251520016\n",
      "24000/49000 loss: 0.4043410017304823\n",
      "26000/49000 loss: 0.35383905648822056\n",
      "28000/49000 loss: 0.3432046575022118\n",
      "30000/49000 loss: 0.30524112824600647\n",
      "32000/49000 loss: 0.37214344783413683\n",
      "34000/49000 loss: 0.4404961144330799\n",
      "36000/49000 loss: 0.36127600572204116\n",
      "38000/49000 loss: 0.3331274185927954\n",
      "40000/49000 loss: 0.44251343072124416\n",
      "42000/49000 loss: 0.4511884439943265\n",
      "44000/49000 loss: 0.3457035430618179\n",
      "46000/49000 loss: 0.375901682002984\n",
      "48000/49000 loss: 0.29054765104449687\n",
      "epoch 9: valid acc = 0.875, new learning rate = 0.0003151247048623045\n",
      "2000/49000 loss: 0.4332799073757411\n",
      "4000/49000 loss: 0.44814239320559746\n",
      "6000/49000 loss: 0.4255293235629714\n",
      "8000/49000 loss: 0.3545561781797221\n",
      "10000/49000 loss: 0.32403847170077266\n",
      "12000/49000 loss: 0.4024663001206279\n",
      "14000/49000 loss: 0.33851668619063147\n",
      "16000/49000 loss: 0.3281608741076903\n",
      "18000/49000 loss: 0.4104944303658715\n",
      "20000/49000 loss: 0.39271552390747017\n",
      "22000/49000 loss: 0.31101142409262417\n",
      "24000/49000 loss: 0.35578328593084246\n",
      "26000/49000 loss: 0.40082300743559623\n",
      "28000/49000 loss: 0.3970717995844232\n",
      "30000/49000 loss: 0.2354890772459798\n",
      "32000/49000 loss: 0.3452160169164993\n",
      "34000/49000 loss: 0.4343553882758088\n",
      "36000/49000 loss: 0.3694735204733245\n",
      "38000/49000 loss: 0.35375187637046485\n",
      "40000/49000 loss: 0.32010544883582837\n",
      "42000/49000 loss: 0.2696508406495857\n",
      "44000/49000 loss: 0.3984008364691437\n",
      "46000/49000 loss: 0.3680959594642556\n",
      "48000/49000 loss: 0.4221411613521515\n",
      "epoch 10: valid acc = 0.871, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.8722857142857143\n",
      "test acc: 0.871\n",
      "test acc: 0.8514\n",
      "number of batches for training: 245\n",
      "epoch 1: valid acc = 0.729, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.806, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.831, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.837, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.847, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.853, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.859, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.866, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.866, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.876, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.8757142857142857\n",
      "test acc: 0.876\n",
      "test acc: 0.8555\n",
      "number of batches for training: 245\n",
      "2000/49000 loss: 4.080295911595371\n",
      "4000/49000 loss: 3.7932548608634424\n",
      "6000/49000 loss: 2.8314851392942475\n",
      "8000/49000 loss: 2.326847994264107\n",
      "10000/49000 loss: 2.3280576768916523\n",
      "12000/49000 loss: 2.0027608315166794\n",
      "14000/49000 loss: 1.9396095618185143\n",
      "16000/49000 loss: 1.4737152442864465\n",
      "18000/49000 loss: 1.3533105290094105\n",
      "20000/49000 loss: 1.2311488121659198\n",
      "22000/49000 loss: 1.1006212343058859\n",
      "24000/49000 loss: 1.2644415526135224\n",
      "26000/49000 loss: 1.157367577155441\n",
      "28000/49000 loss: 0.9893763136484949\n",
      "30000/49000 loss: 1.0323857072677225\n",
      "32000/49000 loss: 1.004697421943364\n",
      "34000/49000 loss: 0.9156506362197496\n",
      "36000/49000 loss: 0.7547771893926369\n",
      "38000/49000 loss: 0.9649438223808363\n",
      "40000/49000 loss: 0.6979621706079928\n",
      "42000/49000 loss: 0.7134343370233454\n",
      "44000/49000 loss: 0.7319389269524575\n",
      "46000/49000 loss: 0.7493402095252755\n",
      "48000/49000 loss: 0.8630399197547652\n",
      "epoch 1: valid acc = 0.747, new learning rate = 0.000475\n",
      "2000/49000 loss: 0.649670063417627\n",
      "4000/49000 loss: 0.6544344237046555\n",
      "6000/49000 loss: 0.7015728433515889\n",
      "8000/49000 loss: 0.6548901724011916\n",
      "10000/49000 loss: 0.676531848912602\n",
      "12000/49000 loss: 0.5969421794654977\n",
      "14000/49000 loss: 0.6204184801761536\n",
      "16000/49000 loss: 0.5815917039889253\n",
      "18000/49000 loss: 0.6891077882252084\n",
      "20000/49000 loss: 0.6769132485768904\n",
      "22000/49000 loss: 0.6720498510295977\n",
      "24000/49000 loss: 0.5927077340544755\n",
      "26000/49000 loss: 0.5840629279145545\n",
      "28000/49000 loss: 0.5321801413563096\n",
      "30000/49000 loss: 0.5402152746111522\n",
      "32000/49000 loss: 0.5852525437524917\n",
      "34000/49000 loss: 0.634499635092278\n",
      "36000/49000 loss: 0.49399778968646\n",
      "38000/49000 loss: 0.5277669450459388\n",
      "40000/49000 loss: 0.6308031808307819\n",
      "42000/49000 loss: 0.5738918952846002\n",
      "44000/49000 loss: 0.448084291431691\n",
      "46000/49000 loss: 0.5191004624066134\n",
      "48000/49000 loss: 0.5163652291280035\n",
      "epoch 2: valid acc = 0.804, new learning rate = 0.00045125\n",
      "2000/49000 loss: 0.5744676997524228\n",
      "4000/49000 loss: 0.5628454499524886\n",
      "6000/49000 loss: 0.6288219294927663\n",
      "8000/49000 loss: 0.4505885716162608\n",
      "10000/49000 loss: 0.4464533308588416\n",
      "12000/49000 loss: 0.5197947930974688\n",
      "14000/49000 loss: 0.5094396959620403\n",
      "16000/49000 loss: 0.39091383343045566\n",
      "18000/49000 loss: 0.5517725391795644\n",
      "20000/49000 loss: 0.4642161792666003\n",
      "22000/49000 loss: 0.5092811548577773\n",
      "24000/49000 loss: 0.5859633589153215\n",
      "26000/49000 loss: 0.5825157483305647\n",
      "28000/49000 loss: 0.5599922406476289\n",
      "30000/49000 loss: 0.46074210768858304\n",
      "32000/49000 loss: 0.509800803459479\n",
      "34000/49000 loss: 0.5217032695853703\n",
      "36000/49000 loss: 0.4859439093082702\n",
      "38000/49000 loss: 0.49984448709193113\n",
      "40000/49000 loss: 0.4041543795508319\n",
      "42000/49000 loss: 0.41373966207844864\n",
      "44000/49000 loss: 0.5097671058412613\n",
      "46000/49000 loss: 0.4324981654357761\n",
      "48000/49000 loss: 0.503860015181371\n",
      "epoch 3: valid acc = 0.829, new learning rate = 0.0004286875\n",
      "2000/49000 loss: 0.46057445757080423\n",
      "4000/49000 loss: 0.5160489186414627\n",
      "6000/49000 loss: 0.4534929204217339\n",
      "8000/49000 loss: 0.48675999681540494\n",
      "10000/49000 loss: 0.388627412131965\n",
      "12000/49000 loss: 0.4246292345057935\n",
      "14000/49000 loss: 0.4353474463919579\n",
      "16000/49000 loss: 0.43000257582711054\n",
      "18000/49000 loss: 0.4182328148071641\n",
      "20000/49000 loss: 0.5961373827035605\n",
      "22000/49000 loss: 0.45079887414477715\n",
      "24000/49000 loss: 0.48894100956921327\n",
      "26000/49000 loss: 0.4472737804436984\n",
      "28000/49000 loss: 0.3620412808185625\n",
      "30000/49000 loss: 0.3115019741698859\n",
      "32000/49000 loss: 0.45828057258583826\n",
      "34000/49000 loss: 0.5309787687932674\n",
      "36000/49000 loss: 0.4157525439341088\n",
      "38000/49000 loss: 0.4453596482029741\n",
      "40000/49000 loss: 0.433894007232686\n",
      "42000/49000 loss: 0.5113175288654228\n",
      "44000/49000 loss: 0.41770292475648807\n",
      "46000/49000 loss: 0.5817665671769878\n",
      "48000/49000 loss: 0.4924710508463791\n",
      "epoch 4: valid acc = 0.835, new learning rate = 0.00040725312499999993\n",
      "2000/49000 loss: 0.3577083978465077\n",
      "4000/49000 loss: 0.48325290372011204\n",
      "6000/49000 loss: 0.52510427755867\n",
      "8000/49000 loss: 0.4090775174403495\n",
      "10000/49000 loss: 0.4206581704378623\n",
      "12000/49000 loss: 0.37939928023295205\n",
      "14000/49000 loss: 0.4730013069009193\n",
      "16000/49000 loss: 0.350940871113742\n",
      "18000/49000 loss: 0.4589253885492434\n",
      "20000/49000 loss: 0.47045734572810194\n",
      "22000/49000 loss: 0.4647648817583099\n",
      "24000/49000 loss: 0.4529393354467666\n",
      "26000/49000 loss: 0.3879833269015183\n",
      "28000/49000 loss: 0.42242940069168766\n",
      "30000/49000 loss: 0.43097876420224446\n",
      "32000/49000 loss: 0.3811597579946479\n",
      "34000/49000 loss: 0.46677977173880825\n",
      "36000/49000 loss: 0.38894466207924583\n",
      "38000/49000 loss: 0.3901404403211254\n",
      "40000/49000 loss: 0.3746040618141824\n",
      "42000/49000 loss: 0.40690265232534145\n",
      "44000/49000 loss: 0.39289917464016477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46000/49000 loss: 0.38405856722015824\n",
      "48000/49000 loss: 0.4212210433957463\n",
      "epoch 5: valid acc = 0.858, new learning rate = 0.0003868904687499999\n",
      "2000/49000 loss: 0.36902114135425473\n",
      "4000/49000 loss: 0.39960520166276886\n",
      "6000/49000 loss: 0.40023827783932153\n",
      "8000/49000 loss: 0.3462776428402332\n",
      "10000/49000 loss: 0.43845255532745014\n",
      "12000/49000 loss: 0.45374582584414974\n",
      "14000/49000 loss: 0.4460251755885325\n",
      "16000/49000 loss: 0.41960073888514765\n",
      "18000/49000 loss: 0.3600963323971482\n",
      "20000/49000 loss: 0.41852787636612127\n",
      "22000/49000 loss: 0.3437980772579729\n",
      "24000/49000 loss: 0.48562079573408534\n",
      "26000/49000 loss: 0.3977275908434084\n",
      "28000/49000 loss: 0.36890904231611393\n",
      "30000/49000 loss: 0.3652341922395704\n",
      "32000/49000 loss: 0.5013300546868789\n",
      "34000/49000 loss: 0.3253823725276091\n",
      "36000/49000 loss: 0.3935035678693586\n",
      "38000/49000 loss: 0.3164485964013324\n",
      "40000/49000 loss: 0.39434540682639796\n",
      "42000/49000 loss: 0.42558646208949946\n",
      "44000/49000 loss: 0.4200884241659141\n",
      "46000/49000 loss: 0.35264844162521053\n",
      "48000/49000 loss: 0.4397759934804247\n",
      "epoch 6: valid acc = 0.859, new learning rate = 0.0003675459453124999\n",
      "2000/49000 loss: 0.30105542086205106\n",
      "4000/49000 loss: 0.370647623548413\n",
      "6000/49000 loss: 0.35446190971951413\n",
      "8000/49000 loss: 0.37092466736474683\n",
      "10000/49000 loss: 0.48828218278844704\n",
      "12000/49000 loss: 0.5189843090664026\n",
      "14000/49000 loss: 0.32063801073431136\n",
      "16000/49000 loss: 0.39986328900301965\n",
      "18000/49000 loss: 0.33645714054394354\n",
      "20000/49000 loss: 0.3883351422787026\n",
      "22000/49000 loss: 0.4386767899208864\n",
      "24000/49000 loss: 0.36159863523634755\n",
      "26000/49000 loss: 0.3836909725588698\n",
      "28000/49000 loss: 0.4004633877370645\n",
      "30000/49000 loss: 0.3443804838095026\n",
      "32000/49000 loss: 0.451421840760652\n",
      "34000/49000 loss: 0.36599409379298625\n",
      "36000/49000 loss: 0.40302835020161715\n",
      "38000/49000 loss: 0.2277762122899667\n",
      "40000/49000 loss: 0.4650791559384705\n",
      "42000/49000 loss: 0.3586783756749041\n",
      "44000/49000 loss: 0.3470838940118738\n",
      "46000/49000 loss: 0.35115446620005125\n",
      "48000/49000 loss: 0.4254781835567078\n",
      "epoch 7: valid acc = 0.866, new learning rate = 0.00034916864804687486\n",
      "2000/49000 loss: 0.2695375084929684\n",
      "4000/49000 loss: 0.316717288667765\n",
      "6000/49000 loss: 0.37570529527656066\n",
      "8000/49000 loss: 0.45826740835399454\n",
      "10000/49000 loss: 0.3487630936545701\n",
      "12000/49000 loss: 0.36341303118122986\n",
      "14000/49000 loss: 0.38964135394919347\n",
      "16000/49000 loss: 0.4015239380788347\n",
      "18000/49000 loss: 0.3334334660735411\n",
      "20000/49000 loss: 0.39308233043349333\n",
      "22000/49000 loss: 0.35867349585004776\n",
      "24000/49000 loss: 0.3424953439822855\n",
      "26000/49000 loss: 0.3511287540638602\n",
      "28000/49000 loss: 0.30920434727062407\n",
      "30000/49000 loss: 0.31283927522244415\n",
      "32000/49000 loss: 0.3053738619319927\n",
      "34000/49000 loss: 0.48956382380813407\n",
      "36000/49000 loss: 0.4263708475301383\n",
      "38000/49000 loss: 0.36131307377204575\n",
      "40000/49000 loss: 0.35612095194013665\n",
      "42000/49000 loss: 0.28261701879092277\n",
      "44000/49000 loss: 0.414550782889921\n",
      "46000/49000 loss: 0.338161975007718\n",
      "48000/49000 loss: 0.3818993424278119\n",
      "epoch 8: valid acc = 0.867, new learning rate = 0.0003317102156445311\n",
      "2000/49000 loss: 0.31869103871650983\n",
      "4000/49000 loss: 0.33160413159972446\n",
      "6000/49000 loss: 0.337306938362961\n",
      "8000/49000 loss: 0.45479605021195585\n",
      "10000/49000 loss: 0.3628464696540274\n",
      "12000/49000 loss: 0.29474533648968837\n",
      "14000/49000 loss: 0.3900442581888113\n",
      "16000/49000 loss: 0.3437237936352825\n",
      "18000/49000 loss: 0.33496765999138167\n",
      "20000/49000 loss: 0.2647296146004951\n",
      "22000/49000 loss: 0.36744895751344475\n",
      "24000/49000 loss: 0.39260986595386094\n",
      "26000/49000 loss: 0.33249509666557975\n",
      "28000/49000 loss: 0.39426803447403314\n",
      "30000/49000 loss: 0.4054589168114594\n",
      "32000/49000 loss: 0.40941504207267043\n",
      "34000/49000 loss: 0.28339894697608325\n",
      "36000/49000 loss: 0.40746648112543704\n",
      "38000/49000 loss: 0.4493771952264081\n",
      "40000/49000 loss: 0.36742485310081996\n",
      "42000/49000 loss: 0.34032997245122776\n",
      "44000/49000 loss: 0.23606668458746635\n",
      "46000/49000 loss: 0.3150622897961491\n",
      "48000/49000 loss: 0.2967634039224679\n",
      "epoch 9: valid acc = 0.865, new learning rate = 0.0003151247048623045\n",
      "2000/49000 loss: 0.32229291130913307\n",
      "4000/49000 loss: 0.3435800322171073\n",
      "6000/49000 loss: 0.3567272932372292\n",
      "8000/49000 loss: 0.3222419337449694\n",
      "10000/49000 loss: 0.4203328914856522\n",
      "12000/49000 loss: 0.3727390256097687\n",
      "14000/49000 loss: 0.2801008134499255\n",
      "16000/49000 loss: 0.3135277862189691\n",
      "18000/49000 loss: 0.43406558231153686\n",
      "20000/49000 loss: 0.4389483569014274\n",
      "22000/49000 loss: 0.29942914904020224\n",
      "24000/49000 loss: 0.41806416730384105\n",
      "26000/49000 loss: 0.42053290949683936\n",
      "28000/49000 loss: 0.3356065353328509\n",
      "30000/49000 loss: 0.38710532835356354\n",
      "32000/49000 loss: 0.3622560961115989\n",
      "34000/49000 loss: 0.30435876122148325\n",
      "36000/49000 loss: 0.2968037174183487\n",
      "38000/49000 loss: 0.42648134696918516\n",
      "40000/49000 loss: 0.4081884820762886\n",
      "42000/49000 loss: 0.33792856655794473\n",
      "44000/49000 loss: 0.34682214668386185\n",
      "46000/49000 loss: 0.3331169046073635\n",
      "48000/49000 loss: 0.33987206614572374\n",
      "epoch 10: valid acc = 0.878, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.8758571428571429\n",
      "test acc: 0.878\n",
      "test acc: 0.8562\n",
      "number of batches for training: 245\n",
      "epoch 1: valid acc = 0.745, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.811, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.833, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.836, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.847, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.848, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.86, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.87, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.867, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.875, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.8765306122448979\n",
      "test acc: 0.875\n",
      "test acc: 0.8559\n",
      "number of batches for training: 245\n",
      "2000/49000 loss: 3.881628925386565\n",
      "4000/49000 loss: 3.450540245287067\n",
      "6000/49000 loss: 2.905861613886497\n",
      "8000/49000 loss: 2.450985720374054\n",
      "10000/49000 loss: 2.275917845444416\n",
      "12000/49000 loss: 2.0152818415567184\n",
      "14000/49000 loss: 1.9808993754271826\n",
      "16000/49000 loss: 1.6565640200587972\n",
      "18000/49000 loss: 1.2202576138859296\n",
      "20000/49000 loss: 1.3503708040278841\n",
      "22000/49000 loss: 1.3126932427447482\n",
      "24000/49000 loss: 1.387823755801765\n",
      "26000/49000 loss: 1.1022169507242296\n",
      "28000/49000 loss: 1.1400791939693147\n",
      "30000/49000 loss: 1.0698738688796166\n",
      "32000/49000 loss: 0.9599577951307295\n",
      "34000/49000 loss: 0.9510284575269284\n",
      "36000/49000 loss: 0.883666917478124\n",
      "38000/49000 loss: 0.7665799425426825\n",
      "40000/49000 loss: 0.8919095656029873\n",
      "42000/49000 loss: 0.9105752263203515\n",
      "44000/49000 loss: 0.7548836842585231\n",
      "46000/49000 loss: 0.7292023416578334\n",
      "48000/49000 loss: 0.7219300671325277\n",
      "epoch 1: valid acc = 0.735, new learning rate = 0.000475\n",
      "2000/49000 loss: 0.7150121091273526\n",
      "4000/49000 loss: 0.6230187736908971\n",
      "6000/49000 loss: 0.7251517187090601\n",
      "8000/49000 loss: 0.6019427266672777\n",
      "10000/49000 loss: 0.7466778580639658\n",
      "12000/49000 loss: 0.677735132427495\n",
      "14000/49000 loss: 0.60855784179997\n",
      "16000/49000 loss: 0.5856467067143878\n",
      "18000/49000 loss: 0.6009727237134257\n",
      "20000/49000 loss: 0.468983200133492\n",
      "22000/49000 loss: 0.5327409616011275\n",
      "24000/49000 loss: 0.6177364745610907\n",
      "26000/49000 loss: 0.574783461400896\n",
      "28000/49000 loss: 0.6577982077804732\n",
      "30000/49000 loss: 0.608763544142451\n",
      "32000/49000 loss: 0.5872872731436156\n",
      "34000/49000 loss: 0.6481252297822103\n",
      "36000/49000 loss: 0.5184238421011499\n",
      "38000/49000 loss: 0.5205144004193347\n",
      "40000/49000 loss: 0.46104915216560893\n",
      "42000/49000 loss: 0.6418592648791144\n",
      "44000/49000 loss: 0.6303940033319828\n",
      "46000/49000 loss: 0.5771736397380208\n",
      "48000/49000 loss: 0.4794607353635898\n",
      "epoch 2: valid acc = 0.794, new learning rate = 0.00045125\n",
      "2000/49000 loss: 0.5976695149657439\n",
      "4000/49000 loss: 0.546220976208757\n",
      "6000/49000 loss: 0.5402464994809123\n",
      "8000/49000 loss: 0.46890314073862166\n",
      "10000/49000 loss: 0.41070664037400423\n",
      "12000/49000 loss: 0.42894925563682\n",
      "14000/49000 loss: 0.3782809122054163\n",
      "16000/49000 loss: 0.531960178470558\n",
      "18000/49000 loss: 0.49787627868633066\n",
      "20000/49000 loss: 0.5652025160924876\n",
      "22000/49000 loss: 0.4900280573373764\n",
      "24000/49000 loss: 0.47113756412449637\n",
      "26000/49000 loss: 0.47643312572799634\n",
      "28000/49000 loss: 0.5668540252104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000/49000 loss: 0.5123753256592025\n",
      "32000/49000 loss: 0.46538061942371445\n",
      "34000/49000 loss: 0.47971525328078074\n",
      "36000/49000 loss: 0.5467824050833144\n",
      "38000/49000 loss: 0.5076325731733641\n",
      "40000/49000 loss: 0.45293001767194124\n",
      "42000/49000 loss: 0.40818625487198096\n",
      "44000/49000 loss: 0.4623961517108986\n",
      "46000/49000 loss: 0.5146801752868709\n",
      "48000/49000 loss: 0.3895065448983014\n",
      "epoch 3: valid acc = 0.824, new learning rate = 0.0004286875\n",
      "2000/49000 loss: 0.39359004959298316\n",
      "4000/49000 loss: 0.44672691113268626\n",
      "6000/49000 loss: 0.4955585596862022\n",
      "8000/49000 loss: 0.3896366420103997\n",
      "10000/49000 loss: 0.5177091588286489\n",
      "12000/49000 loss: 0.4638764194593315\n",
      "14000/49000 loss: 0.4680714729791093\n",
      "16000/49000 loss: 0.4357219923527393\n",
      "18000/49000 loss: 0.3960286380292209\n",
      "20000/49000 loss: 0.46178415151242086\n",
      "22000/49000 loss: 0.4850777265767427\n",
      "24000/49000 loss: 0.42929558786692573\n",
      "26000/49000 loss: 0.46625451030998394\n",
      "28000/49000 loss: 0.42676078188037186\n",
      "30000/49000 loss: 0.42036925667524416\n",
      "32000/49000 loss: 0.5634147471002582\n",
      "34000/49000 loss: 0.4302697417274245\n",
      "36000/49000 loss: 0.43925056352591213\n",
      "38000/49000 loss: 0.4346531570030053\n",
      "40000/49000 loss: 0.4237064317278564\n",
      "42000/49000 loss: 0.4113082085826452\n",
      "44000/49000 loss: 0.4806748076618154\n",
      "46000/49000 loss: 0.3646854846091127\n",
      "48000/49000 loss: 0.4349504106183462\n",
      "epoch 4: valid acc = 0.84, new learning rate = 0.00040725312499999993\n",
      "2000/49000 loss: 0.4380521774723985\n",
      "4000/49000 loss: 0.39748049355867665\n",
      "6000/49000 loss: 0.38623577782026547\n",
      "8000/49000 loss: 0.5276315625116487\n",
      "10000/49000 loss: 0.3179910369223497\n",
      "12000/49000 loss: 0.4784204972922922\n",
      "14000/49000 loss: 0.47107821695073104\n",
      "16000/49000 loss: 0.3362081724304482\n",
      "18000/49000 loss: 0.4126520005621797\n",
      "20000/49000 loss: 0.35779286223112516\n",
      "22000/49000 loss: 0.51157869944477\n",
      "24000/49000 loss: 0.38049132619952486\n",
      "26000/49000 loss: 0.4895595832472168\n",
      "28000/49000 loss: 0.3079087703378806\n",
      "30000/49000 loss: 0.44909392149027394\n",
      "32000/49000 loss: 0.4265412120909318\n",
      "34000/49000 loss: 0.383787878782843\n",
      "36000/49000 loss: 0.3603201607733334\n",
      "38000/49000 loss: 0.4336067365898201\n",
      "40000/49000 loss: 0.42247956682059595\n",
      "42000/49000 loss: 0.34351650704439757\n",
      "44000/49000 loss: 0.4217285767153962\n",
      "46000/49000 loss: 0.4311215481004108\n",
      "48000/49000 loss: 0.4080053229395087\n",
      "epoch 5: valid acc = 0.852, new learning rate = 0.0003868904687499999\n",
      "2000/49000 loss: 0.4080165361259902\n",
      "4000/49000 loss: 0.451191277517942\n",
      "6000/49000 loss: 0.5689229381406238\n",
      "8000/49000 loss: 0.3597122868492424\n",
      "10000/49000 loss: 0.39591844698164597\n",
      "12000/49000 loss: 0.4090985732841168\n",
      "14000/49000 loss: 0.34057912651827854\n",
      "16000/49000 loss: 0.5280145447971863\n",
      "18000/49000 loss: 0.49875906811097076\n",
      "20000/49000 loss: 0.46227167390549984\n",
      "22000/49000 loss: 0.45364647639429956\n",
      "24000/49000 loss: 0.35983309319635215\n",
      "26000/49000 loss: 0.418808529246314\n",
      "28000/49000 loss: 0.3239008147551209\n",
      "30000/49000 loss: 0.37940932523752313\n",
      "32000/49000 loss: 0.44064298928951806\n",
      "34000/49000 loss: 0.3832209608579786\n",
      "36000/49000 loss: 0.44601205965405866\n",
      "38000/49000 loss: 0.40862010781561015\n",
      "40000/49000 loss: 0.49194644945534005\n",
      "42000/49000 loss: 0.4580483843453437\n",
      "44000/49000 loss: 0.4575790119446264\n",
      "46000/49000 loss: 0.4077470643224274\n",
      "48000/49000 loss: 0.4500721386294742\n",
      "epoch 6: valid acc = 0.852, new learning rate = 0.0003675459453124999\n",
      "2000/49000 loss: 0.4401575571919514\n",
      "4000/49000 loss: 0.3870291428309077\n",
      "6000/49000 loss: 0.3557560897808806\n",
      "8000/49000 loss: 0.3869021556181717\n",
      "10000/49000 loss: 0.41386240377260497\n",
      "12000/49000 loss: 0.3911559690685819\n",
      "14000/49000 loss: 0.3717074857013698\n",
      "16000/49000 loss: 0.5051861312149984\n",
      "18000/49000 loss: 0.34090218622967783\n",
      "20000/49000 loss: 0.356831449665841\n",
      "22000/49000 loss: 0.34934804113631995\n",
      "24000/49000 loss: 0.3535002238387471\n",
      "26000/49000 loss: 0.3820399461587105\n",
      "28000/49000 loss: 0.38591741122024525\n",
      "30000/49000 loss: 0.259289953525877\n",
      "32000/49000 loss: 0.42916906962083623\n",
      "34000/49000 loss: 0.3149648508154955\n",
      "36000/49000 loss: 0.36028328661575915\n",
      "38000/49000 loss: 0.3218486532009275\n",
      "40000/49000 loss: 0.33277378193376506\n",
      "42000/49000 loss: 0.3098931973443797\n",
      "44000/49000 loss: 0.34162324353371654\n",
      "46000/49000 loss: 0.3901622087422206\n",
      "48000/49000 loss: 0.3198083553148662\n",
      "epoch 7: valid acc = 0.868, new learning rate = 0.00034916864804687486\n",
      "2000/49000 loss: 0.36112368845072873\n",
      "4000/49000 loss: 0.34993728101472465\n",
      "6000/49000 loss: 0.37473227487330724\n",
      "8000/49000 loss: 0.3342875223546431\n",
      "10000/49000 loss: 0.43347821050687574\n",
      "12000/49000 loss: 0.4088350199762695\n",
      "14000/49000 loss: 0.2993952883047825\n",
      "16000/49000 loss: 0.5462891987900553\n",
      "18000/49000 loss: 0.2969631455521391\n",
      "20000/49000 loss: 0.38159298812034553\n",
      "22000/49000 loss: 0.35126631629688243\n",
      "24000/49000 loss: 0.41306530526464885\n",
      "26000/49000 loss: 0.4004461956443028\n",
      "28000/49000 loss: 0.301211284806288\n",
      "30000/49000 loss: 0.3111907061326364\n",
      "32000/49000 loss: 0.3910778013589269\n",
      "34000/49000 loss: 0.3070775020123246\n",
      "36000/49000 loss: 0.3120569673757823\n",
      "38000/49000 loss: 0.33538129921186755\n",
      "40000/49000 loss: 0.3787214044629924\n",
      "42000/49000 loss: 0.3074589055419733\n",
      "44000/49000 loss: 0.43481335165681195\n",
      "46000/49000 loss: 0.39481372397476794\n",
      "48000/49000 loss: 0.402221387425007\n",
      "epoch 8: valid acc = 0.867, new learning rate = 0.0003317102156445311\n",
      "2000/49000 loss: 0.3876115334376109\n",
      "4000/49000 loss: 0.36422035249581947\n",
      "6000/49000 loss: 0.36796061827089793\n",
      "8000/49000 loss: 0.4178817199374677\n",
      "10000/49000 loss: 0.3841876732043703\n",
      "12000/49000 loss: 0.38112293713447143\n",
      "14000/49000 loss: 0.36425659908739794\n",
      "16000/49000 loss: 0.3814362937732409\n",
      "18000/49000 loss: 0.3312303269929753\n",
      "20000/49000 loss: 0.3656415149557996\n",
      "22000/49000 loss: 0.3827505708364045\n",
      "24000/49000 loss: 0.357462542382898\n",
      "26000/49000 loss: 0.42262924530036683\n",
      "28000/49000 loss: 0.35665395332572686\n",
      "30000/49000 loss: 0.3158721011014964\n",
      "32000/49000 loss: 0.3286401493825325\n",
      "34000/49000 loss: 0.36423673924543865\n",
      "36000/49000 loss: 0.3648750734005621\n",
      "38000/49000 loss: 0.34973396169755505\n",
      "40000/49000 loss: 0.4447936426785598\n",
      "42000/49000 loss: 0.378897616106459\n",
      "44000/49000 loss: 0.30384240353707476\n",
      "46000/49000 loss: 0.29902565133710185\n",
      "48000/49000 loss: 0.4096256778053656\n",
      "epoch 9: valid acc = 0.873, new learning rate = 0.0003151247048623045\n",
      "2000/49000 loss: 0.40123011979180484\n",
      "4000/49000 loss: 0.30655190394231324\n",
      "6000/49000 loss: 0.3885599714454226\n",
      "8000/49000 loss: 0.3538871729323361\n",
      "10000/49000 loss: 0.35949611166442635\n",
      "12000/49000 loss: 0.3174303791943953\n",
      "14000/49000 loss: 0.4866108510526746\n",
      "16000/49000 loss: 0.25845595650838754\n",
      "18000/49000 loss: 0.3728502836779001\n",
      "20000/49000 loss: 0.31840443224185133\n",
      "22000/49000 loss: 0.32451820972863127\n",
      "24000/49000 loss: 0.44681081575182\n",
      "26000/49000 loss: 0.36237328528335594\n",
      "28000/49000 loss: 0.3778767763127231\n",
      "30000/49000 loss: 0.3941856580009413\n",
      "32000/49000 loss: 0.32707477812515423\n",
      "34000/49000 loss: 0.3430148414919057\n",
      "36000/49000 loss: 0.37826357144672157\n",
      "38000/49000 loss: 0.3015104939592968\n",
      "40000/49000 loss: 0.33847845478789695\n",
      "42000/49000 loss: 0.3505843944104064\n",
      "44000/49000 loss: 0.3623737052645875\n",
      "46000/49000 loss: 0.32051500987472026\n",
      "48000/49000 loss: 0.33236292821003427\n",
      "epoch 10: valid acc = 0.869, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.873734693877551\n",
      "test acc: 0.869\n",
      "test acc: 0.853\n",
      "number of batches for training: 245\n",
      "epoch 1: valid acc = 0.746, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.797, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.834, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.843, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.854, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.852, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.862, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.858, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.873, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.881, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.8746734693877551\n",
      "test acc: 0.881\n",
      "test acc: 0.8555\n",
      "number of batches for training: 81\n",
      "6000/49000 loss: 3.3933347886367344\n",
      "12000/49000 loss: 2.978316094964089\n",
      "18000/49000 loss: 2.8236639695069132\n",
      "24000/49000 loss: 2.575695309961906\n",
      "30000/49000 loss: 2.1628976585477866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36000/49000 loss: 2.178678007672106\n",
      "42000/49000 loss: 1.979517821745601\n",
      "48000/49000 loss: 1.6993527858749342\n",
      "epoch 1: valid acc = 0.511, new learning rate = 0.000475\n",
      "6000/49000 loss: 1.399689772961316\n",
      "12000/49000 loss: 1.3711593434312812\n",
      "18000/49000 loss: 1.3287012576869974\n",
      "24000/49000 loss: 1.1256339668057482\n",
      "30000/49000 loss: 1.1330748405290265\n",
      "36000/49000 loss: 1.0587318405536794\n",
      "42000/49000 loss: 1.0464934039668528\n",
      "48000/49000 loss: 1.0220465434282944\n",
      "epoch 2: valid acc = 0.672, new learning rate = 0.00045125\n",
      "6000/49000 loss: 0.9705412826532208\n",
      "12000/49000 loss: 0.981640713497121\n",
      "18000/49000 loss: 0.9155210163827585\n",
      "24000/49000 loss: 0.8405146885246491\n",
      "30000/49000 loss: 0.827503971559366\n",
      "36000/49000 loss: 0.7932959004260872\n",
      "42000/49000 loss: 0.8067107441651\n",
      "48000/49000 loss: 0.7228107187382472\n",
      "epoch 3: valid acc = 0.744, new learning rate = 0.0004286875\n",
      "6000/49000 loss: 0.6657553989245275\n",
      "12000/49000 loss: 0.7277537254293458\n",
      "18000/49000 loss: 0.6848948192276797\n",
      "24000/49000 loss: 0.6442413033691103\n",
      "30000/49000 loss: 0.6846832694320784\n",
      "36000/49000 loss: 0.6273461300058558\n",
      "42000/49000 loss: 0.6544222315196583\n",
      "48000/49000 loss: 0.5837187097772086\n",
      "epoch 4: valid acc = 0.76, new learning rate = 0.00040725312499999993\n",
      "6000/49000 loss: 0.6544898786043624\n",
      "12000/49000 loss: 0.6249589698960187\n",
      "18000/49000 loss: 0.6096963532261627\n",
      "24000/49000 loss: 0.5889134707564601\n",
      "30000/49000 loss: 0.5758392214781992\n",
      "36000/49000 loss: 0.5776525033954011\n",
      "42000/49000 loss: 0.6013553646525528\n",
      "48000/49000 loss: 0.5752917253521189\n",
      "epoch 5: valid acc = 0.785, new learning rate = 0.0003868904687499999\n",
      "6000/49000 loss: 0.5743562478626247\n",
      "12000/49000 loss: 0.4806001761495288\n",
      "18000/49000 loss: 0.6160249045148585\n",
      "24000/49000 loss: 0.5753475341020718\n",
      "30000/49000 loss: 0.575024100374268\n",
      "36000/49000 loss: 0.547301816706652\n",
      "42000/49000 loss: 0.5767788649432425\n",
      "48000/49000 loss: 0.5173591408898026\n",
      "epoch 6: valid acc = 0.801, new learning rate = 0.0003675459453124999\n",
      "6000/49000 loss: 0.5468080463028849\n",
      "12000/49000 loss: 0.5738265105857512\n",
      "18000/49000 loss: 0.5560169028593723\n",
      "24000/49000 loss: 0.532790328131407\n",
      "30000/49000 loss: 0.5643242572384467\n",
      "36000/49000 loss: 0.5085684431840714\n",
      "42000/49000 loss: 0.4348971600400814\n",
      "48000/49000 loss: 0.5189275602232412\n",
      "epoch 7: valid acc = 0.805, new learning rate = 0.00034916864804687486\n",
      "6000/49000 loss: 0.47863475561795427\n",
      "12000/49000 loss: 0.5072017300353322\n",
      "18000/49000 loss: 0.553857187799748\n",
      "24000/49000 loss: 0.5424037186532694\n",
      "30000/49000 loss: 0.471443313564754\n",
      "36000/49000 loss: 0.5800102444700344\n",
      "42000/49000 loss: 0.4818506543864113\n",
      "48000/49000 loss: 0.5521025296122997\n",
      "epoch 8: valid acc = 0.82, new learning rate = 0.0003317102156445311\n",
      "6000/49000 loss: 0.5026234587142308\n",
      "12000/49000 loss: 0.47467866137774944\n",
      "18000/49000 loss: 0.48130586021340127\n",
      "24000/49000 loss: 0.4889316177495557\n",
      "30000/49000 loss: 0.5242970454770167\n",
      "36000/49000 loss: 0.48550858042856293\n",
      "42000/49000 loss: 0.47263259792589835\n",
      "48000/49000 loss: 0.4850685476315972\n",
      "epoch 9: valid acc = 0.822, new learning rate = 0.0003151247048623045\n",
      "6000/49000 loss: 0.5063824631593208\n",
      "12000/49000 loss: 0.5061208242360442\n",
      "18000/49000 loss: 0.5001200601813838\n",
      "24000/49000 loss: 0.46184196858611104\n",
      "30000/49000 loss: 0.5628794052580823\n",
      "36000/49000 loss: 0.5122908162689398\n",
      "42000/49000 loss: 0.4259904719485312\n",
      "48000/49000 loss: 0.5070092147016491\n",
      "epoch 10: valid acc = 0.829, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.835\n",
      "test acc: 0.829\n",
      "test acc: 0.8195\n",
      "number of batches for training: 81\n",
      "epoch 1: valid acc = 0.516, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.67, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.73, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.757, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.788, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.804, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.803, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.819, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.815, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.829, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.836265306122449\n",
      "test acc: 0.829\n",
      "test acc: 0.8206\n",
      "number of batches for training: 81\n",
      "6000/49000 loss: 3.1207259779165906\n",
      "12000/49000 loss: 3.2394971667304326\n",
      "18000/49000 loss: 2.6769685330136253\n",
      "24000/49000 loss: 2.886009454755359\n",
      "30000/49000 loss: 2.4568666905406182\n",
      "36000/49000 loss: 2.266367145319148\n",
      "42000/49000 loss: 2.0325025989068153\n",
      "48000/49000 loss: 1.7001766087603916\n",
      "epoch 1: valid acc = 0.523, new learning rate = 0.000475\n",
      "6000/49000 loss: 1.3952671283060198\n",
      "12000/49000 loss: 1.3422426262185787\n",
      "18000/49000 loss: 1.2180057895746743\n",
      "24000/49000 loss: 1.2402615708791664\n",
      "30000/49000 loss: 1.1697558341784269\n",
      "36000/49000 loss: 1.0849136343933634\n",
      "42000/49000 loss: 1.0450130024449136\n",
      "48000/49000 loss: 0.9709387282291002\n",
      "epoch 2: valid acc = 0.655, new learning rate = 0.00045125\n",
      "6000/49000 loss: 0.9476695935200877\n",
      "12000/49000 loss: 0.882770745067298\n",
      "18000/49000 loss: 0.928761819436153\n",
      "24000/49000 loss: 0.9064879920427422\n",
      "30000/49000 loss: 0.8055055784370011\n",
      "36000/49000 loss: 0.826458546796659\n",
      "42000/49000 loss: 0.8500819540687364\n",
      "48000/49000 loss: 0.8014414276982942\n",
      "epoch 3: valid acc = 0.738, new learning rate = 0.0004286875\n",
      "6000/49000 loss: 0.7327151486225817\n",
      "12000/49000 loss: 0.7735081499599304\n",
      "18000/49000 loss: 0.6497405261765078\n",
      "24000/49000 loss: 0.711692873803761\n",
      "30000/49000 loss: 0.7530699739463184\n",
      "36000/49000 loss: 0.6914697879950382\n",
      "42000/49000 loss: 0.6227691569186924\n",
      "48000/49000 loss: 0.6348887136956166\n",
      "epoch 4: valid acc = 0.764, new learning rate = 0.00040725312499999993\n",
      "6000/49000 loss: 0.6342252377828089\n",
      "12000/49000 loss: 0.6207826554465108\n",
      "18000/49000 loss: 0.573547008717759\n",
      "24000/49000 loss: 0.6524131997338567\n",
      "30000/49000 loss: 0.570426138095491\n",
      "36000/49000 loss: 0.5986358953516914\n",
      "42000/49000 loss: 0.5935030457919839\n",
      "48000/49000 loss: 0.6539711169789069\n",
      "epoch 5: valid acc = 0.782, new learning rate = 0.0003868904687499999\n",
      "6000/49000 loss: 0.558911245557464\n",
      "12000/49000 loss: 0.581396139817691\n",
      "18000/49000 loss: 0.587908278730255\n",
      "24000/49000 loss: 0.5937024413500097\n",
      "30000/49000 loss: 0.5623323388621436\n",
      "36000/49000 loss: 0.5763491198936104\n",
      "42000/49000 loss: 0.5696879975989706\n",
      "48000/49000 loss: 0.5800635639000785\n",
      "epoch 6: valid acc = 0.795, new learning rate = 0.0003675459453124999\n",
      "6000/49000 loss: 0.5119162211626923\n",
      "12000/49000 loss: 0.5610835330151019\n",
      "18000/49000 loss: 0.4891209337821315\n",
      "24000/49000 loss: 0.5110360081235001\n",
      "30000/49000 loss: 0.5060093319508607\n",
      "36000/49000 loss: 0.5088358978421272\n",
      "42000/49000 loss: 0.5434847350704841\n",
      "48000/49000 loss: 0.5843306854153143\n",
      "epoch 7: valid acc = 0.805, new learning rate = 0.00034916864804687486\n",
      "6000/49000 loss: 0.5031594822383517\n",
      "12000/49000 loss: 0.4044113370978936\n",
      "18000/49000 loss: 0.4918964847311723\n",
      "24000/49000 loss: 0.5225869660385696\n",
      "30000/49000 loss: 0.49979339513352256\n",
      "36000/49000 loss: 0.5110063130737965\n",
      "42000/49000 loss: 0.5450587533082952\n",
      "48000/49000 loss: 0.5334121580742057\n",
      "epoch 8: valid acc = 0.812, new learning rate = 0.0003317102156445311\n",
      "6000/49000 loss: 0.48755222789828206\n",
      "12000/49000 loss: 0.519813501504063\n",
      "18000/49000 loss: 0.5098319166951657\n",
      "24000/49000 loss: 0.43836214319090144\n",
      "30000/49000 loss: 0.4911299648020387\n",
      "36000/49000 loss: 0.47939933877246\n",
      "42000/49000 loss: 0.486632657905966\n",
      "48000/49000 loss: 0.5100558622426715\n",
      "epoch 9: valid acc = 0.82, new learning rate = 0.0003151247048623045\n",
      "6000/49000 loss: 0.47447506632320724\n",
      "12000/49000 loss: 0.4965842851500477\n",
      "18000/49000 loss: 0.47003459466138037\n",
      "24000/49000 loss: 0.48202703173826633\n",
      "30000/49000 loss: 0.5023042272335968\n",
      "36000/49000 loss: 0.48235721195083886\n",
      "42000/49000 loss: 0.491967423290972\n",
      "48000/49000 loss: 0.448103703462305\n",
      "epoch 10: valid acc = 0.822, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.8338979591836735\n",
      "test acc: 0.822\n",
      "test acc: 0.8187\n",
      "number of batches for training: 81\n",
      "epoch 1: valid acc = 0.508, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.665, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.741, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.753, new learning rate = 0.00040725312499999993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5: valid acc = 0.775, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.791, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.805, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.811, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.817, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.822, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.8348979591836735\n",
      "test acc: 0.822\n",
      "test acc: 0.8203\n",
      "number of batches for training: 81\n",
      "6000/49000 loss: 4.422427106877379\n",
      "12000/49000 loss: 2.6857565247619815\n",
      "18000/49000 loss: 2.9254677240239304\n",
      "24000/49000 loss: 2.3790795391491892\n",
      "30000/49000 loss: 2.2306665037257143\n",
      "36000/49000 loss: 1.9803034740695373\n",
      "42000/49000 loss: 1.9631778617143285\n",
      "48000/49000 loss: 1.5165877385811393\n",
      "epoch 1: valid acc = 0.543, new learning rate = 0.000475\n",
      "6000/49000 loss: 1.3633293483299682\n",
      "12000/49000 loss: 1.3687502723262217\n",
      "18000/49000 loss: 1.1327481862065687\n",
      "24000/49000 loss: 1.1679786588996925\n",
      "30000/49000 loss: 1.0696209172295574\n",
      "36000/49000 loss: 1.08962156730417\n",
      "42000/49000 loss: 1.0197124177386137\n",
      "48000/49000 loss: 1.0763016149941176\n",
      "epoch 2: valid acc = 0.664, new learning rate = 0.00045125\n",
      "6000/49000 loss: 1.008039905209164\n",
      "12000/49000 loss: 0.9281838291018298\n",
      "18000/49000 loss: 0.8735548018074899\n",
      "24000/49000 loss: 0.9015480561592377\n",
      "30000/49000 loss: 0.828923930681913\n",
      "36000/49000 loss: 0.7998280477038002\n",
      "42000/49000 loss: 0.7925617143211594\n",
      "48000/49000 loss: 0.8398901758816699\n",
      "epoch 3: valid acc = 0.733, new learning rate = 0.0004286875\n",
      "6000/49000 loss: 0.726876469631561\n",
      "12000/49000 loss: 0.7462809036797707\n",
      "18000/49000 loss: 0.6794141070188714\n",
      "24000/49000 loss: 0.72669823725795\n",
      "30000/49000 loss: 0.6896328441652299\n",
      "36000/49000 loss: 0.6631180110588919\n",
      "42000/49000 loss: 0.6540841549964821\n",
      "48000/49000 loss: 0.6626171968244059\n",
      "epoch 4: valid acc = 0.768, new learning rate = 0.00040725312499999993\n",
      "6000/49000 loss: 0.6511899044842354\n",
      "12000/49000 loss: 0.5964151059959552\n",
      "18000/49000 loss: 0.6377048912778623\n",
      "24000/49000 loss: 0.58273316320092\n",
      "30000/49000 loss: 0.6225696297281632\n",
      "36000/49000 loss: 0.5680504173419838\n",
      "42000/49000 loss: 0.5966111139927172\n",
      "48000/49000 loss: 0.6832074533474809\n",
      "epoch 5: valid acc = 0.786, new learning rate = 0.0003868904687499999\n",
      "6000/49000 loss: 0.577485596825072\n",
      "12000/49000 loss: 0.6036230348055731\n",
      "18000/49000 loss: 0.6073694192686541\n",
      "24000/49000 loss: 0.5215264034044581\n",
      "30000/49000 loss: 0.5271534464191694\n",
      "36000/49000 loss: 0.5574825802597027\n",
      "42000/49000 loss: 0.5418083787543327\n",
      "48000/49000 loss: 0.5385826094314221\n",
      "epoch 6: valid acc = 0.801, new learning rate = 0.0003675459453124999\n",
      "6000/49000 loss: 0.4678144594526981\n",
      "12000/49000 loss: 0.569927544875569\n",
      "18000/49000 loss: 0.5427241651422368\n",
      "24000/49000 loss: 0.5224614412177473\n",
      "30000/49000 loss: 0.5583442264455559\n",
      "36000/49000 loss: 0.5346921559653954\n",
      "42000/49000 loss: 0.48913334765421596\n",
      "48000/49000 loss: 0.5431269833721813\n",
      "epoch 7: valid acc = 0.809, new learning rate = 0.00034916864804687486\n",
      "6000/49000 loss: 0.5526783490749821\n",
      "12000/49000 loss: 0.5108562750599187\n",
      "18000/49000 loss: 0.524492583923889\n",
      "24000/49000 loss: 0.5348056687484957\n",
      "30000/49000 loss: 0.5406016297954258\n",
      "36000/49000 loss: 0.5009127119997828\n",
      "42000/49000 loss: 0.4861574696297054\n",
      "48000/49000 loss: 0.5583001353263041\n",
      "epoch 8: valid acc = 0.819, new learning rate = 0.0003317102156445311\n",
      "6000/49000 loss: 0.5323072599965443\n",
      "12000/49000 loss: 0.5452681322514773\n",
      "18000/49000 loss: 0.5096266373064252\n",
      "24000/49000 loss: 0.49593704974387254\n",
      "30000/49000 loss: 0.5094166389454977\n",
      "36000/49000 loss: 0.50056389961791\n",
      "42000/49000 loss: 0.47654814255782413\n",
      "48000/49000 loss: 0.49842266881728303\n",
      "epoch 9: valid acc = 0.826, new learning rate = 0.0003151247048623045\n",
      "6000/49000 loss: 0.5224957315912221\n",
      "12000/49000 loss: 0.42846675567125486\n",
      "18000/49000 loss: 0.4425286922709051\n",
      "24000/49000 loss: 0.4818371945951152\n",
      "30000/49000 loss: 0.4275472537505602\n",
      "36000/49000 loss: 0.5338584596270587\n",
      "42000/49000 loss: 0.44146196855871633\n",
      "48000/49000 loss: 0.49619972149609193\n",
      "epoch 10: valid acc = 0.829, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.8341020408163266\n",
      "test acc: 0.829\n",
      "test acc: 0.8221\n",
      "number of batches for training: 81\n",
      "epoch 1: valid acc = 0.543, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.673, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.743, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.758, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.782, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.791, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.807, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.807, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.817, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.825, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.8317959183673469\n",
      "test acc: 0.825\n",
      "test acc: 0.8171\n",
      "number of batches for training: 49\n",
      "10000/49000 loss: 3.4109999308052523\n",
      "20000/49000 loss: 2.9342418665135166\n",
      "30000/49000 loss: 2.729211612865963\n",
      "40000/49000 loss: 2.2970528412721167\n",
      "epoch 1: valid acc = 0.331, new learning rate = 0.000475\n",
      "10000/49000 loss: 2.1118084804202915\n",
      "20000/49000 loss: 1.9189977909239753\n",
      "30000/49000 loss: 1.7905036210881886\n",
      "40000/49000 loss: 1.4530884829915665\n",
      "epoch 2: valid acc = 0.524, new learning rate = 0.00045125\n",
      "10000/49000 loss: 1.2475274632868185\n",
      "20000/49000 loss: 1.1081748774994296\n",
      "30000/49000 loss: 1.1990422424726273\n",
      "40000/49000 loss: 1.09477002802366\n",
      "epoch 3: valid acc = 0.615, new learning rate = 0.0004286875\n",
      "10000/49000 loss: 1.064934855337327\n",
      "20000/49000 loss: 1.0652707969372928\n",
      "30000/49000 loss: 0.9317135460853208\n",
      "40000/49000 loss: 0.9206824443950121\n",
      "epoch 4: valid acc = 0.687, new learning rate = 0.00040725312499999993\n",
      "10000/49000 loss: 0.8914130364187313\n",
      "20000/49000 loss: 0.882895285434366\n",
      "30000/49000 loss: 0.7942200041404656\n",
      "40000/49000 loss: 0.8460637743300914\n",
      "epoch 5: valid acc = 0.724, new learning rate = 0.0003868904687499999\n",
      "10000/49000 loss: 0.7443924344965468\n",
      "20000/49000 loss: 0.7389431665249627\n",
      "30000/49000 loss: 0.7650956197736747\n",
      "40000/49000 loss: 0.7387400368947891\n",
      "epoch 6: valid acc = 0.745, new learning rate = 0.0003675459453124999\n",
      "10000/49000 loss: 0.7462829017814121\n",
      "20000/49000 loss: 0.66339190404672\n",
      "30000/49000 loss: 0.6620299036945065\n",
      "40000/49000 loss: 0.7124050097426431\n",
      "epoch 7: valid acc = 0.75, new learning rate = 0.00034916864804687486\n",
      "10000/49000 loss: 0.6461020895930618\n",
      "20000/49000 loss: 0.6244445015493741\n",
      "30000/49000 loss: 0.5980256663296187\n",
      "40000/49000 loss: 0.6756795836729494\n",
      "epoch 8: valid acc = 0.767, new learning rate = 0.0003317102156445311\n",
      "10000/49000 loss: 0.6481957420509965\n",
      "20000/49000 loss: 0.5601970553648449\n",
      "30000/49000 loss: 0.603685973625162\n",
      "40000/49000 loss: 0.6179593455132245\n",
      "epoch 9: valid acc = 0.778, new learning rate = 0.0003151247048623045\n",
      "10000/49000 loss: 0.5672036748157268\n",
      "20000/49000 loss: 0.6251398845269202\n",
      "30000/49000 loss: 0.5799099673629547\n",
      "40000/49000 loss: 0.586253673604157\n",
      "epoch 10: valid acc = 0.787, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.7959591836734694\n",
      "test acc: 0.787\n",
      "test acc: 0.7896\n",
      "number of batches for training: 49\n",
      "epoch 1: valid acc = 0.389, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.538, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.624, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.696, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.738, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.745, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.756, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.776, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.785, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.794, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.7981020408163265\n",
      "test acc: 0.794\n",
      "test acc: 0.791\n",
      "number of batches for training: 49\n",
      "10000/49000 loss: 4.493088345701718\n",
      "20000/49000 loss: 3.0244860993749314\n",
      "30000/49000 loss: 2.661966851425902\n",
      "40000/49000 loss: 2.50151357664529\n",
      "epoch 1: valid acc = 0.38, new learning rate = 0.000475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/49000 loss: 2.1451640904794402\n",
      "20000/49000 loss: 2.0073452668487906\n",
      "30000/49000 loss: 1.9409229650331403\n",
      "40000/49000 loss: 1.5564168412784125\n",
      "epoch 2: valid acc = 0.536, new learning rate = 0.00045125\n",
      "10000/49000 loss: 1.2575171262423994\n",
      "20000/49000 loss: 1.2238363013992721\n",
      "30000/49000 loss: 1.15339375564068\n",
      "40000/49000 loss: 1.0994497997564971\n",
      "epoch 3: valid acc = 0.635, new learning rate = 0.0004286875\n",
      "10000/49000 loss: 1.0496476264668049\n",
      "20000/49000 loss: 0.9698092078137542\n",
      "30000/49000 loss: 0.9521089061530589\n",
      "40000/49000 loss: 0.9266309220367325\n",
      "epoch 4: valid acc = 0.698, new learning rate = 0.00040725312499999993\n",
      "10000/49000 loss: 0.8933204444724565\n",
      "20000/49000 loss: 0.8247814159927994\n",
      "30000/49000 loss: 0.8104675753730397\n",
      "40000/49000 loss: 0.7608149670286712\n",
      "epoch 5: valid acc = 0.734, new learning rate = 0.0003868904687499999\n",
      "10000/49000 loss: 0.7446269511307066\n",
      "20000/49000 loss: 0.7353677807248622\n",
      "30000/49000 loss: 0.7135408383635911\n",
      "40000/49000 loss: 0.7007739424047286\n",
      "epoch 6: valid acc = 0.742, new learning rate = 0.0003675459453124999\n",
      "10000/49000 loss: 0.6922682328228112\n",
      "20000/49000 loss: 0.6765512363217727\n",
      "30000/49000 loss: 0.6413881333202758\n",
      "40000/49000 loss: 0.6679351233034824\n",
      "epoch 7: valid acc = 0.764, new learning rate = 0.00034916864804687486\n",
      "10000/49000 loss: 0.6246785998853602\n",
      "20000/49000 loss: 0.6051108113682272\n",
      "30000/49000 loss: 0.6570995528133127\n",
      "40000/49000 loss: 0.5685427888408271\n",
      "epoch 8: valid acc = 0.773, new learning rate = 0.0003317102156445311\n",
      "10000/49000 loss: 0.6208875996955433\n",
      "20000/49000 loss: 0.5541275041789683\n",
      "30000/49000 loss: 0.5747958334756016\n",
      "40000/49000 loss: 0.6029995123048292\n",
      "epoch 9: valid acc = 0.785, new learning rate = 0.0003151247048623045\n",
      "10000/49000 loss: 0.6265401831337595\n",
      "20000/49000 loss: 0.5205064691034782\n",
      "30000/49000 loss: 0.5363443883106825\n",
      "40000/49000 loss: 0.5566105541724226\n",
      "epoch 10: valid acc = 0.788, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.7981428571428572\n",
      "test acc: 0.788\n",
      "test acc: 0.7915\n",
      "number of batches for training: 49\n",
      "epoch 1: valid acc = 0.402, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.512, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.639, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.694, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.734, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.744, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.763, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.779, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.789, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.785, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.7951836734693878\n",
      "test acc: 0.785\n",
      "test acc: 0.7893\n",
      "number of batches for training: 49\n",
      "10000/49000 loss: 3.2792181452408102\n",
      "20000/49000 loss: 2.9773581085172887\n",
      "30000/49000 loss: 2.7424529730899208\n",
      "40000/49000 loss: 2.3631556687844575\n",
      "epoch 1: valid acc = 0.431, new learning rate = 0.000475\n",
      "10000/49000 loss: 2.219744320280414\n",
      "20000/49000 loss: 1.9325495081988353\n",
      "30000/49000 loss: 1.8666574068614672\n",
      "40000/49000 loss: 1.5208368764873634\n",
      "epoch 2: valid acc = 0.529, new learning rate = 0.00045125\n",
      "10000/49000 loss: 1.2429673505955021\n",
      "20000/49000 loss: 1.2155686058887598\n",
      "30000/49000 loss: 1.1129647966434608\n",
      "40000/49000 loss: 1.109412313858074\n",
      "epoch 3: valid acc = 0.633, new learning rate = 0.0004286875\n",
      "10000/49000 loss: 1.060521706968211\n",
      "20000/49000 loss: 0.981376863021755\n",
      "30000/49000 loss: 0.9498153469467177\n",
      "40000/49000 loss: 0.8895157091769974\n",
      "epoch 4: valid acc = 0.707, new learning rate = 0.00040725312499999993\n",
      "10000/49000 loss: 0.8744469613947526\n",
      "20000/49000 loss: 0.8488752398735241\n",
      "30000/49000 loss: 0.7497065979071953\n",
      "40000/49000 loss: 0.8159905833533188\n",
      "epoch 5: valid acc = 0.726, new learning rate = 0.0003868904687499999\n",
      "10000/49000 loss: 0.7734746000234575\n",
      "20000/49000 loss: 0.7554786967451872\n",
      "30000/49000 loss: 0.7163138355203664\n",
      "40000/49000 loss: 0.6712067839409677\n",
      "epoch 6: valid acc = 0.745, new learning rate = 0.0003675459453124999\n",
      "10000/49000 loss: 0.6810321409362033\n",
      "20000/49000 loss: 0.6714233902427595\n",
      "30000/49000 loss: 0.6539158719917553\n",
      "40000/49000 loss: 0.7331427226456949\n",
      "epoch 7: valid acc = 0.758, new learning rate = 0.00034916864804687486\n",
      "10000/49000 loss: 0.66880247641542\n",
      "20000/49000 loss: 0.6317021419200167\n",
      "30000/49000 loss: 0.6378427465268227\n",
      "40000/49000 loss: 0.6150537122931624\n",
      "epoch 8: valid acc = 0.767, new learning rate = 0.0003317102156445311\n",
      "10000/49000 loss: 0.6276208749326707\n",
      "20000/49000 loss: 0.5825475420869615\n",
      "30000/49000 loss: 0.5575039575450126\n",
      "40000/49000 loss: 0.5385487816282373\n",
      "epoch 9: valid acc = 0.786, new learning rate = 0.0003151247048623045\n",
      "10000/49000 loss: 0.5483686492096896\n",
      "20000/49000 loss: 0.5869380437367542\n",
      "30000/49000 loss: 0.6129341687640601\n",
      "40000/49000 loss: 0.5183663753753521\n",
      "epoch 10: valid acc = 0.791, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.7990408163265306\n",
      "test acc: 0.791\n",
      "test acc: 0.7916\n",
      "number of batches for training: 49\n",
      "epoch 1: valid acc = 0.402, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.553, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.645, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.709, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.738, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.737, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.757, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.778, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.788, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.796, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.8024489795918367\n",
      "test acc: 0.796\n",
      "test acc: 0.7921\n",
      "number of batches for training: 245\n",
      "2000/49000 loss: 3.607558977915897\n",
      "4000/49000 loss: 3.4033138262863156\n",
      "6000/49000 loss: 2.607464000423339\n",
      "8000/49000 loss: 2.3633774689170237\n",
      "10000/49000 loss: 2.218358825954217\n",
      "12000/49000 loss: 2.364384784395802\n",
      "14000/49000 loss: 1.9864586750973883\n",
      "16000/49000 loss: 1.4272845756075148\n",
      "18000/49000 loss: 1.291538881053615\n",
      "20000/49000 loss: 1.2154494747003939\n",
      "22000/49000 loss: 1.2550164890359632\n",
      "24000/49000 loss: 1.1249853894953525\n",
      "26000/49000 loss: 1.0693978939461994\n",
      "28000/49000 loss: 0.9158428436645741\n",
      "30000/49000 loss: 1.1025631092359167\n",
      "32000/49000 loss: 1.0557886804139014\n",
      "34000/49000 loss: 0.9163567506883084\n",
      "36000/49000 loss: 0.9506327261151691\n",
      "38000/49000 loss: 0.7246100378750926\n",
      "40000/49000 loss: 0.8858836954447941\n",
      "42000/49000 loss: 0.7652753049734569\n",
      "44000/49000 loss: 0.8859746193736068\n",
      "46000/49000 loss: 0.7884262427088726\n",
      "48000/49000 loss: 0.8566445690793658\n",
      "epoch 1: valid acc = 0.748, new learning rate = 0.000475\n",
      "2000/49000 loss: 0.7674136432278225\n",
      "4000/49000 loss: 0.7160033220398923\n",
      "6000/49000 loss: 0.6433350741528748\n",
      "8000/49000 loss: 0.5969277138964777\n",
      "10000/49000 loss: 0.7125434585874382\n",
      "12000/49000 loss: 0.5968147794653126\n",
      "14000/49000 loss: 0.717508473889832\n",
      "16000/49000 loss: 0.6528433321113504\n",
      "18000/49000 loss: 0.6445963702602169\n",
      "20000/49000 loss: 0.6861426453446815\n",
      "22000/49000 loss: 0.5716390535290228\n",
      "24000/49000 loss: 0.6202584954518099\n",
      "26000/49000 loss: 0.6318779710769312\n",
      "28000/49000 loss: 0.6156239390729261\n",
      "30000/49000 loss: 0.4708675061200304\n",
      "32000/49000 loss: 0.6215711819716216\n",
      "34000/49000 loss: 0.556287223587898\n",
      "36000/49000 loss: 0.5235423217252602\n",
      "38000/49000 loss: 0.7617329664016327\n",
      "40000/49000 loss: 0.5833150599688588\n",
      "42000/49000 loss: 0.5059091581409487\n",
      "44000/49000 loss: 0.6268087034659027\n",
      "46000/49000 loss: 0.5416205240054859\n",
      "48000/49000 loss: 0.6187441507364777\n",
      "epoch 2: valid acc = 0.806, new learning rate = 0.00045125\n",
      "2000/49000 loss: 0.4075649335276621\n",
      "4000/49000 loss: 0.479212022323377\n",
      "6000/49000 loss: 0.5406789886831818\n",
      "8000/49000 loss: 0.4636534881658925\n",
      "10000/49000 loss: 0.5572784640634203\n",
      "12000/49000 loss: 0.5018045734993672\n",
      "14000/49000 loss: 0.5183652773055101\n",
      "16000/49000 loss: 0.4752521804314434\n",
      "18000/49000 loss: 0.5233010770062826\n",
      "20000/49000 loss: 0.4456324345846781\n",
      "22000/49000 loss: 0.45148148010867295\n",
      "24000/49000 loss: 0.4317081040615832\n",
      "26000/49000 loss: 0.41983836376186734\n",
      "28000/49000 loss: 0.521964003680494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000/49000 loss: 0.39037504759984254\n",
      "32000/49000 loss: 0.35222177218035\n",
      "34000/49000 loss: 0.458471133210177\n",
      "36000/49000 loss: 0.507016895155239\n",
      "38000/49000 loss: 0.5219872128714598\n",
      "40000/49000 loss: 0.5733008513481284\n",
      "42000/49000 loss: 0.41526154295694295\n",
      "44000/49000 loss: 0.4774812203270027\n",
      "46000/49000 loss: 0.39076849524468266\n",
      "48000/49000 loss: 0.4193329162436421\n",
      "epoch 3: valid acc = 0.823, new learning rate = 0.0004286875\n",
      "2000/49000 loss: 0.39068684848917895\n",
      "4000/49000 loss: 0.38103248664615236\n",
      "6000/49000 loss: 0.43499960250523534\n",
      "8000/49000 loss: 0.41395963504973166\n",
      "10000/49000 loss: 0.5577284006665959\n",
      "12000/49000 loss: 0.41972873543741895\n",
      "14000/49000 loss: 0.42146154398912483\n",
      "16000/49000 loss: 0.44505208563544724\n",
      "18000/49000 loss: 0.4459277796562366\n",
      "20000/49000 loss: 0.43691469988482406\n",
      "22000/49000 loss: 0.45910558153412984\n",
      "24000/49000 loss: 0.38059560309248813\n",
      "26000/49000 loss: 0.40409972132971284\n",
      "28000/49000 loss: 0.5309871385254452\n",
      "30000/49000 loss: 0.49538535504453984\n",
      "32000/49000 loss: 0.3931983351000638\n",
      "34000/49000 loss: 0.43915952003422914\n",
      "36000/49000 loss: 0.38551419445785534\n",
      "38000/49000 loss: 0.4624779818103602\n",
      "40000/49000 loss: 0.4040534508130637\n",
      "42000/49000 loss: 0.4045220310619736\n",
      "44000/49000 loss: 0.355108293869388\n",
      "46000/49000 loss: 0.47850092193055993\n",
      "48000/49000 loss: 0.4680375313798914\n",
      "epoch 4: valid acc = 0.84, new learning rate = 0.00040725312499999993\n",
      "2000/49000 loss: 0.5590186755715266\n",
      "4000/49000 loss: 0.5373523305601208\n",
      "6000/49000 loss: 0.4184760132536419\n",
      "8000/49000 loss: 0.2911968684006876\n",
      "10000/49000 loss: 0.4124256394982026\n",
      "12000/49000 loss: 0.37202337680698105\n",
      "14000/49000 loss: 0.37354991009838356\n",
      "16000/49000 loss: 0.4205678436901276\n",
      "18000/49000 loss: 0.40515145094915644\n",
      "20000/49000 loss: 0.4476727900281689\n",
      "22000/49000 loss: 0.5289327932316064\n",
      "24000/49000 loss: 0.4941977718068074\n",
      "26000/49000 loss: 0.46472968372342877\n",
      "28000/49000 loss: 0.436257603790891\n",
      "30000/49000 loss: 0.4952242109100159\n",
      "32000/49000 loss: 0.4266813380067808\n",
      "34000/49000 loss: 0.42164261883269094\n",
      "36000/49000 loss: 0.3632797385366364\n",
      "38000/49000 loss: 0.3980260347229419\n",
      "40000/49000 loss: 0.39356895554416654\n",
      "42000/49000 loss: 0.45343079851128115\n",
      "44000/49000 loss: 0.3629646202014283\n",
      "46000/49000 loss: 0.4696448375257515\n",
      "48000/49000 loss: 0.4538099952520826\n",
      "epoch 5: valid acc = 0.852, new learning rate = 0.0003868904687499999\n",
      "2000/49000 loss: 0.39600791431305227\n",
      "4000/49000 loss: 0.30368842927694306\n",
      "6000/49000 loss: 0.416525933743156\n",
      "8000/49000 loss: 0.383684378296022\n",
      "10000/49000 loss: 0.3846009523902323\n",
      "12000/49000 loss: 0.40796584263263275\n",
      "14000/49000 loss: 0.4531108250651452\n",
      "16000/49000 loss: 0.4230986310972283\n",
      "18000/49000 loss: 0.39676780462410444\n",
      "20000/49000 loss: 0.4309478797846672\n",
      "22000/49000 loss: 0.35904528751783665\n",
      "24000/49000 loss: 0.2965665719771733\n",
      "26000/49000 loss: 0.330735048326152\n",
      "28000/49000 loss: 0.38412035531717975\n",
      "30000/49000 loss: 0.36886994634521775\n",
      "32000/49000 loss: 0.3880302868752029\n",
      "34000/49000 loss: 0.4809787498297703\n",
      "36000/49000 loss: 0.4785314853469911\n",
      "38000/49000 loss: 0.35566672994111215\n",
      "40000/49000 loss: 0.3787564347961855\n",
      "42000/49000 loss: 0.3512801461195856\n",
      "44000/49000 loss: 0.3658598485844799\n",
      "46000/49000 loss: 0.425322898511241\n",
      "48000/49000 loss: 0.4354495160469022\n",
      "epoch 6: valid acc = 0.863, new learning rate = 0.0003675459453124999\n",
      "2000/49000 loss: 0.4414196742330716\n",
      "4000/49000 loss: 0.4001904301382694\n",
      "6000/49000 loss: 0.38919622036383794\n",
      "8000/49000 loss: 0.35124263363196895\n",
      "10000/49000 loss: 0.4489546338929609\n",
      "12000/49000 loss: 0.44595008088312854\n",
      "14000/49000 loss: 0.4467628211935137\n",
      "16000/49000 loss: 0.39634851785436953\n",
      "18000/49000 loss: 0.427677507571293\n",
      "20000/49000 loss: 0.38983748629163906\n",
      "22000/49000 loss: 0.37015067940932556\n",
      "24000/49000 loss: 0.358720579802525\n",
      "26000/49000 loss: 0.36522012866547626\n",
      "28000/49000 loss: 0.34383074094078364\n",
      "30000/49000 loss: 0.37996007800178205\n",
      "32000/49000 loss: 0.2970601577306928\n",
      "34000/49000 loss: 0.35939663381218107\n",
      "36000/49000 loss: 0.4054324310840298\n",
      "38000/49000 loss: 0.3330934738623247\n",
      "40000/49000 loss: 0.3671264357252159\n",
      "42000/49000 loss: 0.432251919990018\n",
      "44000/49000 loss: 0.2756666546095656\n",
      "46000/49000 loss: 0.36532349146348836\n",
      "48000/49000 loss: 0.37567442609434176\n",
      "epoch 7: valid acc = 0.867, new learning rate = 0.00034916864804687486\n",
      "2000/49000 loss: 0.3771791378069337\n",
      "4000/49000 loss: 0.39040928706678735\n",
      "6000/49000 loss: 0.35395810052779103\n",
      "8000/49000 loss: 0.3812918181107511\n",
      "10000/49000 loss: 0.31469199023055083\n",
      "12000/49000 loss: 0.3346836262528965\n",
      "14000/49000 loss: 0.38382902848180217\n",
      "16000/49000 loss: 0.3592002034549496\n",
      "18000/49000 loss: 0.30237759115145113\n",
      "20000/49000 loss: 0.36861508462460274\n",
      "22000/49000 loss: 0.4023760855353805\n",
      "24000/49000 loss: 0.34168257293312476\n",
      "26000/49000 loss: 0.3883609023191382\n",
      "28000/49000 loss: 0.40146773645349043\n",
      "30000/49000 loss: 0.39278960455519424\n",
      "32000/49000 loss: 0.29516738621270905\n",
      "34000/49000 loss: 0.39368251775832147\n",
      "36000/49000 loss: 0.4115440411983539\n",
      "38000/49000 loss: 0.3808533443043176\n",
      "40000/49000 loss: 0.47304604056144484\n",
      "42000/49000 loss: 0.3330937285185118\n",
      "44000/49000 loss: 0.3701301919051103\n",
      "46000/49000 loss: 0.47904786469456057\n",
      "48000/49000 loss: 0.38692262438829167\n",
      "epoch 8: valid acc = 0.866, new learning rate = 0.0003317102156445311\n",
      "2000/49000 loss: 0.3188912378396576\n",
      "4000/49000 loss: 0.41235567851966626\n",
      "6000/49000 loss: 0.3425400936418816\n",
      "8000/49000 loss: 0.3341162907840042\n",
      "10000/49000 loss: 0.40790676491745737\n",
      "12000/49000 loss: 0.3871371881315456\n",
      "14000/49000 loss: 0.300183010741062\n",
      "16000/49000 loss: 0.38482683469289647\n",
      "18000/49000 loss: 0.47088551522078714\n",
      "20000/49000 loss: 0.4539989541213898\n",
      "22000/49000 loss: 0.4035646061477999\n",
      "24000/49000 loss: 0.26080907976741624\n",
      "26000/49000 loss: 0.300535071415407\n",
      "28000/49000 loss: 0.30052045199448224\n",
      "30000/49000 loss: 0.2800814204829411\n",
      "32000/49000 loss: 0.27823156018593903\n",
      "34000/49000 loss: 0.3107443166697761\n",
      "36000/49000 loss: 0.32214969594642345\n",
      "38000/49000 loss: 0.30394146024358304\n",
      "40000/49000 loss: 0.31981959904464696\n",
      "42000/49000 loss: 0.3130147237093958\n",
      "44000/49000 loss: 0.41159108018079\n",
      "46000/49000 loss: 0.3816954805630847\n",
      "48000/49000 loss: 0.37266505321268134\n",
      "epoch 9: valid acc = 0.861, new learning rate = 0.0003151247048623045\n",
      "2000/49000 loss: 0.2812850838883923\n",
      "4000/49000 loss: 0.39854063822107255\n",
      "6000/49000 loss: 0.3768444832731209\n",
      "8000/49000 loss: 0.3822463411851075\n",
      "10000/49000 loss: 0.40736548404427875\n",
      "12000/49000 loss: 0.4334977311823669\n",
      "14000/49000 loss: 0.40223571538683095\n",
      "16000/49000 loss: 0.4076427590107297\n",
      "18000/49000 loss: 0.2889421083999171\n",
      "20000/49000 loss: 0.3756648851449392\n",
      "22000/49000 loss: 0.45118612693831445\n",
      "24000/49000 loss: 0.318518854531309\n",
      "26000/49000 loss: 0.3098131077157706\n",
      "28000/49000 loss: 0.3353066662834497\n",
      "30000/49000 loss: 0.29992603982037325\n",
      "32000/49000 loss: 0.41802499909249047\n",
      "34000/49000 loss: 0.4107344995712702\n",
      "36000/49000 loss: 0.26760548512597304\n",
      "38000/49000 loss: 0.2624682193983504\n",
      "40000/49000 loss: 0.2984983471045469\n",
      "42000/49000 loss: 0.44831851604597805\n",
      "44000/49000 loss: 0.25916278788279185\n",
      "46000/49000 loss: 0.33264576432489845\n",
      "48000/49000 loss: 0.31312828580838\n",
      "epoch 10: valid acc = 0.878, new learning rate = 0.00029936846961918924\n",
      "2000/49000 loss: 0.27895228953364276\n",
      "4000/49000 loss: 0.34635909171547263\n",
      "6000/49000 loss: 0.32473101085380013\n",
      "8000/49000 loss: 0.3496788413822845\n",
      "10000/49000 loss: 0.4161514395035751\n",
      "12000/49000 loss: 0.3444807788675104\n",
      "14000/49000 loss: 0.3097275971620451\n",
      "16000/49000 loss: 0.3257533886723762\n",
      "18000/49000 loss: 0.28846537026875235\n",
      "20000/49000 loss: 0.4061608793807654\n",
      "22000/49000 loss: 0.34879745693649994\n",
      "24000/49000 loss: 0.3123782561066762\n",
      "26000/49000 loss: 0.3272710137743095\n",
      "28000/49000 loss: 0.38966353052562014\n",
      "30000/49000 loss: 0.36156150549494265\n",
      "32000/49000 loss: 0.3068827858464816\n",
      "34000/49000 loss: 0.4429837121355786\n",
      "36000/49000 loss: 0.2950325913379475\n",
      "38000/49000 loss: 0.3054580912749211\n",
      "40000/49000 loss: 0.3339643005692132\n",
      "42000/49000 loss: 0.3261714797212275\n",
      "44000/49000 loss: 0.398245608572255\n",
      "46000/49000 loss: 0.3800361798559807\n",
      "48000/49000 loss: 0.3592151648014864\n",
      "epoch 11: valid acc = 0.878, new learning rate = 0.00028440004613822977\n",
      "2000/49000 loss: 0.3266308089271704\n",
      "4000/49000 loss: 0.5106751014910647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/49000 loss: 0.3927806499285028\n",
      "8000/49000 loss: 0.4380854197670851\n",
      "10000/49000 loss: 0.41309886781314403\n",
      "12000/49000 loss: 0.29768210202469236\n",
      "14000/49000 loss: 0.27237488833182144\n",
      "16000/49000 loss: 0.3235341721538974\n",
      "18000/49000 loss: 0.29214031999972634\n",
      "20000/49000 loss: 0.2912662816341116\n",
      "22000/49000 loss: 0.32879335080658556\n",
      "24000/49000 loss: 0.35397169283101965\n",
      "26000/49000 loss: 0.2529729547176216\n",
      "28000/49000 loss: 0.3397484358161744\n",
      "30000/49000 loss: 0.379359847979156\n",
      "32000/49000 loss: 0.3072861073379871\n",
      "34000/49000 loss: 0.42109248591016796\n",
      "36000/49000 loss: 0.3200599523314982\n",
      "38000/49000 loss: 0.3085822138311171\n",
      "40000/49000 loss: 0.4137351715939515\n",
      "42000/49000 loss: 0.3430187182712019\n",
      "44000/49000 loss: 0.3405111135483385\n",
      "46000/49000 loss: 0.27112314069618276\n",
      "48000/49000 loss: 0.3257289830048388\n",
      "epoch 12: valid acc = 0.877, new learning rate = 0.00027018004383131826\n",
      "2000/49000 loss: 0.28383106451115053\n",
      "4000/49000 loss: 0.31541482645503677\n",
      "6000/49000 loss: 0.30966373221109206\n",
      "8000/49000 loss: 0.3128647579232116\n",
      "10000/49000 loss: 0.33698492939029606\n",
      "12000/49000 loss: 0.2668961788251043\n",
      "14000/49000 loss: 0.3665044997912665\n",
      "16000/49000 loss: 0.2997446268253651\n",
      "18000/49000 loss: 0.3361147359588885\n",
      "20000/49000 loss: 0.37997315853141644\n",
      "22000/49000 loss: 0.32303653844101593\n",
      "24000/49000 loss: 0.30013367636393157\n",
      "26000/49000 loss: 0.3446029155526496\n",
      "28000/49000 loss: 0.36817768061749895\n",
      "30000/49000 loss: 0.3236539534288835\n",
      "32000/49000 loss: 0.3845875889471652\n",
      "34000/49000 loss: 0.30496423463228917\n",
      "36000/49000 loss: 0.2844722695014397\n",
      "38000/49000 loss: 0.3096598053691581\n",
      "40000/49000 loss: 0.29770823215600684\n",
      "42000/49000 loss: 0.3139612383483137\n",
      "44000/49000 loss: 0.32393348668459493\n",
      "46000/49000 loss: 0.3252478555600964\n",
      "48000/49000 loss: 0.4079665698378008\n",
      "epoch 13: valid acc = 0.874, new learning rate = 0.00025667104163975234\n",
      "2000/49000 loss: 0.4628955158232279\n",
      "4000/49000 loss: 0.3816433125376202\n",
      "6000/49000 loss: 0.27268697706255746\n",
      "8000/49000 loss: 0.3167940206925827\n",
      "10000/49000 loss: 0.3472433403094849\n",
      "12000/49000 loss: 0.4628632275614839\n",
      "14000/49000 loss: 0.2875108139772309\n",
      "16000/49000 loss: 0.34771529636273585\n",
      "18000/49000 loss: 0.3896669503336779\n",
      "20000/49000 loss: 0.36758118297480324\n",
      "22000/49000 loss: 0.3447308858519127\n",
      "24000/49000 loss: 0.3228277963417959\n",
      "26000/49000 loss: 0.3335564336061934\n",
      "28000/49000 loss: 0.32911242617098935\n",
      "30000/49000 loss: 0.3116105154309269\n",
      "32000/49000 loss: 0.349397586312193\n",
      "34000/49000 loss: 0.3107665207082649\n",
      "36000/49000 loss: 0.2758393621592165\n",
      "38000/49000 loss: 0.3201802532656104\n",
      "40000/49000 loss: 0.3871104403866518\n",
      "42000/49000 loss: 0.39643014517802916\n",
      "44000/49000 loss: 0.29913499478381556\n",
      "46000/49000 loss: 0.265061270066856\n",
      "48000/49000 loss: 0.24928509019983836\n",
      "epoch 14: valid acc = 0.882, new learning rate = 0.00024383748955776472\n",
      "2000/49000 loss: 0.3745731162874433\n",
      "4000/49000 loss: 0.353456469051318\n",
      "6000/49000 loss: 0.2703913285809158\n",
      "8000/49000 loss: 0.37856260094654226\n",
      "10000/49000 loss: 0.3183335175787764\n",
      "12000/49000 loss: 0.2931356865905067\n",
      "14000/49000 loss: 0.3503946890152472\n",
      "16000/49000 loss: 0.28999064876087305\n",
      "18000/49000 loss: 0.18995582629910998\n",
      "20000/49000 loss: 0.19889520457122598\n",
      "22000/49000 loss: 0.28837971141030694\n",
      "24000/49000 loss: 0.3393892296053344\n",
      "26000/49000 loss: 0.3350294469006547\n",
      "28000/49000 loss: 0.2506756472226875\n",
      "30000/49000 loss: 0.2536816363552253\n",
      "32000/49000 loss: 0.26548245974455853\n",
      "34000/49000 loss: 0.4583825971499033\n",
      "36000/49000 loss: 0.37084953386744784\n",
      "38000/49000 loss: 0.3515747019341159\n",
      "40000/49000 loss: 0.24835534212754296\n",
      "42000/49000 loss: 0.30120317634299604\n",
      "44000/49000 loss: 0.2983603169866564\n",
      "46000/49000 loss: 0.25298564346019914\n",
      "48000/49000 loss: 0.22222357551479696\n",
      "epoch 15: valid acc = 0.881, new learning rate = 0.00023164561507987649\n",
      "2000/49000 loss: 0.31301556804335523\n",
      "4000/49000 loss: 0.24366621361271842\n",
      "6000/49000 loss: 0.3224384867231451\n",
      "8000/49000 loss: 0.351476992987783\n",
      "10000/49000 loss: 0.311657391961962\n",
      "12000/49000 loss: 0.3756406834727419\n",
      "14000/49000 loss: 0.37653821995376746\n",
      "16000/49000 loss: 0.32090019892295146\n",
      "18000/49000 loss: 0.2894435909725211\n",
      "20000/49000 loss: 0.3020128588362725\n",
      "22000/49000 loss: 0.34658284016002655\n",
      "24000/49000 loss: 0.29447429835445316\n",
      "26000/49000 loss: 0.39808056825813287\n",
      "28000/49000 loss: 0.293360394058589\n",
      "30000/49000 loss: 0.3219935202127533\n",
      "32000/49000 loss: 0.26721340308737823\n",
      "34000/49000 loss: 0.31245327280821505\n",
      "36000/49000 loss: 0.3443846390379168\n",
      "38000/49000 loss: 0.32401372145029533\n",
      "40000/49000 loss: 0.2987328492670593\n",
      "42000/49000 loss: 0.35913402517905024\n",
      "44000/49000 loss: 0.37481501926475064\n",
      "46000/49000 loss: 0.2525397769133919\n",
      "48000/49000 loss: 0.29419773039220126\n",
      "epoch 16: valid acc = 0.885, new learning rate = 0.00022006333432588265\n",
      "2000/49000 loss: 0.24849467708928366\n",
      "4000/49000 loss: 0.32287348838041235\n",
      "6000/49000 loss: 0.288910211314765\n",
      "8000/49000 loss: 0.26422713179438495\n",
      "10000/49000 loss: 0.30834924533653246\n",
      "12000/49000 loss: 0.41422978899253654\n",
      "14000/49000 loss: 0.3252996677807828\n",
      "16000/49000 loss: 0.2491640934899497\n",
      "18000/49000 loss: 0.26275759679650285\n",
      "20000/49000 loss: 0.3330714680026049\n",
      "22000/49000 loss: 0.4158162328092846\n",
      "24000/49000 loss: 0.36104228204519223\n",
      "26000/49000 loss: 0.33597168359442364\n",
      "28000/49000 loss: 0.41490887836716345\n",
      "30000/49000 loss: 0.36008879719034115\n",
      "32000/49000 loss: 0.30661884730897104\n",
      "34000/49000 loss: 0.4039578481418427\n",
      "36000/49000 loss: 0.2908914120826741\n",
      "38000/49000 loss: 0.27385146601545785\n",
      "40000/49000 loss: 0.20604000646846501\n",
      "42000/49000 loss: 0.2710061568869208\n",
      "44000/49000 loss: 0.3064522830646543\n",
      "46000/49000 loss: 0.3567678864175225\n",
      "48000/49000 loss: 0.2469595142143709\n",
      "epoch 17: valid acc = 0.882, new learning rate = 0.00020906016760958852\n",
      "2000/49000 loss: 0.2919710512395226\n",
      "4000/49000 loss: 0.26127709089204343\n",
      "6000/49000 loss: 0.32527811860933703\n",
      "8000/49000 loss: 0.3314749548046284\n",
      "10000/49000 loss: 0.3152880537260499\n",
      "12000/49000 loss: 0.2990795289023132\n",
      "14000/49000 loss: 0.3561811800466756\n",
      "16000/49000 loss: 0.41054580225178516\n",
      "18000/49000 loss: 0.2631252899417585\n",
      "20000/49000 loss: 0.3295507584565086\n",
      "22000/49000 loss: 0.27929017274947376\n",
      "24000/49000 loss: 0.2882084778840302\n",
      "26000/49000 loss: 0.2651464033509096\n",
      "28000/49000 loss: 0.40388353542193406\n",
      "30000/49000 loss: 0.29478299983675516\n",
      "32000/49000 loss: 0.28539855724795427\n",
      "34000/49000 loss: 0.25428558171058974\n",
      "36000/49000 loss: 0.2869392859322089\n",
      "38000/49000 loss: 0.318984025001703\n",
      "40000/49000 loss: 0.43495983048560144\n",
      "42000/49000 loss: 0.346311351898276\n",
      "44000/49000 loss: 0.36245069526712753\n",
      "46000/49000 loss: 0.3164271929878407\n",
      "48000/49000 loss: 0.2849757656943381\n",
      "epoch 18: valid acc = 0.881, new learning rate = 0.00019860715922910907\n",
      "2000/49000 loss: 0.4524031365034909\n",
      "4000/49000 loss: 0.2833377211818142\n",
      "6000/49000 loss: 0.2574077895139083\n",
      "8000/49000 loss: 0.29902604336115357\n",
      "10000/49000 loss: 0.2890958437324193\n",
      "12000/49000 loss: 0.27323379897270017\n",
      "14000/49000 loss: 0.2819838115056739\n",
      "16000/49000 loss: 0.3880607181720451\n",
      "18000/49000 loss: 0.288118998625286\n",
      "20000/49000 loss: 0.32401053505866095\n",
      "22000/49000 loss: 0.2865759541454336\n",
      "24000/49000 loss: 0.4403583067117886\n",
      "26000/49000 loss: 0.31642854787641866\n",
      "28000/49000 loss: 0.3091227706424839\n",
      "30000/49000 loss: 0.3083137693059125\n",
      "32000/49000 loss: 0.3471456825546465\n",
      "34000/49000 loss: 0.29004411170985994\n",
      "36000/49000 loss: 0.25188482921703126\n",
      "38000/49000 loss: 0.2590314018371866\n",
      "40000/49000 loss: 0.26172009696284865\n",
      "42000/49000 loss: 0.30883273400678557\n",
      "44000/49000 loss: 0.28723732907741445\n",
      "46000/49000 loss: 0.2954018556268339\n",
      "48000/49000 loss: 0.28384510299715204\n",
      "epoch 19: valid acc = 0.882, new learning rate = 0.0001886768012676536\n",
      "2000/49000 loss: 0.25036500587905053\n",
      "4000/49000 loss: 0.3262361195409481\n",
      "6000/49000 loss: 0.31946064145437797\n",
      "8000/49000 loss: 0.2810086058235676\n",
      "10000/49000 loss: 0.30716604835683586\n",
      "12000/49000 loss: 0.3019525222413582\n",
      "14000/49000 loss: 0.2709198945668004\n",
      "16000/49000 loss: 0.3564497307156433\n",
      "18000/49000 loss: 0.30638825726815405\n",
      "20000/49000 loss: 0.2982239361930327\n",
      "22000/49000 loss: 0.24621759872784654\n",
      "24000/49000 loss: 0.4291937278060252\n",
      "26000/49000 loss: 0.37427643759674883\n",
      "28000/49000 loss: 0.3809696221454322\n",
      "30000/49000 loss: 0.3040475829789987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000/49000 loss: 0.31854293755708263\n",
      "34000/49000 loss: 0.3567719956549309\n",
      "36000/49000 loss: 0.2670493425689027\n",
      "38000/49000 loss: 0.2711793760693477\n",
      "40000/49000 loss: 0.3426298249319065\n",
      "42000/49000 loss: 0.2856653049735456\n",
      "44000/49000 loss: 0.24621922964632525\n",
      "46000/49000 loss: 0.32165122405227237\n",
      "48000/49000 loss: 0.2820007022921144\n",
      "epoch 20: valid acc = 0.879, new learning rate = 0.0001792429612042709\n",
      "2000/49000 loss: 0.2990731199048944\n",
      "4000/49000 loss: 0.1908594344890979\n",
      "6000/49000 loss: 0.2541774832532893\n",
      "8000/49000 loss: 0.31222915127836337\n",
      "10000/49000 loss: 0.2834760260742272\n",
      "12000/49000 loss: 0.3405378690412731\n",
      "14000/49000 loss: 0.24230588326388686\n",
      "16000/49000 loss: 0.3998648941220794\n",
      "18000/49000 loss: 0.3013660751972096\n",
      "20000/49000 loss: 0.3025664694640967\n",
      "22000/49000 loss: 0.23081056993532065\n",
      "24000/49000 loss: 0.3781138812080016\n",
      "26000/49000 loss: 0.3638675998575957\n",
      "28000/49000 loss: 0.3164777690970749\n",
      "30000/49000 loss: 0.32299619152488446\n",
      "32000/49000 loss: 0.3018814224856005\n",
      "34000/49000 loss: 0.32434912660963516\n",
      "36000/49000 loss: 0.2870255961659183\n",
      "38000/49000 loss: 0.25987767236997944\n",
      "40000/49000 loss: 0.3966033952407712\n",
      "42000/49000 loss: 0.3464217711163494\n",
      "44000/49000 loss: 0.29022067929629497\n",
      "46000/49000 loss: 0.34932685222861504\n",
      "48000/49000 loss: 0.31500554257192304\n",
      "epoch 21: valid acc = 0.887, new learning rate = 0.00017028081314405735\n",
      "2000/49000 loss: 0.30193803318264356\n",
      "4000/49000 loss: 0.30947063119005175\n",
      "6000/49000 loss: 0.25447412170048306\n",
      "8000/49000 loss: 0.32049208650651695\n",
      "10000/49000 loss: 0.23835506997912753\n",
      "12000/49000 loss: 0.27366515928838014\n",
      "14000/49000 loss: 0.31431060771689545\n",
      "16000/49000 loss: 0.2578315268946318\n",
      "18000/49000 loss: 0.2994854071955155\n",
      "20000/49000 loss: 0.291772339017259\n",
      "22000/49000 loss: 0.2673919811989484\n",
      "24000/49000 loss: 0.3293062382481725\n",
      "26000/49000 loss: 0.34849105237042494\n",
      "28000/49000 loss: 0.3422036239559539\n",
      "30000/49000 loss: 0.4274223854012366\n",
      "32000/49000 loss: 0.26014802818925276\n",
      "34000/49000 loss: 0.2956676779395693\n",
      "36000/49000 loss: 0.36704278032002946\n",
      "38000/49000 loss: 0.41607621108644804\n",
      "40000/49000 loss: 0.33104333238292194\n",
      "42000/49000 loss: 0.2811642352442478\n",
      "44000/49000 loss: 0.2656893681971689\n",
      "46000/49000 loss: 0.3527809617181787\n",
      "48000/49000 loss: 0.29899200613837307\n",
      "epoch 22: valid acc = 0.884, new learning rate = 0.00016176677248685447\n",
      "2000/49000 loss: 0.33671357143139924\n",
      "4000/49000 loss: 0.33142984094410505\n",
      "6000/49000 loss: 0.23143229386207614\n",
      "8000/49000 loss: 0.268286805847533\n",
      "10000/49000 loss: 0.29618120990385205\n",
      "12000/49000 loss: 0.42199783961291754\n",
      "14000/49000 loss: 0.3801003880678074\n",
      "16000/49000 loss: 0.300889803961358\n",
      "18000/49000 loss: 0.35616551300684596\n",
      "20000/49000 loss: 0.32411364776045587\n",
      "22000/49000 loss: 0.3329539287755233\n",
      "24000/49000 loss: 0.29788533170841325\n",
      "26000/49000 loss: 0.2727383436731618\n",
      "28000/49000 loss: 0.21665473593409232\n",
      "30000/49000 loss: 0.4718883237159656\n",
      "32000/49000 loss: 0.3514365084385943\n",
      "34000/49000 loss: 0.3703387199874656\n",
      "36000/49000 loss: 0.32406734366255624\n",
      "38000/49000 loss: 0.23576883309657296\n",
      "40000/49000 loss: 0.2898047230682055\n",
      "42000/49000 loss: 0.3312414583448443\n",
      "44000/49000 loss: 0.322048697464902\n",
      "46000/49000 loss: 0.42752663261268375\n",
      "48000/49000 loss: 0.23985624472120917\n",
      "epoch 23: valid acc = 0.889, new learning rate = 0.00015367843386251173\n",
      "2000/49000 loss: 0.24079841553512718\n",
      "4000/49000 loss: 0.30253539526068807\n",
      "6000/49000 loss: 0.28775975397256137\n",
      "8000/49000 loss: 0.23805732430500612\n",
      "10000/49000 loss: 0.256558691719272\n",
      "12000/49000 loss: 0.26615919802987287\n",
      "14000/49000 loss: 0.33993951072397516\n",
      "16000/49000 loss: 0.29333717322016567\n",
      "18000/49000 loss: 0.331795772685108\n",
      "20000/49000 loss: 0.22694050822555756\n",
      "22000/49000 loss: 0.32111657868919263\n",
      "24000/49000 loss: 0.33002699356282716\n",
      "26000/49000 loss: 0.25030075971085197\n",
      "28000/49000 loss: 0.27555058591487036\n",
      "30000/49000 loss: 0.27981030068100676\n",
      "32000/49000 loss: 0.304348980681132\n",
      "34000/49000 loss: 0.27285836642930894\n",
      "36000/49000 loss: 0.3419838172908414\n",
      "38000/49000 loss: 0.32891902288681624\n",
      "40000/49000 loss: 0.2941394559648483\n",
      "42000/49000 loss: 0.26396214802139306\n",
      "44000/49000 loss: 0.24910927354905418\n",
      "46000/49000 loss: 0.4068046098119999\n",
      "48000/49000 loss: 0.34835215398279945\n",
      "epoch 24: valid acc = 0.888, new learning rate = 0.00014599451216938612\n",
      "2000/49000 loss: 0.3632634131655084\n",
      "4000/49000 loss: 0.23807158106974932\n",
      "6000/49000 loss: 0.2773046878604962\n",
      "8000/49000 loss: 0.3771828829428477\n",
      "10000/49000 loss: 0.31824009045219265\n",
      "12000/49000 loss: 0.36901890039754776\n",
      "14000/49000 loss: 0.2673154751146152\n",
      "16000/49000 loss: 0.27389995181411203\n",
      "18000/49000 loss: 0.3532874125444544\n",
      "20000/49000 loss: 0.24359040808151847\n",
      "22000/49000 loss: 0.27734579252662284\n",
      "24000/49000 loss: 0.3862651744414045\n",
      "26000/49000 loss: 0.3008830092163697\n",
      "28000/49000 loss: 0.28760378159036304\n",
      "30000/49000 loss: 0.26255547988542033\n",
      "32000/49000 loss: 0.2585863465051168\n",
      "34000/49000 loss: 0.28401686011677585\n",
      "36000/49000 loss: 0.2331734459141356\n",
      "38000/49000 loss: 0.37555046333731124\n",
      "40000/49000 loss: 0.2717469669365756\n",
      "42000/49000 loss: 0.2556183255326585\n",
      "44000/49000 loss: 0.37924238007240246\n",
      "46000/49000 loss: 0.31813387250941466\n",
      "48000/49000 loss: 0.28905900417960295\n",
      "epoch 25: valid acc = 0.888, new learning rate = 0.00013869478656091682\n",
      "2000/49000 loss: 0.28065663150174586\n",
      "4000/49000 loss: 0.37978682819396625\n",
      "6000/49000 loss: 0.2293058214918243\n",
      "8000/49000 loss: 0.2598354915452363\n",
      "10000/49000 loss: 0.3104243131947927\n",
      "12000/49000 loss: 0.2836536176001911\n",
      "14000/49000 loss: 0.30289552689041227\n",
      "16000/49000 loss: 0.2734593426609514\n",
      "18000/49000 loss: 0.2803231455868727\n",
      "20000/49000 loss: 0.23783089007155508\n",
      "22000/49000 loss: 0.2721851762819557\n",
      "24000/49000 loss: 0.39570797790255124\n",
      "26000/49000 loss: 0.2379592454212152\n",
      "28000/49000 loss: 0.2750701894268848\n",
      "30000/49000 loss: 0.360547729803313\n",
      "32000/49000 loss: 0.23065705894537478\n",
      "34000/49000 loss: 0.2760340343272803\n",
      "36000/49000 loss: 0.34918466450455343\n",
      "38000/49000 loss: 0.25521527111139325\n",
      "40000/49000 loss: 0.3149129791210147\n",
      "42000/49000 loss: 0.281025230057809\n",
      "44000/49000 loss: 0.4093512152430601\n",
      "46000/49000 loss: 0.34226456255771925\n",
      "48000/49000 loss: 0.36467613990981507\n",
      "epoch 26: valid acc = 0.89, new learning rate = 0.00013176004723287096\n",
      "2000/49000 loss: 0.32424011649145124\n",
      "4000/49000 loss: 0.23604226117200303\n",
      "6000/49000 loss: 0.36592425215328317\n",
      "8000/49000 loss: 0.26199604773480484\n",
      "10000/49000 loss: 0.18582274944855523\n",
      "12000/49000 loss: 0.31809895618117445\n",
      "14000/49000 loss: 0.26100263577133087\n",
      "16000/49000 loss: 0.2519199376230154\n",
      "18000/49000 loss: 0.3381030721945822\n",
      "20000/49000 loss: 0.3194483354458395\n",
      "22000/49000 loss: 0.29354056834528025\n",
      "24000/49000 loss: 0.252304972314723\n",
      "26000/49000 loss: 0.2774854378848822\n",
      "28000/49000 loss: 0.2556411768761395\n",
      "30000/49000 loss: 0.2805615611522162\n",
      "32000/49000 loss: 0.272546854064254\n",
      "34000/49000 loss: 0.2871162537418603\n",
      "36000/49000 loss: 0.28439989311984365\n",
      "38000/49000 loss: 0.3148350842704421\n",
      "40000/49000 loss: 0.2631157529024521\n",
      "42000/49000 loss: 0.30587545246959547\n",
      "44000/49000 loss: 0.3041820934311226\n",
      "46000/49000 loss: 0.28286042385917687\n",
      "48000/49000 loss: 0.304719460585239\n",
      "epoch 27: valid acc = 0.886, new learning rate = 0.0001251720448712274\n",
      "2000/49000 loss: 0.3911914827857446\n",
      "4000/49000 loss: 0.2924513351828919\n",
      "6000/49000 loss: 0.29856707158604434\n",
      "8000/49000 loss: 0.3194192741427673\n",
      "10000/49000 loss: 0.25153517724522706\n",
      "12000/49000 loss: 0.2966118966060489\n",
      "14000/49000 loss: 0.34938709488065556\n",
      "16000/49000 loss: 0.26078924450066726\n",
      "18000/49000 loss: 0.3947386078643681\n",
      "20000/49000 loss: 0.3163043425429907\n",
      "22000/49000 loss: 0.23069214618316566\n",
      "24000/49000 loss: 0.3615041642975696\n",
      "26000/49000 loss: 0.27224009327663967\n",
      "28000/49000 loss: 0.30237930428422255\n",
      "30000/49000 loss: 0.25586051489082234\n",
      "32000/49000 loss: 0.30124186965406574\n",
      "34000/49000 loss: 0.37202223104926324\n",
      "36000/49000 loss: 0.3041689266797679\n",
      "38000/49000 loss: 0.19242823637293124\n",
      "40000/49000 loss: 0.333082124069274\n",
      "42000/49000 loss: 0.32966018522219576\n",
      "44000/49000 loss: 0.3065572231765491\n",
      "46000/49000 loss: 0.34952085070863387\n",
      "48000/49000 loss: 0.3267770997531187\n",
      "epoch 28: valid acc = 0.888, new learning rate = 0.00011891344262766602\n",
      "2000/49000 loss: 0.2675618881262548\n",
      "4000/49000 loss: 0.3010761368823951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/49000 loss: 0.29327287562315657\n",
      "8000/49000 loss: 0.23027051880094543\n",
      "10000/49000 loss: 0.30685890201937005\n",
      "12000/49000 loss: 0.3146552278824571\n",
      "14000/49000 loss: 0.2622015083400788\n",
      "16000/49000 loss: 0.2281369598356745\n",
      "18000/49000 loss: 0.3743945192500885\n",
      "20000/49000 loss: 0.29038504725997877\n",
      "22000/49000 loss: 0.25546806570233443\n",
      "24000/49000 loss: 0.2426090374078012\n",
      "26000/49000 loss: 0.29695115268770456\n",
      "28000/49000 loss: 0.3070353801622145\n",
      "30000/49000 loss: 0.29402089443574625\n",
      "32000/49000 loss: 0.2781143158028806\n",
      "34000/49000 loss: 0.32035619025396683\n",
      "36000/49000 loss: 0.29718351664309883\n",
      "38000/49000 loss: 0.3473317888504224\n",
      "40000/49000 loss: 0.3211308503654954\n",
      "42000/49000 loss: 0.20066493760251475\n",
      "44000/49000 loss: 0.3035393217904937\n",
      "46000/49000 loss: 0.26191125080885075\n",
      "48000/49000 loss: 0.3134567609957995\n",
      "epoch 29: valid acc = 0.888, new learning rate = 0.00011296777049628272\n",
      "2000/49000 loss: 0.22189326384673086\n",
      "4000/49000 loss: 0.32166431207544\n",
      "6000/49000 loss: 0.27055463283024894\n",
      "8000/49000 loss: 0.3050834895855724\n",
      "10000/49000 loss: 0.2175479168821449\n",
      "12000/49000 loss: 0.30906114891627606\n",
      "14000/49000 loss: 0.355041208943693\n",
      "16000/49000 loss: 0.16866595110807034\n",
      "18000/49000 loss: 0.32343591043890535\n",
      "20000/49000 loss: 0.25262646031481023\n",
      "22000/49000 loss: 0.3278964225763224\n",
      "24000/49000 loss: 0.29578857393455127\n",
      "26000/49000 loss: 0.26835766625447044\n",
      "28000/49000 loss: 0.33370416179776685\n",
      "30000/49000 loss: 0.22813903828076845\n",
      "32000/49000 loss: 0.33015458549118465\n",
      "34000/49000 loss: 0.3126268886438179\n",
      "36000/49000 loss: 0.30369525315597845\n",
      "38000/49000 loss: 0.33682398862324137\n",
      "40000/49000 loss: 0.23139096711702475\n",
      "42000/49000 loss: 0.38071305081933987\n",
      "44000/49000 loss: 0.3845738326036048\n",
      "46000/49000 loss: 0.2810544551514026\n",
      "48000/49000 loss: 0.2927935942613315\n",
      "epoch 30: valid acc = 0.889, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.8980408163265307\n",
      "test acc: 0.889\n",
      "test acc: 0.8694\n",
      "number of batches for training: 245\n",
      "epoch 1: valid acc = 0.743, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.807, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.83, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.842, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.854, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.859, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.861, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.867, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.868, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.877, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.872, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.882, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.882, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.882, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.882, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.88, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.886, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.88, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.888, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.882, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.879, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.878, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.882, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.878, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.879, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.882, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.889, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.884, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.884, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.882, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.898734693877551\n",
      "test acc: 0.882\n",
      "test acc: 0.8713\n",
      "number of batches for training: 245\n",
      "2000/49000 loss: 3.767943279087519\n",
      "4000/49000 loss: 2.625965247170773\n",
      "6000/49000 loss: 3.096461169050178\n",
      "8000/49000 loss: 2.452779043994816\n",
      "10000/49000 loss: 2.1708536560234495\n",
      "12000/49000 loss: 2.2178600399990005\n",
      "14000/49000 loss: 1.7570318321893506\n",
      "16000/49000 loss: 1.4712472918054074\n",
      "18000/49000 loss: 1.2646270915026696\n",
      "20000/49000 loss: 1.2615707634618762\n",
      "22000/49000 loss: 1.2026238399694498\n",
      "24000/49000 loss: 1.1497826912717408\n",
      "26000/49000 loss: 1.103751466028498\n",
      "28000/49000 loss: 1.1553911298207402\n",
      "30000/49000 loss: 0.9454695025510068\n",
      "32000/49000 loss: 0.9964445836316037\n",
      "34000/49000 loss: 0.9330884635042451\n",
      "36000/49000 loss: 0.9498217206943109\n",
      "38000/49000 loss: 0.8474894675991829\n",
      "40000/49000 loss: 0.8040169781779481\n",
      "42000/49000 loss: 0.8001817895924479\n",
      "44000/49000 loss: 0.7540662201411517\n",
      "46000/49000 loss: 0.824531634235474\n",
      "48000/49000 loss: 0.7744888079383575\n",
      "epoch 1: valid acc = 0.738, new learning rate = 0.000475\n",
      "2000/49000 loss: 0.743274064370896\n",
      "4000/49000 loss: 0.5342799675251456\n",
      "6000/49000 loss: 0.6897123709328983\n",
      "8000/49000 loss: 0.6331408549964591\n",
      "10000/49000 loss: 0.6881492294414474\n",
      "12000/49000 loss: 0.6258247987964661\n",
      "14000/49000 loss: 0.5447158896604486\n",
      "16000/49000 loss: 0.5771367807284989\n",
      "18000/49000 loss: 0.6935701418146231\n",
      "20000/49000 loss: 0.6195134058430418\n",
      "22000/49000 loss: 0.5554516426984843\n",
      "24000/49000 loss: 0.5732914729205465\n",
      "26000/49000 loss: 0.5381785476446124\n",
      "28000/49000 loss: 0.6178394946757169\n",
      "30000/49000 loss: 0.5593122599096622\n",
      "32000/49000 loss: 0.6593917054654149\n",
      "34000/49000 loss: 0.5469858411383128\n",
      "36000/49000 loss: 0.535792919303784\n",
      "38000/49000 loss: 0.500434858505188\n",
      "40000/49000 loss: 0.5462696399817929\n",
      "42000/49000 loss: 0.5003699740074329\n",
      "44000/49000 loss: 0.6295009454933954\n",
      "46000/49000 loss: 0.5797110859718975\n",
      "48000/49000 loss: 0.4933835656054527\n",
      "epoch 2: valid acc = 0.814, new learning rate = 0.00045125\n",
      "2000/49000 loss: 0.5650105938365781\n",
      "4000/49000 loss: 0.4505392053458576\n",
      "6000/49000 loss: 0.5132124210810025\n",
      "8000/49000 loss: 0.4766967045860207\n",
      "10000/49000 loss: 0.580446098290798\n",
      "12000/49000 loss: 0.48475187532768454\n",
      "14000/49000 loss: 0.48800685628948987\n",
      "16000/49000 loss: 0.4726624394150427\n",
      "18000/49000 loss: 0.48829620125587636\n",
      "20000/49000 loss: 0.5028831269578156\n",
      "22000/49000 loss: 0.5922315479936695\n",
      "24000/49000 loss: 0.512654303590441\n",
      "26000/49000 loss: 0.4839372503575382\n",
      "28000/49000 loss: 0.5284516502213013\n",
      "30000/49000 loss: 0.44283306924398885\n",
      "32000/49000 loss: 0.4780019634029366\n",
      "34000/49000 loss: 0.38654865160262547\n",
      "36000/49000 loss: 0.47820642814528974\n",
      "38000/49000 loss: 0.5388434566982893\n",
      "40000/49000 loss: 0.5008014744070797\n",
      "42000/49000 loss: 0.34852787010189096\n",
      "44000/49000 loss: 0.6096953715740069\n",
      "46000/49000 loss: 0.49571994627782445\n",
      "48000/49000 loss: 0.4222553817968261\n",
      "epoch 3: valid acc = 0.826, new learning rate = 0.0004286875\n",
      "2000/49000 loss: 0.4284247895696948\n",
      "4000/49000 loss: 0.46625473090588926\n",
      "6000/49000 loss: 0.38640595563294683\n",
      "8000/49000 loss: 0.45898765025232185\n",
      "10000/49000 loss: 0.5418177144099253\n",
      "12000/49000 loss: 0.46427858830037066\n",
      "14000/49000 loss: 0.4648034398349622\n",
      "16000/49000 loss: 0.468500830545284\n",
      "18000/49000 loss: 0.3966410177826888\n",
      "20000/49000 loss: 0.4979959497770942\n",
      "22000/49000 loss: 0.4237544112442471\n",
      "24000/49000 loss: 0.4878086888665205\n",
      "26000/49000 loss: 0.33441195214188685\n",
      "28000/49000 loss: 0.44362839512307034\n",
      "30000/49000 loss: 0.42655299338689234\n",
      "32000/49000 loss: 0.49872177279617913\n",
      "34000/49000 loss: 0.4742666376291539\n",
      "36000/49000 loss: 0.48411564327767864\n",
      "38000/49000 loss: 0.4367122537764206\n",
      "40000/49000 loss: 0.4115337498456803\n",
      "42000/49000 loss: 0.5489352405546528\n",
      "44000/49000 loss: 0.47412529802039816\n",
      "46000/49000 loss: 0.4022584942617344\n",
      "48000/49000 loss: 0.3586552697928801\n",
      "epoch 4: valid acc = 0.838, new learning rate = 0.00040725312499999993\n",
      "2000/49000 loss: 0.4388641340858805\n",
      "4000/49000 loss: 0.4476085147889628\n",
      "6000/49000 loss: 0.32771245469856686\n",
      "8000/49000 loss: 0.46583276157199\n",
      "10000/49000 loss: 0.4060828739579471\n",
      "12000/49000 loss: 0.431260992383776\n",
      "14000/49000 loss: 0.3911014330466428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000/49000 loss: 0.3776951699756985\n",
      "18000/49000 loss: 0.3159296940691044\n",
      "20000/49000 loss: 0.4553445353828812\n",
      "22000/49000 loss: 0.45577825127407334\n",
      "24000/49000 loss: 0.4829084140388978\n",
      "26000/49000 loss: 0.32587684431810404\n",
      "28000/49000 loss: 0.42147127622383584\n",
      "30000/49000 loss: 0.44551619345714005\n",
      "32000/49000 loss: 0.32185802352694637\n",
      "34000/49000 loss: 0.49040048269951536\n",
      "36000/49000 loss: 0.3611500871575614\n",
      "38000/49000 loss: 0.44078089931402564\n",
      "40000/49000 loss: 0.42725945650764435\n",
      "42000/49000 loss: 0.45307549963766197\n",
      "44000/49000 loss: 0.4221030082939996\n",
      "46000/49000 loss: 0.5019171551259354\n",
      "48000/49000 loss: 0.33058026530359647\n",
      "epoch 5: valid acc = 0.846, new learning rate = 0.0003868904687499999\n",
      "2000/49000 loss: 0.426964267199088\n",
      "4000/49000 loss: 0.39305112746016896\n",
      "6000/49000 loss: 0.2507642815015298\n",
      "8000/49000 loss: 0.30610896129845805\n",
      "10000/49000 loss: 0.38694872809641695\n",
      "12000/49000 loss: 0.36315938587399915\n",
      "14000/49000 loss: 0.41725335960614024\n",
      "16000/49000 loss: 0.38758105420909394\n",
      "18000/49000 loss: 0.3720265815365983\n",
      "20000/49000 loss: 0.4697036705733273\n",
      "22000/49000 loss: 0.34619518418662376\n",
      "24000/49000 loss: 0.3294820281851216\n",
      "26000/49000 loss: 0.4850213451876838\n",
      "28000/49000 loss: 0.3633230467585739\n",
      "30000/49000 loss: 0.48490719731772636\n",
      "32000/49000 loss: 0.31156792886798096\n",
      "34000/49000 loss: 0.3890658824535407\n",
      "36000/49000 loss: 0.4512589618933217\n",
      "38000/49000 loss: 0.4260395365070049\n",
      "40000/49000 loss: 0.4272060514760978\n",
      "42000/49000 loss: 0.32657381874805375\n",
      "44000/49000 loss: 0.3908056078087687\n",
      "46000/49000 loss: 0.31889217375135237\n",
      "48000/49000 loss: 0.4806331209958215\n",
      "epoch 6: valid acc = 0.854, new learning rate = 0.0003675459453124999\n",
      "2000/49000 loss: 0.3068203552012552\n",
      "4000/49000 loss: 0.3745441554990421\n",
      "6000/49000 loss: 0.37225797369821606\n",
      "8000/49000 loss: 0.3836386385308927\n",
      "10000/49000 loss: 0.40788338129609614\n",
      "12000/49000 loss: 0.40738766677148003\n",
      "14000/49000 loss: 0.4049164442421975\n",
      "16000/49000 loss: 0.42074382503646646\n",
      "18000/49000 loss: 0.3488566388739901\n",
      "20000/49000 loss: 0.5496930755065828\n",
      "22000/49000 loss: 0.42049789460685083\n",
      "24000/49000 loss: 0.36786488713349563\n",
      "26000/49000 loss: 0.4607972369250589\n",
      "28000/49000 loss: 0.4109970314405402\n",
      "30000/49000 loss: 0.3756988163974245\n",
      "32000/49000 loss: 0.4247439673237444\n",
      "34000/49000 loss: 0.49947432677293846\n",
      "36000/49000 loss: 0.594870544969527\n",
      "38000/49000 loss: 0.40164634527363224\n",
      "40000/49000 loss: 0.3428234378156432\n",
      "42000/49000 loss: 0.43757393129964284\n",
      "44000/49000 loss: 0.3353073155498895\n",
      "46000/49000 loss: 0.45973269385570864\n",
      "48000/49000 loss: 0.3493677775912266\n",
      "epoch 7: valid acc = 0.861, new learning rate = 0.00034916864804687486\n",
      "2000/49000 loss: 0.37885048509645813\n",
      "4000/49000 loss: 0.44173704037167694\n",
      "6000/49000 loss: 0.35432288097684755\n",
      "8000/49000 loss: 0.5006896149613783\n",
      "10000/49000 loss: 0.34642559972130754\n",
      "12000/49000 loss: 0.404383991114897\n",
      "14000/49000 loss: 0.3000204097464159\n",
      "16000/49000 loss: 0.32709285142616495\n",
      "18000/49000 loss: 0.5140464199064108\n",
      "20000/49000 loss: 0.3802666704716692\n",
      "22000/49000 loss: 0.29558208949698833\n",
      "24000/49000 loss: 0.3689221548293495\n",
      "26000/49000 loss: 0.3712686052044027\n",
      "28000/49000 loss: 0.2808895389443143\n",
      "30000/49000 loss: 0.33678585234080477\n",
      "32000/49000 loss: 0.3658589785783516\n",
      "34000/49000 loss: 0.4194006205824546\n",
      "36000/49000 loss: 0.4605800420288652\n",
      "38000/49000 loss: 0.3303383718210506\n",
      "40000/49000 loss: 0.42153615018887586\n",
      "42000/49000 loss: 0.27106128659390116\n",
      "44000/49000 loss: 0.36941073925776985\n",
      "46000/49000 loss: 0.3469944867556535\n",
      "48000/49000 loss: 0.3957418620964824\n",
      "epoch 8: valid acc = 0.862, new learning rate = 0.0003317102156445311\n",
      "2000/49000 loss: 0.3781621831386042\n",
      "4000/49000 loss: 0.34161241574714063\n",
      "6000/49000 loss: 0.4119852980587514\n",
      "8000/49000 loss: 0.3306135569308446\n",
      "10000/49000 loss: 0.29675110627517143\n",
      "12000/49000 loss: 0.37065293085111295\n",
      "14000/49000 loss: 0.36735032190408706\n",
      "16000/49000 loss: 0.3427609217747486\n",
      "18000/49000 loss: 0.29868643227158337\n",
      "20000/49000 loss: 0.3825114658652122\n",
      "22000/49000 loss: 0.27844459338469957\n",
      "24000/49000 loss: 0.27800656678621444\n",
      "26000/49000 loss: 0.3985533378160545\n",
      "28000/49000 loss: 0.4505182401758478\n",
      "30000/49000 loss: 0.34157900318823914\n",
      "32000/49000 loss: 0.2686509096411212\n",
      "34000/49000 loss: 0.37689728268677125\n",
      "36000/49000 loss: 0.3370366600092728\n",
      "38000/49000 loss: 0.29946636018292455\n",
      "40000/49000 loss: 0.39127488958459444\n",
      "42000/49000 loss: 0.32334703200766723\n",
      "44000/49000 loss: 0.37014741260945205\n",
      "46000/49000 loss: 0.30692472326270187\n",
      "48000/49000 loss: 0.33714614362433387\n",
      "epoch 9: valid acc = 0.872, new learning rate = 0.0003151247048623045\n",
      "2000/49000 loss: 0.38757202159565085\n",
      "4000/49000 loss: 0.3579000215742188\n",
      "6000/49000 loss: 0.35134185136919094\n",
      "8000/49000 loss: 0.3317199747556423\n",
      "10000/49000 loss: 0.34138085076663266\n",
      "12000/49000 loss: 0.40706653387934844\n",
      "14000/49000 loss: 0.42377931481274955\n",
      "16000/49000 loss: 0.37946640497763323\n",
      "18000/49000 loss: 0.30647173305305414\n",
      "20000/49000 loss: 0.31760541914830925\n",
      "22000/49000 loss: 0.30003889790695826\n",
      "24000/49000 loss: 0.37515863351600565\n",
      "26000/49000 loss: 0.2744255488853409\n",
      "28000/49000 loss: 0.37181860852937737\n",
      "30000/49000 loss: 0.3351883934205905\n",
      "32000/49000 loss: 0.4091753248009811\n",
      "34000/49000 loss: 0.3349814723944642\n",
      "36000/49000 loss: 0.3450959514831831\n",
      "38000/49000 loss: 0.3056954847531709\n",
      "40000/49000 loss: 0.4417039744220227\n",
      "42000/49000 loss: 0.34534028541702166\n",
      "44000/49000 loss: 0.35816108233604776\n",
      "46000/49000 loss: 0.3588681083174604\n",
      "48000/49000 loss: 0.3501455085145398\n",
      "epoch 10: valid acc = 0.866, new learning rate = 0.00029936846961918924\n",
      "2000/49000 loss: 0.34924884330452927\n",
      "4000/49000 loss: 0.27116917605119734\n",
      "6000/49000 loss: 0.38358241283238\n",
      "8000/49000 loss: 0.24791610456245528\n",
      "10000/49000 loss: 0.3215822292716377\n",
      "12000/49000 loss: 0.38382787108342575\n",
      "14000/49000 loss: 0.34032836127717664\n",
      "16000/49000 loss: 0.28652408739011\n",
      "18000/49000 loss: 0.3550299024331103\n",
      "20000/49000 loss: 0.40861555124360655\n",
      "22000/49000 loss: 0.31190435401419925\n",
      "24000/49000 loss: 0.2509369214564167\n",
      "26000/49000 loss: 0.27085673960594286\n",
      "28000/49000 loss: 0.3852609785084204\n",
      "30000/49000 loss: 0.3242393972752869\n",
      "32000/49000 loss: 0.4864912385374211\n",
      "34000/49000 loss: 0.33637043114655746\n",
      "36000/49000 loss: 0.38784680384425024\n",
      "38000/49000 loss: 0.32029459833091556\n",
      "40000/49000 loss: 0.3500102030419596\n",
      "42000/49000 loss: 0.5005314121650501\n",
      "44000/49000 loss: 0.3352250679361354\n",
      "46000/49000 loss: 0.2908567748622859\n",
      "48000/49000 loss: 0.3441375217328417\n",
      "epoch 11: valid acc = 0.876, new learning rate = 0.00028440004613822977\n",
      "2000/49000 loss: 0.2759199808211095\n",
      "4000/49000 loss: 0.37919245639176236\n",
      "6000/49000 loss: 0.35020154236499207\n",
      "8000/49000 loss: 0.3264714348720099\n",
      "10000/49000 loss: 0.33231346347575474\n",
      "12000/49000 loss: 0.4163643304524465\n",
      "14000/49000 loss: 0.32682759016194723\n",
      "16000/49000 loss: 0.41147170586381643\n",
      "18000/49000 loss: 0.3717138933870095\n",
      "20000/49000 loss: 0.33572615472007683\n",
      "22000/49000 loss: 0.348639133895657\n",
      "24000/49000 loss: 0.32176270354912084\n",
      "26000/49000 loss: 0.4472731636142025\n",
      "28000/49000 loss: 0.33207736439652297\n",
      "30000/49000 loss: 0.332540644175893\n",
      "32000/49000 loss: 0.3636025045143502\n",
      "34000/49000 loss: 0.331805026354232\n",
      "36000/49000 loss: 0.33110747555354414\n",
      "38000/49000 loss: 0.24927765557760184\n",
      "40000/49000 loss: 0.35508392560305346\n",
      "42000/49000 loss: 0.3982197400851242\n",
      "44000/49000 loss: 0.3195190847691802\n",
      "46000/49000 loss: 0.3451787864249373\n",
      "48000/49000 loss: 0.2068171492223229\n",
      "epoch 12: valid acc = 0.877, new learning rate = 0.00027018004383131826\n",
      "2000/49000 loss: 0.3120996958387463\n",
      "4000/49000 loss: 0.26988760868010025\n",
      "6000/49000 loss: 0.4578503635199345\n",
      "8000/49000 loss: 0.3451307232988069\n",
      "10000/49000 loss: 0.2594282331827899\n",
      "12000/49000 loss: 0.39158848745141633\n",
      "14000/49000 loss: 0.30389464460274607\n",
      "16000/49000 loss: 0.38410393932271136\n",
      "18000/49000 loss: 0.3169570070835773\n",
      "20000/49000 loss: 0.29639770134950033\n",
      "22000/49000 loss: 0.2820555970832573\n",
      "24000/49000 loss: 0.34405595646237264\n",
      "26000/49000 loss: 0.3135460559910657\n",
      "28000/49000 loss: 0.34856373575556493\n",
      "30000/49000 loss: 0.38485650875770056\n",
      "32000/49000 loss: 0.25417052863291284\n",
      "34000/49000 loss: 0.4199241135217411\n",
      "36000/49000 loss: 0.29639487701675216\n",
      "38000/49000 loss: 0.34260010473071045\n",
      "40000/49000 loss: 0.34549616965966806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/49000 loss: 0.29559181108087423\n",
      "44000/49000 loss: 0.33078603515677546\n",
      "46000/49000 loss: 0.3580492301799559\n",
      "48000/49000 loss: 0.2860818808354501\n",
      "epoch 13: valid acc = 0.877, new learning rate = 0.00025667104163975234\n",
      "2000/49000 loss: 0.36709591740349007\n",
      "4000/49000 loss: 0.3019033497626038\n",
      "6000/49000 loss: 0.2949642103886403\n",
      "8000/49000 loss: 0.30794539380125807\n",
      "10000/49000 loss: 0.22230478719395047\n",
      "12000/49000 loss: 0.32951013487344233\n",
      "14000/49000 loss: 0.33046470333830047\n",
      "16000/49000 loss: 0.28721383282027113\n",
      "18000/49000 loss: 0.2849535194744444\n",
      "20000/49000 loss: 0.2727957311324528\n",
      "22000/49000 loss: 0.3564418929951675\n",
      "24000/49000 loss: 0.35774298984069214\n",
      "26000/49000 loss: 0.39078349537135626\n",
      "28000/49000 loss: 0.2582401111425354\n",
      "30000/49000 loss: 0.3210013723045036\n",
      "32000/49000 loss: 0.43962516172252347\n",
      "34000/49000 loss: 0.328659451735393\n",
      "36000/49000 loss: 0.26794684341682995\n",
      "38000/49000 loss: 0.2346122211387481\n",
      "40000/49000 loss: 0.4062222679907132\n",
      "42000/49000 loss: 0.29437087260948047\n",
      "44000/49000 loss: 0.38347139617111997\n",
      "46000/49000 loss: 0.356417274686808\n",
      "48000/49000 loss: 0.3376697823802631\n",
      "epoch 14: valid acc = 0.881, new learning rate = 0.00024383748955776472\n",
      "2000/49000 loss: 0.3386367350610743\n",
      "4000/49000 loss: 0.35943824149166004\n",
      "6000/49000 loss: 0.4023456896708729\n",
      "8000/49000 loss: 0.37837798208700185\n",
      "10000/49000 loss: 0.28752878960123723\n",
      "12000/49000 loss: 0.3396558049267101\n",
      "14000/49000 loss: 0.3257386590951334\n",
      "16000/49000 loss: 0.32224006744571004\n",
      "18000/49000 loss: 0.2558210586830865\n",
      "20000/49000 loss: 0.3880623568923153\n",
      "22000/49000 loss: 0.2755441167806465\n",
      "24000/49000 loss: 0.2863889923708242\n",
      "26000/49000 loss: 0.2962042316523655\n",
      "28000/49000 loss: 0.2981603546521184\n",
      "30000/49000 loss: 0.33831699481319427\n",
      "32000/49000 loss: 0.35776635804909124\n",
      "34000/49000 loss: 0.32057872483202177\n",
      "36000/49000 loss: 0.2839693832874087\n",
      "38000/49000 loss: 0.29774619685606746\n",
      "40000/49000 loss: 0.3341046772660941\n",
      "42000/49000 loss: 0.38744280739175774\n",
      "44000/49000 loss: 0.37579372351935764\n",
      "46000/49000 loss: 0.26827314139697167\n",
      "48000/49000 loss: 0.3312738878361949\n",
      "epoch 15: valid acc = 0.879, new learning rate = 0.00023164561507987649\n",
      "2000/49000 loss: 0.3322059720940223\n",
      "4000/49000 loss: 0.3344998169391092\n",
      "6000/49000 loss: 0.245977860321462\n",
      "8000/49000 loss: 0.27474195279238617\n",
      "10000/49000 loss: 0.39766432208181496\n",
      "12000/49000 loss: 0.344526087816269\n",
      "14000/49000 loss: 0.3765836158685881\n",
      "16000/49000 loss: 0.33775895678487283\n",
      "18000/49000 loss: 0.2975578939417167\n",
      "20000/49000 loss: 0.32065341227064026\n",
      "22000/49000 loss: 0.35575546535803376\n",
      "24000/49000 loss: 0.31365671253970623\n",
      "26000/49000 loss: 0.33297583972056727\n",
      "28000/49000 loss: 0.35782502498408075\n",
      "30000/49000 loss: 0.31983176944969066\n",
      "32000/49000 loss: 0.31956605363738405\n",
      "34000/49000 loss: 0.260083003215913\n",
      "36000/49000 loss: 0.36169678573640196\n",
      "38000/49000 loss: 0.3428187440503254\n",
      "40000/49000 loss: 0.35972772688729326\n",
      "42000/49000 loss: 0.3166184446543724\n",
      "44000/49000 loss: 0.25470598285105683\n",
      "46000/49000 loss: 0.31444649005062036\n",
      "48000/49000 loss: 0.29436111855566377\n",
      "epoch 16: valid acc = 0.887, new learning rate = 0.00022006333432588265\n",
      "2000/49000 loss: 0.30146265884128004\n",
      "4000/49000 loss: 0.27635776560215974\n",
      "6000/49000 loss: 0.3725237115397447\n",
      "8000/49000 loss: 0.3254677101969375\n",
      "10000/49000 loss: 0.38913936068287497\n",
      "12000/49000 loss: 0.3800777205997572\n",
      "14000/49000 loss: 0.36905977072178214\n",
      "16000/49000 loss: 0.2980536352182705\n",
      "18000/49000 loss: 0.37502871720487824\n",
      "20000/49000 loss: 0.4057077194395967\n",
      "22000/49000 loss: 0.36080672399365465\n",
      "24000/49000 loss: 0.3567362947399132\n",
      "26000/49000 loss: 0.27457126349111555\n",
      "28000/49000 loss: 0.2930633963271836\n",
      "30000/49000 loss: 0.3855622470163386\n",
      "32000/49000 loss: 0.2854671784546691\n",
      "34000/49000 loss: 0.3862985347632728\n",
      "36000/49000 loss: 0.30557330746800854\n",
      "38000/49000 loss: 0.30756244606125543\n",
      "40000/49000 loss: 0.27389830855227976\n",
      "42000/49000 loss: 0.3506244811015439\n",
      "44000/49000 loss: 0.2782824366985739\n",
      "46000/49000 loss: 0.39802243337569226\n",
      "48000/49000 loss: 0.3419527065388188\n",
      "epoch 17: valid acc = 0.883, new learning rate = 0.00020906016760958852\n",
      "2000/49000 loss: 0.3165663434858176\n",
      "4000/49000 loss: 0.35872233572669054\n",
      "6000/49000 loss: 0.3215894833325074\n",
      "8000/49000 loss: 0.2652332740797176\n",
      "10000/49000 loss: 0.40315191886438106\n",
      "12000/49000 loss: 0.3760236353556183\n",
      "14000/49000 loss: 0.2775664598714052\n",
      "16000/49000 loss: 0.4011146243454145\n",
      "18000/49000 loss: 0.30433777916846727\n",
      "20000/49000 loss: 0.3241673720050607\n",
      "22000/49000 loss: 0.36822835900182854\n",
      "24000/49000 loss: 0.4019128300842054\n",
      "26000/49000 loss: 0.23761341434048183\n",
      "28000/49000 loss: 0.2455599084832925\n",
      "30000/49000 loss: 0.28808825375263897\n",
      "32000/49000 loss: 0.28880057576648527\n",
      "34000/49000 loss: 0.40381917600474\n",
      "36000/49000 loss: 0.3520958566576048\n",
      "38000/49000 loss: 0.29259079088080164\n",
      "40000/49000 loss: 0.2954114150030611\n",
      "42000/49000 loss: 0.20709780497161087\n",
      "44000/49000 loss: 0.2747743878570883\n",
      "46000/49000 loss: 0.24531379449583796\n",
      "48000/49000 loss: 0.32836240711006237\n",
      "epoch 18: valid acc = 0.88, new learning rate = 0.00019860715922910907\n",
      "2000/49000 loss: 0.39074631445702246\n",
      "4000/49000 loss: 0.37154286608382076\n",
      "6000/49000 loss: 0.2536396014919915\n",
      "8000/49000 loss: 0.29981461275521765\n",
      "10000/49000 loss: 0.2643647154636251\n",
      "12000/49000 loss: 0.3101193061605191\n",
      "14000/49000 loss: 0.36210940230328453\n",
      "16000/49000 loss: 0.3010836908394804\n",
      "18000/49000 loss: 0.3128853775923341\n",
      "20000/49000 loss: 0.45129088273937296\n",
      "22000/49000 loss: 0.30403874422725385\n",
      "24000/49000 loss: 0.30617214100247847\n",
      "26000/49000 loss: 0.28647710471558857\n",
      "28000/49000 loss: 0.3199095037038037\n",
      "30000/49000 loss: 0.3191335074674974\n",
      "32000/49000 loss: 0.3281144299915069\n",
      "34000/49000 loss: 0.391372912545947\n",
      "36000/49000 loss: 0.35199720318715755\n",
      "38000/49000 loss: 0.4039081688561921\n",
      "40000/49000 loss: 0.23777785323403205\n",
      "42000/49000 loss: 0.31996566724558784\n",
      "44000/49000 loss: 0.2886887511373284\n",
      "46000/49000 loss: 0.3247504102469249\n",
      "48000/49000 loss: 0.24583685822420617\n",
      "epoch 19: valid acc = 0.88, new learning rate = 0.0001886768012676536\n",
      "2000/49000 loss: 0.31530011998236207\n",
      "4000/49000 loss: 0.28264634395332394\n",
      "6000/49000 loss: 0.2754102895527212\n",
      "8000/49000 loss: 0.3426913653893069\n",
      "10000/49000 loss: 0.48023860616783326\n",
      "12000/49000 loss: 0.3873048359678223\n",
      "14000/49000 loss: 0.3723461281954515\n",
      "16000/49000 loss: 0.2828438378682015\n",
      "18000/49000 loss: 0.3032758129918081\n",
      "20000/49000 loss: 0.2921951275627883\n",
      "22000/49000 loss: 0.2638385434520283\n",
      "24000/49000 loss: 0.30009198045665225\n",
      "26000/49000 loss: 0.3016046215021208\n",
      "28000/49000 loss: 0.31328861795900487\n",
      "30000/49000 loss: 0.22344466438360155\n",
      "32000/49000 loss: 0.3606930272996592\n",
      "34000/49000 loss: 0.2595557544524072\n",
      "36000/49000 loss: 0.2653156498860052\n",
      "38000/49000 loss: 0.2629939135309214\n",
      "40000/49000 loss: 0.3604565488349065\n",
      "42000/49000 loss: 0.35999831165619056\n",
      "44000/49000 loss: 0.27945089147470736\n",
      "46000/49000 loss: 0.2573788079946531\n",
      "48000/49000 loss: 0.40805263174844364\n",
      "epoch 20: valid acc = 0.891, new learning rate = 0.0001792429612042709\n",
      "2000/49000 loss: 0.28304226781460684\n",
      "4000/49000 loss: 0.3220251764458301\n",
      "6000/49000 loss: 0.33352531055051055\n",
      "8000/49000 loss: 0.2788934569021988\n",
      "10000/49000 loss: 0.3223085981088164\n",
      "12000/49000 loss: 0.2913590768420805\n",
      "14000/49000 loss: 0.2978600022548201\n",
      "16000/49000 loss: 0.28566355149030653\n",
      "18000/49000 loss: 0.3446919311872949\n",
      "20000/49000 loss: 0.23409612551881348\n",
      "22000/49000 loss: 0.2564289310707908\n",
      "24000/49000 loss: 0.2437116286923616\n",
      "26000/49000 loss: 0.2785512023552018\n",
      "28000/49000 loss: 0.22982791337749608\n",
      "30000/49000 loss: 0.27212899879350344\n",
      "32000/49000 loss: 0.3128191177788076\n",
      "34000/49000 loss: 0.26242401548355687\n",
      "36000/49000 loss: 0.23907868709871338\n",
      "38000/49000 loss: 0.28820047474404553\n",
      "40000/49000 loss: 0.31937484132910865\n",
      "42000/49000 loss: 0.29621806407139467\n",
      "44000/49000 loss: 0.3346388436449456\n",
      "46000/49000 loss: 0.29304396223009893\n",
      "48000/49000 loss: 0.41922008615524414\n",
      "epoch 21: valid acc = 0.883, new learning rate = 0.00017028081314405735\n",
      "2000/49000 loss: 0.2759406111762121\n",
      "4000/49000 loss: 0.3256260020194155\n",
      "6000/49000 loss: 0.268754599405782\n",
      "8000/49000 loss: 0.35395941408100967\n",
      "10000/49000 loss: 0.3688256852497901\n",
      "12000/49000 loss: 0.21253633193344654\n",
      "14000/49000 loss: 0.3423999089190044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000/49000 loss: 0.3166961279640573\n",
      "18000/49000 loss: 0.29540527945543543\n",
      "20000/49000 loss: 0.31997438823402335\n",
      "22000/49000 loss: 0.34787511456123477\n",
      "24000/49000 loss: 0.40352625758416005\n",
      "26000/49000 loss: 0.2780543575047152\n",
      "28000/49000 loss: 0.31104464441080787\n",
      "30000/49000 loss: 0.32002801118846586\n",
      "32000/49000 loss: 0.3394401535312269\n",
      "34000/49000 loss: 0.3233216914166695\n",
      "36000/49000 loss: 0.3938894277023056\n",
      "38000/49000 loss: 0.32767713375292984\n",
      "40000/49000 loss: 0.2791877558479768\n",
      "42000/49000 loss: 0.23254233560808937\n",
      "44000/49000 loss: 0.30258316828989357\n",
      "46000/49000 loss: 0.40421362841977454\n",
      "48000/49000 loss: 0.27756088947385504\n",
      "epoch 22: valid acc = 0.883, new learning rate = 0.00016176677248685447\n",
      "2000/49000 loss: 0.18796899307227613\n",
      "4000/49000 loss: 0.2461252112950623\n",
      "6000/49000 loss: 0.36394086984816176\n",
      "8000/49000 loss: 0.2750237551815814\n",
      "10000/49000 loss: 0.32623657063931133\n",
      "12000/49000 loss: 0.2917422410467843\n",
      "14000/49000 loss: 0.25187645965007643\n",
      "16000/49000 loss: 0.3972235138679687\n",
      "18000/49000 loss: 0.40693396808546106\n",
      "20000/49000 loss: 0.2577783710085384\n",
      "22000/49000 loss: 0.2838026275166993\n",
      "24000/49000 loss: 0.2786206089574212\n",
      "26000/49000 loss: 0.260302521880647\n",
      "28000/49000 loss: 0.274820340887514\n",
      "30000/49000 loss: 0.2994295816060906\n",
      "32000/49000 loss: 0.39058750970229933\n",
      "34000/49000 loss: 0.2850665857110633\n",
      "36000/49000 loss: 0.3343174560609827\n",
      "38000/49000 loss: 0.3908449953301929\n",
      "40000/49000 loss: 0.3351939846079665\n",
      "42000/49000 loss: 0.3071945461711752\n",
      "44000/49000 loss: 0.3630851953055032\n",
      "46000/49000 loss: 0.334157390921475\n",
      "48000/49000 loss: 0.3708151912894684\n",
      "epoch 23: valid acc = 0.882, new learning rate = 0.00015367843386251173\n",
      "2000/49000 loss: 0.29460022690659576\n",
      "4000/49000 loss: 0.24296510776416616\n",
      "6000/49000 loss: 0.2695119073979618\n",
      "8000/49000 loss: 0.2745087713868616\n",
      "10000/49000 loss: 0.2623838666100269\n",
      "12000/49000 loss: 0.28885271364427373\n",
      "14000/49000 loss: 0.25721897164758695\n",
      "16000/49000 loss: 0.20819389858746673\n",
      "18000/49000 loss: 0.27243783284906725\n",
      "20000/49000 loss: 0.2986539128739574\n",
      "22000/49000 loss: 0.2642954784902316\n",
      "24000/49000 loss: 0.2721527601803225\n",
      "26000/49000 loss: 0.22825679164932525\n",
      "28000/49000 loss: 0.29528752046162093\n",
      "30000/49000 loss: 0.2516966493010572\n",
      "32000/49000 loss: 0.31717653810753405\n",
      "34000/49000 loss: 0.42182241042633434\n",
      "36000/49000 loss: 0.2629002922837223\n",
      "38000/49000 loss: 0.2596203259565595\n",
      "40000/49000 loss: 0.28016211081153597\n",
      "42000/49000 loss: 0.3189883607160108\n",
      "44000/49000 loss: 0.2867061401856985\n",
      "46000/49000 loss: 0.2852677932189377\n",
      "48000/49000 loss: 0.3179275387011241\n",
      "epoch 24: valid acc = 0.882, new learning rate = 0.00014599451216938612\n",
      "2000/49000 loss: 0.31419929369544747\n",
      "4000/49000 loss: 0.35187722462726073\n",
      "6000/49000 loss: 0.3692810875274426\n",
      "8000/49000 loss: 0.29250193444685885\n",
      "10000/49000 loss: 0.36434370814760575\n",
      "12000/49000 loss: 0.21880090802109467\n",
      "14000/49000 loss: 0.3675915947333257\n",
      "16000/49000 loss: 0.2628257922181427\n",
      "18000/49000 loss: 0.27752637989763734\n",
      "20000/49000 loss: 0.33794209757034377\n",
      "22000/49000 loss: 0.33872927166083805\n",
      "24000/49000 loss: 0.3342994632850723\n",
      "26000/49000 loss: 0.23417285303541074\n",
      "28000/49000 loss: 0.28264569199788075\n",
      "30000/49000 loss: 0.2876578065638415\n",
      "32000/49000 loss: 0.22553422552962735\n",
      "34000/49000 loss: 0.27481227249691653\n",
      "36000/49000 loss: 0.3358201216511444\n",
      "38000/49000 loss: 0.2928909637852445\n",
      "40000/49000 loss: 0.2645221543241554\n",
      "42000/49000 loss: 0.2603860715546944\n",
      "44000/49000 loss: 0.30146877300763586\n",
      "46000/49000 loss: 0.286383282277979\n",
      "48000/49000 loss: 0.27709740513878833\n",
      "epoch 25: valid acc = 0.88, new learning rate = 0.00013869478656091682\n",
      "2000/49000 loss: 0.2983857146229942\n",
      "4000/49000 loss: 0.3191097218508758\n",
      "6000/49000 loss: 0.20635973395294963\n",
      "8000/49000 loss: 0.3354532949872596\n",
      "10000/49000 loss: 0.2626511866127637\n",
      "12000/49000 loss: 0.4312076702863336\n",
      "14000/49000 loss: 0.29468837244716084\n",
      "16000/49000 loss: 0.3201521829971507\n",
      "18000/49000 loss: 0.2502922074784825\n",
      "20000/49000 loss: 0.3020786753529045\n",
      "22000/49000 loss: 0.3189292919033156\n",
      "24000/49000 loss: 0.33397417818900793\n",
      "26000/49000 loss: 0.27340694764547946\n",
      "28000/49000 loss: 0.24032792218163002\n",
      "30000/49000 loss: 0.31843296164311835\n",
      "32000/49000 loss: 0.35179257058908137\n",
      "34000/49000 loss: 0.30083416008656283\n",
      "36000/49000 loss: 0.28204950414063834\n",
      "38000/49000 loss: 0.314640249777064\n",
      "40000/49000 loss: 0.28045367274670047\n",
      "42000/49000 loss: 0.2399984356478386\n",
      "44000/49000 loss: 0.29908857867629834\n",
      "46000/49000 loss: 0.2514831987617822\n",
      "48000/49000 loss: 0.3262757436344644\n",
      "epoch 26: valid acc = 0.884, new learning rate = 0.00013176004723287096\n",
      "2000/49000 loss: 0.3186025782472391\n",
      "4000/49000 loss: 0.31590123651349594\n",
      "6000/49000 loss: 0.3019854355873471\n",
      "8000/49000 loss: 0.26118391204152863\n",
      "10000/49000 loss: 0.2824940126950145\n",
      "12000/49000 loss: 0.2702896613361197\n",
      "14000/49000 loss: 0.2704679874898662\n",
      "16000/49000 loss: 0.2503192519403483\n",
      "18000/49000 loss: 0.21032364230061987\n",
      "20000/49000 loss: 0.36051536500617204\n",
      "22000/49000 loss: 0.3943098695072923\n",
      "24000/49000 loss: 0.3424521797945482\n",
      "26000/49000 loss: 0.30091697954154395\n",
      "28000/49000 loss: 0.22916702060530308\n",
      "30000/49000 loss: 0.2564711091127756\n",
      "32000/49000 loss: 0.3240271653253624\n",
      "34000/49000 loss: 0.32701407864758886\n",
      "36000/49000 loss: 0.30020881196290894\n",
      "38000/49000 loss: 0.2148252593399077\n",
      "40000/49000 loss: 0.2649981187630876\n",
      "42000/49000 loss: 0.3434051484169204\n",
      "44000/49000 loss: 0.2953447832370201\n",
      "46000/49000 loss: 0.40636109832487116\n",
      "48000/49000 loss: 0.27758808378033223\n",
      "epoch 27: valid acc = 0.88, new learning rate = 0.0001251720448712274\n",
      "2000/49000 loss: 0.256473542824289\n",
      "4000/49000 loss: 0.21063025018614448\n",
      "6000/49000 loss: 0.25779790560685695\n",
      "8000/49000 loss: 0.2564115629701047\n",
      "10000/49000 loss: 0.26315970633764085\n",
      "12000/49000 loss: 0.34545336304896473\n",
      "14000/49000 loss: 0.37030118011925345\n",
      "16000/49000 loss: 0.28589663886773287\n",
      "18000/49000 loss: 0.34689171314365636\n",
      "20000/49000 loss: 0.3681991837948485\n",
      "22000/49000 loss: 0.28770580214618474\n",
      "24000/49000 loss: 0.3171835757911589\n",
      "26000/49000 loss: 0.34560010125446466\n",
      "28000/49000 loss: 0.3082232593043854\n",
      "30000/49000 loss: 0.2945427431865291\n",
      "32000/49000 loss: 0.27001801897832767\n",
      "34000/49000 loss: 0.2894780568743301\n",
      "36000/49000 loss: 0.2809977153117154\n",
      "38000/49000 loss: 0.20700978598396927\n",
      "40000/49000 loss: 0.2873165280318543\n",
      "42000/49000 loss: 0.2779385138540102\n",
      "44000/49000 loss: 0.291871863249291\n",
      "46000/49000 loss: 0.2879735437206065\n",
      "48000/49000 loss: 0.23128250535803582\n",
      "epoch 28: valid acc = 0.885, new learning rate = 0.00011891344262766602\n",
      "2000/49000 loss: 0.23863641718640072\n",
      "4000/49000 loss: 0.3343933565041378\n",
      "6000/49000 loss: 0.3522357990023163\n",
      "8000/49000 loss: 0.3117221733769718\n",
      "10000/49000 loss: 0.36837615899939863\n",
      "12000/49000 loss: 0.28938336348251875\n",
      "14000/49000 loss: 0.25191314290625283\n",
      "16000/49000 loss: 0.2959669861817229\n",
      "18000/49000 loss: 0.25841841251465025\n",
      "20000/49000 loss: 0.28161133460938537\n",
      "22000/49000 loss: 0.3195587257625623\n",
      "24000/49000 loss: 0.28666689345726376\n",
      "26000/49000 loss: 0.30233538715557734\n",
      "28000/49000 loss: 0.2800385787280065\n",
      "30000/49000 loss: 0.23329578067985554\n",
      "32000/49000 loss: 0.3328506953062175\n",
      "34000/49000 loss: 0.30071538149912375\n",
      "36000/49000 loss: 0.3048356912499162\n",
      "38000/49000 loss: 0.3346382943032734\n",
      "40000/49000 loss: 0.24344301669513782\n",
      "42000/49000 loss: 0.22025546815450886\n",
      "44000/49000 loss: 0.30740522090108635\n",
      "46000/49000 loss: 0.21250562789528718\n",
      "48000/49000 loss: 0.2665710203457883\n",
      "epoch 29: valid acc = 0.888, new learning rate = 0.00011296777049628272\n",
      "2000/49000 loss: 0.3144828343017103\n",
      "4000/49000 loss: 0.2940874338839585\n",
      "6000/49000 loss: 0.3026908636878797\n",
      "8000/49000 loss: 0.223298541063895\n",
      "10000/49000 loss: 0.2538116613398837\n",
      "12000/49000 loss: 0.2847627624912156\n",
      "14000/49000 loss: 0.31463212753903635\n",
      "16000/49000 loss: 0.394880170719742\n",
      "18000/49000 loss: 0.36790946635074623\n",
      "20000/49000 loss: 0.25912241867569946\n",
      "22000/49000 loss: 0.3101714028386561\n",
      "24000/49000 loss: 0.3393971402124133\n",
      "26000/49000 loss: 0.2623659046395694\n",
      "28000/49000 loss: 0.3245650301983833\n",
      "30000/49000 loss: 0.21679940115311694\n",
      "32000/49000 loss: 0.3866758531826197\n",
      "34000/49000 loss: 0.34811701788500543\n",
      "36000/49000 loss: 0.2111020626510239\n",
      "38000/49000 loss: 0.3425515442703617\n",
      "40000/49000 loss: 0.3487492555485356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/49000 loss: 0.2949883268759936\n",
      "44000/49000 loss: 0.24828544837599284\n",
      "46000/49000 loss: 0.3543790302582408\n",
      "48000/49000 loss: 0.28535864513433445\n",
      "epoch 30: valid acc = 0.884, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.8987755102040816\n",
      "test acc: 0.884\n",
      "test acc: 0.8683\n",
      "number of batches for training: 245\n",
      "epoch 1: valid acc = 0.751, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.809, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.827, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.844, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.85, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.858, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.861, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.865, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.871, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.87, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.869, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.87, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.88, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.875, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.883, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.882, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.881, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.885, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.884, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.89, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.884, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.887, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.887, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.88, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.885, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.885, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.885, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.884, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.885, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.886, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.8991428571428571\n",
      "test acc: 0.886\n",
      "test acc: 0.8677\n",
      "number of batches for training: 245\n",
      "2000/49000 loss: 4.385289732444341\n",
      "4000/49000 loss: 5.231743671479582\n",
      "6000/49000 loss: 2.8232172841989116\n",
      "8000/49000 loss: 2.248551908778418\n",
      "10000/49000 loss: 2.307034437594996\n",
      "12000/49000 loss: 1.9686236287352858\n",
      "14000/49000 loss: 1.838218232619766\n",
      "16000/49000 loss: 1.5381266905059157\n",
      "18000/49000 loss: 1.2682940522321204\n",
      "20000/49000 loss: 1.2424219749740542\n",
      "22000/49000 loss: 1.2198105962959256\n",
      "24000/49000 loss: 0.988439381571909\n",
      "26000/49000 loss: 1.0809689230334185\n",
      "28000/49000 loss: 0.9854745243005257\n",
      "30000/49000 loss: 0.9940599381975611\n",
      "32000/49000 loss: 0.9053951278829432\n",
      "34000/49000 loss: 0.8608960906793037\n",
      "36000/49000 loss: 1.0502762705197892\n",
      "38000/49000 loss: 0.7959746856842583\n",
      "40000/49000 loss: 0.8649853717939059\n",
      "42000/49000 loss: 0.7865210977605187\n",
      "44000/49000 loss: 0.8196014516026798\n",
      "46000/49000 loss: 0.7025494645912072\n",
      "48000/49000 loss: 0.7844054155244957\n",
      "epoch 1: valid acc = 0.742, new learning rate = 0.000475\n",
      "2000/49000 loss: 0.7241868344458693\n",
      "4000/49000 loss: 0.8248398262763228\n",
      "6000/49000 loss: 0.6158727609991628\n",
      "8000/49000 loss: 0.6984390670311932\n",
      "10000/49000 loss: 0.5235560411679125\n",
      "12000/49000 loss: 0.6186158273704113\n",
      "14000/49000 loss: 0.6320441885159814\n",
      "16000/49000 loss: 0.6344686655648741\n",
      "18000/49000 loss: 0.6791741018647911\n",
      "20000/49000 loss: 0.6032645501767702\n",
      "22000/49000 loss: 0.5386462557339382\n",
      "24000/49000 loss: 0.5424678741038409\n",
      "26000/49000 loss: 0.4852482806575311\n",
      "28000/49000 loss: 0.49864303789164716\n",
      "30000/49000 loss: 0.6060289798997328\n",
      "32000/49000 loss: 0.4949537726667718\n",
      "34000/49000 loss: 0.5843549708025159\n",
      "36000/49000 loss: 0.5230477865899809\n",
      "38000/49000 loss: 0.5689496388212619\n",
      "40000/49000 loss: 0.48030796099492074\n",
      "42000/49000 loss: 0.604213424870003\n",
      "44000/49000 loss: 0.5353193653339486\n",
      "46000/49000 loss: 0.5111143531476046\n",
      "48000/49000 loss: 0.6184899692040097\n",
      "epoch 2: valid acc = 0.808, new learning rate = 0.00045125\n",
      "2000/49000 loss: 0.4978471677592856\n",
      "4000/49000 loss: 0.5330492712620056\n",
      "6000/49000 loss: 0.5485783004700566\n",
      "8000/49000 loss: 0.4391653736765755\n",
      "10000/49000 loss: 0.5270197405738828\n",
      "12000/49000 loss: 0.3758049252312267\n",
      "14000/49000 loss: 0.4007025827759104\n",
      "16000/49000 loss: 0.5672778131553932\n",
      "18000/49000 loss: 0.5539022977343926\n",
      "20000/49000 loss: 0.4601432572579066\n",
      "22000/49000 loss: 0.46567206994368615\n",
      "24000/49000 loss: 0.41247316875895906\n",
      "26000/49000 loss: 0.37153828509474107\n",
      "28000/49000 loss: 0.39178605660789756\n",
      "30000/49000 loss: 0.45340773948724744\n",
      "32000/49000 loss: 0.5402838970289248\n",
      "34000/49000 loss: 0.4616871334477932\n",
      "36000/49000 loss: 0.4741455660602739\n",
      "38000/49000 loss: 0.39409318281176403\n",
      "40000/49000 loss: 0.4585115467008518\n",
      "42000/49000 loss: 0.49869449323768245\n",
      "44000/49000 loss: 0.6531183346358679\n",
      "46000/49000 loss: 0.5023392609759941\n",
      "48000/49000 loss: 0.4523841880680381\n",
      "epoch 3: valid acc = 0.829, new learning rate = 0.0004286875\n",
      "2000/49000 loss: 0.33315531301696527\n",
      "4000/49000 loss: 0.399154679067559\n",
      "6000/49000 loss: 0.5380714419143335\n",
      "8000/49000 loss: 0.425675823861879\n",
      "10000/49000 loss: 0.4611544346997385\n",
      "12000/49000 loss: 0.5975748341057798\n",
      "14000/49000 loss: 0.5046634450847312\n",
      "16000/49000 loss: 0.45768345921815534\n",
      "18000/49000 loss: 0.42053431564397886\n",
      "20000/49000 loss: 0.468975528024924\n",
      "22000/49000 loss: 0.5547579072880926\n",
      "24000/49000 loss: 0.47553191890951974\n",
      "26000/49000 loss: 0.5783313036688628\n",
      "28000/49000 loss: 0.46326243282126345\n",
      "30000/49000 loss: 0.5163068321460244\n",
      "32000/49000 loss: 0.44176423570702916\n",
      "34000/49000 loss: 0.3949294290723078\n",
      "36000/49000 loss: 0.39156995443811116\n",
      "38000/49000 loss: 0.37219505676987474\n",
      "40000/49000 loss: 0.41417725785221404\n",
      "42000/49000 loss: 0.48875796768452334\n",
      "44000/49000 loss: 0.4332732082905937\n",
      "46000/49000 loss: 0.3410230147114634\n",
      "48000/49000 loss: 0.4546015777603176\n",
      "epoch 4: valid acc = 0.842, new learning rate = 0.00040725312499999993\n",
      "2000/49000 loss: 0.4992897302904437\n",
      "4000/49000 loss: 0.412036851591297\n",
      "6000/49000 loss: 0.4635733557197374\n",
      "8000/49000 loss: 0.37744085003990496\n",
      "10000/49000 loss: 0.36254499950131647\n",
      "12000/49000 loss: 0.4022499592269867\n",
      "14000/49000 loss: 0.3206344460481679\n",
      "16000/49000 loss: 0.5292308452462754\n",
      "18000/49000 loss: 0.5134926779452565\n",
      "20000/49000 loss: 0.39909972443829506\n",
      "22000/49000 loss: 0.43505534689160663\n",
      "24000/49000 loss: 0.44632945594674606\n",
      "26000/49000 loss: 0.4316468869564494\n",
      "28000/49000 loss: 0.501229869805387\n",
      "30000/49000 loss: 0.3685675260079796\n",
      "32000/49000 loss: 0.42566278975156707\n",
      "34000/49000 loss: 0.40344300167906405\n",
      "36000/49000 loss: 0.40528265926261187\n",
      "38000/49000 loss: 0.3626445485970424\n",
      "40000/49000 loss: 0.4210113983838845\n",
      "42000/49000 loss: 0.5365594430808\n",
      "44000/49000 loss: 0.4062490218596651\n",
      "46000/49000 loss: 0.4287434802691229\n",
      "48000/49000 loss: 0.4421409513837359\n",
      "epoch 5: valid acc = 0.848, new learning rate = 0.0003868904687499999\n",
      "2000/49000 loss: 0.39766412713304977\n",
      "4000/49000 loss: 0.37656188247030603\n",
      "6000/49000 loss: 0.3044357773309105\n",
      "8000/49000 loss: 0.42685869232811735\n",
      "10000/49000 loss: 0.4897780358275905\n",
      "12000/49000 loss: 0.42561444966549394\n",
      "14000/49000 loss: 0.38843545586230976\n",
      "16000/49000 loss: 0.3509196880247642\n",
      "18000/49000 loss: 0.3805168513754773\n",
      "20000/49000 loss: 0.42245666331770193\n",
      "22000/49000 loss: 0.3548865001570201\n",
      "24000/49000 loss: 0.4280123798855689\n",
      "26000/49000 loss: 0.3657872742285554\n",
      "28000/49000 loss: 0.46538213480087737\n",
      "30000/49000 loss: 0.33777685704199806\n",
      "32000/49000 loss: 0.45928674286109944\n",
      "34000/49000 loss: 0.46093999941863795\n",
      "36000/49000 loss: 0.45796228842643094\n",
      "38000/49000 loss: 0.28358669569481515\n",
      "40000/49000 loss: 0.3559874929622913\n",
      "42000/49000 loss: 0.46414342713379947\n",
      "44000/49000 loss: 0.38359292001871065\n",
      "46000/49000 loss: 0.442271278143791\n",
      "48000/49000 loss: 0.49465555414811474\n",
      "epoch 6: valid acc = 0.853, new learning rate = 0.0003675459453124999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/49000 loss: 0.37606805282265704\n",
      "4000/49000 loss: 0.34955800356581285\n",
      "6000/49000 loss: 0.39697836756546273\n",
      "8000/49000 loss: 0.4389748718676514\n",
      "10000/49000 loss: 0.32424606385558097\n",
      "12000/49000 loss: 0.45823243530797303\n",
      "14000/49000 loss: 0.36173320410701676\n",
      "16000/49000 loss: 0.3053183515408168\n",
      "18000/49000 loss: 0.46343075180677273\n",
      "20000/49000 loss: 0.35409851379526847\n",
      "22000/49000 loss: 0.2566470489844478\n",
      "24000/49000 loss: 0.4272648743720321\n",
      "26000/49000 loss: 0.38519876508761514\n",
      "28000/49000 loss: 0.3843824812394184\n",
      "30000/49000 loss: 0.38869421619753114\n",
      "32000/49000 loss: 0.398509706301636\n",
      "34000/49000 loss: 0.28556124629785395\n",
      "36000/49000 loss: 0.4674346909859197\n",
      "38000/49000 loss: 0.46328176069458815\n",
      "40000/49000 loss: 0.47759311826651607\n",
      "42000/49000 loss: 0.34719175389400836\n",
      "44000/49000 loss: 0.3712537825869177\n",
      "46000/49000 loss: 0.4225324917695531\n",
      "48000/49000 loss: 0.28639057143193947\n",
      "epoch 7: valid acc = 0.866, new learning rate = 0.00034916864804687486\n",
      "2000/49000 loss: 0.46294133197693915\n",
      "4000/49000 loss: 0.29500574899044213\n",
      "6000/49000 loss: 0.37029679175240204\n",
      "8000/49000 loss: 0.47087781523598815\n",
      "10000/49000 loss: 0.511484477243643\n",
      "12000/49000 loss: 0.45359292847007915\n",
      "14000/49000 loss: 0.4375206921703514\n",
      "16000/49000 loss: 0.43740888058744115\n",
      "18000/49000 loss: 0.34620661413063264\n",
      "20000/49000 loss: 0.35710509011727276\n",
      "22000/49000 loss: 0.4271726326141924\n",
      "24000/49000 loss: 0.4115708487564808\n",
      "26000/49000 loss: 0.36989801923013366\n",
      "28000/49000 loss: 0.41478808095410524\n",
      "30000/49000 loss: 0.39342334106955384\n",
      "32000/49000 loss: 0.3684068589843304\n",
      "34000/49000 loss: 0.3440004591060152\n",
      "36000/49000 loss: 0.3960674977722258\n",
      "38000/49000 loss: 0.3250203328930565\n",
      "40000/49000 loss: 0.31172016200980485\n",
      "42000/49000 loss: 0.34172392405217866\n",
      "44000/49000 loss: 0.33234117659816115\n",
      "46000/49000 loss: 0.3795401221220519\n",
      "48000/49000 loss: 0.373309234998718\n",
      "epoch 8: valid acc = 0.868, new learning rate = 0.0003317102156445311\n",
      "2000/49000 loss: 0.33127907751985247\n",
      "4000/49000 loss: 0.3713548139666906\n",
      "6000/49000 loss: 0.3415313714355573\n",
      "8000/49000 loss: 0.3968665560031688\n",
      "10000/49000 loss: 0.38319635567333493\n",
      "12000/49000 loss: 0.37622458947881054\n",
      "14000/49000 loss: 0.3998047320980225\n",
      "16000/49000 loss: 0.3569574192324112\n",
      "18000/49000 loss: 0.2888496437782705\n",
      "20000/49000 loss: 0.37768488743682566\n",
      "22000/49000 loss: 0.36605247170390975\n",
      "24000/49000 loss: 0.4190849245755019\n",
      "26000/49000 loss: 0.28622666187493645\n",
      "28000/49000 loss: 0.3791502896899125\n",
      "30000/49000 loss: 0.37990579586278356\n",
      "32000/49000 loss: 0.33681971584083054\n",
      "34000/49000 loss: 0.3507745779185364\n",
      "36000/49000 loss: 0.3554845284306279\n",
      "38000/49000 loss: 0.3824559824328423\n",
      "40000/49000 loss: 0.28366707915339223\n",
      "42000/49000 loss: 0.4481231698859075\n",
      "44000/49000 loss: 0.29563938453268856\n",
      "46000/49000 loss: 0.33414868029542727\n",
      "48000/49000 loss: 0.49173063872177836\n",
      "epoch 9: valid acc = 0.87, new learning rate = 0.0003151247048623045\n",
      "2000/49000 loss: 0.265167980710147\n",
      "4000/49000 loss: 0.41745349963428324\n",
      "6000/49000 loss: 0.3912312687576881\n",
      "8000/49000 loss: 0.3769679561011508\n",
      "10000/49000 loss: 0.44187077252674667\n",
      "12000/49000 loss: 0.3774499622097134\n",
      "14000/49000 loss: 0.38060788876708634\n",
      "16000/49000 loss: 0.34723865631330103\n",
      "18000/49000 loss: 0.3072164316339891\n",
      "20000/49000 loss: 0.335967567578731\n",
      "22000/49000 loss: 0.2976216277656783\n",
      "24000/49000 loss: 0.3504354674485624\n",
      "26000/49000 loss: 0.3864460843010309\n",
      "28000/49000 loss: 0.3197450642815565\n",
      "30000/49000 loss: 0.3735255097707276\n",
      "32000/49000 loss: 0.37997104374061347\n",
      "34000/49000 loss: 0.4780425940829095\n",
      "36000/49000 loss: 0.3420701770020638\n",
      "38000/49000 loss: 0.3973054003421878\n",
      "40000/49000 loss: 0.34560027529312337\n",
      "42000/49000 loss: 0.3579146919631122\n",
      "44000/49000 loss: 0.3433496907014049\n",
      "46000/49000 loss: 0.3499025050595505\n",
      "48000/49000 loss: 0.35528689776862027\n",
      "epoch 10: valid acc = 0.874, new learning rate = 0.00029936846961918924\n",
      "2000/49000 loss: 0.3184274321180583\n",
      "4000/49000 loss: 0.38409990412007605\n",
      "6000/49000 loss: 0.3305273016812103\n",
      "8000/49000 loss: 0.2491728531958868\n",
      "10000/49000 loss: 0.38751525885154475\n",
      "12000/49000 loss: 0.34374562096669586\n",
      "14000/49000 loss: 0.3351695098469418\n",
      "16000/49000 loss: 0.30915680500221077\n",
      "18000/49000 loss: 0.36053236974238234\n",
      "20000/49000 loss: 0.2884395544866799\n",
      "22000/49000 loss: 0.3097660168483516\n",
      "24000/49000 loss: 0.3753655513730548\n",
      "26000/49000 loss: 0.30269258083617456\n",
      "28000/49000 loss: 0.25063722760767465\n",
      "30000/49000 loss: 0.32343203829482087\n",
      "32000/49000 loss: 0.3676693415407604\n",
      "34000/49000 loss: 0.3801403052750092\n",
      "36000/49000 loss: 0.3458613460847432\n",
      "38000/49000 loss: 0.24646096617341015\n",
      "40000/49000 loss: 0.44418235364769065\n",
      "42000/49000 loss: 0.36728350664698095\n",
      "44000/49000 loss: 0.4429006175902473\n",
      "46000/49000 loss: 0.2907571615992645\n",
      "48000/49000 loss: 0.2968794873955621\n",
      "epoch 11: valid acc = 0.87, new learning rate = 0.00028440004613822977\n",
      "2000/49000 loss: 0.3288846463391739\n",
      "4000/49000 loss: 0.2817358135810553\n",
      "6000/49000 loss: 0.3022305767430337\n",
      "8000/49000 loss: 0.31804459705547466\n",
      "10000/49000 loss: 0.3156325080132907\n",
      "12000/49000 loss: 0.35204829633232837\n",
      "14000/49000 loss: 0.2978771488007058\n",
      "16000/49000 loss: 0.3509593820054251\n",
      "18000/49000 loss: 0.38781554473477003\n",
      "20000/49000 loss: 0.42964174659891835\n",
      "22000/49000 loss: 0.3637834092722086\n",
      "24000/49000 loss: 0.36975339134323176\n",
      "26000/49000 loss: 0.2981782315153506\n",
      "28000/49000 loss: 0.35108889778895613\n",
      "30000/49000 loss: 0.3718385782514352\n",
      "32000/49000 loss: 0.3379297307286178\n",
      "34000/49000 loss: 0.3222417164977958\n",
      "36000/49000 loss: 0.3107735077310754\n",
      "38000/49000 loss: 0.27259362178554636\n",
      "40000/49000 loss: 0.28173037439584586\n",
      "42000/49000 loss: 0.3036833035807665\n",
      "44000/49000 loss: 0.37779856877551155\n",
      "46000/49000 loss: 0.3832197379506962\n",
      "48000/49000 loss: 0.444943796242616\n",
      "epoch 12: valid acc = 0.879, new learning rate = 0.00027018004383131826\n",
      "2000/49000 loss: 0.33270265670545923\n",
      "4000/49000 loss: 0.25860393624638456\n",
      "6000/49000 loss: 0.19205627825594943\n",
      "8000/49000 loss: 0.31486503613675004\n",
      "10000/49000 loss: 0.33677497090699526\n",
      "12000/49000 loss: 0.3263726373648729\n",
      "14000/49000 loss: 0.3221754876854978\n",
      "16000/49000 loss: 0.34200908040980454\n",
      "18000/49000 loss: 0.3559042969207094\n",
      "20000/49000 loss: 0.39143567333097024\n",
      "22000/49000 loss: 0.38917866601043993\n",
      "24000/49000 loss: 0.32224738611918813\n",
      "26000/49000 loss: 0.38504753780716494\n",
      "28000/49000 loss: 0.2910280179645938\n",
      "30000/49000 loss: 0.35700446946841047\n",
      "32000/49000 loss: 0.25032877520247715\n",
      "34000/49000 loss: 0.2899143597658492\n",
      "36000/49000 loss: 0.3711004844428022\n",
      "38000/49000 loss: 0.40029357265161386\n",
      "40000/49000 loss: 0.3539465332101143\n",
      "42000/49000 loss: 0.3616553271330035\n",
      "44000/49000 loss: 0.39121454775425685\n",
      "46000/49000 loss: 0.27684602068034486\n",
      "48000/49000 loss: 0.3572787361380927\n",
      "epoch 13: valid acc = 0.876, new learning rate = 0.00025667104163975234\n",
      "2000/49000 loss: 0.2762669042727681\n",
      "4000/49000 loss: 0.2700994971299643\n",
      "6000/49000 loss: 0.3727440548743364\n",
      "8000/49000 loss: 0.31179976751494953\n",
      "10000/49000 loss: 0.3642541106831973\n",
      "12000/49000 loss: 0.2805242809836228\n",
      "14000/49000 loss: 0.34349016705628155\n",
      "16000/49000 loss: 0.4002985787678021\n",
      "18000/49000 loss: 0.32468188455297176\n",
      "20000/49000 loss: 0.30944031011953604\n",
      "22000/49000 loss: 0.2906100685698728\n",
      "24000/49000 loss: 0.2947709571835839\n",
      "26000/49000 loss: 0.33146108028030874\n",
      "28000/49000 loss: 0.36118903364011895\n",
      "30000/49000 loss: 0.35013556108066857\n",
      "32000/49000 loss: 0.3537127474747295\n",
      "34000/49000 loss: 0.37185791628844106\n",
      "36000/49000 loss: 0.4159134890307088\n",
      "38000/49000 loss: 0.32686252049022874\n",
      "40000/49000 loss: 0.32305899414153594\n",
      "42000/49000 loss: 0.3525495827061083\n",
      "44000/49000 loss: 0.4106600958324352\n",
      "46000/49000 loss: 0.3913630422125498\n",
      "48000/49000 loss: 0.35157323690283954\n",
      "epoch 14: valid acc = 0.884, new learning rate = 0.00024383748955776472\n",
      "2000/49000 loss: 0.41817591088537814\n",
      "4000/49000 loss: 0.25879977460266024\n",
      "6000/49000 loss: 0.26883856627500485\n",
      "8000/49000 loss: 0.3323477353211652\n",
      "10000/49000 loss: 0.38950974923609966\n",
      "12000/49000 loss: 0.34929140903477657\n",
      "14000/49000 loss: 0.3569312151931883\n",
      "16000/49000 loss: 0.3000446468703133\n",
      "18000/49000 loss: 0.3349355475282396\n",
      "20000/49000 loss: 0.39809461661762974\n",
      "22000/49000 loss: 0.27334366012782296\n",
      "24000/49000 loss: 0.31423530311424197\n",
      "26000/49000 loss: 0.3750947742376151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28000/49000 loss: 0.3636265541209157\n",
      "30000/49000 loss: 0.3111742092750463\n",
      "32000/49000 loss: 0.36399261206911765\n",
      "34000/49000 loss: 0.39542009926919025\n",
      "36000/49000 loss: 0.3668937229481498\n",
      "38000/49000 loss: 0.32066303175669364\n",
      "40000/49000 loss: 0.3958717301037141\n",
      "42000/49000 loss: 0.33911698275374047\n",
      "44000/49000 loss: 0.2802593573933135\n",
      "46000/49000 loss: 0.32275636224981974\n",
      "48000/49000 loss: 0.28748875863291395\n",
      "epoch 15: valid acc = 0.883, new learning rate = 0.00023164561507987649\n",
      "2000/49000 loss: 0.3392763785944593\n",
      "4000/49000 loss: 0.225744611739093\n",
      "6000/49000 loss: 0.3130614504875663\n",
      "8000/49000 loss: 0.298897509602827\n",
      "10000/49000 loss: 0.2466308148291524\n",
      "12000/49000 loss: 0.33575166705670617\n",
      "14000/49000 loss: 0.3067713500838026\n",
      "16000/49000 loss: 0.32922167831270666\n",
      "18000/49000 loss: 0.3281755788557914\n",
      "20000/49000 loss: 0.31312932181240793\n",
      "22000/49000 loss: 0.3108386774501498\n",
      "24000/49000 loss: 0.43030184941685795\n",
      "26000/49000 loss: 0.4242012705775846\n",
      "28000/49000 loss: 0.3247418090614021\n",
      "30000/49000 loss: 0.33166599689038473\n",
      "32000/49000 loss: 0.2966029971550774\n",
      "34000/49000 loss: 0.44793037135928493\n",
      "36000/49000 loss: 0.2789254844068234\n",
      "38000/49000 loss: 0.37109568269482257\n",
      "40000/49000 loss: 0.3093355955534574\n",
      "42000/49000 loss: 0.318776467474001\n",
      "44000/49000 loss: 0.3716696482633839\n",
      "46000/49000 loss: 0.31450847571596935\n",
      "48000/49000 loss: 0.28870765921302705\n",
      "epoch 16: valid acc = 0.882, new learning rate = 0.00022006333432588265\n",
      "2000/49000 loss: 0.4238668909127706\n",
      "4000/49000 loss: 0.31193065845309903\n",
      "6000/49000 loss: 0.32435709638117155\n",
      "8000/49000 loss: 0.3840929594861112\n",
      "10000/49000 loss: 0.37572055352974276\n",
      "12000/49000 loss: 0.27492066697404677\n",
      "14000/49000 loss: 0.45957061307610203\n",
      "16000/49000 loss: 0.23515061631551007\n",
      "18000/49000 loss: 0.31482332353911924\n",
      "20000/49000 loss: 0.28958988641831684\n",
      "22000/49000 loss: 0.2631751676107187\n",
      "24000/49000 loss: 0.3438098359806987\n",
      "26000/49000 loss: 0.3314150248314527\n",
      "28000/49000 loss: 0.2756003871427113\n",
      "30000/49000 loss: 0.3512350366405007\n",
      "32000/49000 loss: 0.3156393191014857\n",
      "34000/49000 loss: 0.3920252226558026\n",
      "36000/49000 loss: 0.3195485138113598\n",
      "38000/49000 loss: 0.3757297666399473\n",
      "40000/49000 loss: 0.25652308033079396\n",
      "42000/49000 loss: 0.2452189071841241\n",
      "44000/49000 loss: 0.3727131677790797\n",
      "46000/49000 loss: 0.2567875201637769\n",
      "48000/49000 loss: 0.29691777301390626\n",
      "epoch 17: valid acc = 0.885, new learning rate = 0.00020906016760958852\n",
      "2000/49000 loss: 0.2790531530006492\n",
      "4000/49000 loss: 0.3951500345167028\n",
      "6000/49000 loss: 0.27048359357374047\n",
      "8000/49000 loss: 0.2594749166583558\n",
      "10000/49000 loss: 0.3852943366573413\n",
      "12000/49000 loss: 0.37918840807919524\n",
      "14000/49000 loss: 0.3452411335093129\n",
      "16000/49000 loss: 0.32707971809829933\n",
      "18000/49000 loss: 0.3209071433141805\n",
      "20000/49000 loss: 0.3118567460364963\n",
      "22000/49000 loss: 0.398447917304175\n",
      "24000/49000 loss: 0.32827279190280456\n",
      "26000/49000 loss: 0.3119874011230892\n",
      "28000/49000 loss: 0.33264408539663665\n",
      "30000/49000 loss: 0.4029224489880987\n",
      "32000/49000 loss: 0.2770514308348596\n",
      "34000/49000 loss: 0.280976244662651\n",
      "36000/49000 loss: 0.2492433017318771\n",
      "38000/49000 loss: 0.29279059393060175\n",
      "40000/49000 loss: 0.3985955567228738\n",
      "42000/49000 loss: 0.3136165616999987\n",
      "44000/49000 loss: 0.29341040789896516\n",
      "46000/49000 loss: 0.31855037369203754\n",
      "48000/49000 loss: 0.26594336546992675\n",
      "epoch 18: valid acc = 0.883, new learning rate = 0.00019860715922910907\n",
      "2000/49000 loss: 0.37566376070828106\n",
      "4000/49000 loss: 0.24834462707267738\n",
      "6000/49000 loss: 0.2870117972627669\n",
      "8000/49000 loss: 0.3515771113475539\n",
      "10000/49000 loss: 0.33296592151583854\n",
      "12000/49000 loss: 0.2482887198833577\n",
      "14000/49000 loss: 0.252270153798266\n",
      "16000/49000 loss: 0.3513754726489019\n",
      "18000/49000 loss: 0.3401866426296728\n",
      "20000/49000 loss: 0.3211125287025197\n",
      "22000/49000 loss: 0.34747806111090407\n",
      "24000/49000 loss: 0.3323373219024267\n",
      "26000/49000 loss: 0.3335693564043219\n",
      "28000/49000 loss: 0.329833687101035\n",
      "30000/49000 loss: 0.3631970328575604\n",
      "32000/49000 loss: 0.4125629323088797\n",
      "34000/49000 loss: 0.25956436906256486\n",
      "36000/49000 loss: 0.24779994274825448\n",
      "38000/49000 loss: 0.288240350548508\n",
      "40000/49000 loss: 0.2466731250576849\n",
      "42000/49000 loss: 0.3342652238211372\n",
      "44000/49000 loss: 0.3702896697334245\n",
      "46000/49000 loss: 0.31209874255579134\n",
      "48000/49000 loss: 0.35197661456728085\n",
      "epoch 19: valid acc = 0.882, new learning rate = 0.0001886768012676536\n",
      "2000/49000 loss: 0.344275671033511\n",
      "4000/49000 loss: 0.3897717156992973\n",
      "6000/49000 loss: 0.2898347673023326\n",
      "8000/49000 loss: 0.3041637639385964\n",
      "10000/49000 loss: 0.2707126979774136\n",
      "12000/49000 loss: 0.2787398577718716\n",
      "14000/49000 loss: 0.29235910424382067\n",
      "16000/49000 loss: 0.28901072732138455\n",
      "18000/49000 loss: 0.3011922936625944\n",
      "20000/49000 loss: 0.3384816114258872\n",
      "22000/49000 loss: 0.35931247781393055\n",
      "24000/49000 loss: 0.1933663835912214\n",
      "26000/49000 loss: 0.3129968915323269\n",
      "28000/49000 loss: 0.27059643039227843\n",
      "30000/49000 loss: 0.2423189853854976\n",
      "32000/49000 loss: 0.4092939463849569\n",
      "34000/49000 loss: 0.29459466668511214\n",
      "36000/49000 loss: 0.36770451811992844\n",
      "38000/49000 loss: 0.2984327297020335\n",
      "40000/49000 loss: 0.2732996797258575\n",
      "42000/49000 loss: 0.36706467574143326\n",
      "44000/49000 loss: 0.3227383976082165\n",
      "46000/49000 loss: 0.3042170754296397\n",
      "48000/49000 loss: 0.30316845027562445\n",
      "epoch 20: valid acc = 0.883, new learning rate = 0.0001792429612042709\n",
      "2000/49000 loss: 0.2919479540509141\n",
      "4000/49000 loss: 0.2506194833966476\n",
      "6000/49000 loss: 0.32299286286363765\n",
      "8000/49000 loss: 0.32061180281293805\n",
      "10000/49000 loss: 0.2866169461099928\n",
      "12000/49000 loss: 0.2791624567267354\n",
      "14000/49000 loss: 0.3336244566388503\n",
      "16000/49000 loss: 0.22465994927002292\n",
      "18000/49000 loss: 0.2779276586523959\n",
      "20000/49000 loss: 0.2383294998221888\n",
      "22000/49000 loss: 0.3122434271109244\n",
      "24000/49000 loss: 0.2593883569704324\n",
      "26000/49000 loss: 0.22218951410399349\n",
      "28000/49000 loss: 0.2643967034229884\n",
      "30000/49000 loss: 0.23551749694501758\n",
      "32000/49000 loss: 0.23539013186358687\n",
      "34000/49000 loss: 0.27200670310402947\n",
      "36000/49000 loss: 0.27252957953197604\n",
      "38000/49000 loss: 0.31178399930818895\n",
      "40000/49000 loss: 0.28784700743131736\n",
      "42000/49000 loss: 0.24892635282999312\n",
      "44000/49000 loss: 0.278212776184353\n",
      "46000/49000 loss: 0.2607385363006162\n",
      "48000/49000 loss: 0.2691336348240549\n",
      "epoch 21: valid acc = 0.88, new learning rate = 0.00017028081314405735\n",
      "2000/49000 loss: 0.3062149280062813\n",
      "4000/49000 loss: 0.31111123410580166\n",
      "6000/49000 loss: 0.39703114385923644\n",
      "8000/49000 loss: 0.24439938849851048\n",
      "10000/49000 loss: 0.23886541109758058\n",
      "12000/49000 loss: 0.29558791804750567\n",
      "14000/49000 loss: 0.34925810953789665\n",
      "16000/49000 loss: 0.3129228027746619\n",
      "18000/49000 loss: 0.28252796502270044\n",
      "20000/49000 loss: 0.3428358052171861\n",
      "22000/49000 loss: 0.31956524824354327\n",
      "24000/49000 loss: 0.3452128415341224\n",
      "26000/49000 loss: 0.3486711357044517\n",
      "28000/49000 loss: 0.2746524464238095\n",
      "30000/49000 loss: 0.34716539654962447\n",
      "32000/49000 loss: 0.4101936145102328\n",
      "34000/49000 loss: 0.34105495540293573\n",
      "36000/49000 loss: 0.25956206070619686\n",
      "38000/49000 loss: 0.3448031027371565\n",
      "40000/49000 loss: 0.3352498072531134\n",
      "42000/49000 loss: 0.2672917111674411\n",
      "44000/49000 loss: 0.29451037916290596\n",
      "46000/49000 loss: 0.3732636295036558\n",
      "48000/49000 loss: 0.25723850259960873\n",
      "epoch 22: valid acc = 0.881, new learning rate = 0.00016176677248685447\n",
      "2000/49000 loss: 0.3185697015667042\n",
      "4000/49000 loss: 0.25497188239417545\n",
      "6000/49000 loss: 0.2551190409142027\n",
      "8000/49000 loss: 0.33153367891910174\n",
      "10000/49000 loss: 0.3295625020921585\n",
      "12000/49000 loss: 0.344146377160998\n",
      "14000/49000 loss: 0.25414062407592586\n",
      "16000/49000 loss: 0.35810071523994974\n",
      "18000/49000 loss: 0.34380704731824824\n",
      "20000/49000 loss: 0.35442818837638324\n",
      "22000/49000 loss: 0.3130143274178062\n",
      "24000/49000 loss: 0.33898536214219605\n",
      "26000/49000 loss: 0.4561404539025934\n",
      "28000/49000 loss: 0.30733438787459416\n",
      "30000/49000 loss: 0.34500316349801163\n",
      "32000/49000 loss: 0.2863080933072241\n",
      "34000/49000 loss: 0.2795050717924975\n",
      "36000/49000 loss: 0.26212341237172393\n",
      "38000/49000 loss: 0.3145639898263985\n",
      "40000/49000 loss: 0.31212932791010467\n",
      "42000/49000 loss: 0.2921196804482497\n",
      "44000/49000 loss: 0.2934723689624885\n",
      "46000/49000 loss: 0.30600214296102507\n",
      "48000/49000 loss: 0.3424206830690588\n",
      "epoch 23: valid acc = 0.886, new learning rate = 0.00015367843386251173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/49000 loss: 0.38748152210477915\n",
      "4000/49000 loss: 0.3203131167778003\n",
      "6000/49000 loss: 0.2902484265911659\n",
      "8000/49000 loss: 0.2930963179731681\n",
      "10000/49000 loss: 0.3825558643866543\n",
      "12000/49000 loss: 0.23236448628920248\n",
      "14000/49000 loss: 0.30080184016783235\n",
      "16000/49000 loss: 0.34191632371306635\n",
      "18000/49000 loss: 0.19367312146168614\n",
      "20000/49000 loss: 0.36835726996388085\n",
      "22000/49000 loss: 0.2991201680863041\n",
      "24000/49000 loss: 0.25216522696385346\n",
      "26000/49000 loss: 0.2577314891260509\n",
      "28000/49000 loss: 0.29054102834759654\n",
      "30000/49000 loss: 0.3624605763537619\n",
      "32000/49000 loss: 0.3654096800108528\n",
      "34000/49000 loss: 0.34595610918489295\n",
      "36000/49000 loss: 0.2816131983424857\n",
      "38000/49000 loss: 0.2924928035234495\n",
      "40000/49000 loss: 0.21765139615683987\n",
      "42000/49000 loss: 0.2878722262944997\n",
      "44000/49000 loss: 0.32102661847288383\n",
      "46000/49000 loss: 0.23695552020218427\n",
      "48000/49000 loss: 0.23434156187611166\n",
      "epoch 24: valid acc = 0.885, new learning rate = 0.00014599451216938612\n",
      "2000/49000 loss: 0.37862099935092214\n",
      "4000/49000 loss: 0.3266965095887236\n",
      "6000/49000 loss: 0.23047011865697767\n",
      "8000/49000 loss: 0.3386563140866173\n",
      "10000/49000 loss: 0.25142295859216696\n",
      "12000/49000 loss: 0.3121475164881027\n",
      "14000/49000 loss: 0.31113401511360894\n",
      "16000/49000 loss: 0.27921527015025044\n",
      "18000/49000 loss: 0.3040753783164857\n",
      "20000/49000 loss: 0.2645210518155455\n",
      "22000/49000 loss: 0.263364587563709\n",
      "24000/49000 loss: 0.2341196346689733\n",
      "26000/49000 loss: 0.31554359396830584\n",
      "28000/49000 loss: 0.3293088473691402\n",
      "30000/49000 loss: 0.3307998300407648\n",
      "32000/49000 loss: 0.33708485512791087\n",
      "34000/49000 loss: 0.27423383593722683\n",
      "36000/49000 loss: 0.2928525417778244\n",
      "38000/49000 loss: 0.3530299840239776\n",
      "40000/49000 loss: 0.34196074444922586\n",
      "42000/49000 loss: 0.30788064960651224\n",
      "44000/49000 loss: 0.2917834116353621\n",
      "46000/49000 loss: 0.3784366120906276\n",
      "48000/49000 loss: 0.24213264729354309\n",
      "epoch 25: valid acc = 0.885, new learning rate = 0.00013869478656091682\n",
      "2000/49000 loss: 0.3233047569597594\n",
      "4000/49000 loss: 0.2347908686474432\n",
      "6000/49000 loss: 0.36418738057989347\n",
      "8000/49000 loss: 0.30583182056167874\n",
      "10000/49000 loss: 0.35654784156818187\n",
      "12000/49000 loss: 0.29252342781790963\n",
      "14000/49000 loss: 0.298935411010259\n",
      "16000/49000 loss: 0.34176795096958684\n",
      "18000/49000 loss: 0.27776081580564776\n",
      "20000/49000 loss: 0.30442438384766046\n",
      "22000/49000 loss: 0.25817977279160764\n",
      "24000/49000 loss: 0.3009261380247462\n",
      "26000/49000 loss: 0.3071726214357233\n",
      "28000/49000 loss: 0.3811010921584623\n",
      "30000/49000 loss: 0.2638380389798725\n",
      "32000/49000 loss: 0.31656494266888446\n",
      "34000/49000 loss: 0.37306694422952474\n",
      "36000/49000 loss: 0.3168107244718354\n",
      "38000/49000 loss: 0.24830407296441273\n",
      "40000/49000 loss: 0.34728930595421104\n",
      "42000/49000 loss: 0.21221019337319613\n",
      "44000/49000 loss: 0.2805298725549983\n",
      "46000/49000 loss: 0.32277527990029004\n",
      "48000/49000 loss: 0.2853249916564583\n",
      "epoch 26: valid acc = 0.88, new learning rate = 0.00013176004723287096\n",
      "2000/49000 loss: 0.2812205440299021\n",
      "4000/49000 loss: 0.3089863347832494\n",
      "6000/49000 loss: 0.2446054080361676\n",
      "8000/49000 loss: 0.26774135336312127\n",
      "10000/49000 loss: 0.20977673838861724\n",
      "12000/49000 loss: 0.3412640900270184\n",
      "14000/49000 loss: 0.19273171215336873\n",
      "16000/49000 loss: 0.22648169237185498\n",
      "18000/49000 loss: 0.23100964674794724\n",
      "20000/49000 loss: 0.2643071147125082\n",
      "22000/49000 loss: 0.3019550699328685\n",
      "24000/49000 loss: 0.27402696649364144\n",
      "26000/49000 loss: 0.30722521395759156\n",
      "28000/49000 loss: 0.3494683110944448\n",
      "30000/49000 loss: 0.33162794110860055\n",
      "32000/49000 loss: 0.3587688889890116\n",
      "34000/49000 loss: 0.2942277238007888\n",
      "36000/49000 loss: 0.3297085741841461\n",
      "38000/49000 loss: 0.2549308794516981\n",
      "40000/49000 loss: 0.30607704028836014\n",
      "42000/49000 loss: 0.28860076458509504\n",
      "44000/49000 loss: 0.19102219722725242\n",
      "46000/49000 loss: 0.27665355436689054\n",
      "48000/49000 loss: 0.29466165650857573\n",
      "epoch 27: valid acc = 0.885, new learning rate = 0.0001251720448712274\n",
      "2000/49000 loss: 0.2547109422309687\n",
      "4000/49000 loss: 0.314260297594117\n",
      "6000/49000 loss: 0.3891827407848882\n",
      "8000/49000 loss: 0.2403137752946408\n",
      "10000/49000 loss: 0.32240136395091173\n",
      "12000/49000 loss: 0.2602328888502358\n",
      "14000/49000 loss: 0.3748859616009049\n",
      "16000/49000 loss: 0.3314571505058175\n",
      "18000/49000 loss: 0.3160785592856246\n",
      "20000/49000 loss: 0.2663807371094292\n",
      "22000/49000 loss: 0.2715049274546302\n",
      "24000/49000 loss: 0.24843865581871577\n",
      "26000/49000 loss: 0.3362924987623423\n",
      "28000/49000 loss: 0.2382636120684232\n",
      "30000/49000 loss: 0.2805837457757648\n",
      "32000/49000 loss: 0.20794762471906986\n",
      "34000/49000 loss: 0.27974795102695693\n",
      "36000/49000 loss: 0.20697610203395406\n",
      "38000/49000 loss: 0.32784374429723684\n",
      "40000/49000 loss: 0.29636319530766364\n",
      "42000/49000 loss: 0.3070404859736637\n",
      "44000/49000 loss: 0.24002353251080313\n",
      "46000/49000 loss: 0.3467022266836179\n",
      "48000/49000 loss: 0.2952679265381677\n",
      "epoch 28: valid acc = 0.887, new learning rate = 0.00011891344262766602\n",
      "2000/49000 loss: 0.21194258424277082\n",
      "4000/49000 loss: 0.24752002552378438\n",
      "6000/49000 loss: 0.2825213230805992\n",
      "8000/49000 loss: 0.33688957726139385\n",
      "10000/49000 loss: 0.2210333892113657\n",
      "12000/49000 loss: 0.31016398560270464\n",
      "14000/49000 loss: 0.3064275364019418\n",
      "16000/49000 loss: 0.29138658752068014\n",
      "18000/49000 loss: 0.31994926834848525\n",
      "20000/49000 loss: 0.4136068811645342\n",
      "22000/49000 loss: 0.2515117280602554\n",
      "24000/49000 loss: 0.4232732270112119\n",
      "26000/49000 loss: 0.26291962216910353\n",
      "28000/49000 loss: 0.3209846891804948\n",
      "30000/49000 loss: 0.2864578125281573\n",
      "32000/49000 loss: 0.3185721263840574\n",
      "34000/49000 loss: 0.2862007390209128\n",
      "36000/49000 loss: 0.2661196858217727\n",
      "38000/49000 loss: 0.25340701753821604\n",
      "40000/49000 loss: 0.32495419808702614\n",
      "42000/49000 loss: 0.1873519844300048\n",
      "44000/49000 loss: 0.30040152530239644\n",
      "46000/49000 loss: 0.23663184586831373\n",
      "48000/49000 loss: 0.3636614365305517\n",
      "epoch 29: valid acc = 0.885, new learning rate = 0.00011296777049628272\n",
      "2000/49000 loss: 0.2672771624812941\n",
      "4000/49000 loss: 0.26245355039927054\n",
      "6000/49000 loss: 0.27406420071645987\n",
      "8000/49000 loss: 0.2864647112438147\n",
      "10000/49000 loss: 0.2844868626238873\n",
      "12000/49000 loss: 0.26207515621084776\n",
      "14000/49000 loss: 0.3066278099138948\n",
      "16000/49000 loss: 0.27726786153430044\n",
      "18000/49000 loss: 0.30307377987529943\n",
      "20000/49000 loss: 0.21994557729115616\n",
      "22000/49000 loss: 0.25974849556800184\n",
      "24000/49000 loss: 0.24874498107726978\n",
      "26000/49000 loss: 0.2303252093706908\n",
      "28000/49000 loss: 0.34271662077138265\n",
      "30000/49000 loss: 0.2839688601002289\n",
      "32000/49000 loss: 0.3113683384512955\n",
      "34000/49000 loss: 0.3448519328506524\n",
      "36000/49000 loss: 0.27673688821325354\n",
      "38000/49000 loss: 0.3444867185566754\n",
      "40000/49000 loss: 0.2903552539175912\n",
      "42000/49000 loss: 0.3761638238968848\n",
      "44000/49000 loss: 0.30553443103220695\n",
      "46000/49000 loss: 0.27301494285677064\n",
      "48000/49000 loss: 0.3140386589374613\n",
      "epoch 30: valid acc = 0.888, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.8992040816326531\n",
      "test acc: 0.888\n",
      "test acc: 0.8709\n",
      "number of batches for training: 245\n",
      "epoch 1: valid acc = 0.738, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.805, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.825, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.848, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.85, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.856, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.858, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.853, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.869, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.872, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.878, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.878, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.874, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.882, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.881, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.882, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.888, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.89, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.882, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.885, new learning rate = 0.0001792429612042709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21: valid acc = 0.884, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.887, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.883, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.887, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.884, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.885, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.88, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.884, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.887, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.881, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.8981632653061224\n",
      "test acc: 0.881\n",
      "test acc: 0.8705\n",
      "number of batches for training: 81\n",
      "6000/49000 loss: 4.3446243997234975\n",
      "12000/49000 loss: 3.0706797110559596\n",
      "18000/49000 loss: 2.6174633154363067\n",
      "24000/49000 loss: 2.6741047434950556\n",
      "30000/49000 loss: 2.2275807091912356\n",
      "36000/49000 loss: 2.2047373892274496\n",
      "42000/49000 loss: 1.8176196508703917\n",
      "48000/49000 loss: 1.5190855537964636\n",
      "epoch 1: valid acc = 0.492, new learning rate = 0.000475\n",
      "6000/49000 loss: 1.4086286415403966\n",
      "12000/49000 loss: 1.230702629745579\n",
      "18000/49000 loss: 1.252628425573621\n",
      "24000/49000 loss: 1.2112978045040055\n",
      "30000/49000 loss: 1.1714040452047112\n",
      "36000/49000 loss: 1.2519584970413773\n",
      "42000/49000 loss: 1.0103536755113613\n",
      "48000/49000 loss: 1.080846561022896\n",
      "epoch 2: valid acc = 0.663, new learning rate = 0.00045125\n",
      "6000/49000 loss: 0.947822808973265\n",
      "12000/49000 loss: 0.8867992760804454\n",
      "18000/49000 loss: 0.8473112543727754\n",
      "24000/49000 loss: 0.8792827238062624\n",
      "30000/49000 loss: 0.8222628286089367\n",
      "36000/49000 loss: 0.8770332024008378\n",
      "42000/49000 loss: 0.8062267965005436\n",
      "48000/49000 loss: 0.7747790651658185\n",
      "epoch 3: valid acc = 0.738, new learning rate = 0.0004286875\n",
      "6000/49000 loss: 0.8225748015444752\n",
      "12000/49000 loss: 0.7248032226586759\n",
      "18000/49000 loss: 0.7090837247255134\n",
      "24000/49000 loss: 0.6613401463243732\n",
      "30000/49000 loss: 0.6614427959315257\n",
      "36000/49000 loss: 0.6874393135992036\n",
      "42000/49000 loss: 0.5476376893625797\n",
      "48000/49000 loss: 0.6596743542363189\n",
      "epoch 4: valid acc = 0.774, new learning rate = 0.00040725312499999993\n",
      "6000/49000 loss: 0.620492222174996\n",
      "12000/49000 loss: 0.6326064425076808\n",
      "18000/49000 loss: 0.5948985376710063\n",
      "24000/49000 loss: 0.6208849099331882\n",
      "30000/49000 loss: 0.596087321384326\n",
      "36000/49000 loss: 0.6407772919690182\n",
      "42000/49000 loss: 0.5528148071523948\n",
      "48000/49000 loss: 0.6306675804113379\n",
      "epoch 5: valid acc = 0.787, new learning rate = 0.0003868904687499999\n",
      "6000/49000 loss: 0.5646300404958767\n",
      "12000/49000 loss: 0.6205282706909494\n",
      "18000/49000 loss: 0.5916094140480845\n",
      "24000/49000 loss: 0.6530889462410688\n",
      "30000/49000 loss: 0.5115771067562637\n",
      "36000/49000 loss: 0.5318066320030834\n",
      "42000/49000 loss: 0.5238113209595808\n",
      "48000/49000 loss: 0.5819002365496572\n",
      "epoch 6: valid acc = 0.806, new learning rate = 0.0003675459453124999\n",
      "6000/49000 loss: 0.5588215214151679\n",
      "12000/49000 loss: 0.5347884106446578\n",
      "18000/49000 loss: 0.5192381385331435\n",
      "24000/49000 loss: 0.5146830061323382\n",
      "30000/49000 loss: 0.5278605186126932\n",
      "36000/49000 loss: 0.5531185020423967\n",
      "42000/49000 loss: 0.5669940915135879\n",
      "48000/49000 loss: 0.5201617267788746\n",
      "epoch 7: valid acc = 0.81, new learning rate = 0.00034916864804687486\n",
      "6000/49000 loss: 0.4494599374409131\n",
      "12000/49000 loss: 0.5056605248222151\n",
      "18000/49000 loss: 0.5302111852868084\n",
      "24000/49000 loss: 0.4614460587951385\n",
      "30000/49000 loss: 0.4968691818919286\n",
      "36000/49000 loss: 0.49976808577052967\n",
      "42000/49000 loss: 0.47549759965839067\n",
      "48000/49000 loss: 0.5181042843297197\n",
      "epoch 8: valid acc = 0.814, new learning rate = 0.0003317102156445311\n",
      "6000/49000 loss: 0.49719904950929295\n",
      "12000/49000 loss: 0.44627575996552626\n",
      "18000/49000 loss: 0.45989252246652007\n",
      "24000/49000 loss: 0.5451155396352847\n",
      "30000/49000 loss: 0.46381312991801854\n",
      "36000/49000 loss: 0.4928575177474283\n",
      "42000/49000 loss: 0.4200900539958805\n",
      "48000/49000 loss: 0.4696960444822812\n",
      "epoch 9: valid acc = 0.818, new learning rate = 0.0003151247048623045\n",
      "6000/49000 loss: 0.46564750117263465\n",
      "12000/49000 loss: 0.4940767300638259\n",
      "18000/49000 loss: 0.47940297508645374\n",
      "24000/49000 loss: 0.4755187839058531\n",
      "30000/49000 loss: 0.4274704405798184\n",
      "36000/49000 loss: 0.4570837297129986\n",
      "42000/49000 loss: 0.4720371328918695\n",
      "48000/49000 loss: 0.4697150629730951\n",
      "epoch 10: valid acc = 0.826, new learning rate = 0.00029936846961918924\n",
      "6000/49000 loss: 0.45648203829687395\n",
      "12000/49000 loss: 0.4954355396878302\n",
      "18000/49000 loss: 0.47316297256312784\n",
      "24000/49000 loss: 0.46635640445057597\n",
      "30000/49000 loss: 0.4346093979951543\n",
      "36000/49000 loss: 0.49120777332097154\n",
      "42000/49000 loss: 0.3642796589530881\n",
      "48000/49000 loss: 0.500660654117657\n",
      "epoch 11: valid acc = 0.827, new learning rate = 0.00028440004613822977\n",
      "6000/49000 loss: 0.47318055658889147\n",
      "12000/49000 loss: 0.4231527332061458\n",
      "18000/49000 loss: 0.3932762483569266\n",
      "24000/49000 loss: 0.5160963280558918\n",
      "30000/49000 loss: 0.48828189195219723\n",
      "36000/49000 loss: 0.3849657568552621\n",
      "42000/49000 loss: 0.4754279089609471\n",
      "48000/49000 loss: 0.49194454085785094\n",
      "epoch 12: valid acc = 0.835, new learning rate = 0.00027018004383131826\n",
      "6000/49000 loss: 0.4047734892923719\n",
      "12000/49000 loss: 0.44138816065846015\n",
      "18000/49000 loss: 0.4266583466621362\n",
      "24000/49000 loss: 0.4798467258278385\n",
      "30000/49000 loss: 0.4344658330894227\n",
      "36000/49000 loss: 0.4523911468098052\n",
      "42000/49000 loss: 0.5226711757427948\n",
      "48000/49000 loss: 0.47662354394636286\n",
      "epoch 13: valid acc = 0.837, new learning rate = 0.00025667104163975234\n",
      "6000/49000 loss: 0.4926329425784905\n",
      "12000/49000 loss: 0.44503121191284817\n",
      "18000/49000 loss: 0.4341974715116808\n",
      "24000/49000 loss: 0.3859473449717254\n",
      "30000/49000 loss: 0.43615177862613996\n",
      "36000/49000 loss: 0.47506779694352996\n",
      "42000/49000 loss: 0.44094782725172826\n",
      "48000/49000 loss: 0.4309983271856879\n",
      "epoch 14: valid acc = 0.838, new learning rate = 0.00024383748955776472\n",
      "6000/49000 loss: 0.38386613984028184\n",
      "12000/49000 loss: 0.4065267680321483\n",
      "18000/49000 loss: 0.48314732152160483\n",
      "24000/49000 loss: 0.393270495511708\n",
      "30000/49000 loss: 0.45314965703084265\n",
      "36000/49000 loss: 0.41206552115565825\n",
      "42000/49000 loss: 0.39663272657161\n",
      "48000/49000 loss: 0.46086695502362574\n",
      "epoch 15: valid acc = 0.845, new learning rate = 0.00023164561507987649\n",
      "6000/49000 loss: 0.36558930988907967\n",
      "12000/49000 loss: 0.45385537283178684\n",
      "18000/49000 loss: 0.414516560667032\n",
      "24000/49000 loss: 0.4405345153611213\n",
      "30000/49000 loss: 0.41128898329057195\n",
      "36000/49000 loss: 0.40894955078094786\n",
      "42000/49000 loss: 0.3947964463569214\n",
      "48000/49000 loss: 0.42749854615988503\n",
      "epoch 16: valid acc = 0.844, new learning rate = 0.00022006333432588265\n",
      "6000/49000 loss: 0.3775400402337291\n",
      "12000/49000 loss: 0.4216595078656249\n",
      "18000/49000 loss: 0.4295604489853084\n",
      "24000/49000 loss: 0.451398298378053\n",
      "30000/49000 loss: 0.43259888274469516\n",
      "36000/49000 loss: 0.4726523375895192\n",
      "42000/49000 loss: 0.459537437027213\n",
      "48000/49000 loss: 0.4444311262362779\n",
      "epoch 17: valid acc = 0.847, new learning rate = 0.00020906016760958852\n",
      "6000/49000 loss: 0.44687698510543394\n",
      "12000/49000 loss: 0.4410201390708465\n",
      "18000/49000 loss: 0.40517186958031465\n",
      "24000/49000 loss: 0.4392739720093111\n",
      "30000/49000 loss: 0.4764691013136164\n",
      "36000/49000 loss: 0.46093445667301614\n",
      "42000/49000 loss: 0.3824188037139977\n",
      "48000/49000 loss: 0.5066222306324221\n",
      "epoch 18: valid acc = 0.848, new learning rate = 0.00019860715922910907\n",
      "6000/49000 loss: 0.4235328530981533\n",
      "12000/49000 loss: 0.42978337842473374\n",
      "18000/49000 loss: 0.4170038658554267\n",
      "24000/49000 loss: 0.410373233788124\n",
      "30000/49000 loss: 0.45691980801119725\n",
      "36000/49000 loss: 0.37638745596167705\n",
      "42000/49000 loss: 0.3847037243730686\n",
      "48000/49000 loss: 0.38716528069127654\n",
      "epoch 19: valid acc = 0.852, new learning rate = 0.0001886768012676536\n",
      "6000/49000 loss: 0.42596783831892404\n",
      "12000/49000 loss: 0.37114282987079295\n",
      "18000/49000 loss: 0.4656456069186824\n",
      "24000/49000 loss: 0.4409639708838882\n",
      "30000/49000 loss: 0.38505923750789295\n",
      "36000/49000 loss: 0.4338848378799541\n",
      "42000/49000 loss: 0.4164488798669628\n",
      "48000/49000 loss: 0.3744192171860153\n",
      "epoch 20: valid acc = 0.855, new learning rate = 0.0001792429612042709\n",
      "6000/49000 loss: 0.38268329555433644\n",
      "12000/49000 loss: 0.4221762361292158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/49000 loss: 0.43856307273712036\n",
      "24000/49000 loss: 0.4155732779049051\n",
      "30000/49000 loss: 0.4079714505127741\n",
      "36000/49000 loss: 0.434560674894688\n",
      "42000/49000 loss: 0.3729240073273519\n",
      "48000/49000 loss: 0.43630939499418164\n",
      "epoch 21: valid acc = 0.853, new learning rate = 0.00017028081314405735\n",
      "6000/49000 loss: 0.45050921149586437\n",
      "12000/49000 loss: 0.3861705008624813\n",
      "18000/49000 loss: 0.4471395944560802\n",
      "24000/49000 loss: 0.41258442862298467\n",
      "30000/49000 loss: 0.4016651714521563\n",
      "36000/49000 loss: 0.4047403379680154\n",
      "42000/49000 loss: 0.38383944758424293\n",
      "48000/49000 loss: 0.41582585485120377\n",
      "epoch 22: valid acc = 0.853, new learning rate = 0.00016176677248685447\n",
      "6000/49000 loss: 0.38075230965439355\n",
      "12000/49000 loss: 0.42435630814020786\n",
      "18000/49000 loss: 0.42996179207478663\n",
      "24000/49000 loss: 0.345918568188166\n",
      "30000/49000 loss: 0.411987522960098\n",
      "36000/49000 loss: 0.42639085553665695\n",
      "42000/49000 loss: 0.44993098843981083\n",
      "48000/49000 loss: 0.3929422181299235\n",
      "epoch 23: valid acc = 0.854, new learning rate = 0.00015367843386251173\n",
      "6000/49000 loss: 0.40435671168459725\n",
      "12000/49000 loss: 0.37993824078995697\n",
      "18000/49000 loss: 0.38090866143369345\n",
      "24000/49000 loss: 0.3650310385366679\n",
      "30000/49000 loss: 0.34266089782763015\n",
      "36000/49000 loss: 0.3841400998593325\n",
      "42000/49000 loss: 0.3983086578935588\n",
      "48000/49000 loss: 0.3539810036720181\n",
      "epoch 24: valid acc = 0.859, new learning rate = 0.00014599451216938612\n",
      "6000/49000 loss: 0.398227156767523\n",
      "12000/49000 loss: 0.47561922052019123\n",
      "18000/49000 loss: 0.3788256211086821\n",
      "24000/49000 loss: 0.4055653156159186\n",
      "30000/49000 loss: 0.3778286752716015\n",
      "36000/49000 loss: 0.4140730835967831\n",
      "42000/49000 loss: 0.399940586640938\n",
      "48000/49000 loss: 0.40544797409247607\n",
      "epoch 25: valid acc = 0.86, new learning rate = 0.00013869478656091682\n",
      "6000/49000 loss: 0.382088876968171\n",
      "12000/49000 loss: 0.39925659507322514\n",
      "18000/49000 loss: 0.3416797795965062\n",
      "24000/49000 loss: 0.39398529639154845\n",
      "30000/49000 loss: 0.3687296664260106\n",
      "36000/49000 loss: 0.41436431304011334\n",
      "42000/49000 loss: 0.3627999961803191\n",
      "48000/49000 loss: 0.4151669694479288\n",
      "epoch 26: valid acc = 0.857, new learning rate = 0.00013176004723287096\n",
      "6000/49000 loss: 0.3581797560802341\n",
      "12000/49000 loss: 0.42921027767417236\n",
      "18000/49000 loss: 0.34693451571307554\n",
      "24000/49000 loss: 0.46693165941598325\n",
      "30000/49000 loss: 0.40430934098193155\n",
      "36000/49000 loss: 0.40899545947410926\n",
      "42000/49000 loss: 0.37878469454730346\n",
      "48000/49000 loss: 0.34634118211824383\n",
      "epoch 27: valid acc = 0.862, new learning rate = 0.0001251720448712274\n",
      "6000/49000 loss: 0.4142287453599626\n",
      "12000/49000 loss: 0.4293130949356141\n",
      "18000/49000 loss: 0.40022436195179256\n",
      "24000/49000 loss: 0.33646395897984205\n",
      "30000/49000 loss: 0.3719264952908454\n",
      "36000/49000 loss: 0.4165298619964703\n",
      "42000/49000 loss: 0.3875548610069304\n",
      "48000/49000 loss: 0.40667581918618234\n",
      "epoch 28: valid acc = 0.856, new learning rate = 0.00011891344262766602\n",
      "6000/49000 loss: 0.3550494694279352\n",
      "12000/49000 loss: 0.4104444989850263\n",
      "18000/49000 loss: 0.35286757667938173\n",
      "24000/49000 loss: 0.4262347772444133\n",
      "30000/49000 loss: 0.33944198942550236\n",
      "36000/49000 loss: 0.36991168665968\n",
      "42000/49000 loss: 0.39897317373284014\n",
      "48000/49000 loss: 0.4340799207244353\n",
      "epoch 29: valid acc = 0.855, new learning rate = 0.00011296777049628272\n",
      "6000/49000 loss: 0.38112378182745993\n",
      "12000/49000 loss: 0.42584992974774893\n",
      "18000/49000 loss: 0.42013231550854563\n",
      "24000/49000 loss: 0.3979390696764671\n",
      "30000/49000 loss: 0.4152247958152175\n",
      "36000/49000 loss: 0.29166758516405317\n",
      "42000/49000 loss: 0.3315774292036436\n",
      "48000/49000 loss: 0.3933714391746485\n",
      "epoch 30: valid acc = 0.857, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.8644693877551021\n",
      "test acc: 0.857\n",
      "test acc: 0.8445\n",
      "number of batches for training: 81\n",
      "epoch 1: valid acc = 0.538, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.677, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.735, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.76, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.789, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.801, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.807, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.813, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.816, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.821, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.824, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.835, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.834, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.839, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.841, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.844, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.847, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.847, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.848, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.85, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.847, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.856, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.857, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.857, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.857, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.862, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.859, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.855, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.861, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.86, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.8629591836734694\n",
      "test acc: 0.86\n",
      "test acc: 0.8441\n",
      "number of batches for training: 81\n",
      "6000/49000 loss: 3.5338039807811703\n",
      "12000/49000 loss: 3.118982748757779\n",
      "18000/49000 loss: 3.4210216250421013\n",
      "24000/49000 loss: 2.300497610282702\n",
      "30000/49000 loss: 2.239187094191876\n",
      "36000/49000 loss: 1.997898236975122\n",
      "42000/49000 loss: 2.1276609693192756\n",
      "48000/49000 loss: 1.6135856869304015\n",
      "epoch 1: valid acc = 0.501, new learning rate = 0.000475\n",
      "6000/49000 loss: 1.3577854566930827\n",
      "12000/49000 loss: 1.3635960453817293\n",
      "18000/49000 loss: 1.150360553483078\n",
      "24000/49000 loss: 1.1304122149738638\n",
      "30000/49000 loss: 1.0621510124705584\n",
      "36000/49000 loss: 1.0794940216616284\n",
      "42000/49000 loss: 1.1093699443982743\n",
      "48000/49000 loss: 1.0367411993413234\n",
      "epoch 2: valid acc = 0.659, new learning rate = 0.00045125\n",
      "6000/49000 loss: 0.9143177978940694\n",
      "12000/49000 loss: 0.9204555207669334\n",
      "18000/49000 loss: 0.9337036316771816\n",
      "24000/49000 loss: 0.8787053039230103\n",
      "30000/49000 loss: 0.9205016273517864\n",
      "36000/49000 loss: 0.7639272867094997\n",
      "42000/49000 loss: 0.7482407452632306\n",
      "48000/49000 loss: 0.7856119649712727\n",
      "epoch 3: valid acc = 0.743, new learning rate = 0.0004286875\n",
      "6000/49000 loss: 0.8004518112594349\n",
      "12000/49000 loss: 0.7731047453874398\n",
      "18000/49000 loss: 0.6751160844069579\n",
      "24000/49000 loss: 0.7016436100489875\n",
      "30000/49000 loss: 0.6852929468945381\n",
      "36000/49000 loss: 0.6278651215658472\n",
      "42000/49000 loss: 0.6691729088742113\n",
      "48000/49000 loss: 0.6876075239696477\n",
      "epoch 4: valid acc = 0.755, new learning rate = 0.00040725312499999993\n",
      "6000/49000 loss: 0.6652172574147843\n",
      "12000/49000 loss: 0.5894401615022629\n",
      "18000/49000 loss: 0.6415294854349165\n",
      "24000/49000 loss: 0.587661679539105\n",
      "30000/49000 loss: 0.6185131396295342\n",
      "36000/49000 loss: 0.6358132582203951\n",
      "42000/49000 loss: 0.5979179863411604\n",
      "48000/49000 loss: 0.5554822904788358\n",
      "epoch 5: valid acc = 0.79, new learning rate = 0.0003868904687499999\n",
      "6000/49000 loss: 0.5903288544061003\n",
      "12000/49000 loss: 0.6434106516788453\n",
      "18000/49000 loss: 0.5489370558028539\n",
      "24000/49000 loss: 0.5575337346888558\n",
      "30000/49000 loss: 0.5808984599108253\n",
      "36000/49000 loss: 0.5702720601528148\n",
      "42000/49000 loss: 0.5425831194828312\n",
      "48000/49000 loss: 0.534909112975564\n",
      "epoch 6: valid acc = 0.81, new learning rate = 0.0003675459453124999\n",
      "6000/49000 loss: 0.5432540364534866\n",
      "12000/49000 loss: 0.526583476816083\n",
      "18000/49000 loss: 0.5543818772118643\n",
      "24000/49000 loss: 0.6034556964022958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000/49000 loss: 0.5368520108343736\n",
      "36000/49000 loss: 0.5781457592891882\n",
      "42000/49000 loss: 0.573728180516527\n",
      "48000/49000 loss: 0.48376898972664706\n",
      "epoch 7: valid acc = 0.815, new learning rate = 0.00034916864804687486\n",
      "6000/49000 loss: 0.5128471149279379\n",
      "12000/49000 loss: 0.5395066400793238\n",
      "18000/49000 loss: 0.484398774798491\n",
      "24000/49000 loss: 0.5358290668457969\n",
      "30000/49000 loss: 0.43403337991407925\n",
      "36000/49000 loss: 0.5158586117367787\n",
      "42000/49000 loss: 0.5389442300545684\n",
      "48000/49000 loss: 0.4582316699408839\n",
      "epoch 8: valid acc = 0.818, new learning rate = 0.0003317102156445311\n",
      "6000/49000 loss: 0.5096968344799437\n",
      "12000/49000 loss: 0.5554176817559839\n",
      "18000/49000 loss: 0.49830893878978294\n",
      "24000/49000 loss: 0.5068089711368109\n",
      "30000/49000 loss: 0.5416352533116174\n",
      "36000/49000 loss: 0.46612470522096183\n",
      "42000/49000 loss: 0.4253036858906132\n",
      "48000/49000 loss: 0.465242817642655\n",
      "epoch 9: valid acc = 0.828, new learning rate = 0.0003151247048623045\n",
      "6000/49000 loss: 0.4619048991109222\n",
      "12000/49000 loss: 0.5124716017064402\n",
      "18000/49000 loss: 0.5502259837177832\n",
      "24000/49000 loss: 0.4988551951051684\n",
      "30000/49000 loss: 0.5006750599644265\n",
      "36000/49000 loss: 0.4175050007511308\n",
      "42000/49000 loss: 0.43327555763950176\n",
      "48000/49000 loss: 0.49237256935104956\n",
      "epoch 10: valid acc = 0.826, new learning rate = 0.00029936846961918924\n",
      "6000/49000 loss: 0.48718919196285154\n",
      "12000/49000 loss: 0.4190175573056588\n",
      "18000/49000 loss: 0.45440858999823713\n",
      "24000/49000 loss: 0.45644117134271267\n",
      "30000/49000 loss: 0.4377679200807801\n",
      "36000/49000 loss: 0.45126378804850165\n",
      "42000/49000 loss: 0.43509080075424084\n",
      "48000/49000 loss: 0.4397038596545966\n",
      "epoch 11: valid acc = 0.837, new learning rate = 0.00028440004613822977\n",
      "6000/49000 loss: 0.4664738006439122\n",
      "12000/49000 loss: 0.4882552398561323\n",
      "18000/49000 loss: 0.45805768311328415\n",
      "24000/49000 loss: 0.4152809537022207\n",
      "30000/49000 loss: 0.4006128797558903\n",
      "36000/49000 loss: 0.4395961311881656\n",
      "42000/49000 loss: 0.42450262502910074\n",
      "48000/49000 loss: 0.4760232355278458\n",
      "epoch 12: valid acc = 0.836, new learning rate = 0.00027018004383131826\n",
      "6000/49000 loss: 0.45504384062006675\n",
      "12000/49000 loss: 0.4456340596251612\n",
      "18000/49000 loss: 0.42806473477019674\n",
      "24000/49000 loss: 0.4730325391693644\n",
      "30000/49000 loss: 0.45247309642252864\n",
      "36000/49000 loss: 0.4343786242640389\n",
      "42000/49000 loss: 0.3667288648981301\n",
      "48000/49000 loss: 0.5170496664966873\n",
      "epoch 13: valid acc = 0.838, new learning rate = 0.00025667104163975234\n",
      "6000/49000 loss: 0.44014829306793035\n",
      "12000/49000 loss: 0.42993376580862364\n",
      "18000/49000 loss: 0.44926075788747305\n",
      "24000/49000 loss: 0.3797728603574764\n",
      "30000/49000 loss: 0.43398469817609336\n",
      "36000/49000 loss: 0.4075882732613065\n",
      "42000/49000 loss: 0.4548474757584871\n",
      "48000/49000 loss: 0.37245611659250155\n",
      "epoch 14: valid acc = 0.835, new learning rate = 0.00024383748955776472\n",
      "6000/49000 loss: 0.40583245324277717\n",
      "12000/49000 loss: 0.4851767348242057\n",
      "18000/49000 loss: 0.4277898384505724\n",
      "24000/49000 loss: 0.467295026115086\n",
      "30000/49000 loss: 0.4233573217755917\n",
      "36000/49000 loss: 0.44518310845532755\n",
      "42000/49000 loss: 0.5095544538704527\n",
      "48000/49000 loss: 0.45553525171524434\n",
      "epoch 15: valid acc = 0.846, new learning rate = 0.00023164561507987649\n",
      "6000/49000 loss: 0.40698747219039605\n",
      "12000/49000 loss: 0.4567161459781642\n",
      "18000/49000 loss: 0.44438168905555575\n",
      "24000/49000 loss: 0.4461464125762783\n",
      "30000/49000 loss: 0.39862600783837987\n",
      "36000/49000 loss: 0.3932135561784667\n",
      "42000/49000 loss: 0.39074673751795225\n",
      "48000/49000 loss: 0.4166779032847682\n",
      "epoch 16: valid acc = 0.849, new learning rate = 0.00022006333432588265\n",
      "6000/49000 loss: 0.4118363244355205\n",
      "12000/49000 loss: 0.38380553548639035\n",
      "18000/49000 loss: 0.33670460746668174\n",
      "24000/49000 loss: 0.4997605552335815\n",
      "30000/49000 loss: 0.4757545986873822\n",
      "36000/49000 loss: 0.46106393141115276\n",
      "42000/49000 loss: 0.44926407542994384\n",
      "48000/49000 loss: 0.4071558287407733\n",
      "epoch 17: valid acc = 0.851, new learning rate = 0.00020906016760958852\n",
      "6000/49000 loss: 0.3497782054091066\n",
      "12000/49000 loss: 0.4667881412488168\n",
      "18000/49000 loss: 0.4506356794052113\n",
      "24000/49000 loss: 0.4077873185780369\n",
      "30000/49000 loss: 0.4234468094696924\n",
      "36000/49000 loss: 0.36486502444435026\n",
      "42000/49000 loss: 0.4141758983677493\n",
      "48000/49000 loss: 0.42509735318787134\n",
      "epoch 18: valid acc = 0.848, new learning rate = 0.00019860715922910907\n",
      "6000/49000 loss: 0.40257828493537035\n",
      "12000/49000 loss: 0.5120914787156876\n",
      "18000/49000 loss: 0.38072151077234984\n",
      "24000/49000 loss: 0.3571007488720159\n",
      "30000/49000 loss: 0.4292459398477317\n",
      "36000/49000 loss: 0.3828218686995019\n",
      "42000/49000 loss: 0.41419481377083067\n",
      "48000/49000 loss: 0.4295646361479848\n",
      "epoch 19: valid acc = 0.853, new learning rate = 0.0001886768012676536\n",
      "6000/49000 loss: 0.4285418526894019\n",
      "12000/49000 loss: 0.39003544296088993\n",
      "18000/49000 loss: 0.3624989220390279\n",
      "24000/49000 loss: 0.3909360246103612\n",
      "30000/49000 loss: 0.4165806457972535\n",
      "36000/49000 loss: 0.3907348137676752\n",
      "42000/49000 loss: 0.42425710578771847\n",
      "48000/49000 loss: 0.36207530228577967\n",
      "epoch 20: valid acc = 0.852, new learning rate = 0.0001792429612042709\n",
      "6000/49000 loss: 0.4660705642486823\n",
      "12000/49000 loss: 0.3520270245939234\n",
      "18000/49000 loss: 0.4394590576626269\n",
      "24000/49000 loss: 0.37590678121566795\n",
      "30000/49000 loss: 0.4031077930155176\n",
      "36000/49000 loss: 0.39257921530888756\n",
      "42000/49000 loss: 0.3858935791598829\n",
      "48000/49000 loss: 0.3706721594941826\n",
      "epoch 21: valid acc = 0.857, new learning rate = 0.00017028081314405735\n",
      "6000/49000 loss: 0.4617148116435508\n",
      "12000/49000 loss: 0.37592692686059376\n",
      "18000/49000 loss: 0.42947292938905235\n",
      "24000/49000 loss: 0.4280402627328662\n",
      "30000/49000 loss: 0.38779983436850657\n",
      "36000/49000 loss: 0.4141017143946585\n",
      "42000/49000 loss: 0.43884897228632425\n",
      "48000/49000 loss: 0.4165822475401677\n",
      "epoch 22: valid acc = 0.856, new learning rate = 0.00016176677248685447\n",
      "6000/49000 loss: 0.43903012095254507\n",
      "12000/49000 loss: 0.4292013147235488\n",
      "18000/49000 loss: 0.46533250913272245\n",
      "24000/49000 loss: 0.39129679765928327\n",
      "30000/49000 loss: 0.3909129971540582\n",
      "36000/49000 loss: 0.41696993358835566\n",
      "42000/49000 loss: 0.37439530520809927\n",
      "48000/49000 loss: 0.39770752403021964\n",
      "epoch 23: valid acc = 0.856, new learning rate = 0.00015367843386251173\n",
      "6000/49000 loss: 0.449100466708247\n",
      "12000/49000 loss: 0.3792953760859192\n",
      "18000/49000 loss: 0.41529592321081366\n",
      "24000/49000 loss: 0.3906788167356002\n",
      "30000/49000 loss: 0.414561266837831\n",
      "36000/49000 loss: 0.3885772237424302\n",
      "42000/49000 loss: 0.3925382131538897\n",
      "48000/49000 loss: 0.380811226833016\n",
      "epoch 24: valid acc = 0.858, new learning rate = 0.00014599451216938612\n",
      "6000/49000 loss: 0.3770769353972171\n",
      "12000/49000 loss: 0.34670407215893323\n",
      "18000/49000 loss: 0.4035262798604804\n",
      "24000/49000 loss: 0.3225564197818046\n",
      "30000/49000 loss: 0.3688117487207872\n",
      "36000/49000 loss: 0.3815432089374971\n",
      "42000/49000 loss: 0.3751029905163894\n",
      "48000/49000 loss: 0.3693214921262943\n",
      "epoch 25: valid acc = 0.863, new learning rate = 0.00013869478656091682\n",
      "6000/49000 loss: 0.44862745257937037\n",
      "12000/49000 loss: 0.4001663039488805\n",
      "18000/49000 loss: 0.3622001789812666\n",
      "24000/49000 loss: 0.37582551201705255\n",
      "30000/49000 loss: 0.4210361804812277\n",
      "36000/49000 loss: 0.47283417436433717\n",
      "42000/49000 loss: 0.36810858675816605\n",
      "48000/49000 loss: 0.4439376728145843\n",
      "epoch 26: valid acc = 0.863, new learning rate = 0.00013176004723287096\n",
      "6000/49000 loss: 0.3393458776948603\n",
      "12000/49000 loss: 0.38519831104839064\n",
      "18000/49000 loss: 0.40806741486413184\n",
      "24000/49000 loss: 0.42220786767962293\n",
      "30000/49000 loss: 0.41260653301035766\n",
      "36000/49000 loss: 0.3731770764077103\n",
      "42000/49000 loss: 0.3902165384467807\n",
      "48000/49000 loss: 0.3675534209103475\n",
      "epoch 27: valid acc = 0.858, new learning rate = 0.0001251720448712274\n",
      "6000/49000 loss: 0.3902786923706174\n",
      "12000/49000 loss: 0.3921210806299978\n",
      "18000/49000 loss: 0.3967214116773742\n",
      "24000/49000 loss: 0.3868433632350755\n",
      "30000/49000 loss: 0.3766175973897613\n",
      "36000/49000 loss: 0.39066123440297634\n",
      "42000/49000 loss: 0.400210379946434\n",
      "48000/49000 loss: 0.4029586933262497\n",
      "epoch 28: valid acc = 0.862, new learning rate = 0.00011891344262766602\n",
      "6000/49000 loss: 0.4026700492228987\n",
      "12000/49000 loss: 0.4085317180012112\n",
      "18000/49000 loss: 0.4501080711286638\n",
      "24000/49000 loss: 0.41698478444690845\n",
      "30000/49000 loss: 0.3836483497694431\n",
      "36000/49000 loss: 0.43736008517580927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/49000 loss: 0.4208596711766104\n",
      "48000/49000 loss: 0.38720798150993674\n",
      "epoch 29: valid acc = 0.863, new learning rate = 0.00011296777049628272\n",
      "6000/49000 loss: 0.3487237877336764\n",
      "12000/49000 loss: 0.44770170904673795\n",
      "18000/49000 loss: 0.4292524432365605\n",
      "24000/49000 loss: 0.41975337670745455\n",
      "30000/49000 loss: 0.40030520996362934\n",
      "36000/49000 loss: 0.34772754276718054\n",
      "42000/49000 loss: 0.3673469805727977\n",
      "48000/49000 loss: 0.3850124899788518\n",
      "epoch 30: valid acc = 0.862, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.8645918367346939\n",
      "test acc: 0.862\n",
      "test acc: 0.8443\n",
      "number of batches for training: 81\n",
      "epoch 1: valid acc = 0.511, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.667, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.736, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.758, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.785, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.789, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.813, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.81, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.817, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.831, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.827, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.831, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.828, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.836, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.836, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.842, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.843, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.846, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.849, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.847, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.85, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.853, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.852, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.856, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.856, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.857, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.857, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.861, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.859, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.857, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.8637551020408163\n",
      "test acc: 0.857\n",
      "test acc: 0.8448\n",
      "number of batches for training: 81\n",
      "6000/49000 loss: 4.442258134517506\n",
      "12000/49000 loss: 3.2219808363209497\n",
      "18000/49000 loss: 2.903047006274132\n",
      "24000/49000 loss: 2.6102229461959405\n",
      "30000/49000 loss: 2.134539091171322\n",
      "36000/49000 loss: 2.076423513021624\n",
      "42000/49000 loss: 1.8206524536990072\n",
      "48000/49000 loss: 1.6688279108338524\n",
      "epoch 1: valid acc = 0.534, new learning rate = 0.000475\n",
      "6000/49000 loss: 1.4494898779205785\n",
      "12000/49000 loss: 1.2952287553266328\n",
      "18000/49000 loss: 1.2076794977665013\n",
      "24000/49000 loss: 1.1061278964054098\n",
      "30000/49000 loss: 1.1371172249055574\n",
      "36000/49000 loss: 1.0029729686051647\n",
      "42000/49000 loss: 0.9966671995425672\n",
      "48000/49000 loss: 1.0131926538494778\n",
      "epoch 2: valid acc = 0.68, new learning rate = 0.00045125\n",
      "6000/49000 loss: 0.8444686926547131\n",
      "12000/49000 loss: 0.8792686915192631\n",
      "18000/49000 loss: 0.9413079581106378\n",
      "24000/49000 loss: 0.806207803441074\n",
      "30000/49000 loss: 0.876333756316369\n",
      "36000/49000 loss: 0.7663066641215864\n",
      "42000/49000 loss: 0.7801319632757846\n",
      "48000/49000 loss: 0.7614763651286207\n",
      "epoch 3: valid acc = 0.736, new learning rate = 0.0004286875\n",
      "6000/49000 loss: 0.7480008259744994\n",
      "12000/49000 loss: 0.7412609616265003\n",
      "18000/49000 loss: 0.7377174549819239\n",
      "24000/49000 loss: 0.7214636435461567\n",
      "30000/49000 loss: 0.6147327864353788\n",
      "36000/49000 loss: 0.6553589184484799\n",
      "42000/49000 loss: 0.6074859424155049\n",
      "48000/49000 loss: 0.689341348666281\n",
      "epoch 4: valid acc = 0.76, new learning rate = 0.00040725312499999993\n",
      "6000/49000 loss: 0.6835576517342107\n",
      "12000/49000 loss: 0.6764595020351779\n",
      "18000/49000 loss: 0.6216190756881674\n",
      "24000/49000 loss: 0.5918943559644199\n",
      "30000/49000 loss: 0.6110320959905376\n",
      "36000/49000 loss: 0.6471216464041422\n",
      "42000/49000 loss: 0.6325781064960602\n",
      "48000/49000 loss: 0.6101858658466834\n",
      "epoch 5: valid acc = 0.775, new learning rate = 0.0003868904687499999\n",
      "6000/49000 loss: 0.5778974108116345\n",
      "12000/49000 loss: 0.6068228949592795\n",
      "18000/49000 loss: 0.5186135538451182\n",
      "24000/49000 loss: 0.575410761591161\n",
      "30000/49000 loss: 0.564261143174262\n",
      "36000/49000 loss: 0.5553416141647749\n",
      "42000/49000 loss: 0.5888997241223001\n",
      "48000/49000 loss: 0.5672446259846624\n",
      "epoch 6: valid acc = 0.804, new learning rate = 0.0003675459453124999\n",
      "6000/49000 loss: 0.5932591120395063\n",
      "12000/49000 loss: 0.48790458766619804\n",
      "18000/49000 loss: 0.4660447751481179\n",
      "24000/49000 loss: 0.49321314910869046\n",
      "30000/49000 loss: 0.5374792245199399\n",
      "36000/49000 loss: 0.5195009253909343\n",
      "42000/49000 loss: 0.5089544766943909\n",
      "48000/49000 loss: 0.4875377528810929\n",
      "epoch 7: valid acc = 0.804, new learning rate = 0.00034916864804687486\n",
      "6000/49000 loss: 0.5143198583431441\n",
      "12000/49000 loss: 0.5248555571068182\n",
      "18000/49000 loss: 0.5338117360236343\n",
      "24000/49000 loss: 0.505676515736519\n",
      "30000/49000 loss: 0.5023412408521869\n",
      "36000/49000 loss: 0.4676252809624427\n",
      "42000/49000 loss: 0.5020588724503954\n",
      "48000/49000 loss: 0.47006740896736215\n",
      "epoch 8: valid acc = 0.815, new learning rate = 0.0003317102156445311\n",
      "6000/49000 loss: 0.4784985582216955\n",
      "12000/49000 loss: 0.42652375763095063\n",
      "18000/49000 loss: 0.5180288813817668\n",
      "24000/49000 loss: 0.4517634836705888\n",
      "30000/49000 loss: 0.4865723255678971\n",
      "36000/49000 loss: 0.4830563583663132\n",
      "42000/49000 loss: 0.48956149280785194\n",
      "48000/49000 loss: 0.5151838859103353\n",
      "epoch 9: valid acc = 0.823, new learning rate = 0.0003151247048623045\n",
      "6000/49000 loss: 0.4608629606699799\n",
      "12000/49000 loss: 0.4789487932295977\n",
      "18000/49000 loss: 0.4364241376858413\n",
      "24000/49000 loss: 0.4188748511568365\n",
      "30000/49000 loss: 0.4963383506052813\n",
      "36000/49000 loss: 0.5088941430636843\n",
      "42000/49000 loss: 0.44478977012003174\n",
      "48000/49000 loss: 0.4911005817628234\n",
      "epoch 10: valid acc = 0.829, new learning rate = 0.00029936846961918924\n",
      "6000/49000 loss: 0.5028815144462881\n",
      "12000/49000 loss: 0.5245138280528577\n",
      "18000/49000 loss: 0.5469922872703586\n",
      "24000/49000 loss: 0.4445731497823654\n",
      "30000/49000 loss: 0.4348959272894191\n",
      "36000/49000 loss: 0.4915832783974066\n",
      "42000/49000 loss: 0.4355690890428413\n",
      "48000/49000 loss: 0.45352050983861825\n",
      "epoch 11: valid acc = 0.831, new learning rate = 0.00028440004613822977\n",
      "6000/49000 loss: 0.41516067998835066\n",
      "12000/49000 loss: 0.45409015778485284\n",
      "18000/49000 loss: 0.42520494330209535\n",
      "24000/49000 loss: 0.43178778054731737\n",
      "30000/49000 loss: 0.45737252949879137\n",
      "36000/49000 loss: 0.3890741592646733\n",
      "42000/49000 loss: 0.4334320338619451\n",
      "48000/49000 loss: 0.4492159201742284\n",
      "epoch 12: valid acc = 0.834, new learning rate = 0.00027018004383131826\n",
      "6000/49000 loss: 0.43081145284997274\n",
      "12000/49000 loss: 0.49599877541104465\n",
      "18000/49000 loss: 0.3929364766366958\n",
      "24000/49000 loss: 0.43773610584548356\n",
      "30000/49000 loss: 0.42136573754682016\n",
      "36000/49000 loss: 0.44084637252079784\n",
      "42000/49000 loss: 0.38550267714756\n",
      "48000/49000 loss: 0.41487586457689707\n",
      "epoch 13: valid acc = 0.842, new learning rate = 0.00025667104163975234\n",
      "6000/49000 loss: 0.43606186279723863\n",
      "12000/49000 loss: 0.4081958093373868\n",
      "18000/49000 loss: 0.4258840621069149\n",
      "24000/49000 loss: 0.4166558717683293\n",
      "30000/49000 loss: 0.4218933536187518\n",
      "36000/49000 loss: 0.4156150581032027\n",
      "42000/49000 loss: 0.43916392533400445\n",
      "48000/49000 loss: 0.43493974967349086\n",
      "epoch 14: valid acc = 0.839, new learning rate = 0.00024383748955776472\n",
      "6000/49000 loss: 0.47346117512656666\n",
      "12000/49000 loss: 0.4351860762505274\n",
      "18000/49000 loss: 0.425785509564654\n",
      "24000/49000 loss: 0.4115469922567002\n",
      "30000/49000 loss: 0.4410839945587002\n",
      "36000/49000 loss: 0.4316810446921292\n",
      "42000/49000 loss: 0.3802576793363401\n",
      "48000/49000 loss: 0.4132457180267374\n",
      "epoch 15: valid acc = 0.844, new learning rate = 0.00023164561507987649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/49000 loss: 0.4186451445251861\n",
      "12000/49000 loss: 0.45776998741703206\n",
      "18000/49000 loss: 0.40064967002425583\n",
      "24000/49000 loss: 0.4489909755649894\n",
      "30000/49000 loss: 0.491415091743928\n",
      "36000/49000 loss: 0.4374290998520141\n",
      "42000/49000 loss: 0.3983742896756291\n",
      "48000/49000 loss: 0.4633495627049171\n",
      "epoch 16: valid acc = 0.838, new learning rate = 0.00022006333432588265\n",
      "6000/49000 loss: 0.435415185071054\n",
      "12000/49000 loss: 0.3973676361756673\n",
      "18000/49000 loss: 0.40558883978243065\n",
      "24000/49000 loss: 0.39373255003144375\n",
      "30000/49000 loss: 0.4449772442095389\n",
      "36000/49000 loss: 0.41962849681615605\n",
      "42000/49000 loss: 0.41541763710971846\n",
      "48000/49000 loss: 0.4512371334235097\n",
      "epoch 17: valid acc = 0.846, new learning rate = 0.00020906016760958852\n",
      "6000/49000 loss: 0.49479351514215386\n",
      "12000/49000 loss: 0.41275514841886746\n",
      "18000/49000 loss: 0.4266587093030287\n",
      "24000/49000 loss: 0.3650440379624394\n",
      "30000/49000 loss: 0.45715741728471143\n",
      "36000/49000 loss: 0.46900011963632693\n",
      "42000/49000 loss: 0.4020138520004943\n",
      "48000/49000 loss: 0.42322506639414087\n",
      "epoch 18: valid acc = 0.851, new learning rate = 0.00019860715922910907\n",
      "6000/49000 loss: 0.4753507589575959\n",
      "12000/49000 loss: 0.42271860277746187\n",
      "18000/49000 loss: 0.38451961418556707\n",
      "24000/49000 loss: 0.42773412033258185\n",
      "30000/49000 loss: 0.39839865479531517\n",
      "36000/49000 loss: 0.4325049726439207\n",
      "42000/49000 loss: 0.43603503402778265\n",
      "48000/49000 loss: 0.3645281473543039\n",
      "epoch 19: valid acc = 0.851, new learning rate = 0.0001886768012676536\n",
      "6000/49000 loss: 0.39336085839986606\n",
      "12000/49000 loss: 0.3914655992010746\n",
      "18000/49000 loss: 0.3945829734295354\n",
      "24000/49000 loss: 0.3605193871954719\n",
      "30000/49000 loss: 0.4530423527897413\n",
      "36000/49000 loss: 0.4270137672418132\n",
      "42000/49000 loss: 0.443730656089806\n",
      "48000/49000 loss: 0.43725767176303054\n",
      "epoch 20: valid acc = 0.85, new learning rate = 0.0001792429612042709\n",
      "6000/49000 loss: 0.4123675509019457\n",
      "12000/49000 loss: 0.45164255339585757\n",
      "18000/49000 loss: 0.4329210870712713\n",
      "24000/49000 loss: 0.45634707053992785\n",
      "30000/49000 loss: 0.5087701067753445\n",
      "36000/49000 loss: 0.4133669636751937\n",
      "42000/49000 loss: 0.42954034475129244\n",
      "48000/49000 loss: 0.4105563317653851\n",
      "epoch 21: valid acc = 0.855, new learning rate = 0.00017028081314405735\n",
      "6000/49000 loss: 0.37496736683184184\n",
      "12000/49000 loss: 0.35712081003329704\n",
      "18000/49000 loss: 0.37900522665909586\n",
      "24000/49000 loss: 0.38511393636713676\n",
      "30000/49000 loss: 0.4330929956252558\n",
      "36000/49000 loss: 0.3730599633421811\n",
      "42000/49000 loss: 0.4283500242660887\n",
      "48000/49000 loss: 0.3582529757358834\n",
      "epoch 22: valid acc = 0.857, new learning rate = 0.00016176677248685447\n",
      "6000/49000 loss: 0.42696361441104463\n",
      "12000/49000 loss: 0.40195584373154264\n",
      "18000/49000 loss: 0.39354118105431984\n",
      "24000/49000 loss: 0.423882478191082\n",
      "30000/49000 loss: 0.40346015084585474\n",
      "36000/49000 loss: 0.4053042872182226\n",
      "42000/49000 loss: 0.4568064767445237\n",
      "48000/49000 loss: 0.41697125097732524\n",
      "epoch 23: valid acc = 0.855, new learning rate = 0.00015367843386251173\n",
      "6000/49000 loss: 0.39304227698113336\n",
      "12000/49000 loss: 0.4289117338202429\n",
      "18000/49000 loss: 0.4253227575295075\n",
      "24000/49000 loss: 0.400158384666161\n",
      "30000/49000 loss: 0.37150895110744264\n",
      "36000/49000 loss: 0.38622987581519214\n",
      "42000/49000 loss: 0.36136899245499393\n",
      "48000/49000 loss: 0.3609414137880765\n",
      "epoch 24: valid acc = 0.861, new learning rate = 0.00014599451216938612\n",
      "6000/49000 loss: 0.38568112606422117\n",
      "12000/49000 loss: 0.3775223649991147\n",
      "18000/49000 loss: 0.4285131899344354\n",
      "24000/49000 loss: 0.4206772620604162\n",
      "30000/49000 loss: 0.3994446218649037\n",
      "36000/49000 loss: 0.43966019000721285\n",
      "42000/49000 loss: 0.4291283057150983\n",
      "48000/49000 loss: 0.4064877767614432\n",
      "epoch 25: valid acc = 0.86, new learning rate = 0.00013869478656091682\n",
      "6000/49000 loss: 0.3784102424846997\n",
      "12000/49000 loss: 0.3941702216802628\n",
      "18000/49000 loss: 0.40932239399495174\n",
      "24000/49000 loss: 0.34601599885342266\n",
      "30000/49000 loss: 0.4281166586276588\n",
      "36000/49000 loss: 0.43157250286562493\n",
      "42000/49000 loss: 0.38131995047589073\n",
      "48000/49000 loss: 0.46417278414737917\n",
      "epoch 26: valid acc = 0.857, new learning rate = 0.00013176004723287096\n",
      "6000/49000 loss: 0.37862221737013924\n",
      "12000/49000 loss: 0.41317870794632516\n",
      "18000/49000 loss: 0.36484295318925536\n",
      "24000/49000 loss: 0.4218146289340477\n",
      "30000/49000 loss: 0.4069299853615095\n",
      "36000/49000 loss: 0.46982176010890603\n",
      "42000/49000 loss: 0.4153549680827101\n",
      "48000/49000 loss: 0.42464189031421623\n",
      "epoch 27: valid acc = 0.856, new learning rate = 0.0001251720448712274\n",
      "6000/49000 loss: 0.395276524812079\n",
      "12000/49000 loss: 0.4918077900250083\n",
      "18000/49000 loss: 0.4009713990701427\n",
      "24000/49000 loss: 0.34605850779762676\n",
      "30000/49000 loss: 0.3906295479014439\n",
      "36000/49000 loss: 0.34560790489186055\n",
      "42000/49000 loss: 0.4026896435243039\n",
      "48000/49000 loss: 0.4117558210643947\n",
      "epoch 28: valid acc = 0.859, new learning rate = 0.00011891344262766602\n",
      "6000/49000 loss: 0.4019521636960794\n",
      "12000/49000 loss: 0.39253617668343466\n",
      "18000/49000 loss: 0.3763225605803596\n",
      "24000/49000 loss: 0.3805346340250867\n",
      "30000/49000 loss: 0.3667741779656817\n",
      "36000/49000 loss: 0.36400400278844525\n",
      "42000/49000 loss: 0.3197284370668676\n",
      "48000/49000 loss: 0.3487485105398324\n",
      "epoch 29: valid acc = 0.86, new learning rate = 0.00011296777049628272\n",
      "6000/49000 loss: 0.3921905909615422\n",
      "12000/49000 loss: 0.3414009594525889\n",
      "18000/49000 loss: 0.3335282030246011\n",
      "24000/49000 loss: 0.30863142498040974\n",
      "30000/49000 loss: 0.41048696431205745\n",
      "36000/49000 loss: 0.41052925607633356\n",
      "42000/49000 loss: 0.3903512450724109\n",
      "48000/49000 loss: 0.3992738789680761\n",
      "epoch 30: valid acc = 0.857, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.8635102040816327\n",
      "test acc: 0.857\n",
      "test acc: 0.8445\n",
      "number of batches for training: 81\n",
      "epoch 1: valid acc = 0.508, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.67, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.738, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.764, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.789, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.787, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.81, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.815, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.817, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.828, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.831, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.829, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.84, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.845, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.841, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.851, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.846, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.85, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.853, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.855, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.855, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.855, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.856, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.853, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.856, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.855, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.857, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.859, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.861, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.861, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.8636122448979592\n",
      "test acc: 0.861\n",
      "test acc: 0.8448\n",
      "number of batches for training: 49\n",
      "10000/49000 loss: 3.9308987831436153\n",
      "20000/49000 loss: 3.39747774896341\n",
      "30000/49000 loss: 2.800985401207194\n",
      "40000/49000 loss: 2.6384582947885677\n",
      "epoch 1: valid acc = 0.36, new learning rate = 0.000475\n",
      "10000/49000 loss: 2.103609542896936\n",
      "20000/49000 loss: 1.897300146726711\n",
      "30000/49000 loss: 1.6415641550297768\n",
      "40000/49000 loss: 1.5001224941867963\n",
      "epoch 2: valid acc = 0.53, new learning rate = 0.00045125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/49000 loss: 1.2082153956791968\n",
      "20000/49000 loss: 1.203920461814733\n",
      "30000/49000 loss: 1.1599015859415953\n",
      "40000/49000 loss: 1.1535518293703\n",
      "epoch 3: valid acc = 0.642, new learning rate = 0.0004286875\n",
      "10000/49000 loss: 1.031696252167848\n",
      "20000/49000 loss: 0.9638326680622942\n",
      "30000/49000 loss: 0.9372423446827747\n",
      "40000/49000 loss: 0.9543398331129597\n",
      "epoch 4: valid acc = 0.696, new learning rate = 0.00040725312499999993\n",
      "10000/49000 loss: 0.92907440794638\n",
      "20000/49000 loss: 0.8659409299913281\n",
      "30000/49000 loss: 0.8017699783834167\n",
      "40000/49000 loss: 0.8702701634865294\n",
      "epoch 5: valid acc = 0.733, new learning rate = 0.0003868904687499999\n",
      "10000/49000 loss: 0.7624721043614079\n",
      "20000/49000 loss: 0.7119727528289739\n",
      "30000/49000 loss: 0.7376828827341022\n",
      "40000/49000 loss: 0.7846434038032299\n",
      "epoch 6: valid acc = 0.738, new learning rate = 0.0003675459453124999\n",
      "10000/49000 loss: 0.6953991525895451\n",
      "20000/49000 loss: 0.6604093672678258\n",
      "30000/49000 loss: 0.6347692062586845\n",
      "40000/49000 loss: 0.6619833617065168\n",
      "epoch 7: valid acc = 0.753, new learning rate = 0.00034916864804687486\n",
      "10000/49000 loss: 0.6699184169902965\n",
      "20000/49000 loss: 0.6626755578238154\n",
      "30000/49000 loss: 0.6346561593923328\n",
      "40000/49000 loss: 0.5836396142763716\n",
      "epoch 8: valid acc = 0.776, new learning rate = 0.0003317102156445311\n",
      "10000/49000 loss: 0.6069628151466838\n",
      "20000/49000 loss: 0.5571882546523862\n",
      "30000/49000 loss: 0.6334000279228458\n",
      "40000/49000 loss: 0.5775625280325516\n",
      "epoch 9: valid acc = 0.788, new learning rate = 0.0003151247048623045\n",
      "10000/49000 loss: 0.6025416056274078\n",
      "20000/49000 loss: 0.5825744047536813\n",
      "30000/49000 loss: 0.5660317758054011\n",
      "40000/49000 loss: 0.5782246077203527\n",
      "epoch 10: valid acc = 0.792, new learning rate = 0.00029936846961918924\n",
      "10000/49000 loss: 0.5407697915583076\n",
      "20000/49000 loss: 0.5685023078565922\n",
      "30000/49000 loss: 0.5591907220735204\n",
      "40000/49000 loss: 0.5525086949725789\n",
      "epoch 11: valid acc = 0.8, new learning rate = 0.00028440004613822977\n",
      "10000/49000 loss: 0.5515014633650411\n",
      "20000/49000 loss: 0.570713698341139\n",
      "30000/49000 loss: 0.5769238505714291\n",
      "40000/49000 loss: 0.5322200613577265\n",
      "epoch 12: valid acc = 0.797, new learning rate = 0.00027018004383131826\n",
      "10000/49000 loss: 0.49421182071416997\n",
      "20000/49000 loss: 0.509294444134778\n",
      "30000/49000 loss: 0.527266618932672\n",
      "40000/49000 loss: 0.5824628562622126\n",
      "epoch 13: valid acc = 0.812, new learning rate = 0.00025667104163975234\n",
      "10000/49000 loss: 0.5294273178965898\n",
      "20000/49000 loss: 0.5492788938238865\n",
      "30000/49000 loss: 0.5615823266773253\n",
      "40000/49000 loss: 0.4653885388923148\n",
      "epoch 14: valid acc = 0.817, new learning rate = 0.00024383748955776472\n",
      "10000/49000 loss: 0.4985638750991891\n",
      "20000/49000 loss: 0.4896230328221652\n",
      "30000/49000 loss: 0.47694251955753975\n",
      "40000/49000 loss: 0.5111185671693348\n",
      "epoch 15: valid acc = 0.817, new learning rate = 0.00023164561507987649\n",
      "10000/49000 loss: 0.4941699235574588\n",
      "20000/49000 loss: 0.4547233315122067\n",
      "30000/49000 loss: 0.5487497021679592\n",
      "40000/49000 loss: 0.546143515817167\n",
      "epoch 16: valid acc = 0.815, new learning rate = 0.00022006333432588265\n",
      "10000/49000 loss: 0.4454563705289816\n",
      "20000/49000 loss: 0.5484567590538391\n",
      "30000/49000 loss: 0.47713502830157734\n",
      "40000/49000 loss: 0.46323134315983666\n",
      "epoch 17: valid acc = 0.818, new learning rate = 0.00020906016760958852\n",
      "10000/49000 loss: 0.46973051076004985\n",
      "20000/49000 loss: 0.47909537749195413\n",
      "30000/49000 loss: 0.5057158236960773\n",
      "40000/49000 loss: 0.5192325051523735\n",
      "epoch 18: valid acc = 0.821, new learning rate = 0.00019860715922910907\n",
      "10000/49000 loss: 0.43718194828153806\n",
      "20000/49000 loss: 0.470365358879806\n",
      "30000/49000 loss: 0.47705122166993924\n",
      "40000/49000 loss: 0.46537329774072184\n",
      "epoch 19: valid acc = 0.821, new learning rate = 0.0001886768012676536\n",
      "10000/49000 loss: 0.4400602758175094\n",
      "20000/49000 loss: 0.5120174465500659\n",
      "30000/49000 loss: 0.45892347232712033\n",
      "40000/49000 loss: 0.4875441159308628\n",
      "epoch 20: valid acc = 0.821, new learning rate = 0.0001792429612042709\n",
      "10000/49000 loss: 0.4642192990485373\n",
      "20000/49000 loss: 0.44520277434955696\n",
      "30000/49000 loss: 0.46619315498411135\n",
      "40000/49000 loss: 0.43500597609475816\n",
      "epoch 21: valid acc = 0.824, new learning rate = 0.00017028081314405735\n",
      "10000/49000 loss: 0.4531840364172459\n",
      "20000/49000 loss: 0.5187118918127535\n",
      "30000/49000 loss: 0.46881394273703325\n",
      "40000/49000 loss: 0.4932097921013675\n",
      "epoch 22: valid acc = 0.823, new learning rate = 0.00016176677248685447\n",
      "10000/49000 loss: 0.4482274169155831\n",
      "20000/49000 loss: 0.458485783450472\n",
      "30000/49000 loss: 0.4913198729964417\n",
      "40000/49000 loss: 0.4845024217258039\n",
      "epoch 23: valid acc = 0.828, new learning rate = 0.00015367843386251173\n",
      "10000/49000 loss: 0.5153817665741571\n",
      "20000/49000 loss: 0.4719920268859865\n",
      "30000/49000 loss: 0.4820778485836167\n",
      "40000/49000 loss: 0.4599787148457998\n",
      "epoch 24: valid acc = 0.83, new learning rate = 0.00014599451216938612\n",
      "10000/49000 loss: 0.4573337296207067\n",
      "20000/49000 loss: 0.4483102607173307\n",
      "30000/49000 loss: 0.4738358488995002\n",
      "40000/49000 loss: 0.49749512836177195\n",
      "epoch 25: valid acc = 0.832, new learning rate = 0.00013869478656091682\n",
      "10000/49000 loss: 0.44613021541232506\n",
      "20000/49000 loss: 0.42726603229570526\n",
      "30000/49000 loss: 0.4797864612980867\n",
      "40000/49000 loss: 0.44839919941679424\n",
      "epoch 26: valid acc = 0.831, new learning rate = 0.00013176004723287096\n",
      "10000/49000 loss: 0.44017188082315123\n",
      "20000/49000 loss: 0.417706482948107\n",
      "30000/49000 loss: 0.4299006144675326\n",
      "40000/49000 loss: 0.4368614642707242\n",
      "epoch 27: valid acc = 0.83, new learning rate = 0.0001251720448712274\n",
      "10000/49000 loss: 0.44100882682202175\n",
      "20000/49000 loss: 0.4424805270095778\n",
      "30000/49000 loss: 0.44369438935281647\n",
      "40000/49000 loss: 0.4759687763372823\n",
      "epoch 28: valid acc = 0.834, new learning rate = 0.00011891344262766602\n",
      "10000/49000 loss: 0.4642044590856421\n",
      "20000/49000 loss: 0.48301025075208937\n",
      "30000/49000 loss: 0.44689217491859035\n",
      "40000/49000 loss: 0.45732800816972424\n",
      "epoch 29: valid acc = 0.828, new learning rate = 0.00011296777049628272\n",
      "10000/49000 loss: 0.41864002246498133\n",
      "20000/49000 loss: 0.42642058243505626\n",
      "30000/49000 loss: 0.44584267644262565\n",
      "40000/49000 loss: 0.4277605591399063\n",
      "epoch 30: valid acc = 0.832, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.8436326530612245\n",
      "test acc: 0.832\n",
      "test acc: 0.8278\n",
      "number of batches for training: 49\n",
      "epoch 1: valid acc = 0.37, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.571, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.636, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.69, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.728, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.743, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.755, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.784, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.78, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.797, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.798, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.802, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.802, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.814, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.815, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.813, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.82, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.819, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.819, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.824, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.826, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.828, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.827, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.829, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.831, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.835, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.834, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.832, new learning rate = 0.00011891344262766602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 29: valid acc = 0.832, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.832, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.8448571428571429\n",
      "test acc: 0.832\n",
      "test acc: 0.8286\n",
      "number of batches for training: 49\n",
      "10000/49000 loss: 3.727363314614707\n",
      "20000/49000 loss: 3.4930177998940466\n",
      "30000/49000 loss: 3.214237243914782\n",
      "40000/49000 loss: 2.3353642435387125\n",
      "epoch 1: valid acc = 0.356, new learning rate = 0.000475\n",
      "10000/49000 loss: 2.126865052229064\n",
      "20000/49000 loss: 1.8334560355871734\n",
      "30000/49000 loss: 1.7074583479547696\n",
      "40000/49000 loss: 1.4271425442463495\n",
      "epoch 2: valid acc = 0.528, new learning rate = 0.00045125\n",
      "10000/49000 loss: 1.2729930695952192\n",
      "20000/49000 loss: 1.1339064447565115\n",
      "30000/49000 loss: 1.1775903337339626\n",
      "40000/49000 loss: 1.1621764939130774\n",
      "epoch 3: valid acc = 0.65, new learning rate = 0.0004286875\n",
      "10000/49000 loss: 1.0686408142892934\n",
      "20000/49000 loss: 1.0064920056445257\n",
      "30000/49000 loss: 0.9260464001606973\n",
      "40000/49000 loss: 0.936942460627435\n",
      "epoch 4: valid acc = 0.69, new learning rate = 0.00040725312499999993\n",
      "10000/49000 loss: 0.8864116618050404\n",
      "20000/49000 loss: 0.8327548556387637\n",
      "30000/49000 loss: 0.8400800952242921\n",
      "40000/49000 loss: 0.8150923387176412\n",
      "epoch 5: valid acc = 0.735, new learning rate = 0.0003868904687499999\n",
      "10000/49000 loss: 0.7882376749260344\n",
      "20000/49000 loss: 0.7370638516172929\n",
      "30000/49000 loss: 0.7478838983518485\n",
      "40000/49000 loss: 0.7278524571126769\n",
      "epoch 6: valid acc = 0.752, new learning rate = 0.0003675459453124999\n",
      "10000/49000 loss: 0.6621086171455841\n",
      "20000/49000 loss: 0.6532107079685284\n",
      "30000/49000 loss: 0.7077616269420288\n",
      "40000/49000 loss: 0.6828006006199366\n",
      "epoch 7: valid acc = 0.759, new learning rate = 0.00034916864804687486\n",
      "10000/49000 loss: 0.6501418659326226\n",
      "20000/49000 loss: 0.665409483053272\n",
      "30000/49000 loss: 0.6233802234278545\n",
      "40000/49000 loss: 0.6206557099979072\n",
      "epoch 8: valid acc = 0.763, new learning rate = 0.0003317102156445311\n",
      "10000/49000 loss: 0.586812259485145\n",
      "20000/49000 loss: 0.5821450324562263\n",
      "30000/49000 loss: 0.5803005898245237\n",
      "40000/49000 loss: 0.5852683566489622\n",
      "epoch 9: valid acc = 0.785, new learning rate = 0.0003151247048623045\n",
      "10000/49000 loss: 0.6105006052702912\n",
      "20000/49000 loss: 0.5593727625878856\n",
      "30000/49000 loss: 0.5755275143648142\n",
      "40000/49000 loss: 0.5465916016653489\n",
      "epoch 10: valid acc = 0.789, new learning rate = 0.00029936846961918924\n",
      "10000/49000 loss: 0.5633063589164526\n",
      "20000/49000 loss: 0.5243945729274712\n",
      "30000/49000 loss: 0.5458925901138746\n",
      "40000/49000 loss: 0.5146598146424406\n",
      "epoch 11: valid acc = 0.796, new learning rate = 0.00028440004613822977\n",
      "10000/49000 loss: 0.5070560956578094\n",
      "20000/49000 loss: 0.5188144516939138\n",
      "30000/49000 loss: 0.5602790303234539\n",
      "40000/49000 loss: 0.5147896660130954\n",
      "epoch 12: valid acc = 0.797, new learning rate = 0.00027018004383131826\n",
      "10000/49000 loss: 0.5283794194585743\n",
      "20000/49000 loss: 0.4880648515005021\n",
      "30000/49000 loss: 0.48141133814722925\n",
      "40000/49000 loss: 0.5608136381607841\n",
      "epoch 13: valid acc = 0.806, new learning rate = 0.00025667104163975234\n",
      "10000/49000 loss: 0.5304897563341604\n",
      "20000/49000 loss: 0.5580938422726948\n",
      "30000/49000 loss: 0.522945601221218\n",
      "40000/49000 loss: 0.5103666285623621\n",
      "epoch 14: valid acc = 0.813, new learning rate = 0.00024383748955776472\n",
      "10000/49000 loss: 0.49857898302249753\n",
      "20000/49000 loss: 0.5378548665122093\n",
      "30000/49000 loss: 0.5300575863315644\n",
      "40000/49000 loss: 0.5030282925865037\n",
      "epoch 15: valid acc = 0.813, new learning rate = 0.00023164561507987649\n",
      "10000/49000 loss: 0.5063360589992647\n",
      "20000/49000 loss: 0.49073070049939904\n",
      "30000/49000 loss: 0.540833120837333\n",
      "40000/49000 loss: 0.5535768248984999\n",
      "epoch 16: valid acc = 0.815, new learning rate = 0.00022006333432588265\n",
      "10000/49000 loss: 0.5114374428656352\n",
      "20000/49000 loss: 0.45204057307868134\n",
      "30000/49000 loss: 0.5016215247607894\n",
      "40000/49000 loss: 0.4945227967578762\n",
      "epoch 17: valid acc = 0.817, new learning rate = 0.00020906016760958852\n",
      "10000/49000 loss: 0.48317969332511534\n",
      "20000/49000 loss: 0.4425848921144136\n",
      "30000/49000 loss: 0.4424385476392666\n",
      "40000/49000 loss: 0.5170413534705864\n",
      "epoch 18: valid acc = 0.823, new learning rate = 0.00019860715922910907\n",
      "10000/49000 loss: 0.5043146567951247\n",
      "20000/49000 loss: 0.4803385765905498\n",
      "30000/49000 loss: 0.46831428331250285\n",
      "40000/49000 loss: 0.46883995566627634\n",
      "epoch 19: valid acc = 0.823, new learning rate = 0.0001886768012676536\n",
      "10000/49000 loss: 0.4631187467771284\n",
      "20000/49000 loss: 0.4628763189468005\n",
      "30000/49000 loss: 0.5246831816216964\n",
      "40000/49000 loss: 0.45450611865746215\n",
      "epoch 20: valid acc = 0.822, new learning rate = 0.0001792429612042709\n",
      "10000/49000 loss: 0.5005637531760271\n",
      "20000/49000 loss: 0.4795537374787208\n",
      "30000/49000 loss: 0.49431477467351254\n",
      "40000/49000 loss: 0.4758522200888831\n",
      "epoch 21: valid acc = 0.829, new learning rate = 0.00017028081314405735\n",
      "10000/49000 loss: 0.5171599345713808\n",
      "20000/49000 loss: 0.48587763942406326\n",
      "30000/49000 loss: 0.495348121674232\n",
      "40000/49000 loss: 0.4487882672187467\n",
      "epoch 22: valid acc = 0.827, new learning rate = 0.00016176677248685447\n",
      "10000/49000 loss: 0.4918591533204769\n",
      "20000/49000 loss: 0.4443832875326763\n",
      "30000/49000 loss: 0.4026515434322395\n",
      "40000/49000 loss: 0.452695079271064\n",
      "epoch 23: valid acc = 0.831, new learning rate = 0.00015367843386251173\n",
      "10000/49000 loss: 0.5267609942127353\n",
      "20000/49000 loss: 0.4706259116872778\n",
      "30000/49000 loss: 0.43926677802606895\n",
      "40000/49000 loss: 0.4880214707084242\n",
      "epoch 24: valid acc = 0.833, new learning rate = 0.00014599451216938612\n",
      "10000/49000 loss: 0.4590722153042389\n",
      "20000/49000 loss: 0.44369423281947296\n",
      "30000/49000 loss: 0.4919348578614928\n",
      "40000/49000 loss: 0.48756473012089635\n",
      "epoch 25: valid acc = 0.826, new learning rate = 0.00013869478656091682\n",
      "10000/49000 loss: 0.4459297538837625\n",
      "20000/49000 loss: 0.5026054528498342\n",
      "30000/49000 loss: 0.4708524708512498\n",
      "40000/49000 loss: 0.5010839193847828\n",
      "epoch 26: valid acc = 0.827, new learning rate = 0.00013176004723287096\n",
      "10000/49000 loss: 0.4720776303354011\n",
      "20000/49000 loss: 0.42971535097491237\n",
      "30000/49000 loss: 0.4460040011575685\n",
      "40000/49000 loss: 0.41565202152842057\n",
      "epoch 27: valid acc = 0.829, new learning rate = 0.0001251720448712274\n",
      "10000/49000 loss: 0.47735553136931813\n",
      "20000/49000 loss: 0.4452327273951233\n",
      "30000/49000 loss: 0.43278279627220084\n",
      "40000/49000 loss: 0.4568293941282917\n",
      "epoch 28: valid acc = 0.829, new learning rate = 0.00011891344262766602\n",
      "10000/49000 loss: 0.4489017192278622\n",
      "20000/49000 loss: 0.4361381062895227\n",
      "30000/49000 loss: 0.4481152489601532\n",
      "40000/49000 loss: 0.4108017834349858\n",
      "epoch 29: valid acc = 0.828, new learning rate = 0.00011296777049628272\n",
      "10000/49000 loss: 0.45783754309330643\n",
      "20000/49000 loss: 0.4609501936179443\n",
      "30000/49000 loss: 0.46067557520857366\n",
      "40000/49000 loss: 0.45010329943146365\n",
      "epoch 30: valid acc = 0.829, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.8431632653061224\n",
      "test acc: 0.829\n",
      "test acc: 0.8294\n",
      "number of batches for training: 49\n",
      "epoch 1: valid acc = 0.382, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.554, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.625, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.692, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.727, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.746, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.757, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.772, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.781, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.791, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.791, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.802, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.801, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.812, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.817, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.815, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.818, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.819, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.819, new learning rate = 0.0001886768012676536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20: valid acc = 0.823, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.822, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.825, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.826, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.833, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.833, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.826, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.832, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.832, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.831, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.831, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.8439387755102041\n",
      "test acc: 0.831\n",
      "test acc: 0.8294\n",
      "number of batches for training: 49\n",
      "10000/49000 loss: 3.2580203957829017\n",
      "20000/49000 loss: 3.5265063725654815\n",
      "30000/49000 loss: 2.713472042302774\n",
      "40000/49000 loss: 2.5038520416917787\n",
      "epoch 1: valid acc = 0.406, new learning rate = 0.000475\n",
      "10000/49000 loss: 2.215344324973267\n",
      "20000/49000 loss: 2.0661909461880676\n",
      "30000/49000 loss: 1.7916948426808703\n",
      "40000/49000 loss: 1.5054547020543694\n",
      "epoch 2: valid acc = 0.518, new learning rate = 0.00045125\n",
      "10000/49000 loss: 1.289897723713953\n",
      "20000/49000 loss: 1.2433534655679577\n",
      "30000/49000 loss: 1.2099107738905301\n",
      "40000/49000 loss: 1.1518583514852478\n",
      "epoch 3: valid acc = 0.65, new learning rate = 0.0004286875\n",
      "10000/49000 loss: 1.0428797478766962\n",
      "20000/49000 loss: 1.0023774673371109\n",
      "30000/49000 loss: 0.9163427704455748\n",
      "40000/49000 loss: 0.8673324520053924\n",
      "epoch 4: valid acc = 0.711, new learning rate = 0.00040725312499999993\n",
      "10000/49000 loss: 0.8634660797713338\n",
      "20000/49000 loss: 0.8444851186514724\n",
      "30000/49000 loss: 0.8162590717689417\n",
      "40000/49000 loss: 0.7994688221119988\n",
      "epoch 5: valid acc = 0.716, new learning rate = 0.0003868904687499999\n",
      "10000/49000 loss: 0.7362132363024493\n",
      "20000/49000 loss: 0.7789204523179142\n",
      "30000/49000 loss: 0.7551614942605779\n",
      "40000/49000 loss: 0.7247800533397218\n",
      "epoch 6: valid acc = 0.745, new learning rate = 0.0003675459453124999\n",
      "10000/49000 loss: 0.6565022371195857\n",
      "20000/49000 loss: 0.7118170204236041\n",
      "30000/49000 loss: 0.6432585190896595\n",
      "40000/49000 loss: 0.6744756531145586\n",
      "epoch 7: valid acc = 0.754, new learning rate = 0.00034916864804687486\n",
      "10000/49000 loss: 0.6199539765539361\n",
      "20000/49000 loss: 0.6439740952710805\n",
      "30000/49000 loss: 0.6607668041918715\n",
      "40000/49000 loss: 0.5582893669405904\n",
      "epoch 8: valid acc = 0.772, new learning rate = 0.0003317102156445311\n",
      "10000/49000 loss: 0.5935813359179969\n",
      "20000/49000 loss: 0.5995982268120411\n",
      "30000/49000 loss: 0.5306708325951881\n",
      "40000/49000 loss: 0.5606331982187392\n",
      "epoch 9: valid acc = 0.784, new learning rate = 0.0003151247048623045\n",
      "10000/49000 loss: 0.5996344700433839\n",
      "20000/49000 loss: 0.5401721497569537\n",
      "30000/49000 loss: 0.5749148006700427\n",
      "40000/49000 loss: 0.603252309677869\n",
      "epoch 10: valid acc = 0.791, new learning rate = 0.00029936846961918924\n",
      "10000/49000 loss: 0.5753657810387219\n",
      "20000/49000 loss: 0.569293269057879\n",
      "30000/49000 loss: 0.6003264265710918\n",
      "40000/49000 loss: 0.5440002571710942\n",
      "epoch 11: valid acc = 0.798, new learning rate = 0.00028440004613822977\n",
      "10000/49000 loss: 0.5332241887934853\n",
      "20000/49000 loss: 0.5532741172530325\n",
      "30000/49000 loss: 0.5327641857663346\n",
      "40000/49000 loss: 0.5202689054729974\n",
      "epoch 12: valid acc = 0.802, new learning rate = 0.00027018004383131826\n",
      "10000/49000 loss: 0.5124215433227313\n",
      "20000/49000 loss: 0.5367351861879088\n",
      "30000/49000 loss: 0.5629243400515986\n",
      "40000/49000 loss: 0.4944352347258051\n",
      "epoch 13: valid acc = 0.804, new learning rate = 0.00025667104163975234\n",
      "10000/49000 loss: 0.5179542125371329\n",
      "20000/49000 loss: 0.5104801151059867\n",
      "30000/49000 loss: 0.49235997266737175\n",
      "40000/49000 loss: 0.5254087887875237\n",
      "epoch 14: valid acc = 0.806, new learning rate = 0.00024383748955776472\n",
      "10000/49000 loss: 0.5485208508869529\n",
      "20000/49000 loss: 0.43849386107478594\n",
      "30000/49000 loss: 0.48349930603314945\n",
      "40000/49000 loss: 0.5056629213915902\n",
      "epoch 15: valid acc = 0.811, new learning rate = 0.00023164561507987649\n",
      "10000/49000 loss: 0.5375396954102234\n",
      "20000/49000 loss: 0.42334520324665664\n",
      "30000/49000 loss: 0.5103544533901422\n",
      "40000/49000 loss: 0.48338337697602257\n",
      "epoch 16: valid acc = 0.817, new learning rate = 0.00022006333432588265\n",
      "10000/49000 loss: 0.4753924553061569\n",
      "20000/49000 loss: 0.48628211715563674\n",
      "30000/49000 loss: 0.5204428368278871\n",
      "40000/49000 loss: 0.4905968939558602\n",
      "epoch 17: valid acc = 0.814, new learning rate = 0.00020906016760958852\n",
      "10000/49000 loss: 0.5452599004591667\n",
      "20000/49000 loss: 0.4652772900263496\n",
      "30000/49000 loss: 0.4529642744896272\n",
      "40000/49000 loss: 0.48615481530162025\n",
      "epoch 18: valid acc = 0.819, new learning rate = 0.00019860715922910907\n",
      "10000/49000 loss: 0.47229473177730286\n",
      "20000/49000 loss: 0.47746456804194337\n",
      "30000/49000 loss: 0.4859792625900992\n",
      "40000/49000 loss: 0.5455723650695707\n",
      "epoch 19: valid acc = 0.818, new learning rate = 0.0001886768012676536\n",
      "10000/49000 loss: 0.4431234966282449\n",
      "20000/49000 loss: 0.475113671765457\n",
      "30000/49000 loss: 0.49112410465306516\n",
      "40000/49000 loss: 0.4623706781818726\n",
      "epoch 20: valid acc = 0.822, new learning rate = 0.0001792429612042709\n",
      "10000/49000 loss: 0.4977843684072531\n",
      "20000/49000 loss: 0.44576582566809325\n",
      "30000/49000 loss: 0.4890278242494993\n",
      "40000/49000 loss: 0.5007521662149469\n",
      "epoch 21: valid acc = 0.825, new learning rate = 0.00017028081314405735\n",
      "10000/49000 loss: 0.45253692716579014\n",
      "20000/49000 loss: 0.4561284578025351\n",
      "30000/49000 loss: 0.47451401004655286\n",
      "40000/49000 loss: 0.47405719963946674\n",
      "epoch 22: valid acc = 0.826, new learning rate = 0.00016176677248685447\n",
      "10000/49000 loss: 0.5115190347161979\n",
      "20000/49000 loss: 0.4374407714157092\n",
      "30000/49000 loss: 0.4717778240027414\n",
      "40000/49000 loss: 0.4520862762553272\n",
      "epoch 23: valid acc = 0.829, new learning rate = 0.00015367843386251173\n",
      "10000/49000 loss: 0.48050654536524073\n",
      "20000/49000 loss: 0.4415944293573341\n",
      "30000/49000 loss: 0.4391920021955723\n",
      "40000/49000 loss: 0.48516203640555877\n",
      "epoch 24: valid acc = 0.829, new learning rate = 0.00014599451216938612\n",
      "10000/49000 loss: 0.48780098518643206\n",
      "20000/49000 loss: 0.4855753346890808\n",
      "30000/49000 loss: 0.43123649276233134\n",
      "40000/49000 loss: 0.4408372981360379\n",
      "epoch 25: valid acc = 0.829, new learning rate = 0.00013869478656091682\n",
      "10000/49000 loss: 0.41272753605997525\n",
      "20000/49000 loss: 0.5133882241093973\n",
      "30000/49000 loss: 0.4204729503808867\n",
      "40000/49000 loss: 0.47236652159614717\n",
      "epoch 26: valid acc = 0.829, new learning rate = 0.00013176004723287096\n",
      "10000/49000 loss: 0.4899572441142661\n",
      "20000/49000 loss: 0.43720169286252847\n",
      "30000/49000 loss: 0.4472568527784548\n",
      "40000/49000 loss: 0.4506073770964418\n",
      "epoch 27: valid acc = 0.83, new learning rate = 0.0001251720448712274\n",
      "10000/49000 loss: 0.4676917857553751\n",
      "20000/49000 loss: 0.4600940699856916\n",
      "30000/49000 loss: 0.4020567671735483\n",
      "40000/49000 loss: 0.44735637184276844\n",
      "epoch 28: valid acc = 0.834, new learning rate = 0.00011891344262766602\n",
      "10000/49000 loss: 0.4349848206802307\n",
      "20000/49000 loss: 0.4498660012274077\n",
      "30000/49000 loss: 0.4360849976917926\n",
      "40000/49000 loss: 0.4597686271266927\n",
      "epoch 29: valid acc = 0.832, new learning rate = 0.00011296777049628272\n",
      "10000/49000 loss: 0.4478257492929089\n",
      "20000/49000 loss: 0.43712826171959407\n",
      "30000/49000 loss: 0.45540772634796656\n",
      "40000/49000 loss: 0.4844786472010571\n",
      "epoch 30: valid acc = 0.836, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.842469387755102\n",
      "test acc: 0.836\n",
      "test acc: 0.8275\n",
      "number of batches for training: 49\n",
      "epoch 1: valid acc = 0.377, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.514, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.642, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.692, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.725, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.75, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.75, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.773, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.78, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.796, new learning rate = 0.00029936846961918924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11: valid acc = 0.8, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.803, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.808, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.81, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.814, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.815, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.82, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.822, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.82, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.821, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.825, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.83, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.829, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.829, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.828, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.832, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.829, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.832, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.833, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.833, new learning rate = 0.00010731938197146858\n",
      "test acc: 0.8441632653061224\n",
      "test acc: 0.833\n",
      "test acc: 0.8297\n",
      "number of batches for training: 245\n",
      "2000/49000 loss: 4.0377726758862895\n",
      "4000/49000 loss: 3.7965795896332635\n",
      "6000/49000 loss: 2.598358216192141\n",
      "8000/49000 loss: 2.3269221615341915\n",
      "10000/49000 loss: 2.21666799648663\n",
      "12000/49000 loss: 2.1657834859489538\n",
      "14000/49000 loss: 2.1736434763098957\n",
      "16000/49000 loss: 1.5755065764615244\n",
      "18000/49000 loss: 1.411729556094833\n",
      "20000/49000 loss: 1.274818420322745\n",
      "22000/49000 loss: 1.4621977478761161\n",
      "24000/49000 loss: 1.1648260006518076\n",
      "26000/49000 loss: 1.014437098360093\n",
      "28000/49000 loss: 1.0620352581642727\n",
      "30000/49000 loss: 1.083010468574005\n",
      "32000/49000 loss: 0.8305066355614008\n",
      "34000/49000 loss: 0.9439551731490704\n",
      "36000/49000 loss: 0.8496610358418725\n",
      "38000/49000 loss: 0.8163898042676433\n",
      "40000/49000 loss: 0.9027850481851366\n",
      "42000/49000 loss: 0.805225524286469\n",
      "44000/49000 loss: 0.8512796382099135\n",
      "46000/49000 loss: 0.7874207551354717\n",
      "48000/49000 loss: 0.7872573025758557\n",
      "epoch 1: valid acc = 0.75, new learning rate = 0.000475\n",
      "2000/49000 loss: 0.702420042680663\n",
      "4000/49000 loss: 0.765987595880958\n",
      "6000/49000 loss: 0.6619914400437098\n",
      "8000/49000 loss: 0.6775076662853033\n",
      "10000/49000 loss: 0.6030113629356043\n",
      "12000/49000 loss: 0.7786230045966043\n",
      "14000/49000 loss: 0.6141668963488259\n",
      "16000/49000 loss: 0.6213133672013756\n",
      "18000/49000 loss: 0.6309296416905521\n",
      "20000/49000 loss: 0.5644758649282513\n",
      "22000/49000 loss: 0.5310399415371229\n",
      "24000/49000 loss: 0.6506804042676081\n",
      "26000/49000 loss: 0.7121413733560551\n",
      "28000/49000 loss: 0.608236490159187\n",
      "30000/49000 loss: 0.5532230956873115\n",
      "32000/49000 loss: 0.5346308199635071\n",
      "34000/49000 loss: 0.4821936667303295\n",
      "36000/49000 loss: 0.4753833016767296\n",
      "38000/49000 loss: 0.5020386360017081\n",
      "40000/49000 loss: 0.5163643446546181\n",
      "42000/49000 loss: 0.5545366889959578\n",
      "44000/49000 loss: 0.5112267572650026\n",
      "46000/49000 loss: 0.4631028721018629\n",
      "48000/49000 loss: 0.5898993306132494\n",
      "epoch 2: valid acc = 0.802, new learning rate = 0.00045125\n",
      "2000/49000 loss: 0.5194678218096334\n",
      "4000/49000 loss: 0.5552904236701285\n",
      "6000/49000 loss: 0.6168043172217724\n",
      "8000/49000 loss: 0.5916386760364205\n",
      "10000/49000 loss: 0.5285888674052578\n",
      "12000/49000 loss: 0.56840450453564\n",
      "14000/49000 loss: 0.5466361046211337\n",
      "16000/49000 loss: 0.3852566018903851\n",
      "18000/49000 loss: 0.41193422282544334\n",
      "20000/49000 loss: 0.4576934094551977\n",
      "22000/49000 loss: 0.5412752950937075\n",
      "24000/49000 loss: 0.41154909152074054\n",
      "26000/49000 loss: 0.522607539933891\n",
      "28000/49000 loss: 0.4675700493374348\n",
      "30000/49000 loss: 0.5006163302893284\n",
      "32000/49000 loss: 0.4249799385980951\n",
      "34000/49000 loss: 0.4657965414861905\n",
      "36000/49000 loss: 0.44271162467617226\n",
      "38000/49000 loss: 0.4416734002178371\n",
      "40000/49000 loss: 0.4369298946056593\n",
      "42000/49000 loss: 0.4744951678408182\n",
      "44000/49000 loss: 0.5032052956601607\n",
      "46000/49000 loss: 0.569478356084638\n",
      "48000/49000 loss: 0.4305026923602674\n",
      "epoch 3: valid acc = 0.83, new learning rate = 0.0004286875\n",
      "2000/49000 loss: 0.34681934391396146\n",
      "4000/49000 loss: 0.4494852222324148\n",
      "6000/49000 loss: 0.3763433648240832\n",
      "8000/49000 loss: 0.58624103366769\n",
      "10000/49000 loss: 0.49835524444546286\n",
      "12000/49000 loss: 0.40947309491052997\n",
      "14000/49000 loss: 0.4840978505843828\n",
      "16000/49000 loss: 0.48073832380431036\n",
      "18000/49000 loss: 0.5528154949697364\n",
      "20000/49000 loss: 0.504256890577707\n",
      "22000/49000 loss: 0.5097164815895361\n",
      "24000/49000 loss: 0.42993907730466235\n",
      "26000/49000 loss: 0.3660555119444491\n",
      "28000/49000 loss: 0.4756445287482424\n",
      "30000/49000 loss: 0.5089481985082264\n",
      "32000/49000 loss: 0.45120290538134\n",
      "34000/49000 loss: 0.41457726014608337\n",
      "36000/49000 loss: 0.564919395270012\n",
      "38000/49000 loss: 0.34548886766259596\n",
      "40000/49000 loss: 0.3843126373447197\n",
      "42000/49000 loss: 0.3661432651855451\n",
      "44000/49000 loss: 0.4957592291691973\n",
      "46000/49000 loss: 0.45113188110712077\n",
      "48000/49000 loss: 0.28001516581668373\n",
      "epoch 4: valid acc = 0.836, new learning rate = 0.00040725312499999993\n",
      "2000/49000 loss: 0.5006254018949461\n",
      "4000/49000 loss: 0.4807049427947309\n",
      "6000/49000 loss: 0.374116328915323\n",
      "8000/49000 loss: 0.5043956226089417\n",
      "10000/49000 loss: 0.4333540910011882\n",
      "12000/49000 loss: 0.4456760130756885\n",
      "14000/49000 loss: 0.44815478949220505\n",
      "16000/49000 loss: 0.38014735219008605\n",
      "18000/49000 loss: 0.44301276149509383\n",
      "20000/49000 loss: 0.37214479642201154\n",
      "22000/49000 loss: 0.4344451808535307\n",
      "24000/49000 loss: 0.33992715914546195\n",
      "26000/49000 loss: 0.40119061752383806\n",
      "28000/49000 loss: 0.4858985551151304\n",
      "30000/49000 loss: 0.5037832226257223\n",
      "32000/49000 loss: 0.5010911694354807\n",
      "34000/49000 loss: 0.39898077925134817\n",
      "36000/49000 loss: 0.41740634641339375\n",
      "38000/49000 loss: 0.339107146083446\n",
      "40000/49000 loss: 0.34655442406853115\n",
      "42000/49000 loss: 0.4547474812513318\n",
      "44000/49000 loss: 0.3838872891421013\n",
      "46000/49000 loss: 0.385193386018226\n",
      "48000/49000 loss: 0.3635911864979533\n",
      "epoch 5: valid acc = 0.849, new learning rate = 0.0003868904687499999\n",
      "2000/49000 loss: 0.40683398004905363\n",
      "4000/49000 loss: 0.407133678085688\n",
      "6000/49000 loss: 0.44616175556755944\n",
      "8000/49000 loss: 0.46010241802125446\n",
      "10000/49000 loss: 0.32638123182709394\n",
      "12000/49000 loss: 0.4347747815275816\n",
      "14000/49000 loss: 0.39798775848951173\n",
      "16000/49000 loss: 0.33026359206036465\n",
      "18000/49000 loss: 0.4100238828352296\n",
      "20000/49000 loss: 0.37667901068925025\n",
      "22000/49000 loss: 0.4166553195447839\n",
      "24000/49000 loss: 0.4010249640615072\n",
      "26000/49000 loss: 0.47614157137870344\n",
      "28000/49000 loss: 0.4608106769672518\n",
      "30000/49000 loss: 0.42509739917124\n",
      "32000/49000 loss: 0.40932011290644077\n",
      "34000/49000 loss: 0.49372675999420373\n",
      "36000/49000 loss: 0.4074255502055316\n",
      "38000/49000 loss: 0.4563969649149966\n",
      "40000/49000 loss: 0.44227659734764707\n",
      "42000/49000 loss: 0.33173281717795\n",
      "44000/49000 loss: 0.3209648683342167\n",
      "46000/49000 loss: 0.25003642436387863\n",
      "48000/49000 loss: 0.32470565097837856\n",
      "epoch 6: valid acc = 0.858, new learning rate = 0.0003675459453124999\n",
      "2000/49000 loss: 0.3846980751001629\n",
      "4000/49000 loss: 0.44973620317963875\n",
      "6000/49000 loss: 0.36447743657315745\n",
      "8000/49000 loss: 0.35250251593561954\n",
      "10000/49000 loss: 0.4499805307969355\n",
      "12000/49000 loss: 0.4458687714098023\n",
      "14000/49000 loss: 0.35982749715984846\n",
      "16000/49000 loss: 0.44394635475896665\n",
      "18000/49000 loss: 0.47722943456669503\n",
      "20000/49000 loss: 0.41676896826765214\n",
      "22000/49000 loss: 0.338974370162881\n",
      "24000/49000 loss: 0.45416360329198985\n",
      "26000/49000 loss: 0.3591055800191236\n",
      "28000/49000 loss: 0.43098573603997564\n",
      "30000/49000 loss: 0.3797133999741233\n",
      "32000/49000 loss: 0.40544842621099425\n",
      "34000/49000 loss: 0.4804614906592916\n",
      "36000/49000 loss: 0.3659209624402333\n",
      "38000/49000 loss: 0.42436759033357896\n",
      "40000/49000 loss: 0.2913161436541769\n",
      "42000/49000 loss: 0.43790959468667096\n",
      "44000/49000 loss: 0.36909362999827244\n",
      "46000/49000 loss: 0.5632222198723488\n",
      "48000/49000 loss: 0.36184922356235394\n",
      "epoch 7: valid acc = 0.863, new learning rate = 0.00034916864804687486\n",
      "2000/49000 loss: 0.25639469321509567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/49000 loss: 0.41150260296565006\n",
      "6000/49000 loss: 0.37699066974623285\n",
      "8000/49000 loss: 0.3543541279065181\n",
      "10000/49000 loss: 0.3002514956028352\n",
      "12000/49000 loss: 0.3663474556395467\n",
      "14000/49000 loss: 0.31120776237387676\n",
      "16000/49000 loss: 0.3296651874737523\n",
      "18000/49000 loss: 0.3563883306741985\n",
      "20000/49000 loss: 0.28895295111387165\n",
      "22000/49000 loss: 0.3443879862713463\n",
      "24000/49000 loss: 0.3971216671001749\n",
      "26000/49000 loss: 0.44559690866540896\n",
      "28000/49000 loss: 0.34276601748887153\n",
      "30000/49000 loss: 0.3287363939783692\n",
      "32000/49000 loss: 0.4660709648585805\n",
      "34000/49000 loss: 0.42830637126713134\n",
      "36000/49000 loss: 0.3442986197199251\n",
      "38000/49000 loss: 0.4027255321111679\n",
      "40000/49000 loss: 0.35555210218771127\n",
      "42000/49000 loss: 0.3606785707286373\n",
      "44000/49000 loss: 0.38167348582777205\n",
      "46000/49000 loss: 0.45619318083256366\n",
      "48000/49000 loss: 0.37428170617348877\n",
      "epoch 8: valid acc = 0.862, new learning rate = 0.0003317102156445311\n",
      "2000/49000 loss: 0.42153071100421063\n",
      "4000/49000 loss: 0.4249619632140289\n",
      "6000/49000 loss: 0.33459129991289227\n",
      "8000/49000 loss: 0.3066236624562523\n",
      "10000/49000 loss: 0.3591708090619111\n",
      "12000/49000 loss: 0.36236798436895595\n",
      "14000/49000 loss: 0.4553669146363809\n",
      "16000/49000 loss: 0.43251912900273487\n",
      "18000/49000 loss: 0.28978591256827746\n",
      "20000/49000 loss: 0.34838221310157785\n",
      "22000/49000 loss: 0.29014106369392345\n",
      "24000/49000 loss: 0.39299028380534295\n",
      "26000/49000 loss: 0.3468124360752558\n",
      "28000/49000 loss: 0.370596152628999\n",
      "30000/49000 loss: 0.3374899668582976\n",
      "32000/49000 loss: 0.3750979600954759\n",
      "34000/49000 loss: 0.323264031117373\n",
      "36000/49000 loss: 0.40845971296192535\n",
      "38000/49000 loss: 0.30273959105740084\n",
      "40000/49000 loss: 0.33305180965175557\n",
      "42000/49000 loss: 0.396494881912712\n",
      "44000/49000 loss: 0.3677103491743722\n",
      "46000/49000 loss: 0.4712949484515178\n",
      "48000/49000 loss: 0.37732789565656727\n",
      "epoch 9: valid acc = 0.872, new learning rate = 0.0003151247048623045\n",
      "2000/49000 loss: 0.37020942898995557\n",
      "4000/49000 loss: 0.4092444937846966\n",
      "6000/49000 loss: 0.31242118889768583\n",
      "8000/49000 loss: 0.30236670533361815\n",
      "10000/49000 loss: 0.3913755820110349\n",
      "12000/49000 loss: 0.2784368545510408\n",
      "14000/49000 loss: 0.37236500372543774\n",
      "16000/49000 loss: 0.4424932470173473\n",
      "18000/49000 loss: 0.45764808249198236\n",
      "20000/49000 loss: 0.274373654216024\n",
      "22000/49000 loss: 0.2657047388527168\n",
      "24000/49000 loss: 0.388473747382683\n",
      "26000/49000 loss: 0.2841675513466781\n",
      "28000/49000 loss: 0.442640916934149\n",
      "30000/49000 loss: 0.45118662409510835\n",
      "32000/49000 loss: 0.3497603723825844\n",
      "34000/49000 loss: 0.3331800201023722\n",
      "36000/49000 loss: 0.3020513019529274\n",
      "38000/49000 loss: 0.43096203147273726\n",
      "40000/49000 loss: 0.2952654276150103\n",
      "42000/49000 loss: 0.323569069393967\n",
      "44000/49000 loss: 0.36358558542360925\n",
      "46000/49000 loss: 0.3527165271751352\n",
      "48000/49000 loss: 0.39467608156790857\n",
      "epoch 10: valid acc = 0.875, new learning rate = 0.00029936846961918924\n",
      "2000/49000 loss: 0.3135080835102932\n",
      "4000/49000 loss: 0.3470800717015514\n",
      "6000/49000 loss: 0.3527801529074168\n",
      "8000/49000 loss: 0.41976996087149665\n",
      "10000/49000 loss: 0.29490428723325546\n",
      "12000/49000 loss: 0.48547337790637074\n",
      "14000/49000 loss: 0.3401474271695252\n",
      "16000/49000 loss: 0.3241271130882258\n",
      "18000/49000 loss: 0.3719042725293044\n",
      "20000/49000 loss: 0.3593108986477748\n",
      "22000/49000 loss: 0.3300716807692102\n",
      "24000/49000 loss: 0.2671538655290138\n",
      "26000/49000 loss: 0.31751211038016475\n",
      "28000/49000 loss: 0.325187583656637\n",
      "30000/49000 loss: 0.3347744455582979\n",
      "32000/49000 loss: 0.3248429797380665\n",
      "34000/49000 loss: 0.32547491897688396\n",
      "36000/49000 loss: 0.32420219621748314\n",
      "38000/49000 loss: 0.3045715154503236\n",
      "40000/49000 loss: 0.421058522827863\n",
      "42000/49000 loss: 0.4262926271651063\n",
      "44000/49000 loss: 0.35227291193114507\n",
      "46000/49000 loss: 0.3272275558014289\n",
      "48000/49000 loss: 0.34656791855199676\n",
      "epoch 11: valid acc = 0.875, new learning rate = 0.00028440004613822977\n",
      "2000/49000 loss: 0.4168161564973665\n",
      "4000/49000 loss: 0.30164970782253925\n",
      "6000/49000 loss: 0.3029966560051851\n",
      "8000/49000 loss: 0.2527704932286118\n",
      "10000/49000 loss: 0.3063883888772856\n",
      "12000/49000 loss: 0.3296117574588357\n",
      "14000/49000 loss: 0.34829989610047324\n",
      "16000/49000 loss: 0.44023087648471676\n",
      "18000/49000 loss: 0.2899601719722074\n",
      "20000/49000 loss: 0.3595655641533297\n",
      "22000/49000 loss: 0.32096426039180165\n",
      "24000/49000 loss: 0.2723658278422936\n",
      "26000/49000 loss: 0.2545419699658195\n",
      "28000/49000 loss: 0.25267637360548145\n",
      "30000/49000 loss: 0.36599901087849557\n",
      "32000/49000 loss: 0.3240216905374549\n",
      "34000/49000 loss: 0.28670104174297467\n",
      "36000/49000 loss: 0.3083796597401397\n",
      "38000/49000 loss: 0.34721571740614565\n",
      "40000/49000 loss: 0.3732886230024702\n",
      "42000/49000 loss: 0.33456572487703273\n",
      "44000/49000 loss: 0.27462834837908545\n",
      "46000/49000 loss: 0.3662879968577073\n",
      "48000/49000 loss: 0.3504333812959021\n",
      "epoch 12: valid acc = 0.882, new learning rate = 0.00027018004383131826\n",
      "2000/49000 loss: 0.34744827907106346\n",
      "4000/49000 loss: 0.42899013099630556\n",
      "6000/49000 loss: 0.29985789868019014\n",
      "8000/49000 loss: 0.3011829749429351\n",
      "10000/49000 loss: 0.24311608724947828\n",
      "12000/49000 loss: 0.34989738796137054\n",
      "14000/49000 loss: 0.29086246695370777\n",
      "16000/49000 loss: 0.36390730747434197\n",
      "18000/49000 loss: 0.3202796171276137\n",
      "20000/49000 loss: 0.2958078954282477\n",
      "22000/49000 loss: 0.3746812239783239\n",
      "24000/49000 loss: 0.26945208299143164\n",
      "26000/49000 loss: 0.36540165382591383\n",
      "28000/49000 loss: 0.35013947508682297\n",
      "30000/49000 loss: 0.3805831888401924\n",
      "32000/49000 loss: 0.35200060338734723\n",
      "34000/49000 loss: 0.38143400659468907\n",
      "36000/49000 loss: 0.4013385273844991\n",
      "38000/49000 loss: 0.2983975991027986\n",
      "40000/49000 loss: 0.33630059902225046\n",
      "42000/49000 loss: 0.21831994125882617\n",
      "44000/49000 loss: 0.29333196597168504\n",
      "46000/49000 loss: 0.31360279334194346\n",
      "48000/49000 loss: 0.364174919943353\n",
      "epoch 13: valid acc = 0.88, new learning rate = 0.00025667104163975234\n",
      "2000/49000 loss: 0.2766592095348794\n",
      "4000/49000 loss: 0.2756428114823817\n",
      "6000/49000 loss: 0.40945432896970024\n",
      "8000/49000 loss: 0.36242575261540017\n",
      "10000/49000 loss: 0.25340097868600814\n",
      "12000/49000 loss: 0.2825880282546174\n",
      "14000/49000 loss: 0.25283016970402045\n",
      "16000/49000 loss: 0.4087045581007922\n",
      "18000/49000 loss: 0.35820337678995984\n",
      "20000/49000 loss: 0.26882767808606545\n",
      "22000/49000 loss: 0.313135598788114\n",
      "24000/49000 loss: 0.30357496707951537\n",
      "26000/49000 loss: 0.358270398262785\n",
      "28000/49000 loss: 0.3247292323120423\n",
      "30000/49000 loss: 0.30289148548624395\n",
      "32000/49000 loss: 0.34394900010865326\n",
      "34000/49000 loss: 0.2897338192987338\n",
      "36000/49000 loss: 0.3418295465056094\n",
      "38000/49000 loss: 0.3459083144016402\n",
      "40000/49000 loss: 0.3592503781625874\n",
      "42000/49000 loss: 0.2831363215121903\n",
      "44000/49000 loss: 0.3748380899969216\n",
      "46000/49000 loss: 0.28847676829122143\n",
      "48000/49000 loss: 0.3229743717923474\n",
      "epoch 14: valid acc = 0.881, new learning rate = 0.00024383748955776472\n",
      "2000/49000 loss: 0.28356024875438424\n",
      "4000/49000 loss: 0.31488198160180336\n",
      "6000/49000 loss: 0.33861899532424683\n",
      "8000/49000 loss: 0.38769983232932426\n",
      "10000/49000 loss: 0.3093913809980483\n",
      "12000/49000 loss: 0.3646784402119011\n",
      "14000/49000 loss: 0.24362869148274288\n",
      "16000/49000 loss: 0.3504990787409786\n",
      "18000/49000 loss: 0.33196287897901605\n",
      "20000/49000 loss: 0.30004624974411304\n",
      "22000/49000 loss: 0.2949984884173627\n",
      "24000/49000 loss: 0.3547702630238908\n",
      "26000/49000 loss: 0.3182484025770579\n",
      "28000/49000 loss: 0.36945761141682937\n",
      "30000/49000 loss: 0.3383712655810695\n",
      "32000/49000 loss: 0.35904356770624873\n",
      "34000/49000 loss: 0.4208367956997083\n",
      "36000/49000 loss: 0.3218988535223586\n",
      "38000/49000 loss: 0.3174735702059021\n",
      "40000/49000 loss: 0.4360497829367581\n",
      "42000/49000 loss: 0.27263433293029543\n",
      "44000/49000 loss: 0.32224642432996786\n",
      "46000/49000 loss: 0.3296474779544053\n",
      "48000/49000 loss: 0.3169469430023216\n",
      "epoch 15: valid acc = 0.874, new learning rate = 0.00023164561507987649\n",
      "2000/49000 loss: 0.3253701045487359\n",
      "4000/49000 loss: 0.3053547990506523\n",
      "6000/49000 loss: 0.3319599007958962\n",
      "8000/49000 loss: 0.2963142665613529\n",
      "10000/49000 loss: 0.24326627031432405\n",
      "12000/49000 loss: 0.29008902596998415\n",
      "14000/49000 loss: 0.2878259894292017\n",
      "16000/49000 loss: 0.3108832908358907\n",
      "18000/49000 loss: 0.36712865446277115\n",
      "20000/49000 loss: 0.3974969875655077\n",
      "22000/49000 loss: 0.30490613920931126\n",
      "24000/49000 loss: 0.41583259327696837\n",
      "26000/49000 loss: 0.45784709218612435\n",
      "28000/49000 loss: 0.3327166067322394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000/49000 loss: 0.3923768805108826\n",
      "32000/49000 loss: 0.31782159806722293\n",
      "34000/49000 loss: 0.3908380446108475\n",
      "36000/49000 loss: 0.3204406265396571\n",
      "38000/49000 loss: 0.35227622371093076\n",
      "40000/49000 loss: 0.37419681425438306\n",
      "42000/49000 loss: 0.33242313162740517\n",
      "44000/49000 loss: 0.3607188639981417\n",
      "46000/49000 loss: 0.32303232674845245\n",
      "48000/49000 loss: 0.3025760645238146\n",
      "epoch 16: valid acc = 0.881, new learning rate = 0.00022006333432588265\n",
      "2000/49000 loss: 0.31538579839319725\n",
      "4000/49000 loss: 0.26534152719322474\n",
      "6000/49000 loss: 0.3815026135203684\n",
      "8000/49000 loss: 0.377960905351487\n",
      "10000/49000 loss: 0.2196132374896356\n",
      "12000/49000 loss: 0.28042891527010555\n",
      "14000/49000 loss: 0.3454630243610388\n",
      "16000/49000 loss: 0.33939602655205203\n",
      "18000/49000 loss: 0.2567037364594145\n",
      "20000/49000 loss: 0.3908034462594438\n",
      "22000/49000 loss: 0.42521829614732287\n",
      "24000/49000 loss: 0.26371407165814326\n",
      "26000/49000 loss: 0.3312393560070751\n",
      "28000/49000 loss: 0.2767125264496613\n",
      "30000/49000 loss: 0.3024745160721058\n",
      "32000/49000 loss: 0.34109313762915544\n",
      "34000/49000 loss: 0.31740971879572566\n",
      "36000/49000 loss: 0.3347807989941344\n",
      "38000/49000 loss: 0.34451276327582053\n",
      "40000/49000 loss: 0.3066384958215579\n",
      "42000/49000 loss: 0.37526811051722725\n",
      "44000/49000 loss: 0.3882293345604482\n",
      "46000/49000 loss: 0.29208510300391\n",
      "48000/49000 loss: 0.3479061482567688\n",
      "epoch 17: valid acc = 0.88, new learning rate = 0.00020906016760958852\n",
      "2000/49000 loss: 0.4094126850892474\n",
      "4000/49000 loss: 0.25681926227945856\n",
      "6000/49000 loss: 0.34800088868103174\n",
      "8000/49000 loss: 0.277110239612903\n",
      "10000/49000 loss: 0.3779341738038561\n",
      "12000/49000 loss: 0.3340202476179604\n",
      "14000/49000 loss: 0.35719486884348844\n",
      "16000/49000 loss: 0.24854927774495453\n",
      "18000/49000 loss: 0.3160423667137279\n",
      "20000/49000 loss: 0.2967311109632528\n",
      "22000/49000 loss: 0.24690972316981127\n",
      "24000/49000 loss: 0.33350045489632774\n",
      "26000/49000 loss: 0.3662361938131662\n",
      "28000/49000 loss: 0.27970561247926023\n",
      "30000/49000 loss: 0.24399233815739815\n",
      "32000/49000 loss: 0.3374493479023968\n",
      "34000/49000 loss: 0.37583939480614065\n",
      "36000/49000 loss: 0.28265405809530497\n",
      "38000/49000 loss: 0.25473655509775217\n",
      "40000/49000 loss: 0.3261229122339443\n",
      "42000/49000 loss: 0.33869251703177644\n",
      "44000/49000 loss: 0.3328470261991105\n",
      "46000/49000 loss: 0.24893322232482384\n",
      "48000/49000 loss: 0.2763768913911566\n",
      "epoch 18: valid acc = 0.878, new learning rate = 0.00019860715922910907\n",
      "2000/49000 loss: 0.3380287851242786\n",
      "4000/49000 loss: 0.28568874960566976\n",
      "6000/49000 loss: 0.3554524774232372\n",
      "8000/49000 loss: 0.39330710299025834\n",
      "10000/49000 loss: 0.3209549283702204\n",
      "12000/49000 loss: 0.35661784112418565\n",
      "14000/49000 loss: 0.3560104205320262\n",
      "16000/49000 loss: 0.40955233734996\n",
      "18000/49000 loss: 0.29893724545288153\n",
      "20000/49000 loss: 0.3416326141915854\n",
      "22000/49000 loss: 0.306450490171832\n",
      "24000/49000 loss: 0.23516649936956238\n",
      "26000/49000 loss: 0.2764703785155201\n",
      "28000/49000 loss: 0.2278959309861777\n",
      "30000/49000 loss: 0.39027392293745317\n",
      "32000/49000 loss: 0.25894030740856694\n",
      "34000/49000 loss: 0.35809645689106023\n",
      "36000/49000 loss: 0.346765724782994\n",
      "38000/49000 loss: 0.31438137074454514\n",
      "40000/49000 loss: 0.4168454634358339\n",
      "42000/49000 loss: 0.2568202296626741\n",
      "44000/49000 loss: 0.22063973984811963\n",
      "46000/49000 loss: 0.3156996960626842\n",
      "48000/49000 loss: 0.28909886231056064\n",
      "epoch 19: valid acc = 0.88, new learning rate = 0.0001886768012676536\n",
      "2000/49000 loss: 0.3047347372521103\n",
      "4000/49000 loss: 0.3250849364710745\n",
      "6000/49000 loss: 0.29392650285837446\n",
      "8000/49000 loss: 0.26832623182378407\n",
      "10000/49000 loss: 0.31658427443366727\n",
      "12000/49000 loss: 0.311109294697563\n",
      "14000/49000 loss: 0.30251277510672897\n",
      "16000/49000 loss: 0.34257893620223046\n",
      "18000/49000 loss: 0.3187562413337888\n",
      "20000/49000 loss: 0.37752371987372924\n",
      "22000/49000 loss: 0.3410244524974675\n",
      "24000/49000 loss: 0.3349535705316551\n",
      "26000/49000 loss: 0.3060794104867831\n",
      "28000/49000 loss: 0.28402378188951993\n",
      "30000/49000 loss: 0.3967614363494059\n",
      "32000/49000 loss: 0.27442212327767795\n",
      "34000/49000 loss: 0.3047200271479829\n",
      "36000/49000 loss: 0.331147823403851\n",
      "38000/49000 loss: 0.2445670143218904\n",
      "40000/49000 loss: 0.32565620020325897\n",
      "42000/49000 loss: 0.30652603587899774\n",
      "44000/49000 loss: 0.25352287239025156\n",
      "46000/49000 loss: 0.32429515038450596\n",
      "48000/49000 loss: 0.29553978558789995\n",
      "epoch 20: valid acc = 0.88, new learning rate = 0.0001792429612042709\n",
      "2000/49000 loss: 0.3412738981733223\n",
      "4000/49000 loss: 0.2538777964828575\n",
      "6000/49000 loss: 0.35375674958562797\n",
      "8000/49000 loss: 0.3290583148581187\n",
      "10000/49000 loss: 0.2463923359837722\n",
      "12000/49000 loss: 0.2822008836745994\n",
      "14000/49000 loss: 0.29816158270559334\n",
      "16000/49000 loss: 0.24036333933625956\n",
      "18000/49000 loss: 0.29643584352041896\n",
      "20000/49000 loss: 0.2973088283571225\n",
      "22000/49000 loss: 0.25299345848849353\n",
      "24000/49000 loss: 0.25446265605488627\n",
      "26000/49000 loss: 0.19544990861303851\n",
      "28000/49000 loss: 0.2791584548294777\n",
      "30000/49000 loss: 0.25299548719903064\n",
      "32000/49000 loss: 0.3190095970077717\n",
      "34000/49000 loss: 0.3311636305145234\n",
      "36000/49000 loss: 0.36005916941745797\n",
      "38000/49000 loss: 0.270629989258957\n",
      "40000/49000 loss: 0.23585132922702617\n",
      "42000/49000 loss: 0.3022236795033688\n",
      "44000/49000 loss: 0.2726747384231807\n",
      "46000/49000 loss: 0.3138916162316988\n",
      "48000/49000 loss: 0.30130690319088305\n",
      "epoch 21: valid acc = 0.881, new learning rate = 0.00017028081314405735\n",
      "2000/49000 loss: 0.22552962976458774\n",
      "4000/49000 loss: 0.30839785611032994\n",
      "6000/49000 loss: 0.34974465433494417\n",
      "8000/49000 loss: 0.3357278584211073\n",
      "10000/49000 loss: 0.3194011641930951\n",
      "12000/49000 loss: 0.2786986513005376\n",
      "14000/49000 loss: 0.2922345651703536\n",
      "16000/49000 loss: 0.21520440095361346\n",
      "18000/49000 loss: 0.3742368263315338\n",
      "20000/49000 loss: 0.29949113411140904\n",
      "22000/49000 loss: 0.3762243114032107\n",
      "24000/49000 loss: 0.26238508189264564\n",
      "26000/49000 loss: 0.38763406988655263\n",
      "28000/49000 loss: 0.25577029774029236\n",
      "30000/49000 loss: 0.2655205701273443\n",
      "32000/49000 loss: 0.28495635977360806\n",
      "34000/49000 loss: 0.36804622610930593\n",
      "36000/49000 loss: 0.22790170313932445\n",
      "38000/49000 loss: 0.33403334722514005\n",
      "40000/49000 loss: 0.32602789553048345\n",
      "42000/49000 loss: 0.28366695374576584\n",
      "44000/49000 loss: 0.2984309786268001\n",
      "46000/49000 loss: 0.2523797303335785\n",
      "48000/49000 loss: 0.26024884319267544\n",
      "epoch 22: valid acc = 0.88, new learning rate = 0.00016176677248685447\n",
      "2000/49000 loss: 0.37570566284378987\n",
      "4000/49000 loss: 0.39868634765092353\n",
      "6000/49000 loss: 0.2832452923450315\n",
      "8000/49000 loss: 0.23493724670222194\n",
      "10000/49000 loss: 0.34526363892068784\n",
      "12000/49000 loss: 0.2825836319746903\n",
      "14000/49000 loss: 0.2457169708152258\n",
      "16000/49000 loss: 0.27162720908261917\n",
      "18000/49000 loss: 0.3224327439523557\n",
      "20000/49000 loss: 0.38428830019269306\n",
      "22000/49000 loss: 0.22016995821475485\n",
      "24000/49000 loss: 0.35902648337827603\n",
      "26000/49000 loss: 0.23549410609072116\n",
      "28000/49000 loss: 0.23144315860740153\n",
      "30000/49000 loss: 0.2606174711339014\n",
      "32000/49000 loss: 0.2767286907150112\n",
      "34000/49000 loss: 0.24465822371579427\n",
      "36000/49000 loss: 0.3819467573771241\n",
      "38000/49000 loss: 0.261953603124677\n",
      "40000/49000 loss: 0.38083434654854365\n",
      "42000/49000 loss: 0.2939004482354635\n",
      "44000/49000 loss: 0.3179927021190227\n",
      "46000/49000 loss: 0.25476653354247414\n",
      "48000/49000 loss: 0.28848504279749093\n",
      "epoch 23: valid acc = 0.891, new learning rate = 0.00015367843386251173\n",
      "2000/49000 loss: 0.3080978439611978\n",
      "4000/49000 loss: 0.29819913336835235\n",
      "6000/49000 loss: 0.2941079724208239\n",
      "8000/49000 loss: 0.26847617437184934\n",
      "10000/49000 loss: 0.22403548558680964\n",
      "12000/49000 loss: 0.29488107717036127\n",
      "14000/49000 loss: 0.26257262747071886\n",
      "16000/49000 loss: 0.287495170367918\n",
      "18000/49000 loss: 0.3495306949808471\n",
      "20000/49000 loss: 0.2590184682101571\n",
      "22000/49000 loss: 0.31445803857188165\n",
      "24000/49000 loss: 0.24630657623010743\n",
      "26000/49000 loss: 0.28387283823006054\n",
      "28000/49000 loss: 0.30492738371708683\n",
      "30000/49000 loss: 0.2536537726927843\n",
      "32000/49000 loss: 0.2453335683363711\n",
      "34000/49000 loss: 0.25571078360990124\n",
      "36000/49000 loss: 0.3949913898960379\n",
      "38000/49000 loss: 0.3309335174972727\n",
      "40000/49000 loss: 0.27032498230009433\n",
      "42000/49000 loss: 0.31460883544254536\n",
      "44000/49000 loss: 0.3615727783288803\n",
      "46000/49000 loss: 0.296395134208405\n",
      "48000/49000 loss: 0.2692773300901912\n",
      "epoch 24: valid acc = 0.884, new learning rate = 0.00014599451216938612\n",
      "2000/49000 loss: 0.2451130764517624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/49000 loss: 0.3256785923464768\n",
      "6000/49000 loss: 0.26228565056842096\n",
      "8000/49000 loss: 0.350368810884575\n",
      "10000/49000 loss: 0.2764129919366098\n",
      "12000/49000 loss: 0.23580465526102076\n",
      "14000/49000 loss: 0.24474622820431632\n",
      "16000/49000 loss: 0.310976617779388\n",
      "18000/49000 loss: 0.30997897196662033\n",
      "20000/49000 loss: 0.28823274661872655\n",
      "22000/49000 loss: 0.2856844628462929\n",
      "24000/49000 loss: 0.2542675220988928\n",
      "26000/49000 loss: 0.239908516105778\n",
      "28000/49000 loss: 0.2959095626468413\n",
      "30000/49000 loss: 0.308137078691669\n",
      "32000/49000 loss: 0.32814973056842234\n",
      "34000/49000 loss: 0.41351416343537306\n",
      "36000/49000 loss: 0.21726077407888597\n",
      "38000/49000 loss: 0.18697198544147664\n",
      "40000/49000 loss: 0.2411896369974757\n",
      "42000/49000 loss: 0.34515516796314616\n",
      "44000/49000 loss: 0.19825800959581366\n",
      "46000/49000 loss: 0.2970541709893047\n",
      "48000/49000 loss: 0.2414400450095174\n",
      "epoch 25: valid acc = 0.883, new learning rate = 0.00013869478656091682\n",
      "2000/49000 loss: 0.28081438305819306\n",
      "4000/49000 loss: 0.28407919316357605\n",
      "6000/49000 loss: 0.2080165893970247\n",
      "8000/49000 loss: 0.2918966820941604\n",
      "10000/49000 loss: 0.30766623330170056\n",
      "12000/49000 loss: 0.3686640729989961\n",
      "14000/49000 loss: 0.393190947690922\n",
      "16000/49000 loss: 0.17567783857787306\n",
      "18000/49000 loss: 0.22742443899997933\n",
      "20000/49000 loss: 0.25327104349598106\n",
      "22000/49000 loss: 0.34040289932345213\n",
      "24000/49000 loss: 0.25918982420756026\n",
      "26000/49000 loss: 0.2948658942558653\n",
      "28000/49000 loss: 0.25401105167919075\n",
      "30000/49000 loss: 0.22122369976720932\n",
      "32000/49000 loss: 0.3902529531652219\n",
      "34000/49000 loss: 0.2993505786669938\n",
      "36000/49000 loss: 0.2869468110764656\n",
      "38000/49000 loss: 0.2967405579613817\n",
      "40000/49000 loss: 0.30703854524703467\n",
      "42000/49000 loss: 0.2359351218236799\n",
      "44000/49000 loss: 0.39354937571111015\n",
      "46000/49000 loss: 0.3365955512769766\n",
      "48000/49000 loss: 0.2327249061479608\n",
      "epoch 26: valid acc = 0.885, new learning rate = 0.00013176004723287096\n",
      "2000/49000 loss: 0.34726437638332613\n",
      "4000/49000 loss: 0.23151381074819782\n",
      "6000/49000 loss: 0.24045711552100482\n",
      "8000/49000 loss: 0.36205164517568517\n",
      "10000/49000 loss: 0.23183350055876775\n",
      "12000/49000 loss: 0.27532516461972845\n",
      "14000/49000 loss: 0.3322540940606324\n",
      "16000/49000 loss: 0.2951185449142909\n",
      "18000/49000 loss: 0.34655297066644786\n",
      "20000/49000 loss: 0.37324182569517217\n",
      "22000/49000 loss: 0.2789599653102246\n",
      "24000/49000 loss: 0.25130388968323275\n",
      "26000/49000 loss: 0.25978202700023734\n",
      "28000/49000 loss: 0.2517438439842504\n",
      "30000/49000 loss: 0.2745704296506631\n",
      "32000/49000 loss: 0.3811633152104771\n",
      "34000/49000 loss: 0.29201884597118505\n",
      "36000/49000 loss: 0.33966390266674906\n",
      "38000/49000 loss: 0.21550300180587284\n",
      "40000/49000 loss: 0.25854806749479464\n",
      "42000/49000 loss: 0.2582345014758985\n",
      "44000/49000 loss: 0.3058766349995852\n",
      "46000/49000 loss: 0.3307069349276167\n",
      "48000/49000 loss: 0.35505356215299055\n",
      "epoch 27: valid acc = 0.883, new learning rate = 0.0001251720448712274\n",
      "2000/49000 loss: 0.3659623166480416\n",
      "4000/49000 loss: 0.3644390572179688\n",
      "6000/49000 loss: 0.2965767312806807\n",
      "8000/49000 loss: 0.42403891468054006\n",
      "10000/49000 loss: 0.2890119800936523\n",
      "12000/49000 loss: 0.25178403810090727\n",
      "14000/49000 loss: 0.24290615185793793\n",
      "16000/49000 loss: 0.38592004397186797\n",
      "18000/49000 loss: 0.277464560657559\n",
      "20000/49000 loss: 0.4419481240109852\n",
      "22000/49000 loss: 0.3052542609136274\n",
      "24000/49000 loss: 0.2976864968962672\n",
      "26000/49000 loss: 0.2594047710661077\n",
      "28000/49000 loss: 0.34399351059586036\n",
      "30000/49000 loss: 0.2336575430432634\n",
      "32000/49000 loss: 0.2587557746229271\n",
      "34000/49000 loss: 0.41146440390695954\n",
      "36000/49000 loss: 0.2879747814259096\n",
      "38000/49000 loss: 0.26923366115522157\n",
      "40000/49000 loss: 0.30687069171400244\n",
      "42000/49000 loss: 0.27003754460493506\n",
      "44000/49000 loss: 0.37350292989710693\n",
      "46000/49000 loss: 0.33363959570599\n",
      "48000/49000 loss: 0.2570938480355229\n",
      "epoch 28: valid acc = 0.883, new learning rate = 0.00011891344262766602\n",
      "2000/49000 loss: 0.23508119480106332\n",
      "4000/49000 loss: 0.2553722132991232\n",
      "6000/49000 loss: 0.3104801027146095\n",
      "8000/49000 loss: 0.28491829918302286\n",
      "10000/49000 loss: 0.2527398016303831\n",
      "12000/49000 loss: 0.2664807130704941\n",
      "14000/49000 loss: 0.1932199570046721\n",
      "16000/49000 loss: 0.24492024232833956\n",
      "18000/49000 loss: 0.24121338309145643\n",
      "20000/49000 loss: 0.2606485517545269\n",
      "22000/49000 loss: 0.26501307022450143\n",
      "24000/49000 loss: 0.25741549819416915\n",
      "26000/49000 loss: 0.2938189407342498\n",
      "28000/49000 loss: 0.2716270679021239\n",
      "30000/49000 loss: 0.3217511094084477\n",
      "32000/49000 loss: 0.27280012395178327\n",
      "34000/49000 loss: 0.28315725721463725\n",
      "36000/49000 loss: 0.2259180801271576\n",
      "38000/49000 loss: 0.20721283901763715\n",
      "40000/49000 loss: 0.2532265232995131\n",
      "42000/49000 loss: 0.36654011282962873\n",
      "44000/49000 loss: 0.331857385335143\n",
      "46000/49000 loss: 0.21387424655464324\n",
      "48000/49000 loss: 0.25916178637128073\n",
      "epoch 29: valid acc = 0.886, new learning rate = 0.00011296777049628272\n",
      "2000/49000 loss: 0.30041582268511047\n",
      "4000/49000 loss: 0.28001975846883276\n",
      "6000/49000 loss: 0.2609344702206958\n",
      "8000/49000 loss: 0.3048039237508941\n",
      "10000/49000 loss: 0.25120505882842153\n",
      "12000/49000 loss: 0.30361738359960283\n",
      "14000/49000 loss: 0.3776641960879658\n",
      "16000/49000 loss: 0.3115032456896588\n",
      "18000/49000 loss: 0.4483801847544061\n",
      "20000/49000 loss: 0.2572499059731635\n",
      "22000/49000 loss: 0.24292037482679602\n",
      "24000/49000 loss: 0.2560092462486052\n",
      "26000/49000 loss: 0.23029969049661794\n",
      "28000/49000 loss: 0.2779790347123244\n",
      "30000/49000 loss: 0.28232691840130203\n",
      "32000/49000 loss: 0.2743877653475963\n",
      "34000/49000 loss: 0.33052269142861157\n",
      "36000/49000 loss: 0.33088464447892785\n",
      "38000/49000 loss: 0.25453534745033757\n",
      "40000/49000 loss: 0.24271652724565232\n",
      "42000/49000 loss: 0.2948830164707544\n",
      "44000/49000 loss: 0.33397288983039136\n",
      "46000/49000 loss: 0.2566423938734364\n",
      "48000/49000 loss: 0.23975017123521483\n",
      "epoch 30: valid acc = 0.882, new learning rate = 0.00010731938197146858\n",
      "2000/49000 loss: 0.30783599661554073\n",
      "4000/49000 loss: 0.2928591849408929\n",
      "6000/49000 loss: 0.2927509332720418\n",
      "8000/49000 loss: 0.36631149063376034\n",
      "10000/49000 loss: 0.2885040095216123\n",
      "12000/49000 loss: 0.2766121154466551\n",
      "14000/49000 loss: 0.24465500847615884\n",
      "16000/49000 loss: 0.26920292066595897\n",
      "18000/49000 loss: 0.2549327176230271\n",
      "20000/49000 loss: 0.32444496638330156\n",
      "22000/49000 loss: 0.2863791275078062\n",
      "24000/49000 loss: 0.3044069952114111\n",
      "26000/49000 loss: 0.2449918369754329\n",
      "28000/49000 loss: 0.21805666698175757\n",
      "30000/49000 loss: 0.3741216317819555\n",
      "32000/49000 loss: 0.20341331500322649\n",
      "34000/49000 loss: 0.25819348870740483\n",
      "36000/49000 loss: 0.3114781450646385\n",
      "38000/49000 loss: 0.2635635329829067\n",
      "40000/49000 loss: 0.3552170779275619\n",
      "42000/49000 loss: 0.23528336944460992\n",
      "44000/49000 loss: 0.31700610530966183\n",
      "46000/49000 loss: 0.4197533060105448\n",
      "48000/49000 loss: 0.3016385588788682\n",
      "epoch 31: valid acc = 0.886, new learning rate = 0.00010195341287289515\n",
      "2000/49000 loss: 0.22900769043743632\n",
      "4000/49000 loss: 0.2885841981101124\n",
      "6000/49000 loss: 0.38002830316186276\n",
      "8000/49000 loss: 0.29599107601491115\n",
      "10000/49000 loss: 0.29510869371783405\n",
      "12000/49000 loss: 0.26482226542065856\n",
      "14000/49000 loss: 0.29555051390760434\n",
      "16000/49000 loss: 0.28470522769014395\n",
      "18000/49000 loss: 0.20583301704909823\n",
      "20000/49000 loss: 0.27966344981126245\n",
      "22000/49000 loss: 0.34558137614366274\n",
      "24000/49000 loss: 0.35263759057774124\n",
      "26000/49000 loss: 0.39055827009625993\n",
      "28000/49000 loss: 0.24405594940187988\n",
      "30000/49000 loss: 0.4062871252715008\n",
      "32000/49000 loss: 0.3667570644716698\n",
      "34000/49000 loss: 0.23413291653919152\n",
      "36000/49000 loss: 0.3450489464722831\n",
      "38000/49000 loss: 0.21610268922422543\n",
      "40000/49000 loss: 0.22065537199172028\n",
      "42000/49000 loss: 0.20132839578222084\n",
      "44000/49000 loss: 0.365904163520809\n",
      "46000/49000 loss: 0.2258705302576016\n",
      "48000/49000 loss: 0.49538419716590976\n",
      "epoch 32: valid acc = 0.882, new learning rate = 9.685574222925039e-05\n",
      "2000/49000 loss: 0.22004644580075236\n",
      "4000/49000 loss: 0.24283317358985335\n",
      "6000/49000 loss: 0.29348139744932716\n",
      "8000/49000 loss: 0.2775969119920797\n",
      "10000/49000 loss: 0.37001972221113677\n",
      "12000/49000 loss: 0.24823390912478543\n",
      "14000/49000 loss: 0.2652261191125543\n",
      "16000/49000 loss: 0.33611090591111176\n",
      "18000/49000 loss: 0.27969839303864036\n",
      "20000/49000 loss: 0.25241369435739536\n",
      "22000/49000 loss: 0.4164482514300444\n",
      "24000/49000 loss: 0.22482490378275538\n",
      "26000/49000 loss: 0.26253659944333996\n",
      "28000/49000 loss: 0.29064148273527224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000/49000 loss: 0.3455177879407079\n",
      "32000/49000 loss: 0.29077573601726286\n",
      "34000/49000 loss: 0.22356175822385416\n",
      "36000/49000 loss: 0.30478929926754394\n",
      "38000/49000 loss: 0.25415232017668243\n",
      "40000/49000 loss: 0.2704730229703295\n",
      "42000/49000 loss: 0.27612325815288546\n",
      "44000/49000 loss: 0.2751659122049738\n",
      "46000/49000 loss: 0.28891834237196423\n",
      "48000/49000 loss: 0.2635066945953221\n",
      "epoch 33: valid acc = 0.88, new learning rate = 9.201295511778786e-05\n",
      "2000/49000 loss: 0.23540415190525796\n",
      "4000/49000 loss: 0.2625494506909628\n",
      "6000/49000 loss: 0.20701634790198364\n",
      "8000/49000 loss: 0.30355512709597154\n",
      "10000/49000 loss: 0.3194247475714127\n",
      "12000/49000 loss: 0.341671968942626\n",
      "14000/49000 loss: 0.2858272844180287\n",
      "16000/49000 loss: 0.21850999526977763\n",
      "18000/49000 loss: 0.3267712235655283\n",
      "20000/49000 loss: 0.35883357903077534\n",
      "22000/49000 loss: 0.3127853423860323\n",
      "24000/49000 loss: 0.29167335565047225\n",
      "26000/49000 loss: 0.2711905450421764\n",
      "28000/49000 loss: 0.24084446075023008\n",
      "30000/49000 loss: 0.21645147683624308\n",
      "32000/49000 loss: 0.33610587895375654\n",
      "34000/49000 loss: 0.2832702696994716\n",
      "36000/49000 loss: 0.26062568852439455\n",
      "38000/49000 loss: 0.264848179633568\n",
      "40000/49000 loss: 0.2832884960408702\n",
      "42000/49000 loss: 0.2648775541954691\n",
      "44000/49000 loss: 0.3059589273980337\n",
      "46000/49000 loss: 0.2715932371791872\n",
      "48000/49000 loss: 0.30477104256911586\n",
      "epoch 34: valid acc = 0.89, new learning rate = 8.741230736189846e-05\n",
      "2000/49000 loss: 0.39998716435827103\n",
      "4000/49000 loss: 0.26012814967544323\n",
      "6000/49000 loss: 0.3433944783266674\n",
      "8000/49000 loss: 0.3218453924063628\n",
      "10000/49000 loss: 0.23670870327539584\n",
      "12000/49000 loss: 0.27819673404057305\n",
      "14000/49000 loss: 0.27666344083015576\n",
      "16000/49000 loss: 0.37077236121204554\n",
      "18000/49000 loss: 0.4333261707241694\n",
      "20000/49000 loss: 0.2799136375342111\n",
      "22000/49000 loss: 0.24451535607095157\n",
      "24000/49000 loss: 0.19889841708384956\n",
      "26000/49000 loss: 0.39260260183018514\n",
      "28000/49000 loss: 0.37693658940275326\n",
      "30000/49000 loss: 0.2865356337994495\n",
      "32000/49000 loss: 0.23473458643518733\n",
      "34000/49000 loss: 0.2503017070597404\n",
      "36000/49000 loss: 0.27612447555316505\n",
      "38000/49000 loss: 0.34075623527542814\n",
      "40000/49000 loss: 0.22122543276465592\n",
      "42000/49000 loss: 0.22636522208407606\n",
      "44000/49000 loss: 0.30527193548440335\n",
      "46000/49000 loss: 0.26481793814232485\n",
      "48000/49000 loss: 0.27696576498456993\n",
      "epoch 35: valid acc = 0.887, new learning rate = 8.304169199380353e-05\n",
      "2000/49000 loss: 0.2410359181659822\n",
      "4000/49000 loss: 0.20858989627386948\n",
      "6000/49000 loss: 0.23814497722211442\n",
      "8000/49000 loss: 0.3176712940088866\n",
      "10000/49000 loss: 0.23618577405271757\n",
      "12000/49000 loss: 0.21431183487608127\n",
      "14000/49000 loss: 0.3303166457573464\n",
      "16000/49000 loss: 0.3171191766600444\n",
      "18000/49000 loss: 0.3111214858822893\n",
      "20000/49000 loss: 0.41762932213890375\n",
      "22000/49000 loss: 0.2726695906869744\n",
      "24000/49000 loss: 0.30995186161973387\n",
      "26000/49000 loss: 0.22985614846913424\n",
      "28000/49000 loss: 0.3086920051774034\n",
      "30000/49000 loss: 0.23143784152796656\n",
      "32000/49000 loss: 0.2274542671223588\n",
      "34000/49000 loss: 0.2896567198132661\n",
      "36000/49000 loss: 0.31521438226324405\n",
      "38000/49000 loss: 0.2694779279865126\n",
      "40000/49000 loss: 0.28909645121966354\n",
      "42000/49000 loss: 0.31961558048688876\n",
      "44000/49000 loss: 0.3198249212608695\n",
      "46000/49000 loss: 0.3054863398646013\n",
      "48000/49000 loss: 0.29244159181690826\n",
      "epoch 36: valid acc = 0.885, new learning rate = 7.888960739411335e-05\n",
      "2000/49000 loss: 0.256992916483129\n",
      "4000/49000 loss: 0.2424912349056534\n",
      "6000/49000 loss: 0.20568607816257137\n",
      "8000/49000 loss: 0.28405204447388055\n",
      "10000/49000 loss: 0.27332731854183373\n",
      "12000/49000 loss: 0.21268621594207038\n",
      "14000/49000 loss: 0.21254888743071046\n",
      "16000/49000 loss: 0.22339494446617322\n",
      "18000/49000 loss: 0.24314471494659165\n",
      "20000/49000 loss: 0.2816474771877615\n",
      "22000/49000 loss: 0.26596980776937085\n",
      "24000/49000 loss: 0.25715387203295453\n",
      "26000/49000 loss: 0.19263801776072298\n",
      "28000/49000 loss: 0.20173651004666124\n",
      "30000/49000 loss: 0.25540165234917755\n",
      "32000/49000 loss: 0.26690539607889\n",
      "34000/49000 loss: 0.20637150290091177\n",
      "36000/49000 loss: 0.3054367538574122\n",
      "38000/49000 loss: 0.38077205735357916\n",
      "40000/49000 loss: 0.19424136377898812\n",
      "42000/49000 loss: 0.2807144879304285\n",
      "44000/49000 loss: 0.27949562206390344\n",
      "46000/49000 loss: 0.26086395428212006\n",
      "48000/49000 loss: 0.30938550467667825\n",
      "epoch 37: valid acc = 0.89, new learning rate = 7.494512702440768e-05\n",
      "2000/49000 loss: 0.2548631906910321\n",
      "4000/49000 loss: 0.3084729056238219\n",
      "6000/49000 loss: 0.2978595440756155\n",
      "8000/49000 loss: 0.28707268485765636\n",
      "10000/49000 loss: 0.27388073960483866\n",
      "12000/49000 loss: 0.2856324994363615\n",
      "14000/49000 loss: 0.2844390747134303\n",
      "16000/49000 loss: 0.24424753157772003\n",
      "18000/49000 loss: 0.1747204243766276\n",
      "20000/49000 loss: 0.2625718616968576\n",
      "22000/49000 loss: 0.38951317131638613\n",
      "24000/49000 loss: 0.38654606212371767\n",
      "26000/49000 loss: 0.3042235103243812\n",
      "28000/49000 loss: 0.25203029206248645\n",
      "30000/49000 loss: 0.34005900503324726\n",
      "32000/49000 loss: 0.23928463659229615\n",
      "34000/49000 loss: 0.2655942503128283\n",
      "36000/49000 loss: 0.27272153772266144\n",
      "38000/49000 loss: 0.24189469963037102\n",
      "40000/49000 loss: 0.34839323421952284\n",
      "42000/49000 loss: 0.31505597705023497\n",
      "44000/49000 loss: 0.293070983421309\n",
      "46000/49000 loss: 0.280389396988587\n",
      "48000/49000 loss: 0.2978280087211409\n",
      "epoch 38: valid acc = 0.887, new learning rate = 7.119787067318729e-05\n",
      "2000/49000 loss: 0.1847837898651071\n",
      "4000/49000 loss: 0.2272755887721786\n",
      "6000/49000 loss: 0.35645286211738875\n",
      "8000/49000 loss: 0.2625923147682232\n",
      "10000/49000 loss: 0.22294203038825045\n",
      "12000/49000 loss: 0.2819828974827767\n",
      "14000/49000 loss: 0.3603744927052267\n",
      "16000/49000 loss: 0.31426701179450567\n",
      "18000/49000 loss: 0.2816101693000638\n",
      "20000/49000 loss: 0.3377735203063157\n",
      "22000/49000 loss: 0.3415155076435237\n",
      "24000/49000 loss: 0.3327332705442835\n",
      "26000/49000 loss: 0.32709082149199914\n",
      "28000/49000 loss: 0.3709539794137555\n",
      "30000/49000 loss: 0.28206995184208966\n",
      "32000/49000 loss: 0.20436888732367958\n",
      "34000/49000 loss: 0.3488631491286353\n",
      "36000/49000 loss: 0.24398467000938331\n",
      "38000/49000 loss: 0.33923410302401363\n",
      "40000/49000 loss: 0.2720297564404403\n",
      "42000/49000 loss: 0.3775488639023573\n",
      "44000/49000 loss: 0.2827043185712223\n",
      "46000/49000 loss: 0.2449723758611461\n",
      "48000/49000 loss: 0.28831559600457546\n",
      "epoch 39: valid acc = 0.888, new learning rate = 6.763797713952792e-05\n",
      "2000/49000 loss: 0.2913858742040501\n",
      "4000/49000 loss: 0.25697471895065316\n",
      "6000/49000 loss: 0.35851335926381034\n",
      "8000/49000 loss: 0.29680379648070243\n",
      "10000/49000 loss: 0.21280622354579537\n",
      "12000/49000 loss: 0.2682910707385666\n",
      "14000/49000 loss: 0.3790185275468096\n",
      "16000/49000 loss: 0.1787279731622711\n",
      "18000/49000 loss: 0.2507452234988956\n",
      "20000/49000 loss: 0.3183725687490359\n",
      "22000/49000 loss: 0.38220547925890463\n",
      "24000/49000 loss: 0.25545326624080616\n",
      "26000/49000 loss: 0.24538465862817835\n",
      "28000/49000 loss: 0.28445980871232956\n",
      "30000/49000 loss: 0.24966203638576842\n",
      "32000/49000 loss: 0.2663564457476111\n",
      "34000/49000 loss: 0.3190708483815304\n",
      "36000/49000 loss: 0.24948536368814656\n",
      "38000/49000 loss: 0.26978655912600397\n",
      "40000/49000 loss: 0.28752721958115257\n",
      "42000/49000 loss: 0.2363141236914714\n",
      "44000/49000 loss: 0.23044945544838955\n",
      "46000/49000 loss: 0.272263600824532\n",
      "48000/49000 loss: 0.22520061654529\n",
      "epoch 40: valid acc = 0.885, new learning rate = 6.425607828255152e-05\n",
      "2000/49000 loss: 0.3046469410936691\n",
      "4000/49000 loss: 0.2596631433529226\n",
      "6000/49000 loss: 0.29057905879891605\n",
      "8000/49000 loss: 0.24100832788118884\n",
      "10000/49000 loss: 0.25868593706381193\n",
      "12000/49000 loss: 0.2601951136370539\n",
      "14000/49000 loss: 0.21581304902759327\n",
      "16000/49000 loss: 0.39178161389343347\n",
      "18000/49000 loss: 0.27771724233724754\n",
      "20000/49000 loss: 0.2716632050950963\n",
      "22000/49000 loss: 0.34330688473840937\n",
      "24000/49000 loss: 0.3217223950781184\n",
      "26000/49000 loss: 0.29849496376229234\n",
      "28000/49000 loss: 0.25837319591800123\n",
      "30000/49000 loss: 0.1612407071171604\n",
      "32000/49000 loss: 0.27485871075551516\n",
      "34000/49000 loss: 0.262732322346255\n",
      "36000/49000 loss: 0.281328336497302\n",
      "38000/49000 loss: 0.2794307935854785\n",
      "40000/49000 loss: 0.31366010257827426\n",
      "42000/49000 loss: 0.3599003129090079\n",
      "44000/49000 loss: 0.2971459488681176\n",
      "46000/49000 loss: 0.28550745297161856\n",
      "48000/49000 loss: 0.26185063861309354\n",
      "epoch 41: valid acc = 0.891, new learning rate = 6.104327436842394e-05\n",
      "2000/49000 loss: 0.2824517964747652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/49000 loss: 0.28320852619656134\n",
      "6000/49000 loss: 0.27914392586210984\n",
      "8000/49000 loss: 0.25996698534804585\n",
      "10000/49000 loss: 0.3036312102844479\n",
      "12000/49000 loss: 0.1929633943263649\n",
      "14000/49000 loss: 0.2714051943586358\n",
      "16000/49000 loss: 0.22877482542352542\n",
      "18000/49000 loss: 0.2668846982177697\n",
      "20000/49000 loss: 0.27292544828603366\n",
      "22000/49000 loss: 0.2185887828161954\n",
      "24000/49000 loss: 0.2866787390394401\n",
      "26000/49000 loss: 0.3009506907827674\n",
      "28000/49000 loss: 0.2903512896581864\n",
      "30000/49000 loss: 0.23993691435878298\n",
      "32000/49000 loss: 0.30684137578476833\n",
      "34000/49000 loss: 0.30336985544744677\n",
      "36000/49000 loss: 0.3772934261438221\n",
      "38000/49000 loss: 0.30691453845191385\n",
      "40000/49000 loss: 0.3243432108402954\n",
      "42000/49000 loss: 0.2859537493142905\n",
      "44000/49000 loss: 0.2723849607898711\n",
      "46000/49000 loss: 0.3220632272307433\n",
      "48000/49000 loss: 0.26583706249917055\n",
      "epoch 42: valid acc = 0.889, new learning rate = 5.799111065000274e-05\n",
      "2000/49000 loss: 0.24222324648707336\n",
      "4000/49000 loss: 0.2567189961881297\n",
      "6000/49000 loss: 0.3083828119256708\n",
      "8000/49000 loss: 0.18049875119352882\n",
      "10000/49000 loss: 0.2990702924347529\n",
      "12000/49000 loss: 0.33794903503475815\n",
      "14000/49000 loss: 0.2528846794672991\n",
      "16000/49000 loss: 0.26652020366797435\n",
      "18000/49000 loss: 0.24457269211321173\n",
      "20000/49000 loss: 0.3136680316158602\n",
      "22000/49000 loss: 0.3728758589285593\n",
      "24000/49000 loss: 0.30425146601170605\n",
      "26000/49000 loss: 0.30636964189439264\n",
      "28000/49000 loss: 0.2228781510577841\n",
      "30000/49000 loss: 0.2771372943735141\n",
      "32000/49000 loss: 0.25683563222989125\n",
      "34000/49000 loss: 0.390876611611503\n",
      "36000/49000 loss: 0.29551734569075017\n",
      "38000/49000 loss: 0.28565310631350355\n",
      "40000/49000 loss: 0.22519835261304208\n",
      "42000/49000 loss: 0.26809433633398255\n",
      "44000/49000 loss: 0.3351738104787568\n",
      "46000/49000 loss: 0.3864813754776237\n",
      "48000/49000 loss: 0.23019042965057274\n",
      "epoch 43: valid acc = 0.886, new learning rate = 5.5091555117502596e-05\n",
      "2000/49000 loss: 0.2696190612057117\n",
      "4000/49000 loss: 0.29136628883063664\n",
      "6000/49000 loss: 0.27112508028798343\n",
      "8000/49000 loss: 0.17250146172992845\n",
      "10000/49000 loss: 0.2640398377861404\n",
      "12000/49000 loss: 0.25939699364986113\n",
      "14000/49000 loss: 0.2761032351258024\n",
      "16000/49000 loss: 0.24101987157456445\n",
      "18000/49000 loss: 0.23452940869902236\n",
      "20000/49000 loss: 0.29817905472467426\n",
      "22000/49000 loss: 0.20709936302732687\n",
      "24000/49000 loss: 0.33566284419133385\n",
      "26000/49000 loss: 0.2611464941924967\n",
      "28000/49000 loss: 0.26437819191205864\n",
      "30000/49000 loss: 0.19192093438185193\n",
      "32000/49000 loss: 0.28664638818974636\n",
      "34000/49000 loss: 0.20881016782540668\n",
      "36000/49000 loss: 0.27848420926539913\n",
      "38000/49000 loss: 0.3725777222996207\n",
      "40000/49000 loss: 0.19320492796880248\n",
      "42000/49000 loss: 0.3222600392155986\n",
      "44000/49000 loss: 0.2625513814287382\n",
      "46000/49000 loss: 0.2825677786062398\n",
      "48000/49000 loss: 0.2622920326127212\n",
      "epoch 44: valid acc = 0.887, new learning rate = 5.2336977361627463e-05\n",
      "2000/49000 loss: 0.3779055132715939\n",
      "4000/49000 loss: 0.31205947215668994\n",
      "6000/49000 loss: 0.25257840154194067\n",
      "8000/49000 loss: 0.38876731037350765\n",
      "10000/49000 loss: 0.322266921996098\n",
      "12000/49000 loss: 0.24322363001754196\n",
      "14000/49000 loss: 0.267318932088018\n",
      "16000/49000 loss: 0.2556422189522343\n",
      "18000/49000 loss: 0.3042327440171633\n",
      "20000/49000 loss: 0.2987807986328005\n",
      "22000/49000 loss: 0.3253110601651136\n",
      "24000/49000 loss: 0.2671585061612645\n",
      "26000/49000 loss: 0.3123181181464864\n",
      "28000/49000 loss: 0.3020655275813903\n",
      "30000/49000 loss: 0.2403362366528724\n",
      "32000/49000 loss: 0.22694311842322226\n",
      "34000/49000 loss: 0.3582919289745849\n",
      "36000/49000 loss: 0.34669701530634744\n",
      "38000/49000 loss: 0.23752169231409265\n",
      "40000/49000 loss: 0.2130294382978909\n",
      "42000/49000 loss: 0.25247021792918417\n",
      "44000/49000 loss: 0.24273672956599995\n",
      "46000/49000 loss: 0.28920899679628304\n",
      "48000/49000 loss: 0.27909542684481886\n",
      "epoch 45: valid acc = 0.888, new learning rate = 4.972012849354609e-05\n",
      "2000/49000 loss: 0.25689110076941446\n",
      "4000/49000 loss: 0.23528028030871995\n",
      "6000/49000 loss: 0.32568162934423267\n",
      "8000/49000 loss: 0.22850611056046627\n",
      "10000/49000 loss: 0.32738053271466255\n",
      "12000/49000 loss: 0.23569873188639862\n",
      "14000/49000 loss: 0.18170723803392905\n",
      "16000/49000 loss: 0.25289375292391386\n",
      "18000/49000 loss: 0.2855768810868365\n",
      "20000/49000 loss: 0.3197806637666924\n",
      "22000/49000 loss: 0.26166523882778414\n",
      "24000/49000 loss: 0.23728925944090246\n",
      "26000/49000 loss: 0.2703974318115407\n",
      "28000/49000 loss: 0.28567338987648955\n",
      "30000/49000 loss: 0.2630803310494804\n",
      "32000/49000 loss: 0.22570135149499448\n",
      "34000/49000 loss: 0.37721102679992885\n",
      "36000/49000 loss: 0.26706494522453617\n",
      "38000/49000 loss: 0.23807241609733423\n",
      "40000/49000 loss: 0.2466705402205093\n",
      "42000/49000 loss: 0.28688498629523956\n",
      "44000/49000 loss: 0.2865930419194784\n",
      "46000/49000 loss: 0.26889596262363546\n",
      "48000/49000 loss: 0.3384285456454741\n",
      "epoch 46: valid acc = 0.889, new learning rate = 4.723412206886878e-05\n",
      "2000/49000 loss: 0.31273293662145285\n",
      "4000/49000 loss: 0.338800635383463\n",
      "6000/49000 loss: 0.2644046932196164\n",
      "8000/49000 loss: 0.2249301134129489\n",
      "10000/49000 loss: 0.3268736078134505\n",
      "12000/49000 loss: 0.24457132733585818\n",
      "14000/49000 loss: 0.35318403503916074\n",
      "16000/49000 loss: 0.23384697044747302\n",
      "18000/49000 loss: 0.3153706173614823\n",
      "20000/49000 loss: 0.22860590621782875\n",
      "22000/49000 loss: 0.274112854530271\n",
      "24000/49000 loss: 0.27802762888479127\n",
      "26000/49000 loss: 0.368075734869355\n",
      "28000/49000 loss: 0.23301172945604795\n",
      "30000/49000 loss: 0.3261693468390363\n",
      "32000/49000 loss: 0.25984183022312274\n",
      "34000/49000 loss: 0.28639570070835385\n",
      "36000/49000 loss: 0.18880275336815946\n",
      "38000/49000 loss: 0.2624177894482515\n",
      "40000/49000 loss: 0.2660616863970415\n",
      "42000/49000 loss: 0.2057502115556982\n",
      "44000/49000 loss: 0.3459172431185558\n",
      "46000/49000 loss: 0.29883010562463613\n",
      "48000/49000 loss: 0.2875033328554065\n",
      "epoch 47: valid acc = 0.887, new learning rate = 4.487241596542534e-05\n",
      "2000/49000 loss: 0.3790870944834591\n",
      "4000/49000 loss: 0.30884394000029464\n",
      "6000/49000 loss: 0.2566510850485822\n",
      "8000/49000 loss: 0.21803092243498334\n",
      "10000/49000 loss: 0.2984195066072291\n",
      "12000/49000 loss: 0.3841788771593368\n",
      "14000/49000 loss: 0.27150113544368243\n",
      "16000/49000 loss: 0.2964488283346649\n",
      "18000/49000 loss: 0.2835549937167386\n",
      "20000/49000 loss: 0.3130133551042468\n",
      "22000/49000 loss: 0.21807573215986292\n",
      "24000/49000 loss: 0.253803575043006\n",
      "26000/49000 loss: 0.24517110166632677\n",
      "28000/49000 loss: 0.2738031786939518\n",
      "30000/49000 loss: 0.42249018799358184\n",
      "32000/49000 loss: 0.26051769742462516\n",
      "34000/49000 loss: 0.26599369960482744\n",
      "36000/49000 loss: 0.28568468923035917\n",
      "38000/49000 loss: 0.31053520110699534\n",
      "40000/49000 loss: 0.2675796170296283\n",
      "42000/49000 loss: 0.2640516320370585\n",
      "44000/49000 loss: 0.2518284573581888\n",
      "46000/49000 loss: 0.24962468110821623\n",
      "48000/49000 loss: 0.29461534108568294\n",
      "epoch 48: valid acc = 0.887, new learning rate = 4.262879516715407e-05\n",
      "2000/49000 loss: 0.2521741081833724\n",
      "4000/49000 loss: 0.2909046097543155\n",
      "6000/49000 loss: 0.27691397594513206\n",
      "8000/49000 loss: 0.31088503462282957\n",
      "10000/49000 loss: 0.24962363031802334\n",
      "12000/49000 loss: 0.2692489295973914\n",
      "14000/49000 loss: 0.3519165095328901\n",
      "16000/49000 loss: 0.19911373410452135\n",
      "18000/49000 loss: 0.1863419090312086\n",
      "20000/49000 loss: 0.2744229714810223\n",
      "22000/49000 loss: 0.34172364798182703\n",
      "24000/49000 loss: 0.3165452956732038\n",
      "26000/49000 loss: 0.3329084205945591\n",
      "28000/49000 loss: 0.17768134546305125\n",
      "30000/49000 loss: 0.2202476515759507\n",
      "32000/49000 loss: 0.25183802277203177\n",
      "34000/49000 loss: 0.2526033290386857\n",
      "36000/49000 loss: 0.2432508426971989\n",
      "38000/49000 loss: 0.2101466622825534\n",
      "40000/49000 loss: 0.22749399978420226\n",
      "42000/49000 loss: 0.33079171863662604\n",
      "44000/49000 loss: 0.2835083804402781\n",
      "46000/49000 loss: 0.3789834612494935\n",
      "48000/49000 loss: 0.2969998469285871\n",
      "epoch 49: valid acc = 0.89, new learning rate = 4.049735540879637e-05\n",
      "2000/49000 loss: 0.2725224051478219\n",
      "4000/49000 loss: 0.34484173034719323\n",
      "6000/49000 loss: 0.2924569582984477\n",
      "8000/49000 loss: 0.2306633135450534\n",
      "10000/49000 loss: 0.2699369394317929\n",
      "12000/49000 loss: 0.25238337831948543\n",
      "14000/49000 loss: 0.1834288981027878\n",
      "16000/49000 loss: 0.20445013869547543\n",
      "18000/49000 loss: 0.26934482182229735\n",
      "20000/49000 loss: 0.28662507865076065\n",
      "22000/49000 loss: 0.2000035178099383\n",
      "24000/49000 loss: 0.21597157863851435\n",
      "26000/49000 loss: 0.2996340978532506\n",
      "28000/49000 loss: 0.25455655971737523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000/49000 loss: 0.27151575376458836\n",
      "32000/49000 loss: 0.23627198127020352\n",
      "34000/49000 loss: 0.24374088461051122\n",
      "36000/49000 loss: 0.28820601249229477\n",
      "38000/49000 loss: 0.3128759720094446\n",
      "40000/49000 loss: 0.23883530967262515\n",
      "42000/49000 loss: 0.22395350798075842\n",
      "44000/49000 loss: 0.2729149321220045\n",
      "46000/49000 loss: 0.21431203347434824\n",
      "48000/49000 loss: 0.22685009201871845\n",
      "epoch 50: valid acc = 0.885, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.9045510204081633\n",
      "test acc: 0.885\n",
      "test acc: 0.8724\n",
      "number of batches for training: 245\n",
      "epoch 1: valid acc = 0.745, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.796, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.822, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.842, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.85, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.861, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.86, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.869, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.868, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.87, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.873, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.887, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.884, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.885, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.888, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.886, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.889, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.891, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.881, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.889, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.887, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.887, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.888, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.887, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.887, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.89, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.889, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.89, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.892, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.892, new learning rate = 0.00010731938197146858\n",
      "epoch 31: valid acc = 0.899, new learning rate = 0.00010195341287289515\n",
      "epoch 32: valid acc = 0.889, new learning rate = 9.685574222925039e-05\n",
      "epoch 33: valid acc = 0.888, new learning rate = 9.201295511778786e-05\n",
      "epoch 34: valid acc = 0.888, new learning rate = 8.741230736189846e-05\n",
      "epoch 35: valid acc = 0.889, new learning rate = 8.304169199380353e-05\n",
      "epoch 36: valid acc = 0.888, new learning rate = 7.888960739411335e-05\n",
      "epoch 37: valid acc = 0.888, new learning rate = 7.494512702440768e-05\n",
      "epoch 38: valid acc = 0.885, new learning rate = 7.119787067318729e-05\n",
      "epoch 39: valid acc = 0.891, new learning rate = 6.763797713952792e-05\n",
      "epoch 40: valid acc = 0.893, new learning rate = 6.425607828255152e-05\n",
      "epoch 41: valid acc = 0.89, new learning rate = 6.104327436842394e-05\n",
      "epoch 42: valid acc = 0.889, new learning rate = 5.799111065000274e-05\n",
      "epoch 43: valid acc = 0.888, new learning rate = 5.5091555117502596e-05\n",
      "epoch 44: valid acc = 0.89, new learning rate = 5.2336977361627463e-05\n",
      "epoch 45: valid acc = 0.889, new learning rate = 4.972012849354609e-05\n",
      "epoch 46: valid acc = 0.89, new learning rate = 4.723412206886878e-05\n",
      "epoch 47: valid acc = 0.89, new learning rate = 4.487241596542534e-05\n",
      "epoch 48: valid acc = 0.892, new learning rate = 4.262879516715407e-05\n",
      "epoch 49: valid acc = 0.89, new learning rate = 4.049735540879637e-05\n",
      "epoch 50: valid acc = 0.891, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.9059387755102041\n",
      "test acc: 0.891\n",
      "test acc: 0.8741\n",
      "number of batches for training: 245\n",
      "2000/49000 loss: 3.7935767634350896\n",
      "4000/49000 loss: 3.0369348847791033\n",
      "6000/49000 loss: 2.552379338312145\n",
      "8000/49000 loss: 2.4619481628250064\n",
      "10000/49000 loss: 2.023291723521213\n",
      "12000/49000 loss: 2.0823499616702894\n",
      "14000/49000 loss: 2.037022685771085\n",
      "16000/49000 loss: 1.6028106120501344\n",
      "18000/49000 loss: 1.3184016775589045\n",
      "20000/49000 loss: 1.4015692133644029\n",
      "22000/49000 loss: 1.3385042461549417\n",
      "24000/49000 loss: 1.2623398715780685\n",
      "26000/49000 loss: 1.0936778836177712\n",
      "28000/49000 loss: 1.1253194416317847\n",
      "30000/49000 loss: 1.0125394387754791\n",
      "32000/49000 loss: 0.9505734894007815\n",
      "34000/49000 loss: 0.8745141375401978\n",
      "36000/49000 loss: 0.8828330058965393\n",
      "38000/49000 loss: 0.8706504089298897\n",
      "40000/49000 loss: 0.9695523758277945\n",
      "42000/49000 loss: 0.8015129237933187\n",
      "44000/49000 loss: 0.9062646229364086\n",
      "46000/49000 loss: 0.6767151040490128\n",
      "48000/49000 loss: 0.6098611004482322\n",
      "epoch 1: valid acc = 0.742, new learning rate = 0.000475\n",
      "2000/49000 loss: 0.7522481894802785\n",
      "4000/49000 loss: 0.7362040905431432\n",
      "6000/49000 loss: 0.7192429498860415\n",
      "8000/49000 loss: 0.6665868956995673\n",
      "10000/49000 loss: 0.7306066555120544\n",
      "12000/49000 loss: 0.6219047001981665\n",
      "14000/49000 loss: 0.6346310963990861\n",
      "16000/49000 loss: 0.6886086672473641\n",
      "18000/49000 loss: 0.5698566530254209\n",
      "20000/49000 loss: 0.6189216696184953\n",
      "22000/49000 loss: 0.6842877778679579\n",
      "24000/49000 loss: 0.5181612031453956\n",
      "26000/49000 loss: 0.6031118096086439\n",
      "28000/49000 loss: 0.5653843262777613\n",
      "30000/49000 loss: 0.6250044618867361\n",
      "32000/49000 loss: 0.5917008179671313\n",
      "34000/49000 loss: 0.5686324074416066\n",
      "36000/49000 loss: 0.5395829695116606\n",
      "38000/49000 loss: 0.6393347000879765\n",
      "40000/49000 loss: 0.5186037106658568\n",
      "42000/49000 loss: 0.619937703022065\n",
      "44000/49000 loss: 0.4743686128589554\n",
      "46000/49000 loss: 0.5011144579887326\n",
      "48000/49000 loss: 0.4448503565294096\n",
      "epoch 2: valid acc = 0.801, new learning rate = 0.00045125\n",
      "2000/49000 loss: 0.494836355505322\n",
      "4000/49000 loss: 0.503073943419475\n",
      "6000/49000 loss: 0.4395882773968046\n",
      "8000/49000 loss: 0.4892728525989955\n",
      "10000/49000 loss: 0.4148993336568192\n",
      "12000/49000 loss: 0.459692392395035\n",
      "14000/49000 loss: 0.43213324079868654\n",
      "16000/49000 loss: 0.48552940816283363\n",
      "18000/49000 loss: 0.37942169829753475\n",
      "20000/49000 loss: 0.5608023698893665\n",
      "22000/49000 loss: 0.4651415095187692\n",
      "24000/49000 loss: 0.5284083064419492\n",
      "26000/49000 loss: 0.4929272241699527\n",
      "28000/49000 loss: 0.5003186978193541\n",
      "30000/49000 loss: 0.42055281367430497\n",
      "32000/49000 loss: 0.501634622332744\n",
      "34000/49000 loss: 0.440993704145698\n",
      "36000/49000 loss: 0.5305270375533323\n",
      "38000/49000 loss: 0.48604788987192177\n",
      "40000/49000 loss: 0.455501330771431\n",
      "42000/49000 loss: 0.4194602496779733\n",
      "44000/49000 loss: 0.5281028738324167\n",
      "46000/49000 loss: 0.48495472993836614\n",
      "48000/49000 loss: 0.48785740943586503\n",
      "epoch 3: valid acc = 0.817, new learning rate = 0.0004286875\n",
      "2000/49000 loss: 0.5347448312652183\n",
      "4000/49000 loss: 0.3699053598236304\n",
      "6000/49000 loss: 0.4359250825904046\n",
      "8000/49000 loss: 0.4502019117910944\n",
      "10000/49000 loss: 0.41710523148141193\n",
      "12000/49000 loss: 0.4200178517329321\n",
      "14000/49000 loss: 0.3557464281007772\n",
      "16000/49000 loss: 0.5338822704171416\n",
      "18000/49000 loss: 0.5045290305061461\n",
      "20000/49000 loss: 0.5107129564101772\n",
      "22000/49000 loss: 0.4225356015373594\n",
      "24000/49000 loss: 0.5264586163008647\n",
      "26000/49000 loss: 0.4170198347461227\n",
      "28000/49000 loss: 0.413855309902876\n",
      "30000/49000 loss: 0.5239081605756719\n",
      "32000/49000 loss: 0.44678343435912576\n",
      "34000/49000 loss: 0.4973540526766273\n",
      "36000/49000 loss: 0.4256908242101522\n",
      "38000/49000 loss: 0.508960242256173\n",
      "40000/49000 loss: 0.4053357160175099\n",
      "42000/49000 loss: 0.39553635294380857\n",
      "44000/49000 loss: 0.6061532018293885\n",
      "46000/49000 loss: 0.4016264179103653\n",
      "48000/49000 loss: 0.37815167539658295\n",
      "epoch 4: valid acc = 0.843, new learning rate = 0.00040725312499999993\n",
      "2000/49000 loss: 0.3789873629426045\n",
      "4000/49000 loss: 0.4699805313124925\n",
      "6000/49000 loss: 0.3574485778922109\n",
      "8000/49000 loss: 0.4727538977089172\n",
      "10000/49000 loss: 0.39696702243204574\n",
      "12000/49000 loss: 0.4258442437411452\n",
      "14000/49000 loss: 0.36753247449420867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000/49000 loss: 0.32550953477741407\n",
      "18000/49000 loss: 0.4606162931880808\n",
      "20000/49000 loss: 0.4356484342022186\n",
      "22000/49000 loss: 0.3751982010557698\n",
      "24000/49000 loss: 0.454223485299436\n",
      "26000/49000 loss: 0.40782248348457706\n",
      "28000/49000 loss: 0.49676394453674017\n",
      "30000/49000 loss: 0.4818988836398469\n",
      "32000/49000 loss: 0.392885012550229\n",
      "34000/49000 loss: 0.4648483920964832\n",
      "36000/49000 loss: 0.35301086611467475\n",
      "38000/49000 loss: 0.40271476752434016\n",
      "40000/49000 loss: 0.5253691069854555\n",
      "42000/49000 loss: 0.45699066321502413\n",
      "44000/49000 loss: 0.32576492966880655\n",
      "46000/49000 loss: 0.38892983334320813\n",
      "48000/49000 loss: 0.3855075287393048\n",
      "epoch 5: valid acc = 0.849, new learning rate = 0.0003868904687499999\n",
      "2000/49000 loss: 0.437649506938869\n",
      "4000/49000 loss: 0.47456356602648175\n",
      "6000/49000 loss: 0.4236877452917667\n",
      "8000/49000 loss: 0.3975451571594516\n",
      "10000/49000 loss: 0.45068015789789573\n",
      "12000/49000 loss: 0.4111148290006291\n",
      "14000/49000 loss: 0.45056801231500493\n",
      "16000/49000 loss: 0.38695870133973814\n",
      "18000/49000 loss: 0.362313854282228\n",
      "20000/49000 loss: 0.39888567856547663\n",
      "22000/49000 loss: 0.4297359687670086\n",
      "24000/49000 loss: 0.39228789497365407\n",
      "26000/49000 loss: 0.4666085667036323\n",
      "28000/49000 loss: 0.3815424155833614\n",
      "30000/49000 loss: 0.35680580855554794\n",
      "32000/49000 loss: 0.38596832692580485\n",
      "34000/49000 loss: 0.31660016343872077\n",
      "36000/49000 loss: 0.3992247937814345\n",
      "38000/49000 loss: 0.40287735215709414\n",
      "40000/49000 loss: 0.3924769045478214\n",
      "42000/49000 loss: 0.3573341596104387\n",
      "44000/49000 loss: 0.3929909454732659\n",
      "46000/49000 loss: 0.36442317498053134\n",
      "48000/49000 loss: 0.406220938846945\n",
      "epoch 6: valid acc = 0.857, new learning rate = 0.0003675459453124999\n",
      "2000/49000 loss: 0.46467838807482975\n",
      "4000/49000 loss: 0.4129701470978977\n",
      "6000/49000 loss: 0.34835487825363654\n",
      "8000/49000 loss: 0.3341786127639652\n",
      "10000/49000 loss: 0.35136227525526503\n",
      "12000/49000 loss: 0.3466389738575027\n",
      "14000/49000 loss: 0.3564488881911629\n",
      "16000/49000 loss: 0.35817248565888143\n",
      "18000/49000 loss: 0.4002905999073863\n",
      "20000/49000 loss: 0.38344772120474\n",
      "22000/49000 loss: 0.3653277465143319\n",
      "24000/49000 loss: 0.4298690696049816\n",
      "26000/49000 loss: 0.35349574211269397\n",
      "28000/49000 loss: 0.4308892102844825\n",
      "30000/49000 loss: 0.41964460142881427\n",
      "32000/49000 loss: 0.3618067554073192\n",
      "34000/49000 loss: 0.47969717001807616\n",
      "36000/49000 loss: 0.47926642679027415\n",
      "38000/49000 loss: 0.38866032938719414\n",
      "40000/49000 loss: 0.3242079820716046\n",
      "42000/49000 loss: 0.2906952541008051\n",
      "44000/49000 loss: 0.38410439402781676\n",
      "46000/49000 loss: 0.3796521215120651\n",
      "48000/49000 loss: 0.3664322907654407\n",
      "epoch 7: valid acc = 0.864, new learning rate = 0.00034916864804687486\n",
      "2000/49000 loss: 0.3857287390431726\n",
      "4000/49000 loss: 0.3306987711862782\n",
      "6000/49000 loss: 0.3725002967084411\n",
      "8000/49000 loss: 0.38999063942085743\n",
      "10000/49000 loss: 0.3050970831168949\n",
      "12000/49000 loss: 0.38452508801966406\n",
      "14000/49000 loss: 0.3230765969357617\n",
      "16000/49000 loss: 0.3836451483756629\n",
      "18000/49000 loss: 0.38467725067049047\n",
      "20000/49000 loss: 0.38454123135834806\n",
      "22000/49000 loss: 0.2842086015491848\n",
      "24000/49000 loss: 0.42124662884873104\n",
      "26000/49000 loss: 0.4928283793546202\n",
      "28000/49000 loss: 0.4694643849366331\n",
      "30000/49000 loss: 0.3925437619553283\n",
      "32000/49000 loss: 0.37798459369131715\n",
      "34000/49000 loss: 0.3225594060689058\n",
      "36000/49000 loss: 0.38233489772141666\n",
      "38000/49000 loss: 0.3439663842407265\n",
      "40000/49000 loss: 0.4169886313126408\n",
      "42000/49000 loss: 0.4159565114563016\n",
      "44000/49000 loss: 0.3013844663372254\n",
      "46000/49000 loss: 0.3843124808379215\n",
      "48000/49000 loss: 0.32734368496683225\n",
      "epoch 8: valid acc = 0.86, new learning rate = 0.0003317102156445311\n",
      "2000/49000 loss: 0.5008469574050475\n",
      "4000/49000 loss: 0.3659121740564061\n",
      "6000/49000 loss: 0.3600102935739161\n",
      "8000/49000 loss: 0.3501119747239245\n",
      "10000/49000 loss: 0.3696830163198594\n",
      "12000/49000 loss: 0.35958428549588695\n",
      "14000/49000 loss: 0.4318699709456015\n",
      "16000/49000 loss: 0.3416303946522423\n",
      "18000/49000 loss: 0.3342781336732266\n",
      "20000/49000 loss: 0.3198735430552554\n",
      "22000/49000 loss: 0.394384538815155\n",
      "24000/49000 loss: 0.33761935406479493\n",
      "26000/49000 loss: 0.33400472260661235\n",
      "28000/49000 loss: 0.2504746221090292\n",
      "30000/49000 loss: 0.33911708196986423\n",
      "32000/49000 loss: 0.34498491176439655\n",
      "34000/49000 loss: 0.36716669632847343\n",
      "36000/49000 loss: 0.3673022558594507\n",
      "38000/49000 loss: 0.31814269065413425\n",
      "40000/49000 loss: 0.4191525077422285\n",
      "42000/49000 loss: 0.32426581072089816\n",
      "44000/49000 loss: 0.3735320336241854\n",
      "46000/49000 loss: 0.2852069082768202\n",
      "48000/49000 loss: 0.32550896629886683\n",
      "epoch 9: valid acc = 0.876, new learning rate = 0.0003151247048623045\n",
      "2000/49000 loss: 0.311138716993163\n",
      "4000/49000 loss: 0.41812599575116316\n",
      "6000/49000 loss: 0.38080951602562\n",
      "8000/49000 loss: 0.35538516530034464\n",
      "10000/49000 loss: 0.3763310084616637\n",
      "12000/49000 loss: 0.3161869367087248\n",
      "14000/49000 loss: 0.3314171825365936\n",
      "16000/49000 loss: 0.40560691002990684\n",
      "18000/49000 loss: 0.42003456191643695\n",
      "20000/49000 loss: 0.44018056415886897\n",
      "22000/49000 loss: 0.32927801334728923\n",
      "24000/49000 loss: 0.36424031656790057\n",
      "26000/49000 loss: 0.3117893307188181\n",
      "28000/49000 loss: 0.37891046811457046\n",
      "30000/49000 loss: 0.4132750258462942\n",
      "32000/49000 loss: 0.3720179103489194\n",
      "34000/49000 loss: 0.3604996907011096\n",
      "36000/49000 loss: 0.3332950144033259\n",
      "38000/49000 loss: 0.3185175572409511\n",
      "40000/49000 loss: 0.25388679380424806\n",
      "42000/49000 loss: 0.39703762202219045\n",
      "44000/49000 loss: 0.3424526791074797\n",
      "46000/49000 loss: 0.39057617074420187\n",
      "48000/49000 loss: 0.26342084489024986\n",
      "epoch 10: valid acc = 0.877, new learning rate = 0.00029936846961918924\n",
      "2000/49000 loss: 0.383818674688485\n",
      "4000/49000 loss: 0.36786667106664456\n",
      "6000/49000 loss: 0.35992728940031815\n",
      "8000/49000 loss: 0.3341621300596597\n",
      "10000/49000 loss: 0.2939508939457215\n",
      "12000/49000 loss: 0.30292723218536094\n",
      "14000/49000 loss: 0.26907013211346026\n",
      "16000/49000 loss: 0.40566487223153835\n",
      "18000/49000 loss: 0.36683403060231834\n",
      "20000/49000 loss: 0.3863512472480834\n",
      "22000/49000 loss: 0.39806488929672146\n",
      "24000/49000 loss: 0.4058477075224696\n",
      "26000/49000 loss: 0.44438142172243056\n",
      "28000/49000 loss: 0.35770438144723654\n",
      "30000/49000 loss: 0.4016040489917607\n",
      "32000/49000 loss: 0.2991090393673104\n",
      "34000/49000 loss: 0.4709294720546289\n",
      "36000/49000 loss: 0.27266205067711285\n",
      "38000/49000 loss: 0.2937445224910391\n",
      "40000/49000 loss: 0.3773515644572549\n",
      "42000/49000 loss: 0.40200126270040254\n",
      "44000/49000 loss: 0.366751358541142\n",
      "46000/49000 loss: 0.3249060843872623\n",
      "48000/49000 loss: 0.304071849876341\n",
      "epoch 11: valid acc = 0.873, new learning rate = 0.00028440004613822977\n",
      "2000/49000 loss: 0.39804086594290167\n",
      "4000/49000 loss: 0.3005752417743403\n",
      "6000/49000 loss: 0.29506265741422155\n",
      "8000/49000 loss: 0.3600742536904479\n",
      "10000/49000 loss: 0.45655153878364485\n",
      "12000/49000 loss: 0.4226097451262134\n",
      "14000/49000 loss: 0.3657270975242744\n",
      "16000/49000 loss: 0.39543541125573317\n",
      "18000/49000 loss: 0.401631854975628\n",
      "20000/49000 loss: 0.25420438918033994\n",
      "22000/49000 loss: 0.35166325863883313\n",
      "24000/49000 loss: 0.30321400545559574\n",
      "26000/49000 loss: 0.32308445827894455\n",
      "28000/49000 loss: 0.41516695447147295\n",
      "30000/49000 loss: 0.4275279325494172\n",
      "32000/49000 loss: 0.33114378995576915\n",
      "34000/49000 loss: 0.35742034411490237\n",
      "36000/49000 loss: 0.37633975204483483\n",
      "38000/49000 loss: 0.3614573698224201\n",
      "40000/49000 loss: 0.33062907253242857\n",
      "42000/49000 loss: 0.3177984563368846\n",
      "44000/49000 loss: 0.26300407768316064\n",
      "46000/49000 loss: 0.3424925775444753\n",
      "48000/49000 loss: 0.3587722937465483\n",
      "epoch 12: valid acc = 0.882, new learning rate = 0.00027018004383131826\n",
      "2000/49000 loss: 0.3743277542011859\n",
      "4000/49000 loss: 0.31230079254450555\n",
      "6000/49000 loss: 0.26600439138728027\n",
      "8000/49000 loss: 0.4041563621500904\n",
      "10000/49000 loss: 0.32999142761060574\n",
      "12000/49000 loss: 0.33753387763258746\n",
      "14000/49000 loss: 0.3895262329399873\n",
      "16000/49000 loss: 0.3032457474155971\n",
      "18000/49000 loss: 0.3315331555266345\n",
      "20000/49000 loss: 0.3852026051163189\n",
      "22000/49000 loss: 0.33892776781995376\n",
      "24000/49000 loss: 0.34930611204349304\n",
      "26000/49000 loss: 0.36230684487713066\n",
      "28000/49000 loss: 0.3707379663060372\n",
      "30000/49000 loss: 0.38016921891868943\n",
      "32000/49000 loss: 0.3388504900848142\n",
      "34000/49000 loss: 0.32107486706688376\n",
      "36000/49000 loss: 0.370609803030331\n",
      "38000/49000 loss: 0.35900575000747004\n",
      "40000/49000 loss: 0.4441906865583644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/49000 loss: 0.31879999859872943\n",
      "44000/49000 loss: 0.2586629479950337\n",
      "46000/49000 loss: 0.2379373675447579\n",
      "48000/49000 loss: 0.3781880223959153\n",
      "epoch 13: valid acc = 0.882, new learning rate = 0.00025667104163975234\n",
      "2000/49000 loss: 0.5080138592422903\n",
      "4000/49000 loss: 0.30352953141371714\n",
      "6000/49000 loss: 0.3301746802256453\n",
      "8000/49000 loss: 0.3445485648559757\n",
      "10000/49000 loss: 0.3984987846866447\n",
      "12000/49000 loss: 0.31510651181485533\n",
      "14000/49000 loss: 0.42215244970192417\n",
      "16000/49000 loss: 0.3693019854347114\n",
      "18000/49000 loss: 0.3175504377386723\n",
      "20000/49000 loss: 0.2601795680024369\n",
      "22000/49000 loss: 0.3302454663749528\n",
      "24000/49000 loss: 0.28478272163969004\n",
      "26000/49000 loss: 0.38421533535759067\n",
      "28000/49000 loss: 0.38600176071035486\n",
      "30000/49000 loss: 0.37000295638606345\n",
      "32000/49000 loss: 0.33879571290156546\n",
      "34000/49000 loss: 0.21603989987720898\n",
      "36000/49000 loss: 0.3766101805294569\n",
      "38000/49000 loss: 0.31548365134143946\n",
      "40000/49000 loss: 0.32250960817164676\n",
      "42000/49000 loss: 0.34743370446631655\n",
      "44000/49000 loss: 0.3508652298425817\n",
      "46000/49000 loss: 0.33638134749168513\n",
      "48000/49000 loss: 0.4126381723991719\n",
      "epoch 14: valid acc = 0.881, new learning rate = 0.00024383748955776472\n",
      "2000/49000 loss: 0.399573252928548\n",
      "4000/49000 loss: 0.3471208398302232\n",
      "6000/49000 loss: 0.2411391945692446\n",
      "8000/49000 loss: 0.34425015299460116\n",
      "10000/49000 loss: 0.27847350468739285\n",
      "12000/49000 loss: 0.3483997829806625\n",
      "14000/49000 loss: 0.2525499864089335\n",
      "16000/49000 loss: 0.3426035268442933\n",
      "18000/49000 loss: 0.2941603845268193\n",
      "20000/49000 loss: 0.3233406574845635\n",
      "22000/49000 loss: 0.2278039049384142\n",
      "24000/49000 loss: 0.36053360279584185\n",
      "26000/49000 loss: 0.31469387667615323\n",
      "28000/49000 loss: 0.2818809839223626\n",
      "30000/49000 loss: 0.41820831654284213\n",
      "32000/49000 loss: 0.38147562213571223\n",
      "34000/49000 loss: 0.332439409424012\n",
      "36000/49000 loss: 0.32800843444414185\n",
      "38000/49000 loss: 0.28158983122702513\n",
      "40000/49000 loss: 0.2779888039571382\n",
      "42000/49000 loss: 0.4462894875172105\n",
      "44000/49000 loss: 0.39826248722017155\n",
      "46000/49000 loss: 0.33538270012677646\n",
      "48000/49000 loss: 0.308050968991628\n",
      "epoch 15: valid acc = 0.88, new learning rate = 0.00023164561507987649\n",
      "2000/49000 loss: 0.2379554368246154\n",
      "4000/49000 loss: 0.3590188699963028\n",
      "6000/49000 loss: 0.30155761351413995\n",
      "8000/49000 loss: 0.3308952182026282\n",
      "10000/49000 loss: 0.3171247536328039\n",
      "12000/49000 loss: 0.30732711023605463\n",
      "14000/49000 loss: 0.29794624892449145\n",
      "16000/49000 loss: 0.3373818812060585\n",
      "18000/49000 loss: 0.3107465317837225\n",
      "20000/49000 loss: 0.3658633886570181\n",
      "22000/49000 loss: 0.3093005789329332\n",
      "24000/49000 loss: 0.3270714280887936\n",
      "26000/49000 loss: 0.3512543971683356\n",
      "28000/49000 loss: 0.3534985188810888\n",
      "30000/49000 loss: 0.30319913480704497\n",
      "32000/49000 loss: 0.3985628052600992\n",
      "34000/49000 loss: 0.31319776867872046\n",
      "36000/49000 loss: 0.21943191587493502\n",
      "38000/49000 loss: 0.27532532397401216\n",
      "40000/49000 loss: 0.4412781434279695\n",
      "42000/49000 loss: 0.35621161578356486\n",
      "44000/49000 loss: 0.23293251373504048\n",
      "46000/49000 loss: 0.3912080335335691\n",
      "48000/49000 loss: 0.37118100648058844\n",
      "epoch 16: valid acc = 0.889, new learning rate = 0.00022006333432588265\n",
      "2000/49000 loss: 0.3989084835480146\n",
      "4000/49000 loss: 0.3572022267787993\n",
      "6000/49000 loss: 0.26777010356692077\n",
      "8000/49000 loss: 0.30530913645187374\n",
      "10000/49000 loss: 0.23748333317583936\n",
      "12000/49000 loss: 0.34362888078775367\n",
      "14000/49000 loss: 0.30995667067493793\n",
      "16000/49000 loss: 0.31832908683192157\n",
      "18000/49000 loss: 0.3671126607997452\n",
      "20000/49000 loss: 0.2489931359115731\n",
      "22000/49000 loss: 0.298592352490846\n",
      "24000/49000 loss: 0.3512142510323309\n",
      "26000/49000 loss: 0.3476454214029444\n",
      "28000/49000 loss: 0.3543007228481673\n",
      "30000/49000 loss: 0.2606943821369146\n",
      "32000/49000 loss: 0.2951887732722245\n",
      "34000/49000 loss: 0.2769147036533366\n",
      "36000/49000 loss: 0.2798799723045891\n",
      "38000/49000 loss: 0.257675152195203\n",
      "40000/49000 loss: 0.25178416335818926\n",
      "42000/49000 loss: 0.3603538023193125\n",
      "44000/49000 loss: 0.27838469992704246\n",
      "46000/49000 loss: 0.2856529148401494\n",
      "48000/49000 loss: 0.26262704836927214\n",
      "epoch 17: valid acc = 0.886, new learning rate = 0.00020906016760958852\n",
      "2000/49000 loss: 0.3466421712219103\n",
      "4000/49000 loss: 0.2805767923172475\n",
      "6000/49000 loss: 0.3494972013819256\n",
      "8000/49000 loss: 0.43615668839374283\n",
      "10000/49000 loss: 0.39685248733938755\n",
      "12000/49000 loss: 0.3392352550988642\n",
      "14000/49000 loss: 0.24319086885079344\n",
      "16000/49000 loss: 0.34509202287813207\n",
      "18000/49000 loss: 0.33796600734969473\n",
      "20000/49000 loss: 0.365909791947916\n",
      "22000/49000 loss: 0.27725369216906637\n",
      "24000/49000 loss: 0.2945715647445709\n",
      "26000/49000 loss: 0.33185931105141364\n",
      "28000/49000 loss: 0.34062817286350117\n",
      "30000/49000 loss: 0.31425591528193336\n",
      "32000/49000 loss: 0.3477560607649982\n",
      "34000/49000 loss: 0.2745563713543766\n",
      "36000/49000 loss: 0.34391926921154536\n",
      "38000/49000 loss: 0.26206485465601087\n",
      "40000/49000 loss: 0.23066407661020336\n",
      "42000/49000 loss: 0.3605366066656557\n",
      "44000/49000 loss: 0.28658596742325276\n",
      "46000/49000 loss: 0.3476371414492924\n",
      "48000/49000 loss: 0.28629445482679916\n",
      "epoch 18: valid acc = 0.882, new learning rate = 0.00019860715922910907\n",
      "2000/49000 loss: 0.2305729884686885\n",
      "4000/49000 loss: 0.3539025394918122\n",
      "6000/49000 loss: 0.2834171697659216\n",
      "8000/49000 loss: 0.35970400330600677\n",
      "10000/49000 loss: 0.3517550907975592\n",
      "12000/49000 loss: 0.25254773923817386\n",
      "14000/49000 loss: 0.2597613292907997\n",
      "16000/49000 loss: 0.2793011649674262\n",
      "18000/49000 loss: 0.27647429396151724\n",
      "20000/49000 loss: 0.3605818998680736\n",
      "22000/49000 loss: 0.31372949088461766\n",
      "24000/49000 loss: 0.2876876017861943\n",
      "26000/49000 loss: 0.28490815673362535\n",
      "28000/49000 loss: 0.3788956749148756\n",
      "30000/49000 loss: 0.30879075504587883\n",
      "32000/49000 loss: 0.3903342306178851\n",
      "34000/49000 loss: 0.255308094292072\n",
      "36000/49000 loss: 0.31345899185214127\n",
      "38000/49000 loss: 0.24153096478272054\n",
      "40000/49000 loss: 0.2889445481017544\n",
      "42000/49000 loss: 0.31225096384429074\n",
      "44000/49000 loss: 0.2517757542509862\n",
      "46000/49000 loss: 0.2935561500622059\n",
      "48000/49000 loss: 0.3668506697505959\n",
      "epoch 19: valid acc = 0.883, new learning rate = 0.0001886768012676536\n",
      "2000/49000 loss: 0.2549210426699565\n",
      "4000/49000 loss: 0.3650219451483865\n",
      "6000/49000 loss: 0.34644741115940864\n",
      "8000/49000 loss: 0.21718914504255765\n",
      "10000/49000 loss: 0.3775816235881409\n",
      "12000/49000 loss: 0.32534989276473797\n",
      "14000/49000 loss: 0.3411170058015627\n",
      "16000/49000 loss: 0.37153089869467193\n",
      "18000/49000 loss: 0.27043075881916634\n",
      "20000/49000 loss: 0.3980598556627324\n",
      "22000/49000 loss: 0.30517600490804486\n",
      "24000/49000 loss: 0.2563442333396377\n",
      "26000/49000 loss: 0.27301157927249764\n",
      "28000/49000 loss: 0.2933713499712835\n",
      "30000/49000 loss: 0.29352935096095584\n",
      "32000/49000 loss: 0.3047080696238612\n",
      "34000/49000 loss: 0.3924669752359474\n",
      "36000/49000 loss: 0.2767182464216952\n",
      "38000/49000 loss: 0.31147729933426077\n",
      "40000/49000 loss: 0.332870427068564\n",
      "42000/49000 loss: 0.3234574534073256\n",
      "44000/49000 loss: 0.32919677476686654\n",
      "46000/49000 loss: 0.36310122338450895\n",
      "48000/49000 loss: 0.2858583709310582\n",
      "epoch 20: valid acc = 0.884, new learning rate = 0.0001792429612042709\n",
      "2000/49000 loss: 0.32927674508573573\n",
      "4000/49000 loss: 0.28257334391544053\n",
      "6000/49000 loss: 0.2730231348284337\n",
      "8000/49000 loss: 0.2454540799131904\n",
      "10000/49000 loss: 0.3691207849432597\n",
      "12000/49000 loss: 0.3718245138999143\n",
      "14000/49000 loss: 0.28835792477296757\n",
      "16000/49000 loss: 0.325200830615967\n",
      "18000/49000 loss: 0.29744179525312525\n",
      "20000/49000 loss: 0.3359194867010308\n",
      "22000/49000 loss: 0.3930765607097858\n",
      "24000/49000 loss: 0.3037116125721837\n",
      "26000/49000 loss: 0.3243147909379205\n",
      "28000/49000 loss: 0.23511345636915731\n",
      "30000/49000 loss: 0.2838203793460173\n",
      "32000/49000 loss: 0.27814528709320485\n",
      "34000/49000 loss: 0.23062409389373822\n",
      "36000/49000 loss: 0.3467580476582118\n",
      "38000/49000 loss: 0.29168675229628527\n",
      "40000/49000 loss: 0.37137198702608143\n",
      "42000/49000 loss: 0.29149715869333326\n",
      "44000/49000 loss: 0.3875489704371022\n",
      "46000/49000 loss: 0.2676128523368693\n",
      "48000/49000 loss: 0.28505131906490466\n",
      "epoch 21: valid acc = 0.887, new learning rate = 0.00017028081314405735\n",
      "2000/49000 loss: 0.34960659943516187\n",
      "4000/49000 loss: 0.22494572472521815\n",
      "6000/49000 loss: 0.3982897047427721\n",
      "8000/49000 loss: 0.23577021976732415\n",
      "10000/49000 loss: 0.2793933024460358\n",
      "12000/49000 loss: 0.24652513200682408\n",
      "14000/49000 loss: 0.24149564708220134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000/49000 loss: 0.27049438947340754\n",
      "18000/49000 loss: 0.3023330543139744\n",
      "20000/49000 loss: 0.2711744273278113\n",
      "22000/49000 loss: 0.4413702269713231\n",
      "24000/49000 loss: 0.24864751350678477\n",
      "26000/49000 loss: 0.3607797136216399\n",
      "28000/49000 loss: 0.36159993449114686\n",
      "30000/49000 loss: 0.2928913654634871\n",
      "32000/49000 loss: 0.25417955042726204\n",
      "34000/49000 loss: 0.2488027689355945\n",
      "36000/49000 loss: 0.22439905563789495\n",
      "38000/49000 loss: 0.23538169673647535\n",
      "40000/49000 loss: 0.3450300148430063\n",
      "42000/49000 loss: 0.3517576030057125\n",
      "44000/49000 loss: 0.28119753810397646\n",
      "46000/49000 loss: 0.3277871156618724\n",
      "48000/49000 loss: 0.36868584123353876\n",
      "epoch 22: valid acc = 0.884, new learning rate = 0.00016176677248685447\n",
      "2000/49000 loss: 0.3459389803471256\n",
      "4000/49000 loss: 0.24615118926267407\n",
      "6000/49000 loss: 0.29692050717864743\n",
      "8000/49000 loss: 0.3563778242757396\n",
      "10000/49000 loss: 0.29403303708748574\n",
      "12000/49000 loss: 0.25854179592148896\n",
      "14000/49000 loss: 0.27538561943119627\n",
      "16000/49000 loss: 0.2114140643176212\n",
      "18000/49000 loss: 0.2479479179219098\n",
      "20000/49000 loss: 0.2677838205253707\n",
      "22000/49000 loss: 0.38436170476601467\n",
      "24000/49000 loss: 0.2870548948331425\n",
      "26000/49000 loss: 0.31082278411587216\n",
      "28000/49000 loss: 0.32912994294914927\n",
      "30000/49000 loss: 0.37651702005194704\n",
      "32000/49000 loss: 0.37205205932915675\n",
      "34000/49000 loss: 0.2442651449689817\n",
      "36000/49000 loss: 0.3111440571877968\n",
      "38000/49000 loss: 0.3200893008026358\n",
      "40000/49000 loss: 0.32517578031873384\n",
      "42000/49000 loss: 0.24543796646956487\n",
      "44000/49000 loss: 0.34461150155835546\n",
      "46000/49000 loss: 0.3277480593160466\n",
      "48000/49000 loss: 0.30938183740304015\n",
      "epoch 23: valid acc = 0.884, new learning rate = 0.00015367843386251173\n",
      "2000/49000 loss: 0.28661803763071714\n",
      "4000/49000 loss: 0.392152960289673\n",
      "6000/49000 loss: 0.3316279662639919\n",
      "8000/49000 loss: 0.33246932420717407\n",
      "10000/49000 loss: 0.2591683184031293\n",
      "12000/49000 loss: 0.31488619163287235\n",
      "14000/49000 loss: 0.27333511267933513\n",
      "16000/49000 loss: 0.3108157872067649\n",
      "18000/49000 loss: 0.30497107229775716\n",
      "20000/49000 loss: 0.3275624260183506\n",
      "22000/49000 loss: 0.2483473526722782\n",
      "24000/49000 loss: 0.22174514385762764\n",
      "26000/49000 loss: 0.2767830427712487\n",
      "28000/49000 loss: 0.3500792419631032\n",
      "30000/49000 loss: 0.32951853616432386\n",
      "32000/49000 loss: 0.3119988723562468\n",
      "34000/49000 loss: 0.36973259975133294\n",
      "36000/49000 loss: 0.2745844793566498\n",
      "38000/49000 loss: 0.2698801949651745\n",
      "40000/49000 loss: 0.3549899792079445\n",
      "42000/49000 loss: 0.3502938447439856\n",
      "44000/49000 loss: 0.3790302928147592\n",
      "46000/49000 loss: 0.2629194357133002\n",
      "48000/49000 loss: 0.3024281378372487\n",
      "epoch 24: valid acc = 0.887, new learning rate = 0.00014599451216938612\n",
      "2000/49000 loss: 0.3142909908661706\n",
      "4000/49000 loss: 0.24628655488005868\n",
      "6000/49000 loss: 0.34632772050620597\n",
      "8000/49000 loss: 0.25651731978232456\n",
      "10000/49000 loss: 0.28849850633831997\n",
      "12000/49000 loss: 0.3182469542006934\n",
      "14000/49000 loss: 0.3333469018589782\n",
      "16000/49000 loss: 0.3974442719930467\n",
      "18000/49000 loss: 0.2530376520755256\n",
      "20000/49000 loss: 0.30493971032006767\n",
      "22000/49000 loss: 0.41076315016445286\n",
      "24000/49000 loss: 0.3039151280301306\n",
      "26000/49000 loss: 0.23883270538872947\n",
      "28000/49000 loss: 0.31318512236071283\n",
      "30000/49000 loss: 0.36226177718367997\n",
      "32000/49000 loss: 0.30517296027218316\n",
      "34000/49000 loss: 0.34995302952447205\n",
      "36000/49000 loss: 0.27180979849072096\n",
      "38000/49000 loss: 0.25553943287874675\n",
      "40000/49000 loss: 0.3885486560641661\n",
      "42000/49000 loss: 0.23672055402576409\n",
      "44000/49000 loss: 0.2679293414693326\n",
      "46000/49000 loss: 0.3819937758967994\n",
      "48000/49000 loss: 0.3200787175000301\n",
      "epoch 25: valid acc = 0.89, new learning rate = 0.00013869478656091682\n",
      "2000/49000 loss: 0.30177070162762315\n",
      "4000/49000 loss: 0.34663895588395355\n",
      "6000/49000 loss: 0.27326105906738246\n",
      "8000/49000 loss: 0.28338611563099997\n",
      "10000/49000 loss: 0.21210062300444513\n",
      "12000/49000 loss: 0.2671529180252632\n",
      "14000/49000 loss: 0.26416226077821137\n",
      "16000/49000 loss: 0.22932660964246757\n",
      "18000/49000 loss: 0.32922442369013716\n",
      "20000/49000 loss: 0.3133123756663535\n",
      "22000/49000 loss: 0.3327452575691548\n",
      "24000/49000 loss: 0.19580962244459796\n",
      "26000/49000 loss: 0.3709550737338281\n",
      "28000/49000 loss: 0.35402687526852555\n",
      "30000/49000 loss: 0.4075849523778631\n",
      "32000/49000 loss: 0.2634868296499917\n",
      "34000/49000 loss: 0.29198339202966844\n",
      "36000/49000 loss: 0.23783517852000485\n",
      "38000/49000 loss: 0.30398959818805604\n",
      "40000/49000 loss: 0.3210448544735925\n",
      "42000/49000 loss: 0.30111879513789985\n",
      "44000/49000 loss: 0.23908323010779017\n",
      "46000/49000 loss: 0.26315859704134037\n",
      "48000/49000 loss: 0.3741804188903583\n",
      "epoch 26: valid acc = 0.894, new learning rate = 0.00013176004723287096\n",
      "2000/49000 loss: 0.26415561752636246\n",
      "4000/49000 loss: 0.3236218586959105\n",
      "6000/49000 loss: 0.3430399831416651\n",
      "8000/49000 loss: 0.2501724742813208\n",
      "10000/49000 loss: 0.2682805923514021\n",
      "12000/49000 loss: 0.26004999267452933\n",
      "14000/49000 loss: 0.2620088169325573\n",
      "16000/49000 loss: 0.3053078496422537\n",
      "18000/49000 loss: 0.2004707152881746\n",
      "20000/49000 loss: 0.32050138526372235\n",
      "22000/49000 loss: 0.3777322580849007\n",
      "24000/49000 loss: 0.2335009357268284\n",
      "26000/49000 loss: 0.23597659772678528\n",
      "28000/49000 loss: 0.2728501993988926\n",
      "30000/49000 loss: 0.2379994672048974\n",
      "32000/49000 loss: 0.2582386988393888\n",
      "34000/49000 loss: 0.30893602121090574\n",
      "36000/49000 loss: 0.3066793396246787\n",
      "38000/49000 loss: 0.29478677626886296\n",
      "40000/49000 loss: 0.2727770843382935\n",
      "42000/49000 loss: 0.2716384271623358\n",
      "44000/49000 loss: 0.30968800183894424\n",
      "46000/49000 loss: 0.36082484423818245\n",
      "48000/49000 loss: 0.244553375661218\n",
      "epoch 27: valid acc = 0.888, new learning rate = 0.0001251720448712274\n",
      "2000/49000 loss: 0.21238791697019616\n",
      "4000/49000 loss: 0.31511982965408497\n",
      "6000/49000 loss: 0.3289143892666389\n",
      "8000/49000 loss: 0.3894177975713959\n",
      "10000/49000 loss: 0.3212387455443718\n",
      "12000/49000 loss: 0.31373539717261445\n",
      "14000/49000 loss: 0.27581250339614\n",
      "16000/49000 loss: 0.2831530853857248\n",
      "18000/49000 loss: 0.31799414109449675\n",
      "20000/49000 loss: 0.22777992524577317\n",
      "22000/49000 loss: 0.2697369796535671\n",
      "24000/49000 loss: 0.2788991464004161\n",
      "26000/49000 loss: 0.29029698069734133\n",
      "28000/49000 loss: 0.2683301935809\n",
      "30000/49000 loss: 0.25207926467359554\n",
      "32000/49000 loss: 0.2730556516862839\n",
      "34000/49000 loss: 0.26325012889583665\n",
      "36000/49000 loss: 0.3039486658239285\n",
      "38000/49000 loss: 0.3071432381377345\n",
      "40000/49000 loss: 0.37707127524301387\n",
      "42000/49000 loss: 0.33367123325306064\n",
      "44000/49000 loss: 0.2992666930604441\n",
      "46000/49000 loss: 0.28201166962788526\n",
      "48000/49000 loss: 0.31615646822442034\n",
      "epoch 28: valid acc = 0.889, new learning rate = 0.00011891344262766602\n",
      "2000/49000 loss: 0.35253682454149693\n",
      "4000/49000 loss: 0.23888012859003854\n",
      "6000/49000 loss: 0.2721834353221624\n",
      "8000/49000 loss: 0.24550081692434225\n",
      "10000/49000 loss: 0.31964301833745506\n",
      "12000/49000 loss: 0.2816458214607956\n",
      "14000/49000 loss: 0.27690115593418013\n",
      "16000/49000 loss: 0.2347197672294083\n",
      "18000/49000 loss: 0.3176891768791362\n",
      "20000/49000 loss: 0.33944919467952595\n",
      "22000/49000 loss: 0.35102484476707624\n",
      "24000/49000 loss: 0.3992074064561478\n",
      "26000/49000 loss: 0.19645563231844126\n",
      "28000/49000 loss: 0.30682242373257224\n",
      "30000/49000 loss: 0.3957111952857785\n",
      "32000/49000 loss: 0.22915905262898087\n",
      "34000/49000 loss: 0.3446016381222994\n",
      "36000/49000 loss: 0.2899592093390682\n",
      "38000/49000 loss: 0.2589512628205008\n",
      "40000/49000 loss: 0.28536733587261154\n",
      "42000/49000 loss: 0.33395896840403244\n",
      "44000/49000 loss: 0.2610665798618972\n",
      "46000/49000 loss: 0.2661874407508443\n",
      "48000/49000 loss: 0.29447609710152023\n",
      "epoch 29: valid acc = 0.884, new learning rate = 0.00011296777049628272\n",
      "2000/49000 loss: 0.31201197479050147\n",
      "4000/49000 loss: 0.312201451525432\n",
      "6000/49000 loss: 0.28641430106654275\n",
      "8000/49000 loss: 0.3872530750930759\n",
      "10000/49000 loss: 0.2848780031376929\n",
      "12000/49000 loss: 0.28699126537386693\n",
      "14000/49000 loss: 0.19644140155459852\n",
      "16000/49000 loss: 0.27825210469001155\n",
      "18000/49000 loss: 0.3343756612387502\n",
      "20000/49000 loss: 0.2640359007916403\n",
      "22000/49000 loss: 0.2742753401462042\n",
      "24000/49000 loss: 0.24489697004283775\n",
      "26000/49000 loss: 0.3026332182005027\n",
      "28000/49000 loss: 0.2951744391751281\n",
      "30000/49000 loss: 0.28575304795405715\n",
      "32000/49000 loss: 0.2468307812650914\n",
      "34000/49000 loss: 0.3240585750928103\n",
      "36000/49000 loss: 0.2764929634677313\n",
      "38000/49000 loss: 0.25153150594986684\n",
      "40000/49000 loss: 0.3044085378543396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/49000 loss: 0.2646527855430602\n",
      "44000/49000 loss: 0.28029303437251746\n",
      "46000/49000 loss: 0.3789069192400874\n",
      "48000/49000 loss: 0.3012937407699887\n",
      "epoch 30: valid acc = 0.889, new learning rate = 0.00010731938197146858\n",
      "2000/49000 loss: 0.3547792657203289\n",
      "4000/49000 loss: 0.2756160101622336\n",
      "6000/49000 loss: 0.20244720348716877\n",
      "8000/49000 loss: 0.27700167779413004\n",
      "10000/49000 loss: 0.2721815925026932\n",
      "12000/49000 loss: 0.26651773920186267\n",
      "14000/49000 loss: 0.2939978531233506\n",
      "16000/49000 loss: 0.27716650515611474\n",
      "18000/49000 loss: 0.18614191930241467\n",
      "20000/49000 loss: 0.2708327335726674\n",
      "22000/49000 loss: 0.33628577232471407\n",
      "24000/49000 loss: 0.30059564863914723\n",
      "26000/49000 loss: 0.3403577021633059\n",
      "28000/49000 loss: 0.20418310101257917\n",
      "30000/49000 loss: 0.2973433089048775\n",
      "32000/49000 loss: 0.3595548084074091\n",
      "34000/49000 loss: 0.3112896465991056\n",
      "36000/49000 loss: 0.3344410258621479\n",
      "38000/49000 loss: 0.31190299160776563\n",
      "40000/49000 loss: 0.2666922840044518\n",
      "42000/49000 loss: 0.253164348652863\n",
      "44000/49000 loss: 0.21926116261473247\n",
      "46000/49000 loss: 0.22383162292088618\n",
      "48000/49000 loss: 0.28699684874824083\n",
      "epoch 31: valid acc = 0.889, new learning rate = 0.00010195341287289515\n",
      "2000/49000 loss: 0.2821228948725654\n",
      "4000/49000 loss: 0.3252961128880323\n",
      "6000/49000 loss: 0.2975933598866698\n",
      "8000/49000 loss: 0.2361710059025346\n",
      "10000/49000 loss: 0.29277559003077136\n",
      "12000/49000 loss: 0.4324907055711364\n",
      "14000/49000 loss: 0.30041009757112\n",
      "16000/49000 loss: 0.25582371535915693\n",
      "18000/49000 loss: 0.3202852174670238\n",
      "20000/49000 loss: 0.343561459783036\n",
      "22000/49000 loss: 0.29552904117371687\n",
      "24000/49000 loss: 0.28237260533495584\n",
      "26000/49000 loss: 0.27238813926338834\n",
      "28000/49000 loss: 0.28363595673472003\n",
      "30000/49000 loss: 0.3276924208900153\n",
      "32000/49000 loss: 0.407079210693526\n",
      "34000/49000 loss: 0.3537807995124345\n",
      "36000/49000 loss: 0.19981746560886254\n",
      "38000/49000 loss: 0.2933500082678446\n",
      "40000/49000 loss: 0.30060920117810447\n",
      "42000/49000 loss: 0.26719410237164154\n",
      "44000/49000 loss: 0.2641493196395143\n",
      "46000/49000 loss: 0.23817112724620615\n",
      "48000/49000 loss: 0.21588541730475913\n",
      "epoch 32: valid acc = 0.891, new learning rate = 9.685574222925039e-05\n",
      "2000/49000 loss: 0.3549411309390395\n",
      "4000/49000 loss: 0.2445563313002712\n",
      "6000/49000 loss: 0.24326206919955798\n",
      "8000/49000 loss: 0.3435651000996805\n",
      "10000/49000 loss: 0.3718464659707254\n",
      "12000/49000 loss: 0.23396611074468865\n",
      "14000/49000 loss: 0.2731545223321569\n",
      "16000/49000 loss: 0.28837974551002077\n",
      "18000/49000 loss: 0.26611952866171856\n",
      "20000/49000 loss: 0.33466407912394763\n",
      "22000/49000 loss: 0.25229778727467805\n",
      "24000/49000 loss: 0.28780100139329734\n",
      "26000/49000 loss: 0.275439877781157\n",
      "28000/49000 loss: 0.281398155559938\n",
      "30000/49000 loss: 0.2166151032151227\n",
      "32000/49000 loss: 0.3069711987872463\n",
      "34000/49000 loss: 0.25087387180910525\n",
      "36000/49000 loss: 0.2713336553496182\n",
      "38000/49000 loss: 0.23986235807565714\n",
      "40000/49000 loss: 0.2523161172605539\n",
      "42000/49000 loss: 0.28859760544827145\n",
      "44000/49000 loss: 0.27867768235501156\n",
      "46000/49000 loss: 0.2671744352706725\n",
      "48000/49000 loss: 0.2714890516243134\n",
      "epoch 33: valid acc = 0.891, new learning rate = 9.201295511778786e-05\n",
      "2000/49000 loss: 0.27371468659711956\n",
      "4000/49000 loss: 0.3145846260473495\n",
      "6000/49000 loss: 0.3447452714217274\n",
      "8000/49000 loss: 0.32662407069124644\n",
      "10000/49000 loss: 0.2999864990023961\n",
      "12000/49000 loss: 0.2550467635269456\n",
      "14000/49000 loss: 0.3074822746371162\n",
      "16000/49000 loss: 0.3542011436348345\n",
      "18000/49000 loss: 0.34177756324954234\n",
      "20000/49000 loss: 0.2721069229478574\n",
      "22000/49000 loss: 0.24708796348015097\n",
      "24000/49000 loss: 0.31758451669797705\n",
      "26000/49000 loss: 0.2430578443156583\n",
      "28000/49000 loss: 0.3154834240331044\n",
      "30000/49000 loss: 0.3186791577867292\n",
      "32000/49000 loss: 0.35923799784361826\n",
      "34000/49000 loss: 0.2549836290665959\n",
      "36000/49000 loss: 0.31246966881012583\n",
      "38000/49000 loss: 0.2600126456617335\n",
      "40000/49000 loss: 0.2276186972978637\n",
      "42000/49000 loss: 0.24861303600475324\n",
      "44000/49000 loss: 0.2763934662193928\n",
      "46000/49000 loss: 0.19662196788727926\n",
      "48000/49000 loss: 0.2630778056795744\n",
      "epoch 34: valid acc = 0.889, new learning rate = 8.741230736189846e-05\n",
      "2000/49000 loss: 0.2753759135951496\n",
      "4000/49000 loss: 0.22949936941340024\n",
      "6000/49000 loss: 0.19375446724700218\n",
      "8000/49000 loss: 0.28703777338264197\n",
      "10000/49000 loss: 0.3150310831308619\n",
      "12000/49000 loss: 0.23553967338764592\n",
      "14000/49000 loss: 0.30140187206028335\n",
      "16000/49000 loss: 0.29282052020316496\n",
      "18000/49000 loss: 0.2491959492500112\n",
      "20000/49000 loss: 0.30100514446299087\n",
      "22000/49000 loss: 0.22044821154744879\n",
      "24000/49000 loss: 0.25332284800935406\n",
      "26000/49000 loss: 0.36689235379946844\n",
      "28000/49000 loss: 0.2636797167393396\n",
      "30000/49000 loss: 0.37309684290202594\n",
      "32000/49000 loss: 0.2720799382292164\n",
      "34000/49000 loss: 0.27808408648605754\n",
      "36000/49000 loss: 0.25242511904305714\n",
      "38000/49000 loss: 0.4256894736440383\n",
      "40000/49000 loss: 0.2775659182462676\n",
      "42000/49000 loss: 0.24059967629937232\n",
      "44000/49000 loss: 0.29480160887513224\n",
      "46000/49000 loss: 0.30958804106710197\n",
      "48000/49000 loss: 0.17423791014090362\n",
      "epoch 35: valid acc = 0.889, new learning rate = 8.304169199380353e-05\n",
      "2000/49000 loss: 0.2875385471506271\n",
      "4000/49000 loss: 0.32166640737040514\n",
      "6000/49000 loss: 0.2419262655416608\n",
      "8000/49000 loss: 0.29783796842922255\n",
      "10000/49000 loss: 0.3670109552864996\n",
      "12000/49000 loss: 0.2999513998467721\n",
      "14000/49000 loss: 0.23694686735004095\n",
      "16000/49000 loss: 0.2593336569341698\n",
      "18000/49000 loss: 0.3090839013666717\n",
      "20000/49000 loss: 0.29651868665994585\n",
      "22000/49000 loss: 0.20919149771440362\n",
      "24000/49000 loss: 0.2976876858148651\n",
      "26000/49000 loss: 0.18873171669099897\n",
      "28000/49000 loss: 0.2993621422714276\n",
      "30000/49000 loss: 0.27139073269176406\n",
      "32000/49000 loss: 0.27174312544773094\n",
      "34000/49000 loss: 0.3108751672808668\n",
      "36000/49000 loss: 0.31344612508734854\n",
      "38000/49000 loss: 0.2628073354132616\n",
      "40000/49000 loss: 0.3084071312716457\n",
      "42000/49000 loss: 0.25836573445082944\n",
      "44000/49000 loss: 0.36133619820832874\n",
      "46000/49000 loss: 0.2805796709470265\n",
      "48000/49000 loss: 0.26661080422769634\n",
      "epoch 36: valid acc = 0.888, new learning rate = 7.888960739411335e-05\n",
      "2000/49000 loss: 0.27617603496811055\n",
      "4000/49000 loss: 0.2784512973929177\n",
      "6000/49000 loss: 0.26689154160808537\n",
      "8000/49000 loss: 0.2581992377029971\n",
      "10000/49000 loss: 0.32093432706914876\n",
      "12000/49000 loss: 0.32796755328742777\n",
      "14000/49000 loss: 0.2160551323657991\n",
      "16000/49000 loss: 0.24211302127825052\n",
      "18000/49000 loss: 0.24013243690607253\n",
      "20000/49000 loss: 0.3069200652844742\n",
      "22000/49000 loss: 0.22486851598337926\n",
      "24000/49000 loss: 0.2958505864263745\n",
      "26000/49000 loss: 0.3085911936781682\n",
      "28000/49000 loss: 0.3008784220621599\n",
      "30000/49000 loss: 0.2894717697822431\n",
      "32000/49000 loss: 0.2150899095240398\n",
      "34000/49000 loss: 0.23660258062860207\n",
      "36000/49000 loss: 0.25572243772982406\n",
      "38000/49000 loss: 0.2852332132434786\n",
      "40000/49000 loss: 0.25935717693486\n",
      "42000/49000 loss: 0.29213009826475117\n",
      "44000/49000 loss: 0.24097336722428955\n",
      "46000/49000 loss: 0.35043568114385437\n",
      "48000/49000 loss: 0.28467263760299083\n",
      "epoch 37: valid acc = 0.892, new learning rate = 7.494512702440768e-05\n",
      "2000/49000 loss: 0.32496555077638045\n",
      "4000/49000 loss: 0.2818861022874201\n",
      "6000/49000 loss: 0.32765463980757964\n",
      "8000/49000 loss: 0.3621979053737157\n",
      "10000/49000 loss: 0.18672439464647964\n",
      "12000/49000 loss: 0.402051364238898\n",
      "14000/49000 loss: 0.3080802808117692\n",
      "16000/49000 loss: 0.26192472889871543\n",
      "18000/49000 loss: 0.35242269006882154\n",
      "20000/49000 loss: 0.3232733359066992\n",
      "22000/49000 loss: 0.3380584068031613\n",
      "24000/49000 loss: 0.32877124232866967\n",
      "26000/49000 loss: 0.27426268941503\n",
      "28000/49000 loss: 0.32784745475132904\n",
      "30000/49000 loss: 0.33110017901680716\n",
      "32000/49000 loss: 0.29970590195486657\n",
      "34000/49000 loss: 0.3413047440850637\n",
      "36000/49000 loss: 0.2933134876219063\n",
      "38000/49000 loss: 0.26114239797989397\n",
      "40000/49000 loss: 0.4264640119007087\n",
      "42000/49000 loss: 0.2698854548644946\n",
      "44000/49000 loss: 0.24946163068917174\n",
      "46000/49000 loss: 0.29907870370750894\n",
      "48000/49000 loss: 0.27791714710142956\n",
      "epoch 38: valid acc = 0.887, new learning rate = 7.119787067318729e-05\n",
      "2000/49000 loss: 0.3235246643783094\n",
      "4000/49000 loss: 0.39301689152914343\n",
      "6000/49000 loss: 0.24584025029519152\n",
      "8000/49000 loss: 0.20828220775766126\n",
      "10000/49000 loss: 0.3123836765159539\n",
      "12000/49000 loss: 0.2351921749260439\n",
      "14000/49000 loss: 0.34648685725050665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000/49000 loss: 0.36899624443885\n",
      "18000/49000 loss: 0.3149725286122908\n",
      "20000/49000 loss: 0.20893699293774082\n",
      "22000/49000 loss: 0.32757327984747525\n",
      "24000/49000 loss: 0.3761720890260544\n",
      "26000/49000 loss: 0.28460459415586553\n",
      "28000/49000 loss: 0.28087561441499503\n",
      "30000/49000 loss: 0.2701185107766323\n",
      "32000/49000 loss: 0.2321481072950682\n",
      "34000/49000 loss: 0.3573148065552493\n",
      "36000/49000 loss: 0.29992335194365793\n",
      "38000/49000 loss: 0.2938424684391073\n",
      "40000/49000 loss: 0.30247572174584314\n",
      "42000/49000 loss: 0.2865226643610299\n",
      "44000/49000 loss: 0.18958405837650186\n",
      "46000/49000 loss: 0.28515342817506484\n",
      "48000/49000 loss: 0.24948762968196284\n",
      "epoch 39: valid acc = 0.888, new learning rate = 6.763797713952792e-05\n",
      "2000/49000 loss: 0.23674551086831588\n",
      "4000/49000 loss: 0.222917841488207\n",
      "6000/49000 loss: 0.3583448492078763\n",
      "8000/49000 loss: 0.24298524253496176\n",
      "10000/49000 loss: 0.27801952602535934\n",
      "12000/49000 loss: 0.23595086544777177\n",
      "14000/49000 loss: 0.299480591123296\n",
      "16000/49000 loss: 0.30324061301113775\n",
      "18000/49000 loss: 0.2906579345329083\n",
      "20000/49000 loss: 0.25337717564241147\n",
      "22000/49000 loss: 0.21704405280150077\n",
      "24000/49000 loss: 0.2411743869653125\n",
      "26000/49000 loss: 0.3838220914211376\n",
      "28000/49000 loss: 0.23515722007065365\n",
      "30000/49000 loss: 0.25792672718779774\n",
      "32000/49000 loss: 0.39482763547970595\n",
      "34000/49000 loss: 0.26700186186435704\n",
      "36000/49000 loss: 0.18094732912574932\n",
      "38000/49000 loss: 0.21702375447363717\n",
      "40000/49000 loss: 0.30118745427346366\n",
      "42000/49000 loss: 0.35001883745338136\n",
      "44000/49000 loss: 0.26523788826877914\n",
      "46000/49000 loss: 0.32113038711686936\n",
      "48000/49000 loss: 0.33469442907986063\n",
      "epoch 40: valid acc = 0.891, new learning rate = 6.425607828255152e-05\n",
      "2000/49000 loss: 0.38070643124529574\n",
      "4000/49000 loss: 0.25502126503843964\n",
      "6000/49000 loss: 0.2842780304584323\n",
      "8000/49000 loss: 0.2703558302185867\n",
      "10000/49000 loss: 0.2812870487422683\n",
      "12000/49000 loss: 0.2615571085446336\n",
      "14000/49000 loss: 0.26277985855998465\n",
      "16000/49000 loss: 0.345480138321429\n",
      "18000/49000 loss: 0.3059217288613385\n",
      "20000/49000 loss: 0.2493098655975837\n",
      "22000/49000 loss: 0.2569299545445302\n",
      "24000/49000 loss: 0.3086940342292561\n",
      "26000/49000 loss: 0.3561855425753868\n",
      "28000/49000 loss: 0.28890528199969334\n",
      "30000/49000 loss: 0.23783198666231598\n",
      "32000/49000 loss: 0.31651977503572426\n",
      "34000/49000 loss: 0.25331832859019915\n",
      "36000/49000 loss: 0.32721936097872684\n",
      "38000/49000 loss: 0.3623592980132888\n",
      "40000/49000 loss: 0.24493529544622397\n",
      "42000/49000 loss: 0.27098104938732986\n",
      "44000/49000 loss: 0.34444091018356715\n",
      "46000/49000 loss: 0.3765208676946586\n",
      "48000/49000 loss: 0.2869908772639414\n",
      "epoch 41: valid acc = 0.889, new learning rate = 6.104327436842394e-05\n",
      "2000/49000 loss: 0.2375165690431351\n",
      "4000/49000 loss: 0.379632056300001\n",
      "6000/49000 loss: 0.22806900351487774\n",
      "8000/49000 loss: 0.2874953731701091\n",
      "10000/49000 loss: 0.2517944102028679\n",
      "12000/49000 loss: 0.23815065333566446\n",
      "14000/49000 loss: 0.26194097702798963\n",
      "16000/49000 loss: 0.23126214799886785\n",
      "18000/49000 loss: 0.3147398645234353\n",
      "20000/49000 loss: 0.2016075925400145\n",
      "22000/49000 loss: 0.24397256980287263\n",
      "24000/49000 loss: 0.20769476571182896\n",
      "26000/49000 loss: 0.24700794148040012\n",
      "28000/49000 loss: 0.20929996275051418\n",
      "30000/49000 loss: 0.2469995673768209\n",
      "32000/49000 loss: 0.28795993578195905\n",
      "34000/49000 loss: 0.25103417013993246\n",
      "36000/49000 loss: 0.2568507351979574\n",
      "38000/49000 loss: 0.2929306742831285\n",
      "40000/49000 loss: 0.23517679357213858\n",
      "42000/49000 loss: 0.24510362927539472\n",
      "44000/49000 loss: 0.2488213472449332\n",
      "46000/49000 loss: 0.4331909930939426\n",
      "48000/49000 loss: 0.24505923639310556\n",
      "epoch 42: valid acc = 0.884, new learning rate = 5.799111065000274e-05\n",
      "2000/49000 loss: 0.2579169887034676\n",
      "4000/49000 loss: 0.18966173083275986\n",
      "6000/49000 loss: 0.2825742628046105\n",
      "8000/49000 loss: 0.29786580646571564\n",
      "10000/49000 loss: 0.26637481669407276\n",
      "12000/49000 loss: 0.24270469973515696\n",
      "14000/49000 loss: 0.295499488535662\n",
      "16000/49000 loss: 0.31442593883722575\n",
      "18000/49000 loss: 0.2492784302080682\n",
      "20000/49000 loss: 0.2387706705052719\n",
      "22000/49000 loss: 0.2018585999040395\n",
      "24000/49000 loss: 0.22606875753419922\n",
      "26000/49000 loss: 0.33062906217500954\n",
      "28000/49000 loss: 0.30472644813929095\n",
      "30000/49000 loss: 0.2950554168624599\n",
      "32000/49000 loss: 0.2574552229687339\n",
      "34000/49000 loss: 0.210575110050351\n",
      "36000/49000 loss: 0.3284456077225457\n",
      "38000/49000 loss: 0.3099706326703953\n",
      "40000/49000 loss: 0.27894037477722083\n",
      "42000/49000 loss: 0.3084179071346922\n",
      "44000/49000 loss: 0.3582530162895445\n",
      "46000/49000 loss: 0.2838998287659086\n",
      "48000/49000 loss: 0.2837535423669591\n",
      "epoch 43: valid acc = 0.89, new learning rate = 5.5091555117502596e-05\n",
      "2000/49000 loss: 0.2087966045894155\n",
      "4000/49000 loss: 0.2758780774741485\n",
      "6000/49000 loss: 0.21684204476752977\n",
      "8000/49000 loss: 0.259204060195614\n",
      "10000/49000 loss: 0.26494953704848717\n",
      "12000/49000 loss: 0.26358790985674196\n",
      "14000/49000 loss: 0.23821291286199636\n",
      "16000/49000 loss: 0.3692510235561923\n",
      "18000/49000 loss: 0.264539470239074\n",
      "20000/49000 loss: 0.24641361603736248\n",
      "22000/49000 loss: 0.36225413405454265\n",
      "24000/49000 loss: 0.32708245739890757\n",
      "26000/49000 loss: 0.25619431760267675\n",
      "28000/49000 loss: 0.26776969392114847\n",
      "30000/49000 loss: 0.3106928064180295\n",
      "32000/49000 loss: 0.32103042045790225\n",
      "34000/49000 loss: 0.2379118451626989\n",
      "36000/49000 loss: 0.27509784783288865\n",
      "38000/49000 loss: 0.33395873884030347\n",
      "40000/49000 loss: 0.3584897959728325\n",
      "42000/49000 loss: 0.29459050772292256\n",
      "44000/49000 loss: 0.25894635504614333\n",
      "46000/49000 loss: 0.2509853279347384\n",
      "48000/49000 loss: 0.24830039526233416\n",
      "epoch 44: valid acc = 0.887, new learning rate = 5.2336977361627463e-05\n",
      "2000/49000 loss: 0.2743269727229654\n",
      "4000/49000 loss: 0.2965435807809797\n",
      "6000/49000 loss: 0.25912068563276214\n",
      "8000/49000 loss: 0.19586405319688902\n",
      "10000/49000 loss: 0.28675719877487543\n",
      "12000/49000 loss: 0.22675397085721857\n",
      "14000/49000 loss: 0.25384134923174473\n",
      "16000/49000 loss: 0.23916671711538345\n",
      "18000/49000 loss: 0.23191438790373342\n",
      "20000/49000 loss: 0.29653878357848906\n",
      "22000/49000 loss: 0.26137722892152915\n",
      "24000/49000 loss: 0.20828266045571792\n",
      "26000/49000 loss: 0.25287844957229133\n",
      "28000/49000 loss: 0.21474554078012803\n",
      "30000/49000 loss: 0.19357053178491596\n",
      "32000/49000 loss: 0.19187962242891804\n",
      "34000/49000 loss: 0.24522880453784207\n",
      "36000/49000 loss: 0.27519123248909094\n",
      "38000/49000 loss: 0.25090445583130305\n",
      "40000/49000 loss: 0.2040332441509687\n",
      "42000/49000 loss: 0.2909752818127313\n",
      "44000/49000 loss: 0.2917437985090227\n",
      "46000/49000 loss: 0.2522006456010168\n",
      "48000/49000 loss: 0.2490468582471299\n",
      "epoch 45: valid acc = 0.887, new learning rate = 4.972012849354609e-05\n",
      "2000/49000 loss: 0.29621737737578\n",
      "4000/49000 loss: 0.2541958851674877\n",
      "6000/49000 loss: 0.25492893381489345\n",
      "8000/49000 loss: 0.25819753327136713\n",
      "10000/49000 loss: 0.25576438561513265\n",
      "12000/49000 loss: 0.3372983725511091\n",
      "14000/49000 loss: 0.3051180633828778\n",
      "16000/49000 loss: 0.30832483868354654\n",
      "18000/49000 loss: 0.30492689322651617\n",
      "20000/49000 loss: 0.2912790906719688\n",
      "22000/49000 loss: 0.3622814884752728\n",
      "24000/49000 loss: 0.3243213280076597\n",
      "26000/49000 loss: 0.29881467137020923\n",
      "28000/49000 loss: 0.36554649704254266\n",
      "30000/49000 loss: 0.2641953265336648\n",
      "32000/49000 loss: 0.26159928390940856\n",
      "34000/49000 loss: 0.338943004510423\n",
      "36000/49000 loss: 0.25590809107296236\n",
      "38000/49000 loss: 0.24474902706703358\n",
      "40000/49000 loss: 0.24557998717970236\n",
      "42000/49000 loss: 0.2676180519263276\n",
      "44000/49000 loss: 0.2724088245171738\n",
      "46000/49000 loss: 0.31430831262386566\n",
      "48000/49000 loss: 0.2915686907914135\n",
      "epoch 46: valid acc = 0.892, new learning rate = 4.723412206886878e-05\n",
      "2000/49000 loss: 0.3101302641063992\n",
      "4000/49000 loss: 0.2500651449697311\n",
      "6000/49000 loss: 0.25493324035788123\n",
      "8000/49000 loss: 0.32997061893415347\n",
      "10000/49000 loss: 0.3045532427300498\n",
      "12000/49000 loss: 0.24990195735462492\n",
      "14000/49000 loss: 0.32255685286854385\n",
      "16000/49000 loss: 0.3050192527639918\n",
      "18000/49000 loss: 0.2933386515630453\n",
      "20000/49000 loss: 0.2818493555104989\n",
      "22000/49000 loss: 0.3062674325742764\n",
      "24000/49000 loss: 0.28773230400881206\n",
      "26000/49000 loss: 0.29868444329756677\n",
      "28000/49000 loss: 0.2610348226061543\n",
      "30000/49000 loss: 0.2362276209413336\n",
      "32000/49000 loss: 0.22351252107561498\n",
      "34000/49000 loss: 0.2515397452547698\n",
      "36000/49000 loss: 0.28509426629370904\n",
      "38000/49000 loss: 0.291314081962318\n",
      "40000/49000 loss: 0.25328974679793315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/49000 loss: 0.30573410540140383\n",
      "44000/49000 loss: 0.2898131877931959\n",
      "46000/49000 loss: 0.3251690208676972\n",
      "48000/49000 loss: 0.30001148483501633\n",
      "epoch 47: valid acc = 0.888, new learning rate = 4.487241596542534e-05\n",
      "2000/49000 loss: 0.29074830043922406\n",
      "4000/49000 loss: 0.30056158549333467\n",
      "6000/49000 loss: 0.24753643097390696\n",
      "8000/49000 loss: 0.3113508045659259\n",
      "10000/49000 loss: 0.25465956181324595\n",
      "12000/49000 loss: 0.21796766387534552\n",
      "14000/49000 loss: 0.2828107737373458\n",
      "16000/49000 loss: 0.23720241992380878\n",
      "18000/49000 loss: 0.2110219151653517\n",
      "20000/49000 loss: 0.2810387437480366\n",
      "22000/49000 loss: 0.2550501393467567\n",
      "24000/49000 loss: 0.2903801511577195\n",
      "26000/49000 loss: 0.2159211454611072\n",
      "28000/49000 loss: 0.29409174549886874\n",
      "30000/49000 loss: 0.26854644971738495\n",
      "32000/49000 loss: 0.2463005836554711\n",
      "34000/49000 loss: 0.24797195119493598\n",
      "36000/49000 loss: 0.24597376746924937\n",
      "38000/49000 loss: 0.2826758635250489\n",
      "40000/49000 loss: 0.2766528795065579\n",
      "42000/49000 loss: 0.3798896882454604\n",
      "44000/49000 loss: 0.238326075377432\n",
      "46000/49000 loss: 0.336623487868891\n",
      "48000/49000 loss: 0.28967795019625797\n",
      "epoch 48: valid acc = 0.886, new learning rate = 4.262879516715407e-05\n",
      "2000/49000 loss: 0.29925252271423974\n",
      "4000/49000 loss: 0.23974939069307555\n",
      "6000/49000 loss: 0.24053405430508948\n",
      "8000/49000 loss: 0.26041281298224167\n",
      "10000/49000 loss: 0.2554412151318896\n",
      "12000/49000 loss: 0.24634118047590034\n",
      "14000/49000 loss: 0.26013938426589683\n",
      "16000/49000 loss: 0.2804222644177956\n",
      "18000/49000 loss: 0.2910773116066983\n",
      "20000/49000 loss: 0.3028177172491376\n",
      "22000/49000 loss: 0.27256379349542775\n",
      "24000/49000 loss: 0.26019286751176374\n",
      "26000/49000 loss: 0.2864249401461799\n",
      "28000/49000 loss: 0.21973691035047127\n",
      "30000/49000 loss: 0.22549506915874976\n",
      "32000/49000 loss: 0.26179291391539594\n",
      "34000/49000 loss: 0.2409303454493432\n",
      "36000/49000 loss: 0.22092227096786476\n",
      "38000/49000 loss: 0.24592673581175115\n",
      "40000/49000 loss: 0.2943525951638793\n",
      "42000/49000 loss: 0.2784989260259306\n",
      "44000/49000 loss: 0.3487752680534466\n",
      "46000/49000 loss: 0.21261481525832365\n",
      "48000/49000 loss: 0.3845377063377148\n",
      "epoch 49: valid acc = 0.887, new learning rate = 4.049735540879637e-05\n",
      "2000/49000 loss: 0.21980579611797996\n",
      "4000/49000 loss: 0.18900896822899052\n",
      "6000/49000 loss: 0.22605174935317632\n",
      "8000/49000 loss: 0.19287411067400217\n",
      "10000/49000 loss: 0.34461520263743634\n",
      "12000/49000 loss: 0.33210917479571517\n",
      "14000/49000 loss: 0.29650204471699754\n",
      "16000/49000 loss: 0.2729629851296895\n",
      "18000/49000 loss: 0.2939021598759541\n",
      "20000/49000 loss: 0.3606792153353064\n",
      "22000/49000 loss: 0.26861644843148313\n",
      "24000/49000 loss: 0.18908057505473527\n",
      "26000/49000 loss: 0.3438408890063868\n",
      "28000/49000 loss: 0.2596519325326516\n",
      "30000/49000 loss: 0.18814561940816904\n",
      "32000/49000 loss: 0.2285719493587972\n",
      "34000/49000 loss: 0.3024254413041352\n",
      "36000/49000 loss: 0.242337415853118\n",
      "38000/49000 loss: 0.30491005867858323\n",
      "40000/49000 loss: 0.2813046267447026\n",
      "42000/49000 loss: 0.3301209758344172\n",
      "44000/49000 loss: 0.2730228011328153\n",
      "46000/49000 loss: 0.3266871898612957\n",
      "48000/49000 loss: 0.24591348402449942\n",
      "epoch 50: valid acc = 0.891, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.9050612244897959\n",
      "test acc: 0.891\n",
      "test acc: 0.8725\n",
      "number of batches for training: 245\n",
      "epoch 1: valid acc = 0.745, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.808, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.836, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.84, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.851, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.862, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.865, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.868, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.87, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.874, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.874, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.876, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.884, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.879, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.884, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.886, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.884, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.883, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.89, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.887, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.889, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.887, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.89, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.889, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.887, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.892, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.89, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.888, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.891, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.884, new learning rate = 0.00010731938197146858\n",
      "epoch 31: valid acc = 0.885, new learning rate = 0.00010195341287289515\n",
      "epoch 32: valid acc = 0.887, new learning rate = 9.685574222925039e-05\n",
      "epoch 33: valid acc = 0.891, new learning rate = 9.201295511778786e-05\n",
      "epoch 34: valid acc = 0.884, new learning rate = 8.741230736189846e-05\n",
      "epoch 35: valid acc = 0.883, new learning rate = 8.304169199380353e-05\n",
      "epoch 36: valid acc = 0.882, new learning rate = 7.888960739411335e-05\n",
      "epoch 37: valid acc = 0.889, new learning rate = 7.494512702440768e-05\n",
      "epoch 38: valid acc = 0.885, new learning rate = 7.119787067318729e-05\n",
      "epoch 39: valid acc = 0.891, new learning rate = 6.763797713952792e-05\n",
      "epoch 40: valid acc = 0.888, new learning rate = 6.425607828255152e-05\n",
      "epoch 41: valid acc = 0.89, new learning rate = 6.104327436842394e-05\n",
      "epoch 42: valid acc = 0.884, new learning rate = 5.799111065000274e-05\n",
      "epoch 43: valid acc = 0.889, new learning rate = 5.5091555117502596e-05\n",
      "epoch 44: valid acc = 0.889, new learning rate = 5.2336977361627463e-05\n",
      "epoch 45: valid acc = 0.889, new learning rate = 4.972012849354609e-05\n",
      "epoch 46: valid acc = 0.889, new learning rate = 4.723412206886878e-05\n",
      "epoch 47: valid acc = 0.886, new learning rate = 4.487241596542534e-05\n",
      "epoch 48: valid acc = 0.887, new learning rate = 4.262879516715407e-05\n",
      "epoch 49: valid acc = 0.885, new learning rate = 4.049735540879637e-05\n",
      "epoch 50: valid acc = 0.886, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.9052448979591837\n",
      "test acc: 0.886\n",
      "test acc: 0.8726\n",
      "number of batches for training: 245\n",
      "2000/49000 loss: 4.768537465394634\n",
      "4000/49000 loss: 3.4192627973103265\n",
      "6000/49000 loss: 3.3981763724059677\n",
      "8000/49000 loss: 2.3780101593371095\n",
      "10000/49000 loss: 2.264940963181568\n",
      "12000/49000 loss: 2.0111688769293687\n",
      "14000/49000 loss: 1.843679404743134\n",
      "16000/49000 loss: 1.8050619174882345\n",
      "18000/49000 loss: 1.4429621316725385\n",
      "20000/49000 loss: 1.3894106502085122\n",
      "22000/49000 loss: 1.050409436571199\n",
      "24000/49000 loss: 1.0835252777215358\n",
      "26000/49000 loss: 1.216586936233339\n",
      "28000/49000 loss: 1.0483305393729732\n",
      "30000/49000 loss: 1.0533290541190499\n",
      "32000/49000 loss: 0.9178857189329561\n",
      "34000/49000 loss: 0.979219504664503\n",
      "36000/49000 loss: 0.7986247041311685\n",
      "38000/49000 loss: 1.0051063843508041\n",
      "40000/49000 loss: 0.7978090554444729\n",
      "42000/49000 loss: 0.871148589072456\n",
      "44000/49000 loss: 0.7472629910679323\n",
      "46000/49000 loss: 0.7298635437130568\n",
      "48000/49000 loss: 0.7575003545528429\n",
      "epoch 1: valid acc = 0.757, new learning rate = 0.000475\n",
      "2000/49000 loss: 0.6710770069408909\n",
      "4000/49000 loss: 0.6944078932917692\n",
      "6000/49000 loss: 0.629008804887092\n",
      "8000/49000 loss: 0.8627316840214472\n",
      "10000/49000 loss: 0.6294098704218266\n",
      "12000/49000 loss: 0.6205086670656642\n",
      "14000/49000 loss: 0.5652674920560818\n",
      "16000/49000 loss: 0.7747135931395153\n",
      "18000/49000 loss: 0.5564703944590905\n",
      "20000/49000 loss: 0.5346381240383521\n",
      "22000/49000 loss: 0.6460696489589772\n",
      "24000/49000 loss: 0.5740999096571361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26000/49000 loss: 0.6272654466610996\n",
      "28000/49000 loss: 0.6193800439090789\n",
      "30000/49000 loss: 0.5156673402575699\n",
      "32000/49000 loss: 0.5266699677602268\n",
      "34000/49000 loss: 0.5641182131927385\n",
      "36000/49000 loss: 0.4556072264103085\n",
      "38000/49000 loss: 0.5142395518985861\n",
      "40000/49000 loss: 0.5956228739314621\n",
      "42000/49000 loss: 0.5219678089269291\n",
      "44000/49000 loss: 0.6181214312883069\n",
      "46000/49000 loss: 0.601201673517029\n",
      "48000/49000 loss: 0.570849089779735\n",
      "epoch 2: valid acc = 0.796, new learning rate = 0.00045125\n",
      "2000/49000 loss: 0.4878453262653058\n",
      "4000/49000 loss: 0.5269950582683303\n",
      "6000/49000 loss: 0.4813891225305754\n",
      "8000/49000 loss: 0.5050728946983303\n",
      "10000/49000 loss: 0.5190128036888585\n",
      "12000/49000 loss: 0.5168489409896938\n",
      "14000/49000 loss: 0.44255446080097605\n",
      "16000/49000 loss: 0.5357630865872468\n",
      "18000/49000 loss: 0.5716514323317288\n",
      "20000/49000 loss: 0.5161259571079709\n",
      "22000/49000 loss: 0.39559941456674075\n",
      "24000/49000 loss: 0.46320654862160504\n",
      "26000/49000 loss: 0.5290981306335352\n",
      "28000/49000 loss: 0.5679561133756337\n",
      "30000/49000 loss: 0.43950244107910796\n",
      "32000/49000 loss: 0.6056900755902382\n",
      "34000/49000 loss: 0.46283061073268356\n",
      "36000/49000 loss: 0.593383351800116\n",
      "38000/49000 loss: 0.4666029592626504\n",
      "40000/49000 loss: 0.5426583737802718\n",
      "42000/49000 loss: 0.48928842855554966\n",
      "44000/49000 loss: 0.40576753041852576\n",
      "46000/49000 loss: 0.42978645751027494\n",
      "48000/49000 loss: 0.43951995888901196\n",
      "epoch 3: valid acc = 0.823, new learning rate = 0.0004286875\n",
      "2000/49000 loss: 0.4561869645129837\n",
      "4000/49000 loss: 0.5013333621175919\n",
      "6000/49000 loss: 0.40788921159555624\n",
      "8000/49000 loss: 0.4504081556834321\n",
      "10000/49000 loss: 0.5264907828687723\n",
      "12000/49000 loss: 0.44799759455030946\n",
      "14000/49000 loss: 0.34452979594950617\n",
      "16000/49000 loss: 0.43357807626493977\n",
      "18000/49000 loss: 0.5808661405825009\n",
      "20000/49000 loss: 0.47718516020731755\n",
      "22000/49000 loss: 0.39492001407560867\n",
      "24000/49000 loss: 0.42176052875337605\n",
      "26000/49000 loss: 0.51917850026482\n",
      "28000/49000 loss: 0.44307955149496264\n",
      "30000/49000 loss: 0.4454034778743338\n",
      "32000/49000 loss: 0.48516926733541815\n",
      "34000/49000 loss: 0.4742414057721468\n",
      "36000/49000 loss: 0.43781908622690013\n",
      "38000/49000 loss: 0.5023550387968381\n",
      "40000/49000 loss: 0.5200455686119551\n",
      "42000/49000 loss: 0.5229121415999671\n",
      "44000/49000 loss: 0.5354863868193183\n",
      "46000/49000 loss: 0.4854753601164548\n",
      "48000/49000 loss: 0.4400989624079223\n",
      "epoch 4: valid acc = 0.836, new learning rate = 0.00040725312499999993\n",
      "2000/49000 loss: 0.3887968603136943\n",
      "4000/49000 loss: 0.4973785963021354\n",
      "6000/49000 loss: 0.3783199730737752\n",
      "8000/49000 loss: 0.3559161454949936\n",
      "10000/49000 loss: 0.4602662071936348\n",
      "12000/49000 loss: 0.4917360931432225\n",
      "14000/49000 loss: 0.3574122361052472\n",
      "16000/49000 loss: 0.3786434049343234\n",
      "18000/49000 loss: 0.44930614482791276\n",
      "20000/49000 loss: 0.420154708568263\n",
      "22000/49000 loss: 0.35153877632131353\n",
      "24000/49000 loss: 0.5346173564557108\n",
      "26000/49000 loss: 0.4557715951209485\n",
      "28000/49000 loss: 0.35730343434179596\n",
      "30000/49000 loss: 0.4220808539195835\n",
      "32000/49000 loss: 0.5161990364714216\n",
      "34000/49000 loss: 0.40787939399138684\n",
      "36000/49000 loss: 0.3701409548904285\n",
      "38000/49000 loss: 0.4107881803701845\n",
      "40000/49000 loss: 0.35243223039632204\n",
      "42000/49000 loss: 0.3899755092115505\n",
      "44000/49000 loss: 0.5055393110723951\n",
      "46000/49000 loss: 0.51584669209922\n",
      "48000/49000 loss: 0.40647314236653137\n",
      "epoch 5: valid acc = 0.857, new learning rate = 0.0003868904687499999\n",
      "2000/49000 loss: 0.5058106537589779\n",
      "4000/49000 loss: 0.5058149266104391\n",
      "6000/49000 loss: 0.39866542437409247\n",
      "8000/49000 loss: 0.4350594318704345\n",
      "10000/49000 loss: 0.3541138654163782\n",
      "12000/49000 loss: 0.3101654080943195\n",
      "14000/49000 loss: 0.43673666519088356\n",
      "16000/49000 loss: 0.36049791741176435\n",
      "18000/49000 loss: 0.3708159105405524\n",
      "20000/49000 loss: 0.42441787090244415\n",
      "22000/49000 loss: 0.45092638885995046\n",
      "24000/49000 loss: 0.39762316440216977\n",
      "26000/49000 loss: 0.36431997275338796\n",
      "28000/49000 loss: 0.3555429153846991\n",
      "30000/49000 loss: 0.3948446703902178\n",
      "32000/49000 loss: 0.44582793619074684\n",
      "34000/49000 loss: 0.3941498518198943\n",
      "36000/49000 loss: 0.47614066168619273\n",
      "38000/49000 loss: 0.38969798036469155\n",
      "40000/49000 loss: 0.4430539723592806\n",
      "42000/49000 loss: 0.3934683040006731\n",
      "44000/49000 loss: 0.4062214196622744\n",
      "46000/49000 loss: 0.2861673928382147\n",
      "48000/49000 loss: 0.4024242160854558\n",
      "epoch 6: valid acc = 0.862, new learning rate = 0.0003675459453124999\n",
      "2000/49000 loss: 0.3013474357286687\n",
      "4000/49000 loss: 0.36340114058835915\n",
      "6000/49000 loss: 0.4067380502966444\n",
      "8000/49000 loss: 0.3909471143850093\n",
      "10000/49000 loss: 0.35500442854767933\n",
      "12000/49000 loss: 0.36503449592120824\n",
      "14000/49000 loss: 0.41614497442980797\n",
      "16000/49000 loss: 0.3552611769840074\n",
      "18000/49000 loss: 0.32583876571079723\n",
      "20000/49000 loss: 0.387562486885779\n",
      "22000/49000 loss: 0.3983211221281699\n",
      "24000/49000 loss: 0.3834957872472206\n",
      "26000/49000 loss: 0.40300330570338694\n",
      "28000/49000 loss: 0.3962435613032533\n",
      "30000/49000 loss: 0.3300005362429522\n",
      "32000/49000 loss: 0.3433572297037635\n",
      "34000/49000 loss: 0.40725014431176254\n",
      "36000/49000 loss: 0.37699527607226757\n",
      "38000/49000 loss: 0.4303674114703195\n",
      "40000/49000 loss: 0.38122376897989424\n",
      "42000/49000 loss: 0.41774824665620086\n",
      "44000/49000 loss: 0.3730032339711138\n",
      "46000/49000 loss: 0.39429563335201584\n",
      "48000/49000 loss: 0.4690912175766275\n",
      "epoch 7: valid acc = 0.862, new learning rate = 0.00034916864804687486\n",
      "2000/49000 loss: 0.37966765932527574\n",
      "4000/49000 loss: 0.39701363690172414\n",
      "6000/49000 loss: 0.3232278671193371\n",
      "8000/49000 loss: 0.42077402777596423\n",
      "10000/49000 loss: 0.4097827458978469\n",
      "12000/49000 loss: 0.3360988151161804\n",
      "14000/49000 loss: 0.43423538743531365\n",
      "16000/49000 loss: 0.44457175433762364\n",
      "18000/49000 loss: 0.37159473427689094\n",
      "20000/49000 loss: 0.3142308603890209\n",
      "22000/49000 loss: 0.4191668444573533\n",
      "24000/49000 loss: 0.3443582788529663\n",
      "26000/49000 loss: 0.3095900450027001\n",
      "28000/49000 loss: 0.3709708049859536\n",
      "30000/49000 loss: 0.37948171074022063\n",
      "32000/49000 loss: 0.3065945718551154\n",
      "34000/49000 loss: 0.3821516220019622\n",
      "36000/49000 loss: 0.3802741150478889\n",
      "38000/49000 loss: 0.3641292734419824\n",
      "40000/49000 loss: 0.3991504729718461\n",
      "42000/49000 loss: 0.453406711244485\n",
      "44000/49000 loss: 0.3562633758837255\n",
      "46000/49000 loss: 0.2739427107924665\n",
      "48000/49000 loss: 0.3205691616226266\n",
      "epoch 8: valid acc = 0.865, new learning rate = 0.0003317102156445311\n",
      "2000/49000 loss: 0.32215548143784906\n",
      "4000/49000 loss: 0.42645749993179677\n",
      "6000/49000 loss: 0.4155181185798274\n",
      "8000/49000 loss: 0.3251779824818305\n",
      "10000/49000 loss: 0.41214586411469323\n",
      "12000/49000 loss: 0.3861423629484134\n",
      "14000/49000 loss: 0.41133963160927667\n",
      "16000/49000 loss: 0.34156797396010635\n",
      "18000/49000 loss: 0.34526280603480486\n",
      "20000/49000 loss: 0.37358803450777633\n",
      "22000/49000 loss: 0.36376839817262974\n",
      "24000/49000 loss: 0.36470012441535443\n",
      "26000/49000 loss: 0.3743929489767129\n",
      "28000/49000 loss: 0.370782170047385\n",
      "30000/49000 loss: 0.42752935571196554\n",
      "32000/49000 loss: 0.3892479408620813\n",
      "34000/49000 loss: 0.3618536462212083\n",
      "36000/49000 loss: 0.38013985192939903\n",
      "38000/49000 loss: 0.3693246842360531\n",
      "40000/49000 loss: 0.362339619989688\n",
      "42000/49000 loss: 0.2965432184476125\n",
      "44000/49000 loss: 0.3084301503948849\n",
      "46000/49000 loss: 0.4419742030246093\n",
      "48000/49000 loss: 0.3377051450763426\n",
      "epoch 9: valid acc = 0.876, new learning rate = 0.0003151247048623045\n",
      "2000/49000 loss: 0.39697807776360433\n",
      "4000/49000 loss: 0.3092964350569339\n",
      "6000/49000 loss: 0.2859602206445042\n",
      "8000/49000 loss: 0.3781487469841671\n",
      "10000/49000 loss: 0.33143628190844476\n",
      "12000/49000 loss: 0.31311268521196334\n",
      "14000/49000 loss: 0.33109374232786076\n",
      "16000/49000 loss: 0.3406852181707959\n",
      "18000/49000 loss: 0.35503810830848814\n",
      "20000/49000 loss: 0.3800479599456822\n",
      "22000/49000 loss: 0.35976973616448915\n",
      "24000/49000 loss: 0.38968223809799557\n",
      "26000/49000 loss: 0.44137161238669104\n",
      "28000/49000 loss: 0.2198063963493666\n",
      "30000/49000 loss: 0.31485721101935366\n",
      "32000/49000 loss: 0.3218760283615366\n",
      "34000/49000 loss: 0.32293512781186484\n",
      "36000/49000 loss: 0.3018567472553876\n",
      "38000/49000 loss: 0.3022783702189374\n",
      "40000/49000 loss: 0.3703401010260572\n",
      "42000/49000 loss: 0.4404073310330724\n",
      "44000/49000 loss: 0.35784984578652146\n",
      "46000/49000 loss: 0.2722509052291009\n",
      "48000/49000 loss: 0.29397917434946064\n",
      "epoch 10: valid acc = 0.875, new learning rate = 0.00029936846961918924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/49000 loss: 0.3594594656470996\n",
      "4000/49000 loss: 0.33920498184479164\n",
      "6000/49000 loss: 0.3038090051588225\n",
      "8000/49000 loss: 0.31164119049228045\n",
      "10000/49000 loss: 0.3194459157719155\n",
      "12000/49000 loss: 0.3165997442320498\n",
      "14000/49000 loss: 0.26246330569957277\n",
      "16000/49000 loss: 0.32111038587063445\n",
      "18000/49000 loss: 0.28528234264397273\n",
      "20000/49000 loss: 0.3691742018142676\n",
      "22000/49000 loss: 0.3115040933889224\n",
      "24000/49000 loss: 0.37228093075768787\n",
      "26000/49000 loss: 0.3586883788892647\n",
      "28000/49000 loss: 0.35433579492863887\n",
      "30000/49000 loss: 0.4539168946807011\n",
      "32000/49000 loss: 0.3806017831920114\n",
      "34000/49000 loss: 0.40461756106688007\n",
      "36000/49000 loss: 0.3106464149745108\n",
      "38000/49000 loss: 0.27497306107174957\n",
      "40000/49000 loss: 0.3744271698897468\n",
      "42000/49000 loss: 0.34409072895626097\n",
      "44000/49000 loss: 0.3286637352466017\n",
      "46000/49000 loss: 0.40284507680743065\n",
      "48000/49000 loss: 0.33788760877770363\n",
      "epoch 11: valid acc = 0.873, new learning rate = 0.00028440004613822977\n",
      "2000/49000 loss: 0.3075614534401995\n",
      "4000/49000 loss: 0.32699336758755426\n",
      "6000/49000 loss: 0.3225364271946618\n",
      "8000/49000 loss: 0.35901941361534295\n",
      "10000/49000 loss: 0.43719492301023327\n",
      "12000/49000 loss: 0.37034245603850585\n",
      "14000/49000 loss: 0.5392098363087902\n",
      "16000/49000 loss: 0.3327428581316055\n",
      "18000/49000 loss: 0.3037396295686081\n",
      "20000/49000 loss: 0.349188141109023\n",
      "22000/49000 loss: 0.3231376942960301\n",
      "24000/49000 loss: 0.35894703024898417\n",
      "26000/49000 loss: 0.38299513759536263\n",
      "28000/49000 loss: 0.3531588756558446\n",
      "30000/49000 loss: 0.4122464888787752\n",
      "32000/49000 loss: 0.31644549692869245\n",
      "34000/49000 loss: 0.38695000092193826\n",
      "36000/49000 loss: 0.2915868648806648\n",
      "38000/49000 loss: 0.3270226715887857\n",
      "40000/49000 loss: 0.41902305121501887\n",
      "42000/49000 loss: 0.33407847997100115\n",
      "44000/49000 loss: 0.28193593325187444\n",
      "46000/49000 loss: 0.3308951148368717\n",
      "48000/49000 loss: 0.3814819276284135\n",
      "epoch 12: valid acc = 0.881, new learning rate = 0.00027018004383131826\n",
      "2000/49000 loss: 0.40682608447289514\n",
      "4000/49000 loss: 0.3574384302490082\n",
      "6000/49000 loss: 0.428987578624531\n",
      "8000/49000 loss: 0.3542118045410092\n",
      "10000/49000 loss: 0.43924175731132964\n",
      "12000/49000 loss: 0.3421532791269884\n",
      "14000/49000 loss: 0.2654648671898617\n",
      "16000/49000 loss: 0.3144066106430469\n",
      "18000/49000 loss: 0.26431725927026767\n",
      "20000/49000 loss: 0.30297044748161706\n",
      "22000/49000 loss: 0.31271185690230147\n",
      "24000/49000 loss: 0.26353508869751624\n",
      "26000/49000 loss: 0.2792429339080223\n",
      "28000/49000 loss: 0.4354391902484975\n",
      "30000/49000 loss: 0.3962424652783405\n",
      "32000/49000 loss: 0.366818120082148\n",
      "34000/49000 loss: 0.3173874090031688\n",
      "36000/49000 loss: 0.3558902736481375\n",
      "38000/49000 loss: 0.3805249232441939\n",
      "40000/49000 loss: 0.31826148030581713\n",
      "42000/49000 loss: 0.3602433410967801\n",
      "44000/49000 loss: 0.2777335746327924\n",
      "46000/49000 loss: 0.41765800678579074\n",
      "48000/49000 loss: 0.2829287063174282\n",
      "epoch 13: valid acc = 0.873, new learning rate = 0.00025667104163975234\n",
      "2000/49000 loss: 0.36470783081151886\n",
      "4000/49000 loss: 0.4599580372679847\n",
      "6000/49000 loss: 0.27020396277109465\n",
      "8000/49000 loss: 0.38613351158347514\n",
      "10000/49000 loss: 0.3372029479885066\n",
      "12000/49000 loss: 0.3496362228818476\n",
      "14000/49000 loss: 0.3162116648619142\n",
      "16000/49000 loss: 0.3825030660501284\n",
      "18000/49000 loss: 0.2943367502478535\n",
      "20000/49000 loss: 0.3515070070263738\n",
      "22000/49000 loss: 0.25316654396226973\n",
      "24000/49000 loss: 0.303382226879446\n",
      "26000/49000 loss: 0.35690396392874174\n",
      "28000/49000 loss: 0.2609040530459241\n",
      "30000/49000 loss: 0.3231766794337608\n",
      "32000/49000 loss: 0.49277849702263704\n",
      "34000/49000 loss: 0.2624926457494998\n",
      "36000/49000 loss: 0.3357883354701173\n",
      "38000/49000 loss: 0.3499263399734732\n",
      "40000/49000 loss: 0.32025991960111644\n",
      "42000/49000 loss: 0.2964835570523533\n",
      "44000/49000 loss: 0.2804816632553349\n",
      "46000/49000 loss: 0.42311860323074757\n",
      "48000/49000 loss: 0.41145777568823294\n",
      "epoch 14: valid acc = 0.881, new learning rate = 0.00024383748955776472\n",
      "2000/49000 loss: 0.404481837533361\n",
      "4000/49000 loss: 0.26862402072142655\n",
      "6000/49000 loss: 0.33307022261907976\n",
      "8000/49000 loss: 0.3598596268538876\n",
      "10000/49000 loss: 0.24719165437463583\n",
      "12000/49000 loss: 0.3042160408943657\n",
      "14000/49000 loss: 0.30206750142478866\n",
      "16000/49000 loss: 0.3748161637441146\n",
      "18000/49000 loss: 0.3308433880727438\n",
      "20000/49000 loss: 0.35241133346044995\n",
      "22000/49000 loss: 0.3712029380631658\n",
      "24000/49000 loss: 0.30490576438888856\n",
      "26000/49000 loss: 0.3194689349740102\n",
      "28000/49000 loss: 0.23195333327848947\n",
      "30000/49000 loss: 0.20699811199484835\n",
      "32000/49000 loss: 0.3112670659540532\n",
      "34000/49000 loss: 0.29063107430806606\n",
      "36000/49000 loss: 0.3001018141293549\n",
      "38000/49000 loss: 0.35406801784514097\n",
      "40000/49000 loss: 0.35259678145654005\n",
      "42000/49000 loss: 0.3843684412877126\n",
      "44000/49000 loss: 0.2466757291166294\n",
      "46000/49000 loss: 0.3651575004015827\n",
      "48000/49000 loss: 0.24551838124622405\n",
      "epoch 15: valid acc = 0.879, new learning rate = 0.00023164561507987649\n",
      "2000/49000 loss: 0.3321034773795232\n",
      "4000/49000 loss: 0.27708228649075406\n",
      "6000/49000 loss: 0.3043724317176667\n",
      "8000/49000 loss: 0.28201932151760806\n",
      "10000/49000 loss: 0.2450967880352345\n",
      "12000/49000 loss: 0.2978216871449734\n",
      "14000/49000 loss: 0.4245787993657839\n",
      "16000/49000 loss: 0.3725048818343251\n",
      "18000/49000 loss: 0.31933930452424847\n",
      "20000/49000 loss: 0.25050281619180104\n",
      "22000/49000 loss: 0.3303114314380889\n",
      "24000/49000 loss: 0.3102206130426141\n",
      "26000/49000 loss: 0.3732967325894611\n",
      "28000/49000 loss: 0.3474587862491754\n",
      "30000/49000 loss: 0.3787584153577387\n",
      "32000/49000 loss: 0.33923926608301075\n",
      "34000/49000 loss: 0.3623286944670622\n",
      "36000/49000 loss: 0.3454224533647756\n",
      "38000/49000 loss: 0.27934456582484474\n",
      "40000/49000 loss: 0.2941268838901065\n",
      "42000/49000 loss: 0.39070326850725806\n",
      "44000/49000 loss: 0.32788083162380105\n",
      "46000/49000 loss: 0.30291453721340117\n",
      "48000/49000 loss: 0.2576145625598483\n",
      "epoch 16: valid acc = 0.876, new learning rate = 0.00022006333432588265\n",
      "2000/49000 loss: 0.28443817637698193\n",
      "4000/49000 loss: 0.26231607502251597\n",
      "6000/49000 loss: 0.3355951894344944\n",
      "8000/49000 loss: 0.36178933799818985\n",
      "10000/49000 loss: 0.2985640793095101\n",
      "12000/49000 loss: 0.32564266831439975\n",
      "14000/49000 loss: 0.31190664889940634\n",
      "16000/49000 loss: 0.2850528233494237\n",
      "18000/49000 loss: 0.2953203553730168\n",
      "20000/49000 loss: 0.28339267749747027\n",
      "22000/49000 loss: 0.3216319812478458\n",
      "24000/49000 loss: 0.29776314552243394\n",
      "26000/49000 loss: 0.2584058700156803\n",
      "28000/49000 loss: 0.31457391046224453\n",
      "30000/49000 loss: 0.26939196781154195\n",
      "32000/49000 loss: 0.3003297355679493\n",
      "34000/49000 loss: 0.31861977216135334\n",
      "36000/49000 loss: 0.3112622016555312\n",
      "38000/49000 loss: 0.26786816022240667\n",
      "40000/49000 loss: 0.3735536853238545\n",
      "42000/49000 loss: 0.31435669752270173\n",
      "44000/49000 loss: 0.330109299393567\n",
      "46000/49000 loss: 0.298553969547105\n",
      "48000/49000 loss: 0.33722578293083594\n",
      "epoch 17: valid acc = 0.876, new learning rate = 0.00020906016760958852\n",
      "2000/49000 loss: 0.44300988282362125\n",
      "4000/49000 loss: 0.34745677340657954\n",
      "6000/49000 loss: 0.2982159612194743\n",
      "8000/49000 loss: 0.32572328945275963\n",
      "10000/49000 loss: 0.3096175016102902\n",
      "12000/49000 loss: 0.3330052698559541\n",
      "14000/49000 loss: 0.2632057567266124\n",
      "16000/49000 loss: 0.23354410998045189\n",
      "18000/49000 loss: 0.332854207660715\n",
      "20000/49000 loss: 0.34974565352094733\n",
      "22000/49000 loss: 0.31896337806449754\n",
      "24000/49000 loss: 0.23376922800998587\n",
      "26000/49000 loss: 0.28967353148652375\n",
      "28000/49000 loss: 0.31839144120933033\n",
      "30000/49000 loss: 0.3811573329948045\n",
      "32000/49000 loss: 0.28664075827656177\n",
      "34000/49000 loss: 0.338386049856863\n",
      "36000/49000 loss: 0.27320332514853185\n",
      "38000/49000 loss: 0.23966145539350317\n",
      "40000/49000 loss: 0.2791206689491069\n",
      "42000/49000 loss: 0.3204752038550723\n",
      "44000/49000 loss: 0.32739989183497065\n",
      "46000/49000 loss: 0.35276402683164065\n",
      "48000/49000 loss: 0.3459305700183358\n",
      "epoch 18: valid acc = 0.883, new learning rate = 0.00019860715922910907\n",
      "2000/49000 loss: 0.35617528928310094\n",
      "4000/49000 loss: 0.2889861651881071\n",
      "6000/49000 loss: 0.2563170548312127\n",
      "8000/49000 loss: 0.40685799627915537\n",
      "10000/49000 loss: 0.31150952254518877\n",
      "12000/49000 loss: 0.42989056889582017\n",
      "14000/49000 loss: 0.23843408704926075\n",
      "16000/49000 loss: 0.25315469727743184\n",
      "18000/49000 loss: 0.3050680727044088\n",
      "20000/49000 loss: 0.28672040602073084\n",
      "22000/49000 loss: 0.24688228363597448\n",
      "24000/49000 loss: 0.2662870798265237\n",
      "26000/49000 loss: 0.27676022640721043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28000/49000 loss: 0.2155275371900978\n",
      "30000/49000 loss: 0.3259759756277408\n",
      "32000/49000 loss: 0.3096778359220203\n",
      "34000/49000 loss: 0.2691113143605844\n",
      "36000/49000 loss: 0.28082541546413636\n",
      "38000/49000 loss: 0.27391175987761435\n",
      "40000/49000 loss: 0.23586281580947935\n",
      "42000/49000 loss: 0.416918645155369\n",
      "44000/49000 loss: 0.34388717902166877\n",
      "46000/49000 loss: 0.43312522479012056\n",
      "48000/49000 loss: 0.350251050443546\n",
      "epoch 19: valid acc = 0.878, new learning rate = 0.0001886768012676536\n",
      "2000/49000 loss: 0.3219706018433202\n",
      "4000/49000 loss: 0.2743425109863972\n",
      "6000/49000 loss: 0.34904386856034575\n",
      "8000/49000 loss: 0.31474816348561124\n",
      "10000/49000 loss: 0.3673232246068526\n",
      "12000/49000 loss: 0.24399212293385628\n",
      "14000/49000 loss: 0.40561203138285373\n",
      "16000/49000 loss: 0.2256952139459672\n",
      "18000/49000 loss: 0.3361245293601844\n",
      "20000/49000 loss: 0.3696825592361884\n",
      "22000/49000 loss: 0.37283356974565085\n",
      "24000/49000 loss: 0.3349905713754937\n",
      "26000/49000 loss: 0.399623419948343\n",
      "28000/49000 loss: 0.27252839531691825\n",
      "30000/49000 loss: 0.3493939565664606\n",
      "32000/49000 loss: 0.3273623593888536\n",
      "34000/49000 loss: 0.2736150375096649\n",
      "36000/49000 loss: 0.2649349395667849\n",
      "38000/49000 loss: 0.2732612413955271\n",
      "40000/49000 loss: 0.2400049201278381\n",
      "42000/49000 loss: 0.2860627120759092\n",
      "44000/49000 loss: 0.26749892420011495\n",
      "46000/49000 loss: 0.3203789683712634\n",
      "48000/49000 loss: 0.31700698503939145\n",
      "epoch 20: valid acc = 0.879, new learning rate = 0.0001792429612042709\n",
      "2000/49000 loss: 0.3178996782929035\n",
      "4000/49000 loss: 0.29417145411562545\n",
      "6000/49000 loss: 0.4228264133769798\n",
      "8000/49000 loss: 0.1704934075877549\n",
      "10000/49000 loss: 0.29292008681508563\n",
      "12000/49000 loss: 0.27609158516144283\n",
      "14000/49000 loss: 0.29547648662363746\n",
      "16000/49000 loss: 0.25674290796007104\n",
      "18000/49000 loss: 0.29301773134282055\n",
      "20000/49000 loss: 0.2781415626565785\n",
      "22000/49000 loss: 0.2985519881923597\n",
      "24000/49000 loss: 0.337357458912401\n",
      "26000/49000 loss: 0.3395370176058889\n",
      "28000/49000 loss: 0.3510318695558166\n",
      "30000/49000 loss: 0.30018145671656016\n",
      "32000/49000 loss: 0.2789941687595035\n",
      "34000/49000 loss: 0.26202820688753853\n",
      "36000/49000 loss: 0.27426846360294405\n",
      "38000/49000 loss: 0.2801454206084333\n",
      "40000/49000 loss: 0.2788599882399346\n",
      "42000/49000 loss: 0.2254444333494652\n",
      "44000/49000 loss: 0.34727019207576404\n",
      "46000/49000 loss: 0.3634354283152873\n",
      "48000/49000 loss: 0.30596850134649545\n",
      "epoch 21: valid acc = 0.883, new learning rate = 0.00017028081314405735\n",
      "2000/49000 loss: 0.26553554992671996\n",
      "4000/49000 loss: 0.2432577103803429\n",
      "6000/49000 loss: 0.29479028171724375\n",
      "8000/49000 loss: 0.3641315477644414\n",
      "10000/49000 loss: 0.3310637929438688\n",
      "12000/49000 loss: 0.35981076896952385\n",
      "14000/49000 loss: 0.3454559982408817\n",
      "16000/49000 loss: 0.27203418335578\n",
      "18000/49000 loss: 0.25858609128166543\n",
      "20000/49000 loss: 0.32496138614997383\n",
      "22000/49000 loss: 0.24224555259431194\n",
      "24000/49000 loss: 0.28298410694644555\n",
      "26000/49000 loss: 0.26907078054827144\n",
      "28000/49000 loss: 0.3177243930982874\n",
      "30000/49000 loss: 0.26591047590140526\n",
      "32000/49000 loss: 0.3229740900024566\n",
      "34000/49000 loss: 0.2568657820170126\n",
      "36000/49000 loss: 0.3159139981142749\n",
      "38000/49000 loss: 0.3133258322142411\n",
      "40000/49000 loss: 0.2645461192556001\n",
      "42000/49000 loss: 0.32630763597411117\n",
      "44000/49000 loss: 0.40479176884545687\n",
      "46000/49000 loss: 0.32367563519914916\n",
      "48000/49000 loss: 0.23297050443355166\n",
      "epoch 22: valid acc = 0.872, new learning rate = 0.00016176677248685447\n",
      "2000/49000 loss: 0.3003015426199594\n",
      "4000/49000 loss: 0.3417725630906971\n",
      "6000/49000 loss: 0.2848092715442394\n",
      "8000/49000 loss: 0.3160625411818663\n",
      "10000/49000 loss: 0.2601837263945152\n",
      "12000/49000 loss: 0.30533580921105236\n",
      "14000/49000 loss: 0.3138052935436318\n",
      "16000/49000 loss: 0.3003276185496446\n",
      "18000/49000 loss: 0.39034185330457094\n",
      "20000/49000 loss: 0.28017088550753383\n",
      "22000/49000 loss: 0.27984181334764363\n",
      "24000/49000 loss: 0.3414757022034211\n",
      "26000/49000 loss: 0.2316304892741534\n",
      "28000/49000 loss: 0.25835368055691127\n",
      "30000/49000 loss: 0.35954361268166685\n",
      "32000/49000 loss: 0.2628343026361189\n",
      "34000/49000 loss: 0.3024398028479447\n",
      "36000/49000 loss: 0.3386357135878528\n",
      "38000/49000 loss: 0.2624535450079313\n",
      "40000/49000 loss: 0.31727000536282385\n",
      "42000/49000 loss: 0.2675325885885309\n",
      "44000/49000 loss: 0.2494212169879998\n",
      "46000/49000 loss: 0.2924537544892034\n",
      "48000/49000 loss: 0.27984063784075447\n",
      "epoch 23: valid acc = 0.885, new learning rate = 0.00015367843386251173\n",
      "2000/49000 loss: 0.22405694852714397\n",
      "4000/49000 loss: 0.27783264291758303\n",
      "6000/49000 loss: 0.2563064047210225\n",
      "8000/49000 loss: 0.25872883087799553\n",
      "10000/49000 loss: 0.30548816936638273\n",
      "12000/49000 loss: 0.24799601570096924\n",
      "14000/49000 loss: 0.27636019859992966\n",
      "16000/49000 loss: 0.3350699901602563\n",
      "18000/49000 loss: 0.2616746272883363\n",
      "20000/49000 loss: 0.2851654087623575\n",
      "22000/49000 loss: 0.2914001026816277\n",
      "24000/49000 loss: 0.23967693759578496\n",
      "26000/49000 loss: 0.3032250611599557\n",
      "28000/49000 loss: 0.2520505885952016\n",
      "30000/49000 loss: 0.3413216615669803\n",
      "32000/49000 loss: 0.29610195128167754\n",
      "34000/49000 loss: 0.26256647207986844\n",
      "36000/49000 loss: 0.2924250815859821\n",
      "38000/49000 loss: 0.3546464849772035\n",
      "40000/49000 loss: 0.2891935321383585\n",
      "42000/49000 loss: 0.2695885957229014\n",
      "44000/49000 loss: 0.2855073577893034\n",
      "46000/49000 loss: 0.2970324655963439\n",
      "48000/49000 loss: 0.28985493945404606\n",
      "epoch 24: valid acc = 0.877, new learning rate = 0.00014599451216938612\n",
      "2000/49000 loss: 0.40783611336347303\n",
      "4000/49000 loss: 0.34760848219132634\n",
      "6000/49000 loss: 0.2590300625626055\n",
      "8000/49000 loss: 0.2379311379856779\n",
      "10000/49000 loss: 0.37067962869079407\n",
      "12000/49000 loss: 0.37205783044258056\n",
      "14000/49000 loss: 0.3126647179364339\n",
      "16000/49000 loss: 0.3356673836647341\n",
      "18000/49000 loss: 0.31343440594533006\n",
      "20000/49000 loss: 0.3492118054019024\n",
      "22000/49000 loss: 0.29820997184803244\n",
      "24000/49000 loss: 0.21114989013167154\n",
      "26000/49000 loss: 0.31780120431354175\n",
      "28000/49000 loss: 0.20180223786787366\n",
      "30000/49000 loss: 0.29058571317582055\n",
      "32000/49000 loss: 0.31159756157013924\n",
      "34000/49000 loss: 0.33477389913172395\n",
      "36000/49000 loss: 0.2526023511162468\n",
      "38000/49000 loss: 0.2694973602750802\n",
      "40000/49000 loss: 0.2752107796998518\n",
      "42000/49000 loss: 0.2938112997647961\n",
      "44000/49000 loss: 0.27083179730468393\n",
      "46000/49000 loss: 0.2682755496883961\n",
      "48000/49000 loss: 0.33010400007203405\n",
      "epoch 25: valid acc = 0.881, new learning rate = 0.00013869478656091682\n",
      "2000/49000 loss: 0.35476648423225027\n",
      "4000/49000 loss: 0.30801575744388304\n",
      "6000/49000 loss: 0.34656778089292023\n",
      "8000/49000 loss: 0.25064868722488576\n",
      "10000/49000 loss: 0.3919609722022739\n",
      "12000/49000 loss: 0.37753282777488445\n",
      "14000/49000 loss: 0.3602591229467623\n",
      "16000/49000 loss: 0.2951780377465161\n",
      "18000/49000 loss: 0.2658645024538311\n",
      "20000/49000 loss: 0.2486073729937768\n",
      "22000/49000 loss: 0.28594318395807344\n",
      "24000/49000 loss: 0.27517631858004\n",
      "26000/49000 loss: 0.33447760389840875\n",
      "28000/49000 loss: 0.37654137344424354\n",
      "30000/49000 loss: 0.30939379167416153\n",
      "32000/49000 loss: 0.3848843794281437\n",
      "34000/49000 loss: 0.31935568422008825\n",
      "36000/49000 loss: 0.2752209275205773\n",
      "38000/49000 loss: 0.2750373806417491\n",
      "40000/49000 loss: 0.3778486534733854\n",
      "42000/49000 loss: 0.30315266949202324\n",
      "44000/49000 loss: 0.2887980598003098\n",
      "46000/49000 loss: 0.3377898409892835\n",
      "48000/49000 loss: 0.29617110123837925\n",
      "epoch 26: valid acc = 0.886, new learning rate = 0.00013176004723287096\n",
      "2000/49000 loss: 0.27140067386489275\n",
      "4000/49000 loss: 0.28981874415482567\n",
      "6000/49000 loss: 0.2335652427896363\n",
      "8000/49000 loss: 0.29353213075189016\n",
      "10000/49000 loss: 0.284865712934109\n",
      "12000/49000 loss: 0.2781436997669071\n",
      "14000/49000 loss: 0.2889856773405788\n",
      "16000/49000 loss: 0.25983236648459207\n",
      "18000/49000 loss: 0.31961836265270027\n",
      "20000/49000 loss: 0.2867299563805686\n",
      "22000/49000 loss: 0.29115890481046874\n",
      "24000/49000 loss: 0.38955508732930194\n",
      "26000/49000 loss: 0.30743267564792637\n",
      "28000/49000 loss: 0.20729089529364658\n",
      "30000/49000 loss: 0.3309417405840893\n",
      "32000/49000 loss: 0.2836359022068912\n",
      "34000/49000 loss: 0.3509579939989911\n",
      "36000/49000 loss: 0.3160087825992933\n",
      "38000/49000 loss: 0.35237058712655706\n",
      "40000/49000 loss: 0.2730457009167893\n",
      "42000/49000 loss: 0.24388636173330425\n",
      "44000/49000 loss: 0.3132642751863457\n",
      "46000/49000 loss: 0.2311937883736872\n",
      "48000/49000 loss: 0.2815189691850367\n",
      "epoch 27: valid acc = 0.882, new learning rate = 0.0001251720448712274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/49000 loss: 0.29395194552940423\n",
      "4000/49000 loss: 0.2719028177555229\n",
      "6000/49000 loss: 0.271236629701606\n",
      "8000/49000 loss: 0.3580754785762893\n",
      "10000/49000 loss: 0.38693927336004674\n",
      "12000/49000 loss: 0.2525357642749243\n",
      "14000/49000 loss: 0.2738289063525277\n",
      "16000/49000 loss: 0.2487861197019547\n",
      "18000/49000 loss: 0.2724931576037736\n",
      "20000/49000 loss: 0.2728830582136646\n",
      "22000/49000 loss: 0.3088620110350258\n",
      "24000/49000 loss: 0.3528354567777027\n",
      "26000/49000 loss: 0.2620756325777565\n",
      "28000/49000 loss: 0.4372477537172058\n",
      "30000/49000 loss: 0.41444805348507485\n",
      "32000/49000 loss: 0.2548142557068164\n",
      "34000/49000 loss: 0.2493750947606824\n",
      "36000/49000 loss: 0.39483580940793234\n",
      "38000/49000 loss: 0.4144573022635903\n",
      "40000/49000 loss: 0.3464888714145963\n",
      "42000/49000 loss: 0.2506655720529175\n",
      "44000/49000 loss: 0.3630020364189889\n",
      "46000/49000 loss: 0.3189161949854164\n",
      "48000/49000 loss: 0.2929576147813562\n",
      "epoch 28: valid acc = 0.884, new learning rate = 0.00011891344262766602\n",
      "2000/49000 loss: 0.2778614866289714\n",
      "4000/49000 loss: 0.20649832992554384\n",
      "6000/49000 loss: 0.3259706695394396\n",
      "8000/49000 loss: 0.25424318271708163\n",
      "10000/49000 loss: 0.32953918271747434\n",
      "12000/49000 loss: 0.1643963321230132\n",
      "14000/49000 loss: 0.3044806563295642\n",
      "16000/49000 loss: 0.2538253825874063\n",
      "18000/49000 loss: 0.19504478090554375\n",
      "20000/49000 loss: 0.2593780105854005\n",
      "22000/49000 loss: 0.26284920420572117\n",
      "24000/49000 loss: 0.32261973043670805\n",
      "26000/49000 loss: 0.2615172269032999\n",
      "28000/49000 loss: 0.27334664138188947\n",
      "30000/49000 loss: 0.25097190899622973\n",
      "32000/49000 loss: 0.3760611557399908\n",
      "34000/49000 loss: 0.17402521512035243\n",
      "36000/49000 loss: 0.2627236932740965\n",
      "38000/49000 loss: 0.32648154094775206\n",
      "40000/49000 loss: 0.3148158962665121\n",
      "42000/49000 loss: 0.31426161445155015\n",
      "44000/49000 loss: 0.2703597448887926\n",
      "46000/49000 loss: 0.301628436942234\n",
      "48000/49000 loss: 0.26830049701285913\n",
      "epoch 29: valid acc = 0.883, new learning rate = 0.00011296777049628272\n",
      "2000/49000 loss: 0.2619539440115128\n",
      "4000/49000 loss: 0.31744475136844996\n",
      "6000/49000 loss: 0.3864657618336675\n",
      "8000/49000 loss: 0.3672848020931372\n",
      "10000/49000 loss: 0.28492742316521097\n",
      "12000/49000 loss: 0.2865417865605212\n",
      "14000/49000 loss: 0.2937191293943038\n",
      "16000/49000 loss: 0.34376345748944076\n",
      "18000/49000 loss: 0.2493342962502414\n",
      "20000/49000 loss: 0.25503065607641295\n",
      "22000/49000 loss: 0.37589385601383024\n",
      "24000/49000 loss: 0.22575558429811518\n",
      "26000/49000 loss: 0.2786015407263619\n",
      "28000/49000 loss: 0.30697228242535674\n",
      "30000/49000 loss: 0.2933431549650584\n",
      "32000/49000 loss: 0.29326152394123894\n",
      "34000/49000 loss: 0.3629721594912934\n",
      "36000/49000 loss: 0.23128121849659355\n",
      "38000/49000 loss: 0.24305513408893145\n",
      "40000/49000 loss: 0.23596682076714795\n",
      "42000/49000 loss: 0.3131513348100382\n",
      "44000/49000 loss: 0.24770436294248255\n",
      "46000/49000 loss: 0.21479820637552557\n",
      "48000/49000 loss: 0.4207050381732816\n",
      "epoch 30: valid acc = 0.886, new learning rate = 0.00010731938197146858\n",
      "2000/49000 loss: 0.2513378850737892\n",
      "4000/49000 loss: 0.2667962448886569\n",
      "6000/49000 loss: 0.25770363936333635\n",
      "8000/49000 loss: 0.22672924211057133\n",
      "10000/49000 loss: 0.29849239155186824\n",
      "12000/49000 loss: 0.2198062034404586\n",
      "14000/49000 loss: 0.3587596333596181\n",
      "16000/49000 loss: 0.2538429489201694\n",
      "18000/49000 loss: 0.25802004147032176\n",
      "20000/49000 loss: 0.27353730854974667\n",
      "22000/49000 loss: 0.3175510016630403\n",
      "24000/49000 loss: 0.22500322148429863\n",
      "26000/49000 loss: 0.26569313393907723\n",
      "28000/49000 loss: 0.24656115554753202\n",
      "30000/49000 loss: 0.24402196372300994\n",
      "32000/49000 loss: 0.32795807075497485\n",
      "34000/49000 loss: 0.2555815385936971\n",
      "36000/49000 loss: 0.34333305685710946\n",
      "38000/49000 loss: 0.27039917869865926\n",
      "40000/49000 loss: 0.26017226856264997\n",
      "42000/49000 loss: 0.28234948340073973\n",
      "44000/49000 loss: 0.30797036610636963\n",
      "46000/49000 loss: 0.2648353062013173\n",
      "48000/49000 loss: 0.311946437116678\n",
      "epoch 31: valid acc = 0.885, new learning rate = 0.00010195341287289515\n",
      "2000/49000 loss: 0.2603757843344408\n",
      "4000/49000 loss: 0.33466857617158235\n",
      "6000/49000 loss: 0.27091522476504704\n",
      "8000/49000 loss: 0.1586050373280626\n",
      "10000/49000 loss: 0.30782588122439697\n",
      "12000/49000 loss: 0.3439968004227432\n",
      "14000/49000 loss: 0.33301676826851384\n",
      "16000/49000 loss: 0.27546573963837523\n",
      "18000/49000 loss: 0.2882690404425447\n",
      "20000/49000 loss: 0.28752406998918384\n",
      "22000/49000 loss: 0.31726305092457496\n",
      "24000/49000 loss: 0.3583084950872927\n",
      "26000/49000 loss: 0.23036070802350292\n",
      "28000/49000 loss: 0.2534673570697842\n",
      "30000/49000 loss: 0.2917037247565084\n",
      "32000/49000 loss: 0.248417075398187\n",
      "34000/49000 loss: 0.2094954961786797\n",
      "36000/49000 loss: 0.2834590031481887\n",
      "38000/49000 loss: 0.3016841162733169\n",
      "40000/49000 loss: 0.24897735673248528\n",
      "42000/49000 loss: 0.32885464497856703\n",
      "44000/49000 loss: 0.2593717162562979\n",
      "46000/49000 loss: 0.3663140928337875\n",
      "48000/49000 loss: 0.2529568216665833\n",
      "epoch 32: valid acc = 0.887, new learning rate = 9.685574222925039e-05\n",
      "2000/49000 loss: 0.2694899321048972\n",
      "4000/49000 loss: 0.2895480242463695\n",
      "6000/49000 loss: 0.32899360987536186\n",
      "8000/49000 loss: 0.2314163657364039\n",
      "10000/49000 loss: 0.27106212951069014\n",
      "12000/49000 loss: 0.3484456507820973\n",
      "14000/49000 loss: 0.2628608142696911\n",
      "16000/49000 loss: 0.2957109840468839\n",
      "18000/49000 loss: 0.32101207064717885\n",
      "20000/49000 loss: 0.3347199773657986\n",
      "22000/49000 loss: 0.312943485437956\n",
      "24000/49000 loss: 0.23045587848266494\n",
      "26000/49000 loss: 0.3065825601031375\n",
      "28000/49000 loss: 0.2766170648325627\n",
      "30000/49000 loss: 0.2601089762482356\n",
      "32000/49000 loss: 0.25086397979316066\n",
      "34000/49000 loss: 0.31335818851983066\n",
      "36000/49000 loss: 0.28023674985361663\n",
      "38000/49000 loss: 0.22572138960396027\n",
      "40000/49000 loss: 0.2794587522869534\n",
      "42000/49000 loss: 0.42654860135595013\n",
      "44000/49000 loss: 0.2930841743059032\n",
      "46000/49000 loss: 0.28374584365655164\n",
      "48000/49000 loss: 0.22190115078834793\n",
      "epoch 33: valid acc = 0.884, new learning rate = 9.201295511778786e-05\n",
      "2000/49000 loss: 0.2629228933081716\n",
      "4000/49000 loss: 0.2768066333574416\n",
      "6000/49000 loss: 0.2885908866005999\n",
      "8000/49000 loss: 0.2931598092938238\n",
      "10000/49000 loss: 0.3524478019307266\n",
      "12000/49000 loss: 0.2946208053480195\n",
      "14000/49000 loss: 0.305938232144226\n",
      "16000/49000 loss: 0.3086001434097685\n",
      "18000/49000 loss: 0.1983859585254759\n",
      "20000/49000 loss: 0.3470774684573749\n",
      "22000/49000 loss: 0.2586757538937176\n",
      "24000/49000 loss: 0.2789288596915717\n",
      "26000/49000 loss: 0.32826125160529934\n",
      "28000/49000 loss: 0.2961452074590729\n",
      "30000/49000 loss: 0.2454011920166213\n",
      "32000/49000 loss: 0.27420278973632495\n",
      "34000/49000 loss: 0.2899937202166501\n",
      "36000/49000 loss: 0.2402676290313821\n",
      "38000/49000 loss: 0.21313238203295848\n",
      "40000/49000 loss: 0.35960176690723483\n",
      "42000/49000 loss: 0.3377538578512338\n",
      "44000/49000 loss: 0.165612853363369\n",
      "46000/49000 loss: 0.3028533830878429\n",
      "48000/49000 loss: 0.3325427197112524\n",
      "epoch 34: valid acc = 0.884, new learning rate = 8.741230736189846e-05\n",
      "2000/49000 loss: 0.25195429561179133\n",
      "4000/49000 loss: 0.26854536980020066\n",
      "6000/49000 loss: 0.2731042398964179\n",
      "8000/49000 loss: 0.37382869245155326\n",
      "10000/49000 loss: 0.22179184354498713\n",
      "12000/49000 loss: 0.26302840830957097\n",
      "14000/49000 loss: 0.2682864685996071\n",
      "16000/49000 loss: 0.3179622575901841\n",
      "18000/49000 loss: 0.2609098278538254\n",
      "20000/49000 loss: 0.3147108892882143\n",
      "22000/49000 loss: 0.30158774504345004\n",
      "24000/49000 loss: 0.2757453857685849\n",
      "26000/49000 loss: 0.3064152864884781\n",
      "28000/49000 loss: 0.27878631184967184\n",
      "30000/49000 loss: 0.2182664151500016\n",
      "32000/49000 loss: 0.2315067039126731\n",
      "34000/49000 loss: 0.40492801960836805\n",
      "36000/49000 loss: 0.28200103291705586\n",
      "38000/49000 loss: 0.25633655285212065\n",
      "40000/49000 loss: 0.24149190469112358\n",
      "42000/49000 loss: 0.2532084847156651\n",
      "44000/49000 loss: 0.2584194970658497\n",
      "46000/49000 loss: 0.32993788376451527\n",
      "48000/49000 loss: 0.253207255969493\n",
      "epoch 35: valid acc = 0.88, new learning rate = 8.304169199380353e-05\n",
      "2000/49000 loss: 0.26538787107179834\n",
      "4000/49000 loss: 0.3451343380379673\n",
      "6000/49000 loss: 0.2859432625893492\n",
      "8000/49000 loss: 0.2436207325929439\n",
      "10000/49000 loss: 0.24767276565402624\n",
      "12000/49000 loss: 0.27257711492820696\n",
      "14000/49000 loss: 0.2731274006667077\n",
      "16000/49000 loss: 0.2726181177282265\n",
      "18000/49000 loss: 0.2652466389401284\n",
      "20000/49000 loss: 0.25825288430921883\n",
      "22000/49000 loss: 0.2876365868356484\n",
      "24000/49000 loss: 0.2915878136276099\n",
      "26000/49000 loss: 0.23490646758730768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28000/49000 loss: 0.2299540703582128\n",
      "30000/49000 loss: 0.23179991355515675\n",
      "32000/49000 loss: 0.3033386499529115\n",
      "34000/49000 loss: 0.30070650080147654\n",
      "36000/49000 loss: 0.3446829320657098\n",
      "38000/49000 loss: 0.3417480912724009\n",
      "40000/49000 loss: 0.16807876628794913\n",
      "42000/49000 loss: 0.2766131075226255\n",
      "44000/49000 loss: 0.2441675270902545\n",
      "46000/49000 loss: 0.3082478813530915\n",
      "48000/49000 loss: 0.32285782914351324\n",
      "epoch 36: valid acc = 0.887, new learning rate = 7.888960739411335e-05\n",
      "2000/49000 loss: 0.23222885016027703\n",
      "4000/49000 loss: 0.3194797163665771\n",
      "6000/49000 loss: 0.3555818866671943\n",
      "8000/49000 loss: 0.2757605852175419\n",
      "10000/49000 loss: 0.35494512529355243\n",
      "12000/49000 loss: 0.1883147035317543\n",
      "14000/49000 loss: 0.30024263037533805\n",
      "16000/49000 loss: 0.3434384343377048\n",
      "18000/49000 loss: 0.3163127903230985\n",
      "20000/49000 loss: 0.33371174863713204\n",
      "22000/49000 loss: 0.27982884353935067\n",
      "24000/49000 loss: 0.24149995078351738\n",
      "26000/49000 loss: 0.3191450432879641\n",
      "28000/49000 loss: 0.28355970289131316\n",
      "30000/49000 loss: 0.3310379245255653\n",
      "32000/49000 loss: 0.2980918289635626\n",
      "34000/49000 loss: 0.2731750255317732\n",
      "36000/49000 loss: 0.232613559593907\n",
      "38000/49000 loss: 0.20464652056898447\n",
      "40000/49000 loss: 0.24623047946479407\n",
      "42000/49000 loss: 0.31697832448724084\n",
      "44000/49000 loss: 0.35156349474186416\n",
      "46000/49000 loss: 0.33874991971435114\n",
      "48000/49000 loss: 0.30402611261532586\n",
      "epoch 37: valid acc = 0.888, new learning rate = 7.494512702440768e-05\n",
      "2000/49000 loss: 0.21796692323503383\n",
      "4000/49000 loss: 0.15747760779517433\n",
      "6000/49000 loss: 0.2787899301228052\n",
      "8000/49000 loss: 0.29229904645797133\n",
      "10000/49000 loss: 0.22962800573925757\n",
      "12000/49000 loss: 0.2630623332796806\n",
      "14000/49000 loss: 0.3869296031226515\n",
      "16000/49000 loss: 0.25553145859826454\n",
      "18000/49000 loss: 0.4067986771475773\n",
      "20000/49000 loss: 0.31899033002814114\n",
      "22000/49000 loss: 0.2908268052153996\n",
      "24000/49000 loss: 0.26621467791768555\n",
      "26000/49000 loss: 0.2395057173669663\n",
      "28000/49000 loss: 0.33846520467412355\n",
      "30000/49000 loss: 0.20593600895877787\n",
      "32000/49000 loss: 0.25734440809576964\n",
      "34000/49000 loss: 0.27582956015696986\n",
      "36000/49000 loss: 0.2305035793312006\n",
      "38000/49000 loss: 0.24406275643237085\n",
      "40000/49000 loss: 0.2711137352625726\n",
      "42000/49000 loss: 0.3706109151366778\n",
      "44000/49000 loss: 0.3181306153851415\n",
      "46000/49000 loss: 0.29639170407527327\n",
      "48000/49000 loss: 0.38123645202861794\n",
      "epoch 38: valid acc = 0.88, new learning rate = 7.119787067318729e-05\n",
      "2000/49000 loss: 0.27766037167833835\n",
      "4000/49000 loss: 0.30773497992949816\n",
      "6000/49000 loss: 0.28957074636451674\n",
      "8000/49000 loss: 0.27467208036627766\n",
      "10000/49000 loss: 0.29868496970690656\n",
      "12000/49000 loss: 0.29346060653742445\n",
      "14000/49000 loss: 0.28221226141680816\n",
      "16000/49000 loss: 0.23818094326250736\n",
      "18000/49000 loss: 0.25511822786106036\n",
      "20000/49000 loss: 0.35115515982448503\n",
      "22000/49000 loss: 0.2984284406239835\n",
      "24000/49000 loss: 0.2515240552075628\n",
      "26000/49000 loss: 0.36207577371397187\n",
      "28000/49000 loss: 0.3263775619642096\n",
      "30000/49000 loss: 0.27340529805528924\n",
      "32000/49000 loss: 0.24138667733402977\n",
      "34000/49000 loss: 0.34782908669721174\n",
      "36000/49000 loss: 0.21008800276028636\n",
      "38000/49000 loss: 0.25516632770130576\n",
      "40000/49000 loss: 0.34354890390947895\n",
      "42000/49000 loss: 0.21359097166387442\n",
      "44000/49000 loss: 0.38658820909896935\n",
      "46000/49000 loss: 0.23003809351318702\n",
      "48000/49000 loss: 0.3280390149806443\n",
      "epoch 39: valid acc = 0.887, new learning rate = 6.763797713952792e-05\n",
      "2000/49000 loss: 0.2800257889496489\n",
      "4000/49000 loss: 0.25466968356677594\n",
      "6000/49000 loss: 0.2422712321930871\n",
      "8000/49000 loss: 0.2244233786303188\n",
      "10000/49000 loss: 0.22117784659688075\n",
      "12000/49000 loss: 0.29421098609241025\n",
      "14000/49000 loss: 0.28111874447915114\n",
      "16000/49000 loss: 0.27984570197145314\n",
      "18000/49000 loss: 0.3283910152067257\n",
      "20000/49000 loss: 0.2732514426594171\n",
      "22000/49000 loss: 0.32004924158468295\n",
      "24000/49000 loss: 0.2967942566154255\n",
      "26000/49000 loss: 0.2881414433081861\n",
      "28000/49000 loss: 0.3748388793912554\n",
      "30000/49000 loss: 0.2558629369608667\n",
      "32000/49000 loss: 0.26813226641275795\n",
      "34000/49000 loss: 0.2554833711050437\n",
      "36000/49000 loss: 0.29550753059539864\n",
      "38000/49000 loss: 0.2745854949435282\n",
      "40000/49000 loss: 0.2503005758171573\n",
      "42000/49000 loss: 0.20590723200646013\n",
      "44000/49000 loss: 0.21526870954674382\n",
      "46000/49000 loss: 0.35067574506551186\n",
      "48000/49000 loss: 0.2137264184699313\n",
      "epoch 40: valid acc = 0.885, new learning rate = 6.425607828255152e-05\n",
      "2000/49000 loss: 0.2753058042590748\n",
      "4000/49000 loss: 0.26762141590423677\n",
      "6000/49000 loss: 0.2372199761142243\n",
      "8000/49000 loss: 0.25195341515941516\n",
      "10000/49000 loss: 0.28824259124822377\n",
      "12000/49000 loss: 0.2445444167953024\n",
      "14000/49000 loss: 0.26656622758900345\n",
      "16000/49000 loss: 0.3831263487217249\n",
      "18000/49000 loss: 0.2199689464564814\n",
      "20000/49000 loss: 0.28499577909153034\n",
      "22000/49000 loss: 0.26745629194529386\n",
      "24000/49000 loss: 0.2832792884632563\n",
      "26000/49000 loss: 0.248138617991489\n",
      "28000/49000 loss: 0.2524518185985799\n",
      "30000/49000 loss: 0.21964677057242218\n",
      "32000/49000 loss: 0.3300298433301518\n",
      "34000/49000 loss: 0.22238870152502405\n",
      "36000/49000 loss: 0.3750529919506781\n",
      "38000/49000 loss: 0.37844395090960886\n",
      "40000/49000 loss: 0.27437999506059313\n",
      "42000/49000 loss: 0.2377807171212568\n",
      "44000/49000 loss: 0.3044077875888306\n",
      "46000/49000 loss: 0.2446408569881227\n",
      "48000/49000 loss: 0.3289441240132614\n",
      "epoch 41: valid acc = 0.888, new learning rate = 6.104327436842394e-05\n",
      "2000/49000 loss: 0.34495302384177573\n",
      "4000/49000 loss: 0.28573579166626284\n",
      "6000/49000 loss: 0.23654547100298587\n",
      "8000/49000 loss: 0.3284227470598231\n",
      "10000/49000 loss: 0.34224847185344603\n",
      "12000/49000 loss: 0.2751816172100366\n",
      "14000/49000 loss: 0.17801199188024316\n",
      "16000/49000 loss: 0.20943468822591463\n",
      "18000/49000 loss: 0.28304296267470486\n",
      "20000/49000 loss: 0.3306556384645354\n",
      "22000/49000 loss: 0.25324344803397514\n",
      "24000/49000 loss: 0.28279879226989274\n",
      "26000/49000 loss: 0.28841389220213326\n",
      "28000/49000 loss: 0.2225748544908069\n",
      "30000/49000 loss: 0.3212210713624193\n",
      "32000/49000 loss: 0.27323268990553307\n",
      "34000/49000 loss: 0.2381499913740725\n",
      "36000/49000 loss: 0.27295185672317096\n",
      "38000/49000 loss: 0.24674149896554146\n",
      "40000/49000 loss: 0.2215114031161195\n",
      "42000/49000 loss: 0.3224729574540228\n",
      "44000/49000 loss: 0.2844424609100312\n",
      "46000/49000 loss: 0.2893366741348564\n",
      "48000/49000 loss: 0.34094962140668633\n",
      "epoch 42: valid acc = 0.888, new learning rate = 5.799111065000274e-05\n",
      "2000/49000 loss: 0.30967105148437346\n",
      "4000/49000 loss: 0.2737789671730713\n",
      "6000/49000 loss: 0.28316889178672505\n",
      "8000/49000 loss: 0.28893592452322553\n",
      "10000/49000 loss: 0.21998224942544414\n",
      "12000/49000 loss: 0.3134541349949174\n",
      "14000/49000 loss: 0.2790617057996364\n",
      "16000/49000 loss: 0.28169276757690304\n",
      "18000/49000 loss: 0.2857639980667196\n",
      "20000/49000 loss: 0.31456442735078216\n",
      "22000/49000 loss: 0.24437166724606907\n",
      "24000/49000 loss: 0.2546616169263255\n",
      "26000/49000 loss: 0.34346610264856137\n",
      "28000/49000 loss: 0.2613705145413623\n",
      "30000/49000 loss: 0.28281199245545774\n",
      "32000/49000 loss: 0.3550817879470031\n",
      "34000/49000 loss: 0.31691248185862886\n",
      "36000/49000 loss: 0.25450073811335294\n",
      "38000/49000 loss: 0.29905704567377145\n",
      "40000/49000 loss: 0.22590980628157023\n",
      "42000/49000 loss: 0.22841066914089572\n",
      "44000/49000 loss: 0.24333097316624386\n",
      "46000/49000 loss: 0.2649391566941743\n",
      "48000/49000 loss: 0.3161548651014427\n",
      "epoch 43: valid acc = 0.886, new learning rate = 5.5091555117502596e-05\n",
      "2000/49000 loss: 0.3053994272844161\n",
      "4000/49000 loss: 0.28934223777770046\n",
      "6000/49000 loss: 0.33982484786986933\n",
      "8000/49000 loss: 0.2907619668345225\n",
      "10000/49000 loss: 0.28785299585739976\n",
      "12000/49000 loss: 0.33065205103057316\n",
      "14000/49000 loss: 0.22488317241182798\n",
      "16000/49000 loss: 0.24652923666886287\n",
      "18000/49000 loss: 0.22784875550765235\n",
      "20000/49000 loss: 0.29564095967409276\n",
      "22000/49000 loss: 0.32517947206032727\n",
      "24000/49000 loss: 0.26915706414530016\n",
      "26000/49000 loss: 0.2415016004018881\n",
      "28000/49000 loss: 0.39233845612674767\n",
      "30000/49000 loss: 0.24241814308375403\n",
      "32000/49000 loss: 0.44788871632752614\n",
      "34000/49000 loss: 0.3290387278708599\n",
      "36000/49000 loss: 0.26607168904033\n",
      "38000/49000 loss: 0.2898753225138785\n",
      "40000/49000 loss: 0.31553738689257727\n",
      "42000/49000 loss: 0.2849680982189916\n",
      "44000/49000 loss: 0.31053291573109826\n",
      "46000/49000 loss: 0.25695911730210175\n",
      "48000/49000 loss: 0.4133864933722412\n",
      "epoch 44: valid acc = 0.881, new learning rate = 5.2336977361627463e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/49000 loss: 0.2468866588932202\n",
      "4000/49000 loss: 0.2150219485615926\n",
      "6000/49000 loss: 0.24647462692239047\n",
      "8000/49000 loss: 0.32815824128658494\n",
      "10000/49000 loss: 0.25045402237133474\n",
      "12000/49000 loss: 0.25339726061103124\n",
      "14000/49000 loss: 0.36628189281588414\n",
      "16000/49000 loss: 0.20652769694370982\n",
      "18000/49000 loss: 0.30042166683334254\n",
      "20000/49000 loss: 0.18213391320514438\n",
      "22000/49000 loss: 0.2709089089486501\n",
      "24000/49000 loss: 0.2956048622155409\n",
      "26000/49000 loss: 0.24442837097636874\n",
      "28000/49000 loss: 0.28216956929909187\n",
      "30000/49000 loss: 0.3164363843991857\n",
      "32000/49000 loss: 0.27287288059271686\n",
      "34000/49000 loss: 0.2147492094684729\n",
      "36000/49000 loss: 0.2521069212966498\n",
      "38000/49000 loss: 0.3461636267184837\n",
      "40000/49000 loss: 0.2546224023168047\n",
      "42000/49000 loss: 0.3013735410586556\n",
      "44000/49000 loss: 0.2018552542984305\n",
      "46000/49000 loss: 0.1759675878446036\n",
      "48000/49000 loss: 0.2878552085999173\n",
      "epoch 45: valid acc = 0.888, new learning rate = 4.972012849354609e-05\n",
      "2000/49000 loss: 0.22883172236139676\n",
      "4000/49000 loss: 0.3107786329209613\n",
      "6000/49000 loss: 0.2637621877591529\n",
      "8000/49000 loss: 0.3211740916486594\n",
      "10000/49000 loss: 0.2455640059798958\n",
      "12000/49000 loss: 0.3128458708839692\n",
      "14000/49000 loss: 0.2789103875630633\n",
      "16000/49000 loss: 0.29325977660248953\n",
      "18000/49000 loss: 0.3383339373426258\n",
      "20000/49000 loss: 0.23782775950554527\n",
      "22000/49000 loss: 0.3740031040402873\n",
      "24000/49000 loss: 0.26754458852820723\n",
      "26000/49000 loss: 0.25681784328269547\n",
      "28000/49000 loss: 0.1882473407032473\n",
      "30000/49000 loss: 0.19751193459513905\n",
      "32000/49000 loss: 0.31492158017801825\n",
      "34000/49000 loss: 0.2403978374770586\n",
      "36000/49000 loss: 0.25884922415609246\n",
      "38000/49000 loss: 0.34932233461377205\n",
      "40000/49000 loss: 0.2857667335433794\n",
      "42000/49000 loss: 0.2633557355397365\n",
      "44000/49000 loss: 0.29384854646118747\n",
      "46000/49000 loss: 0.16120279800033988\n",
      "48000/49000 loss: 0.2400169344138912\n",
      "epoch 46: valid acc = 0.882, new learning rate = 4.723412206886878e-05\n",
      "2000/49000 loss: 0.38740429963506334\n",
      "4000/49000 loss: 0.24054601952532376\n",
      "6000/49000 loss: 0.2718743028939793\n",
      "8000/49000 loss: 0.2492987442743593\n",
      "10000/49000 loss: 0.2630325023564705\n",
      "12000/49000 loss: 0.29507916286388164\n",
      "14000/49000 loss: 0.24533751159651856\n",
      "16000/49000 loss: 0.26225040306341724\n",
      "18000/49000 loss: 0.2462313334317652\n",
      "20000/49000 loss: 0.21009360025900345\n",
      "22000/49000 loss: 0.2141490883866499\n",
      "24000/49000 loss: 0.25266924389224216\n",
      "26000/49000 loss: 0.3254682371181553\n",
      "28000/49000 loss: 0.2864282095384334\n",
      "30000/49000 loss: 0.3279790960377123\n",
      "32000/49000 loss: 0.2937036169626363\n",
      "34000/49000 loss: 0.3004321106566023\n",
      "36000/49000 loss: 0.27446896771171264\n",
      "38000/49000 loss: 0.2682643954106579\n",
      "40000/49000 loss: 0.2841725514502563\n",
      "42000/49000 loss: 0.24475782546162675\n",
      "44000/49000 loss: 0.2542988270930177\n",
      "46000/49000 loss: 0.4429786139825239\n",
      "48000/49000 loss: 0.3515655815031954\n",
      "epoch 47: valid acc = 0.886, new learning rate = 4.487241596542534e-05\n",
      "2000/49000 loss: 0.2210826677752641\n",
      "4000/49000 loss: 0.3028480995342563\n",
      "6000/49000 loss: 0.27474855753754307\n",
      "8000/49000 loss: 0.3146492603370262\n",
      "10000/49000 loss: 0.26657591541104625\n",
      "12000/49000 loss: 0.3405836816843839\n",
      "14000/49000 loss: 0.2420813604749645\n",
      "16000/49000 loss: 0.25528639218241705\n",
      "18000/49000 loss: 0.22870470748868763\n",
      "20000/49000 loss: 0.23010245403633947\n",
      "22000/49000 loss: 0.2644284229707417\n",
      "24000/49000 loss: 0.30382990854602937\n",
      "26000/49000 loss: 0.21261242195530516\n",
      "28000/49000 loss: 0.29849041132154486\n",
      "30000/49000 loss: 0.27058364940625085\n",
      "32000/49000 loss: 0.33769395459833\n",
      "34000/49000 loss: 0.2776158715166387\n",
      "36000/49000 loss: 0.3235667581112908\n",
      "38000/49000 loss: 0.24708453243061218\n",
      "40000/49000 loss: 0.32186305490749006\n",
      "42000/49000 loss: 0.270951532661689\n",
      "44000/49000 loss: 0.3529436439467594\n",
      "46000/49000 loss: 0.23129607206940597\n",
      "48000/49000 loss: 0.20845761076006591\n",
      "epoch 48: valid acc = 0.885, new learning rate = 4.262879516715407e-05\n",
      "2000/49000 loss: 0.30942078248692073\n",
      "4000/49000 loss: 0.21307734614369997\n",
      "6000/49000 loss: 0.28497164919937074\n",
      "8000/49000 loss: 0.26605804747882666\n",
      "10000/49000 loss: 0.30611518461379617\n",
      "12000/49000 loss: 0.32293291495466636\n",
      "14000/49000 loss: 0.1819467568921445\n",
      "16000/49000 loss: 0.39241596468044043\n",
      "18000/49000 loss: 0.29066087535722873\n",
      "20000/49000 loss: 0.23714035110200005\n",
      "22000/49000 loss: 0.28715436534627703\n",
      "24000/49000 loss: 0.2691064469523997\n",
      "26000/49000 loss: 0.2973610817886958\n",
      "28000/49000 loss: 0.3063792280007657\n",
      "30000/49000 loss: 0.28702285324074495\n",
      "32000/49000 loss: 0.22038260774466606\n",
      "34000/49000 loss: 0.3135496437956499\n",
      "36000/49000 loss: 0.29599986319661103\n",
      "38000/49000 loss: 0.26833899611637585\n",
      "40000/49000 loss: 0.3273431572958823\n",
      "42000/49000 loss: 0.20751635472614463\n",
      "44000/49000 loss: 0.3250443169479204\n",
      "46000/49000 loss: 0.3168171510267468\n",
      "48000/49000 loss: 0.2010134208582134\n",
      "epoch 49: valid acc = 0.884, new learning rate = 4.049735540879637e-05\n",
      "2000/49000 loss: 0.288536534052579\n",
      "4000/49000 loss: 0.2611678087857298\n",
      "6000/49000 loss: 0.22266909392449596\n",
      "8000/49000 loss: 0.2673547465859145\n",
      "10000/49000 loss: 0.2993630373874239\n",
      "12000/49000 loss: 0.34869389397845685\n",
      "14000/49000 loss: 0.2837278999480581\n",
      "16000/49000 loss: 0.23248160787327438\n",
      "18000/49000 loss: 0.36944890747245573\n",
      "20000/49000 loss: 0.28436279700705247\n",
      "22000/49000 loss: 0.25966010433150843\n",
      "24000/49000 loss: 0.3359103049051895\n",
      "26000/49000 loss: 0.2540653779244303\n",
      "28000/49000 loss: 0.3257141235975678\n",
      "30000/49000 loss: 0.298185783536388\n",
      "32000/49000 loss: 0.3221912230456726\n",
      "34000/49000 loss: 0.23463523152017687\n",
      "36000/49000 loss: 0.2823548004780249\n",
      "38000/49000 loss: 0.3178350615689546\n",
      "40000/49000 loss: 0.3300742319872747\n",
      "42000/49000 loss: 0.31287491199020584\n",
      "44000/49000 loss: 0.28476330212284773\n",
      "46000/49000 loss: 0.27690856300235417\n",
      "48000/49000 loss: 0.2863953405859636\n",
      "epoch 50: valid acc = 0.885, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.9056530612244897\n",
      "test acc: 0.885\n",
      "test acc: 0.8724\n",
      "number of batches for training: 245\n",
      "epoch 1: valid acc = 0.74, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.811, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.828, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.853, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.858, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.86, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.864, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.866, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.867, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.874, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.88, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.882, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.88, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.881, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.881, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.882, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.884, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.885, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.882, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.884, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.892, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.887, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.885, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.882, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.885, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.893, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.886, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.884, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.893, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.889, new learning rate = 0.00010731938197146858\n",
      "epoch 31: valid acc = 0.885, new learning rate = 0.00010195341287289515\n",
      "epoch 32: valid acc = 0.888, new learning rate = 9.685574222925039e-05\n",
      "epoch 33: valid acc = 0.885, new learning rate = 9.201295511778786e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 34: valid acc = 0.889, new learning rate = 8.741230736189846e-05\n",
      "epoch 35: valid acc = 0.884, new learning rate = 8.304169199380353e-05\n",
      "epoch 36: valid acc = 0.889, new learning rate = 7.888960739411335e-05\n",
      "epoch 37: valid acc = 0.885, new learning rate = 7.494512702440768e-05\n",
      "epoch 38: valid acc = 0.89, new learning rate = 7.119787067318729e-05\n",
      "epoch 39: valid acc = 0.887, new learning rate = 6.763797713952792e-05\n",
      "epoch 40: valid acc = 0.888, new learning rate = 6.425607828255152e-05\n",
      "epoch 41: valid acc = 0.882, new learning rate = 6.104327436842394e-05\n",
      "epoch 42: valid acc = 0.886, new learning rate = 5.799111065000274e-05\n",
      "epoch 43: valid acc = 0.888, new learning rate = 5.5091555117502596e-05\n",
      "epoch 44: valid acc = 0.884, new learning rate = 5.2336977361627463e-05\n",
      "epoch 45: valid acc = 0.886, new learning rate = 4.972012849354609e-05\n",
      "epoch 46: valid acc = 0.887, new learning rate = 4.723412206886878e-05\n",
      "epoch 47: valid acc = 0.885, new learning rate = 4.487241596542534e-05\n",
      "epoch 48: valid acc = 0.887, new learning rate = 4.262879516715407e-05\n",
      "epoch 49: valid acc = 0.886, new learning rate = 4.049735540879637e-05\n",
      "epoch 50: valid acc = 0.89, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.9052653061224489\n",
      "test acc: 0.89\n",
      "test acc: 0.8732\n",
      "number of batches for training: 81\n",
      "6000/49000 loss: 3.557694510835552\n",
      "12000/49000 loss: 3.2941235315795456\n",
      "18000/49000 loss: 2.586360613747863\n",
      "24000/49000 loss: 2.9936416359372817\n",
      "30000/49000 loss: 2.1249189717235795\n",
      "36000/49000 loss: 2.3010550829187713\n",
      "42000/49000 loss: 1.9303427429150317\n",
      "48000/49000 loss: 1.6895913992953588\n",
      "epoch 1: valid acc = 0.553, new learning rate = 0.000475\n",
      "6000/49000 loss: 1.3098344864313796\n",
      "12000/49000 loss: 1.3209860421135868\n",
      "18000/49000 loss: 1.4697526434604602\n",
      "24000/49000 loss: 1.1058714599154489\n",
      "30000/49000 loss: 1.040915346916213\n",
      "36000/49000 loss: 1.0401524232601895\n",
      "42000/49000 loss: 1.0160442995066596\n",
      "48000/49000 loss: 1.0140288894836025\n",
      "epoch 2: valid acc = 0.666, new learning rate = 0.00045125\n",
      "6000/49000 loss: 0.882473834323967\n",
      "12000/49000 loss: 0.8955186391160751\n",
      "18000/49000 loss: 0.8494066362156383\n",
      "24000/49000 loss: 0.8865969664328217\n",
      "30000/49000 loss: 0.8426981554293027\n",
      "36000/49000 loss: 0.7494536520150272\n",
      "42000/49000 loss: 0.7553736882507446\n",
      "48000/49000 loss: 0.7750170732609819\n",
      "epoch 3: valid acc = 0.732, new learning rate = 0.0004286875\n",
      "6000/49000 loss: 0.7639568752359815\n",
      "12000/49000 loss: 0.6665795530923982\n",
      "18000/49000 loss: 0.6668211207048939\n",
      "24000/49000 loss: 0.7386381985237096\n",
      "30000/49000 loss: 0.6976884980853768\n",
      "36000/49000 loss: 0.6883614072859064\n",
      "42000/49000 loss: 0.6195922615349123\n",
      "48000/49000 loss: 0.6586929341077625\n",
      "epoch 4: valid acc = 0.756, new learning rate = 0.00040725312499999993\n",
      "6000/49000 loss: 0.5695919373599052\n",
      "12000/49000 loss: 0.6099883528839561\n",
      "18000/49000 loss: 0.6072476220084095\n",
      "24000/49000 loss: 0.5861720309898566\n",
      "30000/49000 loss: 0.6434479191561767\n",
      "36000/49000 loss: 0.6160150603861543\n",
      "42000/49000 loss: 0.5667539407724516\n",
      "48000/49000 loss: 0.5483362627959814\n",
      "epoch 5: valid acc = 0.787, new learning rate = 0.0003868904687499999\n",
      "6000/49000 loss: 0.6160995338935329\n",
      "12000/49000 loss: 0.5312330671726094\n",
      "18000/49000 loss: 0.5188378460339065\n",
      "24000/49000 loss: 0.5353631294697216\n",
      "30000/49000 loss: 0.5689440099162449\n",
      "36000/49000 loss: 0.5171173149258629\n",
      "42000/49000 loss: 0.5400050925646566\n",
      "48000/49000 loss: 0.5192512776244667\n",
      "epoch 6: valid acc = 0.799, new learning rate = 0.0003675459453124999\n",
      "6000/49000 loss: 0.5023932169639008\n",
      "12000/49000 loss: 0.5277098627272712\n",
      "18000/49000 loss: 0.5299181594849021\n",
      "24000/49000 loss: 0.5162537209447366\n",
      "30000/49000 loss: 0.47720412501987647\n",
      "36000/49000 loss: 0.4762408098952689\n",
      "42000/49000 loss: 0.49447341208624096\n",
      "48000/49000 loss: 0.5102073783263809\n",
      "epoch 7: valid acc = 0.802, new learning rate = 0.00034916864804687486\n",
      "6000/49000 loss: 0.5156058652802301\n",
      "12000/49000 loss: 0.6048457757845418\n",
      "18000/49000 loss: 0.44599707741960365\n",
      "24000/49000 loss: 0.46487314359327053\n",
      "30000/49000 loss: 0.5433568690013854\n",
      "36000/49000 loss: 0.49183595841091954\n",
      "42000/49000 loss: 0.5320856521324424\n",
      "48000/49000 loss: 0.5317822969161887\n",
      "epoch 8: valid acc = 0.812, new learning rate = 0.0003317102156445311\n",
      "6000/49000 loss: 0.5199265232930809\n",
      "12000/49000 loss: 0.4659189075778652\n",
      "18000/49000 loss: 0.4814886557312771\n",
      "24000/49000 loss: 0.4175986717290788\n",
      "30000/49000 loss: 0.5209857294740067\n",
      "36000/49000 loss: 0.47031514180179035\n",
      "42000/49000 loss: 0.4759832176458359\n",
      "48000/49000 loss: 0.5162810030712527\n",
      "epoch 9: valid acc = 0.82, new learning rate = 0.0003151247048623045\n",
      "6000/49000 loss: 0.5066253772635906\n",
      "12000/49000 loss: 0.5044821563753351\n",
      "18000/49000 loss: 0.5071807491296016\n",
      "24000/49000 loss: 0.45355043880210516\n",
      "30000/49000 loss: 0.44175037019446695\n",
      "36000/49000 loss: 0.45590917538412196\n",
      "42000/49000 loss: 0.4654601083554292\n",
      "48000/49000 loss: 0.47885763154669664\n",
      "epoch 10: valid acc = 0.829, new learning rate = 0.00029936846961918924\n",
      "6000/49000 loss: 0.44604097640100776\n",
      "12000/49000 loss: 0.49205831877546025\n",
      "18000/49000 loss: 0.45151280735163307\n",
      "24000/49000 loss: 0.4984558452153527\n",
      "30000/49000 loss: 0.40738491931773546\n",
      "36000/49000 loss: 0.4623125970242282\n",
      "42000/49000 loss: 0.41696736741681345\n",
      "48000/49000 loss: 0.43240593743671835\n",
      "epoch 11: valid acc = 0.822, new learning rate = 0.00028440004613822977\n",
      "6000/49000 loss: 0.45057259622935497\n",
      "12000/49000 loss: 0.48733103883680073\n",
      "18000/49000 loss: 0.47577992573129296\n",
      "24000/49000 loss: 0.5446480029644073\n",
      "30000/49000 loss: 0.45198081017905084\n",
      "36000/49000 loss: 0.41473997089062437\n",
      "42000/49000 loss: 0.4766436657888196\n",
      "48000/49000 loss: 0.46247819902988935\n",
      "epoch 12: valid acc = 0.835, new learning rate = 0.00027018004383131826\n",
      "6000/49000 loss: 0.4633438871648466\n",
      "12000/49000 loss: 0.41947650113276325\n",
      "18000/49000 loss: 0.4105642281390134\n",
      "24000/49000 loss: 0.3865814957933305\n",
      "30000/49000 loss: 0.4546440770807887\n",
      "36000/49000 loss: 0.48458564337039034\n",
      "42000/49000 loss: 0.4438000881986423\n",
      "48000/49000 loss: 0.38739941614919066\n",
      "epoch 13: valid acc = 0.84, new learning rate = 0.00025667104163975234\n",
      "6000/49000 loss: 0.3942997055877988\n",
      "12000/49000 loss: 0.4662729815470394\n",
      "18000/49000 loss: 0.4523894929875033\n",
      "24000/49000 loss: 0.42210457482236624\n",
      "30000/49000 loss: 0.44002381945549623\n",
      "36000/49000 loss: 0.49338315413823\n",
      "42000/49000 loss: 0.5059267093734245\n",
      "48000/49000 loss: 0.45506835536888923\n",
      "epoch 14: valid acc = 0.843, new learning rate = 0.00024383748955776472\n",
      "6000/49000 loss: 0.4573112989535532\n",
      "12000/49000 loss: 0.38556672638966755\n",
      "18000/49000 loss: 0.4461876959810369\n",
      "24000/49000 loss: 0.4691672969840307\n",
      "30000/49000 loss: 0.4411257431429006\n",
      "36000/49000 loss: 0.41983468884835345\n",
      "42000/49000 loss: 0.42038354418927676\n",
      "48000/49000 loss: 0.388563917309012\n",
      "epoch 15: valid acc = 0.844, new learning rate = 0.00023164561507987649\n",
      "6000/49000 loss: 0.4362379796005715\n",
      "12000/49000 loss: 0.4024121819617627\n",
      "18000/49000 loss: 0.47396155263834383\n",
      "24000/49000 loss: 0.44000888279771294\n",
      "30000/49000 loss: 0.43678637723817393\n",
      "36000/49000 loss: 0.40163013846346296\n",
      "42000/49000 loss: 0.42316415923006595\n",
      "48000/49000 loss: 0.4329355595049474\n",
      "epoch 16: valid acc = 0.847, new learning rate = 0.00022006333432588265\n",
      "6000/49000 loss: 0.4452803605268551\n",
      "12000/49000 loss: 0.4157322405509596\n",
      "18000/49000 loss: 0.47477742957772734\n",
      "24000/49000 loss: 0.4376633667686026\n",
      "30000/49000 loss: 0.43892943938849627\n",
      "36000/49000 loss: 0.42710080616481616\n",
      "42000/49000 loss: 0.3682830601177349\n",
      "48000/49000 loss: 0.437283337388265\n",
      "epoch 17: valid acc = 0.844, new learning rate = 0.00020906016760958852\n",
      "6000/49000 loss: 0.37165808669519906\n",
      "12000/49000 loss: 0.4284148687587822\n",
      "18000/49000 loss: 0.4084751620527268\n",
      "24000/49000 loss: 0.42088811190535214\n",
      "30000/49000 loss: 0.37896282889476574\n",
      "36000/49000 loss: 0.41410822979477474\n",
      "42000/49000 loss: 0.3997177973154078\n",
      "48000/49000 loss: 0.4399677563233934\n",
      "epoch 18: valid acc = 0.854, new learning rate = 0.00019860715922910907\n",
      "6000/49000 loss: 0.4041020934772948\n",
      "12000/49000 loss: 0.40377179320693857\n",
      "18000/49000 loss: 0.4111587913437598\n",
      "24000/49000 loss: 0.4105370146276844\n",
      "30000/49000 loss: 0.4017569807962615\n",
      "36000/49000 loss: 0.41436994116359765\n",
      "42000/49000 loss: 0.3742349080891015\n",
      "48000/49000 loss: 0.42712210158338054\n",
      "epoch 19: valid acc = 0.855, new learning rate = 0.0001886768012676536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/49000 loss: 0.4384251267932397\n",
      "12000/49000 loss: 0.37803128131490543\n",
      "18000/49000 loss: 0.42071409749659167\n",
      "24000/49000 loss: 0.3935050979961221\n",
      "30000/49000 loss: 0.4127826707507439\n",
      "36000/49000 loss: 0.3230380758855313\n",
      "42000/49000 loss: 0.4110252397021151\n",
      "48000/49000 loss: 0.41068358568516006\n",
      "epoch 20: valid acc = 0.857, new learning rate = 0.0001792429612042709\n",
      "6000/49000 loss: 0.34921826347023116\n",
      "12000/49000 loss: 0.4073360865476729\n",
      "18000/49000 loss: 0.3965548091404486\n",
      "24000/49000 loss: 0.3972939205287944\n",
      "30000/49000 loss: 0.3924385256562245\n",
      "36000/49000 loss: 0.31934171983449244\n",
      "42000/49000 loss: 0.4238084349091289\n",
      "48000/49000 loss: 0.396471534400291\n",
      "epoch 21: valid acc = 0.854, new learning rate = 0.00017028081314405735\n",
      "6000/49000 loss: 0.3809184199376031\n",
      "12000/49000 loss: 0.4073467137741148\n",
      "18000/49000 loss: 0.39685490784131006\n",
      "24000/49000 loss: 0.4150177089610421\n",
      "30000/49000 loss: 0.39855726386269064\n",
      "36000/49000 loss: 0.39459402425254075\n",
      "42000/49000 loss: 0.4155218663696159\n",
      "48000/49000 loss: 0.36583549166433543\n",
      "epoch 22: valid acc = 0.856, new learning rate = 0.00016176677248685447\n",
      "6000/49000 loss: 0.35739621796849386\n",
      "12000/49000 loss: 0.34508502240824235\n",
      "18000/49000 loss: 0.36804745428128705\n",
      "24000/49000 loss: 0.41029945167902176\n",
      "30000/49000 loss: 0.36716909336721193\n",
      "36000/49000 loss: 0.387707383879057\n",
      "42000/49000 loss: 0.3982979147580015\n",
      "48000/49000 loss: 0.40258402436988605\n",
      "epoch 23: valid acc = 0.855, new learning rate = 0.00015367843386251173\n",
      "6000/49000 loss: 0.5211741012644319\n",
      "12000/49000 loss: 0.36342943237343905\n",
      "18000/49000 loss: 0.4246084101614237\n",
      "24000/49000 loss: 0.41533891617524504\n",
      "30000/49000 loss: 0.4539860597926679\n",
      "36000/49000 loss: 0.39374216584924204\n",
      "42000/49000 loss: 0.391978239795728\n",
      "48000/49000 loss: 0.38773707035845356\n",
      "epoch 24: valid acc = 0.856, new learning rate = 0.00014599451216938612\n",
      "6000/49000 loss: 0.46258424843766177\n",
      "12000/49000 loss: 0.3896855086222901\n",
      "18000/49000 loss: 0.4101322923329335\n",
      "24000/49000 loss: 0.40252710756462207\n",
      "30000/49000 loss: 0.40170831672283663\n",
      "36000/49000 loss: 0.38634907802481444\n",
      "42000/49000 loss: 0.42471792479228754\n",
      "48000/49000 loss: 0.4185951325085783\n",
      "epoch 25: valid acc = 0.859, new learning rate = 0.00013869478656091682\n",
      "6000/49000 loss: 0.3851667251741502\n",
      "12000/49000 loss: 0.39035367623281647\n",
      "18000/49000 loss: 0.44429472365142564\n",
      "24000/49000 loss: 0.4240783790149141\n",
      "30000/49000 loss: 0.3994632608485223\n",
      "36000/49000 loss: 0.35069621814509\n",
      "42000/49000 loss: 0.39045380952314335\n",
      "48000/49000 loss: 0.38665303727135125\n",
      "epoch 26: valid acc = 0.86, new learning rate = 0.00013176004723287096\n",
      "6000/49000 loss: 0.3935126759928273\n",
      "12000/49000 loss: 0.4145309126746947\n",
      "18000/49000 loss: 0.4236088321642823\n",
      "24000/49000 loss: 0.3662961486527492\n",
      "30000/49000 loss: 0.40498097921809223\n",
      "36000/49000 loss: 0.3894122820448533\n",
      "42000/49000 loss: 0.37266237965114074\n",
      "48000/49000 loss: 0.3952618198955244\n",
      "epoch 27: valid acc = 0.864, new learning rate = 0.0001251720448712274\n",
      "6000/49000 loss: 0.42498487843676175\n",
      "12000/49000 loss: 0.41452528140992\n",
      "18000/49000 loss: 0.33313570795398517\n",
      "24000/49000 loss: 0.36729488891610207\n",
      "30000/49000 loss: 0.37843520453571594\n",
      "36000/49000 loss: 0.38398449579041777\n",
      "42000/49000 loss: 0.40671686596195167\n",
      "48000/49000 loss: 0.4348572864151915\n",
      "epoch 28: valid acc = 0.862, new learning rate = 0.00011891344262766602\n",
      "6000/49000 loss: 0.4092484226061377\n",
      "12000/49000 loss: 0.38209233990988634\n",
      "18000/49000 loss: 0.3833261731505019\n",
      "24000/49000 loss: 0.4295573487292734\n",
      "30000/49000 loss: 0.46499854644067296\n",
      "36000/49000 loss: 0.44511331284376465\n",
      "42000/49000 loss: 0.3656979744598301\n",
      "48000/49000 loss: 0.406855626297035\n",
      "epoch 29: valid acc = 0.862, new learning rate = 0.00011296777049628272\n",
      "6000/49000 loss: 0.41600174283144337\n",
      "12000/49000 loss: 0.4099509924714896\n",
      "18000/49000 loss: 0.4262965399397636\n",
      "24000/49000 loss: 0.3757572950093847\n",
      "30000/49000 loss: 0.41110680340176353\n",
      "36000/49000 loss: 0.4037539024615844\n",
      "42000/49000 loss: 0.4142618464109516\n",
      "48000/49000 loss: 0.4381710447106199\n",
      "epoch 30: valid acc = 0.863, new learning rate = 0.00010731938197146858\n",
      "6000/49000 loss: 0.38495699015250845\n",
      "12000/49000 loss: 0.4082720581001659\n",
      "18000/49000 loss: 0.39771617580144136\n",
      "24000/49000 loss: 0.3326015038322395\n",
      "30000/49000 loss: 0.3311729357320647\n",
      "36000/49000 loss: 0.4156426776822381\n",
      "42000/49000 loss: 0.379091697924647\n",
      "48000/49000 loss: 0.3519775552910763\n",
      "epoch 31: valid acc = 0.859, new learning rate = 0.00010195341287289515\n",
      "6000/49000 loss: 0.39309479928330543\n",
      "12000/49000 loss: 0.3285866479673825\n",
      "18000/49000 loss: 0.36748676231199423\n",
      "24000/49000 loss: 0.36265069124144095\n",
      "30000/49000 loss: 0.4047622770231572\n",
      "36000/49000 loss: 0.4306282469786272\n",
      "42000/49000 loss: 0.35071898346313196\n",
      "48000/49000 loss: 0.3837814150443223\n",
      "epoch 32: valid acc = 0.863, new learning rate = 9.685574222925039e-05\n",
      "6000/49000 loss: 0.4123920819973763\n",
      "12000/49000 loss: 0.39733805483169804\n",
      "18000/49000 loss: 0.33956378105012985\n",
      "24000/49000 loss: 0.4338461597125503\n",
      "30000/49000 loss: 0.3765139284909124\n",
      "36000/49000 loss: 0.3622828094895711\n",
      "42000/49000 loss: 0.3799957687334204\n",
      "48000/49000 loss: 0.40037963727575004\n",
      "epoch 33: valid acc = 0.864, new learning rate = 9.201295511778786e-05\n",
      "6000/49000 loss: 0.4352451945382259\n",
      "12000/49000 loss: 0.43028112940177327\n",
      "18000/49000 loss: 0.3928512550224213\n",
      "24000/49000 loss: 0.4291263932004302\n",
      "30000/49000 loss: 0.4066181636704669\n",
      "36000/49000 loss: 0.36291894443145306\n",
      "42000/49000 loss: 0.45669271149840546\n",
      "48000/49000 loss: 0.3824205001768185\n",
      "epoch 34: valid acc = 0.862, new learning rate = 8.741230736189846e-05\n",
      "6000/49000 loss: 0.4310979246161534\n",
      "12000/49000 loss: 0.3708408205757149\n",
      "18000/49000 loss: 0.36100224972145534\n",
      "24000/49000 loss: 0.43076726818079875\n",
      "30000/49000 loss: 0.39583908049635563\n",
      "36000/49000 loss: 0.40575769348864216\n",
      "42000/49000 loss: 0.35278602059756653\n",
      "48000/49000 loss: 0.3647864900541779\n",
      "epoch 35: valid acc = 0.863, new learning rate = 8.304169199380353e-05\n",
      "6000/49000 loss: 0.34446567114073023\n",
      "12000/49000 loss: 0.3499897525844926\n",
      "18000/49000 loss: 0.37751211017915726\n",
      "24000/49000 loss: 0.36606250338034385\n",
      "30000/49000 loss: 0.41136585434827394\n",
      "36000/49000 loss: 0.4238942760880601\n",
      "42000/49000 loss: 0.37938452872094663\n",
      "48000/49000 loss: 0.4214141455032213\n",
      "epoch 36: valid acc = 0.86, new learning rate = 7.888960739411335e-05\n",
      "6000/49000 loss: 0.4104623181073853\n",
      "12000/49000 loss: 0.46970388851964046\n",
      "18000/49000 loss: 0.3817309852333053\n",
      "24000/49000 loss: 0.36447949316439276\n",
      "30000/49000 loss: 0.3703567551601403\n",
      "36000/49000 loss: 0.31063074169016447\n",
      "42000/49000 loss: 0.3819749912301875\n",
      "48000/49000 loss: 0.3415360080179547\n",
      "epoch 37: valid acc = 0.862, new learning rate = 7.494512702440768e-05\n",
      "6000/49000 loss: 0.36711160097414636\n",
      "12000/49000 loss: 0.38359272398108885\n",
      "18000/49000 loss: 0.3725177029119852\n",
      "24000/49000 loss: 0.33867875895712074\n",
      "30000/49000 loss: 0.37216497296959167\n",
      "36000/49000 loss: 0.4035971762746741\n",
      "42000/49000 loss: 0.36374955328680153\n",
      "48000/49000 loss: 0.3768738374214106\n",
      "epoch 38: valid acc = 0.866, new learning rate = 7.119787067318729e-05\n",
      "6000/49000 loss: 0.3974656930583136\n",
      "12000/49000 loss: 0.3910349777792581\n",
      "18000/49000 loss: 0.40606100930705613\n",
      "24000/49000 loss: 0.3803346535651862\n",
      "30000/49000 loss: 0.39326082209295155\n",
      "36000/49000 loss: 0.43461512135099234\n",
      "42000/49000 loss: 0.41537251539722997\n",
      "48000/49000 loss: 0.3458337730533513\n",
      "epoch 39: valid acc = 0.861, new learning rate = 6.763797713952792e-05\n",
      "6000/49000 loss: 0.37445904186000534\n",
      "12000/49000 loss: 0.3352998666209058\n",
      "18000/49000 loss: 0.32615426662652747\n",
      "24000/49000 loss: 0.37519550644434296\n",
      "30000/49000 loss: 0.32849511645193646\n",
      "36000/49000 loss: 0.43215868481167646\n",
      "42000/49000 loss: 0.347934650823419\n",
      "48000/49000 loss: 0.3327245427391303\n",
      "epoch 40: valid acc = 0.863, new learning rate = 6.425607828255152e-05\n",
      "6000/49000 loss: 0.35667302213281127\n",
      "12000/49000 loss: 0.3815997661387903\n",
      "18000/49000 loss: 0.3464095965486919\n",
      "24000/49000 loss: 0.3804946742966002\n",
      "30000/49000 loss: 0.3192554576526835\n",
      "36000/49000 loss: 0.46571934408745413\n",
      "42000/49000 loss: 0.37022539858881387\n",
      "48000/49000 loss: 0.39569019679480916\n",
      "epoch 41: valid acc = 0.862, new learning rate = 6.104327436842394e-05\n",
      "6000/49000 loss: 0.35126140247165827\n",
      "12000/49000 loss: 0.3786028522338645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/49000 loss: 0.37576534998320105\n",
      "24000/49000 loss: 0.3777265152004607\n",
      "30000/49000 loss: 0.42429444805944555\n",
      "36000/49000 loss: 0.4328429833587356\n",
      "42000/49000 loss: 0.39703879766576006\n",
      "48000/49000 loss: 0.3471381403974768\n",
      "epoch 42: valid acc = 0.861, new learning rate = 5.799111065000274e-05\n",
      "6000/49000 loss: 0.3638173929028453\n",
      "12000/49000 loss: 0.4313953422504659\n",
      "18000/49000 loss: 0.3651784833353371\n",
      "24000/49000 loss: 0.4336074090317915\n",
      "30000/49000 loss: 0.32232366237462184\n",
      "36000/49000 loss: 0.34573662416792694\n",
      "42000/49000 loss: 0.38121850384189204\n",
      "48000/49000 loss: 0.3989645603362391\n",
      "epoch 43: valid acc = 0.862, new learning rate = 5.5091555117502596e-05\n",
      "6000/49000 loss: 0.40768105379129727\n",
      "12000/49000 loss: 0.36205059836179787\n",
      "18000/49000 loss: 0.3386045773034852\n",
      "24000/49000 loss: 0.3914177465129921\n",
      "30000/49000 loss: 0.3606360940512531\n",
      "36000/49000 loss: 0.3788764763035962\n",
      "42000/49000 loss: 0.33439152556995927\n",
      "48000/49000 loss: 0.4069658258352372\n",
      "epoch 44: valid acc = 0.866, new learning rate = 5.2336977361627463e-05\n",
      "6000/49000 loss: 0.3322519472641802\n",
      "12000/49000 loss: 0.426007938688653\n",
      "18000/49000 loss: 0.4251413497424171\n",
      "24000/49000 loss: 0.40738374460988963\n",
      "30000/49000 loss: 0.38400263263608564\n",
      "36000/49000 loss: 0.3774039354731291\n",
      "42000/49000 loss: 0.37103671312724434\n",
      "48000/49000 loss: 0.37340771400456924\n",
      "epoch 45: valid acc = 0.864, new learning rate = 4.972012849354609e-05\n",
      "6000/49000 loss: 0.4137367629547719\n",
      "12000/49000 loss: 0.359500671624656\n",
      "18000/49000 loss: 0.40588054148688457\n",
      "24000/49000 loss: 0.3720184095972321\n",
      "30000/49000 loss: 0.35360140940485657\n",
      "36000/49000 loss: 0.34123113648141157\n",
      "42000/49000 loss: 0.4031490641295173\n",
      "48000/49000 loss: 0.41145983301108485\n",
      "epoch 46: valid acc = 0.86, new learning rate = 4.723412206886878e-05\n",
      "6000/49000 loss: 0.38606677792310296\n",
      "12000/49000 loss: 0.4330633486854714\n",
      "18000/49000 loss: 0.3410995640939098\n",
      "24000/49000 loss: 0.37081987278132506\n",
      "30000/49000 loss: 0.3918725997905143\n",
      "36000/49000 loss: 0.4014000616104185\n",
      "42000/49000 loss: 0.30937524874183703\n",
      "48000/49000 loss: 0.3530304571250678\n",
      "epoch 47: valid acc = 0.861, new learning rate = 4.487241596542534e-05\n",
      "6000/49000 loss: 0.4222976217611498\n",
      "12000/49000 loss: 0.38375133997425454\n",
      "18000/49000 loss: 0.34039215041223886\n",
      "24000/49000 loss: 0.3587532139368862\n",
      "30000/49000 loss: 0.38020270533373907\n",
      "36000/49000 loss: 0.3097413314927794\n",
      "42000/49000 loss: 0.32355414948383787\n",
      "48000/49000 loss: 0.3528396123634522\n",
      "epoch 48: valid acc = 0.862, new learning rate = 4.262879516715407e-05\n",
      "6000/49000 loss: 0.38837980499760777\n",
      "12000/49000 loss: 0.3606626543065135\n",
      "18000/49000 loss: 0.3739287611611624\n",
      "24000/49000 loss: 0.39269486207155446\n",
      "30000/49000 loss: 0.36401328147336903\n",
      "36000/49000 loss: 0.42047781808626156\n",
      "42000/49000 loss: 0.395415663520725\n",
      "48000/49000 loss: 0.39433043077333413\n",
      "epoch 49: valid acc = 0.861, new learning rate = 4.049735540879637e-05\n",
      "6000/49000 loss: 0.292603672009422\n",
      "12000/49000 loss: 0.40534483525640663\n",
      "18000/49000 loss: 0.33515922325508846\n",
      "24000/49000 loss: 0.3564614680985106\n",
      "30000/49000 loss: 0.3547800000367595\n",
      "36000/49000 loss: 0.37735744938978505\n",
      "42000/49000 loss: 0.36486849344790695\n",
      "48000/49000 loss: 0.3892787963513412\n",
      "epoch 50: valid acc = 0.863, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.8699183673469387\n",
      "test acc: 0.863\n",
      "test acc: 0.8505\n",
      "number of batches for training: 81\n",
      "epoch 1: valid acc = 0.513, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.649, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.745, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.754, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.788, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.802, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.808, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.816, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.824, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.823, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.835, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.835, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.834, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.843, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.844, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.845, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.846, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.853, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.852, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.858, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.861, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.858, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.86, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.861, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.862, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.863, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.863, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.861, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.862, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.864, new learning rate = 0.00010731938197146858\n",
      "epoch 31: valid acc = 0.866, new learning rate = 0.00010195341287289515\n",
      "epoch 32: valid acc = 0.861, new learning rate = 9.685574222925039e-05\n",
      "epoch 33: valid acc = 0.867, new learning rate = 9.201295511778786e-05\n",
      "epoch 34: valid acc = 0.864, new learning rate = 8.741230736189846e-05\n",
      "epoch 35: valid acc = 0.868, new learning rate = 8.304169199380353e-05\n",
      "epoch 36: valid acc = 0.867, new learning rate = 7.888960739411335e-05\n",
      "epoch 37: valid acc = 0.868, new learning rate = 7.494512702440768e-05\n",
      "epoch 38: valid acc = 0.864, new learning rate = 7.119787067318729e-05\n",
      "epoch 39: valid acc = 0.868, new learning rate = 6.763797713952792e-05\n",
      "epoch 40: valid acc = 0.866, new learning rate = 6.425607828255152e-05\n",
      "epoch 41: valid acc = 0.867, new learning rate = 6.104327436842394e-05\n",
      "epoch 42: valid acc = 0.873, new learning rate = 5.799111065000274e-05\n",
      "epoch 43: valid acc = 0.867, new learning rate = 5.5091555117502596e-05\n",
      "epoch 44: valid acc = 0.869, new learning rate = 5.2336977361627463e-05\n",
      "epoch 45: valid acc = 0.871, new learning rate = 4.972012849354609e-05\n",
      "epoch 46: valid acc = 0.868, new learning rate = 4.723412206886878e-05\n",
      "epoch 47: valid acc = 0.867, new learning rate = 4.487241596542534e-05\n",
      "epoch 48: valid acc = 0.869, new learning rate = 4.262879516715407e-05\n",
      "epoch 49: valid acc = 0.869, new learning rate = 4.049735540879637e-05\n",
      "epoch 50: valid acc = 0.866, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.8693877551020408\n",
      "test acc: 0.866\n",
      "test acc: 0.8469\n",
      "number of batches for training: 81\n",
      "6000/49000 loss: 3.58553616213424\n",
      "12000/49000 loss: 3.478504511688498\n",
      "18000/49000 loss: 3.1225654580065094\n",
      "24000/49000 loss: 2.6342507576551255\n",
      "30000/49000 loss: 2.288640186038864\n",
      "36000/49000 loss: 2.342956264635777\n",
      "42000/49000 loss: 1.736075154547814\n",
      "48000/49000 loss: 1.5456681270104315\n",
      "epoch 1: valid acc = 0.523, new learning rate = 0.000475\n",
      "6000/49000 loss: 1.2446318905150777\n",
      "12000/49000 loss: 1.1973185323973659\n",
      "18000/49000 loss: 1.1584555227608888\n",
      "24000/49000 loss: 1.0555786214491165\n",
      "30000/49000 loss: 1.1777067000136618\n",
      "36000/49000 loss: 1.0453024675902933\n",
      "42000/49000 loss: 0.9557095850948726\n",
      "48000/49000 loss: 0.9508622526891425\n",
      "epoch 2: valid acc = 0.674, new learning rate = 0.00045125\n",
      "6000/49000 loss: 0.8049140712070386\n",
      "12000/49000 loss: 0.8741236835344948\n",
      "18000/49000 loss: 0.9126961036118383\n",
      "24000/49000 loss: 0.8999338790190584\n",
      "30000/49000 loss: 0.8044074522292486\n",
      "36000/49000 loss: 0.8393548503176459\n",
      "42000/49000 loss: 0.7408555887167096\n",
      "48000/49000 loss: 0.7261327507281985\n",
      "epoch 3: valid acc = 0.744, new learning rate = 0.0004286875\n",
      "6000/49000 loss: 0.7695966559184497\n",
      "12000/49000 loss: 0.7154516616509262\n",
      "18000/49000 loss: 0.7604325744948859\n",
      "24000/49000 loss: 0.6552016921191266\n",
      "30000/49000 loss: 0.7455831752641898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36000/49000 loss: 0.5942177038237076\n",
      "42000/49000 loss: 0.6413549822196195\n",
      "48000/49000 loss: 0.6562230742542654\n",
      "epoch 4: valid acc = 0.751, new learning rate = 0.00040725312499999993\n",
      "6000/49000 loss: 0.6016274389082812\n",
      "12000/49000 loss: 0.6255499464580973\n",
      "18000/49000 loss: 0.6061845963239909\n",
      "24000/49000 loss: 0.6257529970781311\n",
      "30000/49000 loss: 0.5571118350305673\n",
      "36000/49000 loss: 0.5698874387608068\n",
      "42000/49000 loss: 0.5720246167399378\n",
      "48000/49000 loss: 0.5615835578291526\n",
      "epoch 5: valid acc = 0.786, new learning rate = 0.0003868904687499999\n",
      "6000/49000 loss: 0.5859529431035685\n",
      "12000/49000 loss: 0.5458086208040938\n",
      "18000/49000 loss: 0.5919397653150346\n",
      "24000/49000 loss: 0.557119832966243\n",
      "30000/49000 loss: 0.5549545758970205\n",
      "36000/49000 loss: 0.5119417568612289\n",
      "42000/49000 loss: 0.5750383187671723\n",
      "48000/49000 loss: 0.509929829074774\n",
      "epoch 6: valid acc = 0.807, new learning rate = 0.0003675459453124999\n",
      "6000/49000 loss: 0.5328039419667665\n",
      "12000/49000 loss: 0.5219435167269126\n",
      "18000/49000 loss: 0.567147275127983\n",
      "24000/49000 loss: 0.5656062530991365\n",
      "30000/49000 loss: 0.5769919404499261\n",
      "36000/49000 loss: 0.4988090436401939\n",
      "42000/49000 loss: 0.4717770454711543\n",
      "48000/49000 loss: 0.49764969420795024\n",
      "epoch 7: valid acc = 0.807, new learning rate = 0.00034916864804687486\n",
      "6000/49000 loss: 0.4881423182704898\n",
      "12000/49000 loss: 0.5265352605126579\n",
      "18000/49000 loss: 0.48582818788800597\n",
      "24000/49000 loss: 0.5248425706084933\n",
      "30000/49000 loss: 0.5689768926886106\n",
      "36000/49000 loss: 0.5588303411354977\n",
      "42000/49000 loss: 0.5361228734666029\n",
      "48000/49000 loss: 0.5071243733574485\n",
      "epoch 8: valid acc = 0.813, new learning rate = 0.0003317102156445311\n",
      "6000/49000 loss: 0.47023596088432407\n",
      "12000/49000 loss: 0.4432483083267222\n",
      "18000/49000 loss: 0.5552691685880351\n",
      "24000/49000 loss: 0.454586165581074\n",
      "30000/49000 loss: 0.4824559956409342\n",
      "36000/49000 loss: 0.46970958846491745\n",
      "42000/49000 loss: 0.5146600011817708\n",
      "48000/49000 loss: 0.4797093883120345\n",
      "epoch 9: valid acc = 0.827, new learning rate = 0.0003151247048623045\n",
      "6000/49000 loss: 0.4997358320684298\n",
      "12000/49000 loss: 0.4422385535574952\n",
      "18000/49000 loss: 0.5048501039955655\n",
      "24000/49000 loss: 0.481687766177632\n",
      "30000/49000 loss: 0.44724385109823417\n",
      "36000/49000 loss: 0.5048476088788835\n",
      "42000/49000 loss: 0.5424495918746853\n",
      "48000/49000 loss: 0.4915907675613354\n",
      "epoch 10: valid acc = 0.83, new learning rate = 0.00029936846961918924\n",
      "6000/49000 loss: 0.4882010148117123\n",
      "12000/49000 loss: 0.4578982117935256\n",
      "18000/49000 loss: 0.4920346123165197\n",
      "24000/49000 loss: 0.5164542989129884\n",
      "30000/49000 loss: 0.47438303076141436\n",
      "36000/49000 loss: 0.4695173639793972\n",
      "42000/49000 loss: 0.480757659961898\n",
      "48000/49000 loss: 0.44242506199382786\n",
      "epoch 11: valid acc = 0.831, new learning rate = 0.00028440004613822977\n",
      "6000/49000 loss: 0.41846264383458986\n",
      "12000/49000 loss: 0.4379352692182673\n",
      "18000/49000 loss: 0.44833002182899967\n",
      "24000/49000 loss: 0.4910325421132599\n",
      "30000/49000 loss: 0.4508477372593489\n",
      "36000/49000 loss: 0.4408288742641686\n",
      "42000/49000 loss: 0.46637872652808\n",
      "48000/49000 loss: 0.48078836839238115\n",
      "epoch 12: valid acc = 0.832, new learning rate = 0.00027018004383131826\n",
      "6000/49000 loss: 0.41613384165920525\n",
      "12000/49000 loss: 0.5056656305211763\n",
      "18000/49000 loss: 0.4388387597163376\n",
      "24000/49000 loss: 0.4330449751032693\n",
      "30000/49000 loss: 0.4909899282836564\n",
      "36000/49000 loss: 0.505294057297958\n",
      "42000/49000 loss: 0.4549696542855729\n",
      "48000/49000 loss: 0.5135505458131563\n",
      "epoch 13: valid acc = 0.835, new learning rate = 0.00025667104163975234\n",
      "6000/49000 loss: 0.4021764831817071\n",
      "12000/49000 loss: 0.48428043752439187\n",
      "18000/49000 loss: 0.43047126413446474\n",
      "24000/49000 loss: 0.42938375624767305\n",
      "30000/49000 loss: 0.4538905877941021\n",
      "36000/49000 loss: 0.4525394199423003\n",
      "42000/49000 loss: 0.42415336492819605\n",
      "48000/49000 loss: 0.45164455223386607\n",
      "epoch 14: valid acc = 0.837, new learning rate = 0.00024383748955776472\n",
      "6000/49000 loss: 0.3946908449814742\n",
      "12000/49000 loss: 0.46049160413752105\n",
      "18000/49000 loss: 0.41588514779825747\n",
      "24000/49000 loss: 0.4910557106933207\n",
      "30000/49000 loss: 0.4410271375255986\n",
      "36000/49000 loss: 0.42789844643522235\n",
      "42000/49000 loss: 0.4169637890660976\n",
      "48000/49000 loss: 0.427695081485196\n",
      "epoch 15: valid acc = 0.846, new learning rate = 0.00023164561507987649\n",
      "6000/49000 loss: 0.4136880320508878\n",
      "12000/49000 loss: 0.4463557787888955\n",
      "18000/49000 loss: 0.4333639085568539\n",
      "24000/49000 loss: 0.4470798524526585\n",
      "30000/49000 loss: 0.46494601752968984\n",
      "36000/49000 loss: 0.3720177899587336\n",
      "42000/49000 loss: 0.3945360407304282\n",
      "48000/49000 loss: 0.4130265595924103\n",
      "epoch 16: valid acc = 0.846, new learning rate = 0.00022006333432588265\n",
      "6000/49000 loss: 0.4036382224472634\n",
      "12000/49000 loss: 0.45196951600024315\n",
      "18000/49000 loss: 0.4160135367797744\n",
      "24000/49000 loss: 0.4149849876493216\n",
      "30000/49000 loss: 0.4661887066175172\n",
      "36000/49000 loss: 0.4375528318007531\n",
      "42000/49000 loss: 0.4153590525169217\n",
      "48000/49000 loss: 0.4838182354646135\n",
      "epoch 17: valid acc = 0.849, new learning rate = 0.00020906016760958852\n",
      "6000/49000 loss: 0.40489038407984357\n",
      "12000/49000 loss: 0.4069269152380606\n",
      "18000/49000 loss: 0.3842528542843723\n",
      "24000/49000 loss: 0.407742251813087\n",
      "30000/49000 loss: 0.4298429429664431\n",
      "36000/49000 loss: 0.4821664247041666\n",
      "42000/49000 loss: 0.3597733480756666\n",
      "48000/49000 loss: 0.4300028828921753\n",
      "epoch 18: valid acc = 0.852, new learning rate = 0.00019860715922910907\n",
      "6000/49000 loss: 0.4025823041176087\n",
      "12000/49000 loss: 0.41751598248207283\n",
      "18000/49000 loss: 0.4149944186959355\n",
      "24000/49000 loss: 0.3788300176587558\n",
      "30000/49000 loss: 0.37081602434918853\n",
      "36000/49000 loss: 0.4531050833198724\n",
      "42000/49000 loss: 0.38839768792294405\n",
      "48000/49000 loss: 0.38521831023138087\n",
      "epoch 19: valid acc = 0.855, new learning rate = 0.0001886768012676536\n",
      "6000/49000 loss: 0.4456631144076273\n",
      "12000/49000 loss: 0.39295423556101594\n",
      "18000/49000 loss: 0.4117653859554927\n",
      "24000/49000 loss: 0.3951678483579802\n",
      "30000/49000 loss: 0.3880260564866173\n",
      "36000/49000 loss: 0.39680685905486907\n",
      "42000/49000 loss: 0.396276135800308\n",
      "48000/49000 loss: 0.3822451662943425\n",
      "epoch 20: valid acc = 0.854, new learning rate = 0.0001792429612042709\n",
      "6000/49000 loss: 0.4062094554869183\n",
      "12000/49000 loss: 0.3873926994609466\n",
      "18000/49000 loss: 0.35421006957786044\n",
      "24000/49000 loss: 0.395390574728423\n",
      "30000/49000 loss: 0.38265196260393913\n",
      "36000/49000 loss: 0.40021348183125055\n",
      "42000/49000 loss: 0.3615386570796197\n",
      "48000/49000 loss: 0.3541666764222101\n",
      "epoch 21: valid acc = 0.857, new learning rate = 0.00017028081314405735\n",
      "6000/49000 loss: 0.38944000295790715\n",
      "12000/49000 loss: 0.4391149960720594\n",
      "18000/49000 loss: 0.44162344760617017\n",
      "24000/49000 loss: 0.4322599850693919\n",
      "30000/49000 loss: 0.3295017034310322\n",
      "36000/49000 loss: 0.4339814341816222\n",
      "42000/49000 loss: 0.444312508696962\n",
      "48000/49000 loss: 0.40588871335295995\n",
      "epoch 22: valid acc = 0.853, new learning rate = 0.00016176677248685447\n",
      "6000/49000 loss: 0.3979927648430388\n",
      "12000/49000 loss: 0.38244011822751933\n",
      "18000/49000 loss: 0.3694421001544572\n",
      "24000/49000 loss: 0.3839814391563684\n",
      "30000/49000 loss: 0.3706890710047419\n",
      "36000/49000 loss: 0.3871717361142767\n",
      "42000/49000 loss: 0.39515589920362565\n",
      "48000/49000 loss: 0.3909284498564348\n",
      "epoch 23: valid acc = 0.855, new learning rate = 0.00015367843386251173\n",
      "6000/49000 loss: 0.4296396075070796\n",
      "12000/49000 loss: 0.3761662612619961\n",
      "18000/49000 loss: 0.43554585807435836\n",
      "24000/49000 loss: 0.3617931650703882\n",
      "30000/49000 loss: 0.46704864179605676\n",
      "36000/49000 loss: 0.4430341284742457\n",
      "42000/49000 loss: 0.3884735095248653\n",
      "48000/49000 loss: 0.4223491080934614\n",
      "epoch 24: valid acc = 0.859, new learning rate = 0.00014599451216938612\n",
      "6000/49000 loss: 0.41886503401862235\n",
      "12000/49000 loss: 0.37034113926669804\n",
      "18000/49000 loss: 0.40836755893587295\n",
      "24000/49000 loss: 0.37173394928110337\n",
      "30000/49000 loss: 0.3670554737520014\n",
      "36000/49000 loss: 0.39092303016787755\n",
      "42000/49000 loss: 0.38699202069740574\n",
      "48000/49000 loss: 0.40047390703068086\n",
      "epoch 25: valid acc = 0.858, new learning rate = 0.00013869478656091682\n",
      "6000/49000 loss: 0.307147315488195\n",
      "12000/49000 loss: 0.4082554117443907\n",
      "18000/49000 loss: 0.31655648596518215\n",
      "24000/49000 loss: 0.34061702796876786\n",
      "30000/49000 loss: 0.4360396621370219\n",
      "36000/49000 loss: 0.4172102522834147\n",
      "42000/49000 loss: 0.4307896387772142\n",
      "48000/49000 loss: 0.4071128242733218\n",
      "epoch 26: valid acc = 0.86, new learning rate = 0.00013176004723287096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/49000 loss: 0.39947758591606475\n",
      "12000/49000 loss: 0.3918137887027265\n",
      "18000/49000 loss: 0.3999412127471128\n",
      "24000/49000 loss: 0.39858704687174845\n",
      "30000/49000 loss: 0.3978058893458052\n",
      "36000/49000 loss: 0.4255756194543843\n",
      "42000/49000 loss: 0.40392712375262707\n",
      "48000/49000 loss: 0.3837121407636448\n",
      "epoch 27: valid acc = 0.858, new learning rate = 0.0001251720448712274\n",
      "6000/49000 loss: 0.4119699436309749\n",
      "12000/49000 loss: 0.4087468767119667\n",
      "18000/49000 loss: 0.40696039271803336\n",
      "24000/49000 loss: 0.4546185194412799\n",
      "30000/49000 loss: 0.39472368062548396\n",
      "36000/49000 loss: 0.39403402366878115\n",
      "42000/49000 loss: 0.395937330977559\n",
      "48000/49000 loss: 0.4476050226591325\n",
      "epoch 28: valid acc = 0.863, new learning rate = 0.00011891344262766602\n",
      "6000/49000 loss: 0.38798548181459225\n",
      "12000/49000 loss: 0.39430819035391973\n",
      "18000/49000 loss: 0.4134976309651203\n",
      "24000/49000 loss: 0.36334775057369106\n",
      "30000/49000 loss: 0.383022892685082\n",
      "36000/49000 loss: 0.37868494862680263\n",
      "42000/49000 loss: 0.3637218265629216\n",
      "48000/49000 loss: 0.3792327409802813\n",
      "epoch 29: valid acc = 0.864, new learning rate = 0.00011296777049628272\n",
      "6000/49000 loss: 0.3664403075134992\n",
      "12000/49000 loss: 0.3818642928707015\n",
      "18000/49000 loss: 0.4734857894407619\n",
      "24000/49000 loss: 0.428460208756532\n",
      "30000/49000 loss: 0.4356723456419587\n",
      "36000/49000 loss: 0.3801318461807882\n",
      "42000/49000 loss: 0.372028055351955\n",
      "48000/49000 loss: 0.4398593504934288\n",
      "epoch 30: valid acc = 0.862, new learning rate = 0.00010731938197146858\n",
      "6000/49000 loss: 0.3991480061596295\n",
      "12000/49000 loss: 0.3910065141826938\n",
      "18000/49000 loss: 0.4164680874435688\n",
      "24000/49000 loss: 0.3807643772033096\n",
      "30000/49000 loss: 0.4458112555354931\n",
      "36000/49000 loss: 0.3806788165660069\n",
      "42000/49000 loss: 0.40164597583265327\n",
      "48000/49000 loss: 0.4298343448708545\n",
      "epoch 31: valid acc = 0.861, new learning rate = 0.00010195341287289515\n",
      "6000/49000 loss: 0.34977501860616667\n",
      "12000/49000 loss: 0.33548627037380546\n",
      "18000/49000 loss: 0.40294256165078957\n",
      "24000/49000 loss: 0.43458919461469975\n",
      "30000/49000 loss: 0.37776769167972757\n",
      "36000/49000 loss: 0.3819932933636271\n",
      "42000/49000 loss: 0.3916123114321773\n",
      "48000/49000 loss: 0.4131157981535139\n",
      "epoch 32: valid acc = 0.863, new learning rate = 9.685574222925039e-05\n",
      "6000/49000 loss: 0.3656344556555466\n",
      "12000/49000 loss: 0.32878031833154925\n",
      "18000/49000 loss: 0.4267618324097004\n",
      "24000/49000 loss: 0.4458288883058013\n",
      "30000/49000 loss: 0.41291015047822255\n",
      "36000/49000 loss: 0.36710665097966994\n",
      "42000/49000 loss: 0.4122535727708411\n",
      "48000/49000 loss: 0.37149893469610884\n",
      "epoch 33: valid acc = 0.863, new learning rate = 9.201295511778786e-05\n",
      "6000/49000 loss: 0.4086045829956849\n",
      "12000/49000 loss: 0.39148372485016514\n",
      "18000/49000 loss: 0.4058406018299064\n",
      "24000/49000 loss: 0.3786617992449427\n",
      "30000/49000 loss: 0.38531901969262466\n",
      "36000/49000 loss: 0.36076968278512217\n",
      "42000/49000 loss: 0.3944751980333893\n",
      "48000/49000 loss: 0.3808891808555997\n",
      "epoch 34: valid acc = 0.864, new learning rate = 8.741230736189846e-05\n",
      "6000/49000 loss: 0.3001508104161059\n",
      "12000/49000 loss: 0.38136055729242657\n",
      "18000/49000 loss: 0.3187912700160641\n",
      "24000/49000 loss: 0.39599596519961194\n",
      "30000/49000 loss: 0.39071927753886215\n",
      "36000/49000 loss: 0.3446993951109079\n",
      "42000/49000 loss: 0.32489445888549484\n",
      "48000/49000 loss: 0.3455189293826009\n",
      "epoch 35: valid acc = 0.865, new learning rate = 8.304169199380353e-05\n",
      "6000/49000 loss: 0.4165195774599256\n",
      "12000/49000 loss: 0.39619951920035235\n",
      "18000/49000 loss: 0.3250252587607699\n",
      "24000/49000 loss: 0.42384909498363826\n",
      "30000/49000 loss: 0.3850419435006994\n",
      "36000/49000 loss: 0.3858228675170886\n",
      "42000/49000 loss: 0.40397656084879224\n",
      "48000/49000 loss: 0.3928103023447051\n",
      "epoch 36: valid acc = 0.864, new learning rate = 7.888960739411335e-05\n",
      "6000/49000 loss: 0.3453159885660244\n",
      "12000/49000 loss: 0.35061065445772993\n",
      "18000/49000 loss: 0.37598007393118504\n",
      "24000/49000 loss: 0.34195605859026335\n",
      "30000/49000 loss: 0.37869167913347584\n",
      "36000/49000 loss: 0.35255492996658355\n",
      "42000/49000 loss: 0.4181016285566139\n",
      "48000/49000 loss: 0.4009020167144814\n",
      "epoch 37: valid acc = 0.863, new learning rate = 7.494512702440768e-05\n",
      "6000/49000 loss: 0.3928969712424049\n",
      "12000/49000 loss: 0.393235897431731\n",
      "18000/49000 loss: 0.3637251873079849\n",
      "24000/49000 loss: 0.33506679842257214\n",
      "30000/49000 loss: 0.38145847505210584\n",
      "36000/49000 loss: 0.3409476545097567\n",
      "42000/49000 loss: 0.36414785888618095\n",
      "48000/49000 loss: 0.3278680704494069\n",
      "epoch 38: valid acc = 0.862, new learning rate = 7.119787067318729e-05\n",
      "6000/49000 loss: 0.3742942949119721\n",
      "12000/49000 loss: 0.37814965305529086\n",
      "18000/49000 loss: 0.4065225975914799\n",
      "24000/49000 loss: 0.43587351624278287\n",
      "30000/49000 loss: 0.3460293771514916\n",
      "36000/49000 loss: 0.3752753429870436\n",
      "42000/49000 loss: 0.359226785816263\n",
      "48000/49000 loss: 0.3589625076338338\n",
      "epoch 39: valid acc = 0.866, new learning rate = 6.763797713952792e-05\n",
      "6000/49000 loss: 0.37259948398912984\n",
      "12000/49000 loss: 0.3842114896729018\n",
      "18000/49000 loss: 0.38178852016141357\n",
      "24000/49000 loss: 0.37989359177220244\n",
      "30000/49000 loss: 0.31233274861657495\n",
      "36000/49000 loss: 0.35508851685559023\n",
      "42000/49000 loss: 0.37867313009732007\n",
      "48000/49000 loss: 0.3964255532560995\n",
      "epoch 40: valid acc = 0.863, new learning rate = 6.425607828255152e-05\n",
      "6000/49000 loss: 0.3470043217424457\n",
      "12000/49000 loss: 0.3690271168589055\n",
      "18000/49000 loss: 0.3361632662910668\n",
      "24000/49000 loss: 0.372873892059401\n",
      "30000/49000 loss: 0.3768964105882858\n",
      "36000/49000 loss: 0.3583160746219062\n",
      "42000/49000 loss: 0.37166900936493763\n",
      "48000/49000 loss: 0.3769018752654004\n",
      "epoch 41: valid acc = 0.866, new learning rate = 6.104327436842394e-05\n",
      "6000/49000 loss: 0.3121294806062424\n",
      "12000/49000 loss: 0.3734388542620333\n",
      "18000/49000 loss: 0.3926257797140755\n",
      "24000/49000 loss: 0.40157071145658396\n",
      "30000/49000 loss: 0.4180845189160559\n",
      "36000/49000 loss: 0.33862344553235096\n",
      "42000/49000 loss: 0.44838289560078975\n",
      "48000/49000 loss: 0.36721212309934737\n",
      "epoch 42: valid acc = 0.865, new learning rate = 5.799111065000274e-05\n",
      "6000/49000 loss: 0.4229137562247565\n",
      "12000/49000 loss: 0.35365842092299016\n",
      "18000/49000 loss: 0.3388677531510625\n",
      "24000/49000 loss: 0.37697571100887833\n",
      "30000/49000 loss: 0.3213337202298986\n",
      "36000/49000 loss: 0.41437925256731156\n",
      "42000/49000 loss: 0.37747834032419686\n",
      "48000/49000 loss: 0.34976076481850465\n",
      "epoch 43: valid acc = 0.862, new learning rate = 5.5091555117502596e-05\n",
      "6000/49000 loss: 0.327461573771481\n",
      "12000/49000 loss: 0.3844955808423867\n",
      "18000/49000 loss: 0.3538732440296878\n",
      "24000/49000 loss: 0.43413260750447674\n",
      "30000/49000 loss: 0.45478737584696444\n",
      "36000/49000 loss: 0.39098866009700717\n",
      "42000/49000 loss: 0.37951925627691935\n",
      "48000/49000 loss: 0.3090668055733766\n",
      "epoch 44: valid acc = 0.864, new learning rate = 5.2336977361627463e-05\n",
      "6000/49000 loss: 0.3777227089743084\n",
      "12000/49000 loss: 0.3434158722037416\n",
      "18000/49000 loss: 0.3263882552670137\n",
      "24000/49000 loss: 0.35357912105404127\n",
      "30000/49000 loss: 0.3330362844767199\n",
      "36000/49000 loss: 0.4031781287373731\n",
      "42000/49000 loss: 0.3552517787874912\n",
      "48000/49000 loss: 0.3394411020255502\n",
      "epoch 45: valid acc = 0.866, new learning rate = 4.972012849354609e-05\n",
      "6000/49000 loss: 0.3650664872426946\n",
      "12000/49000 loss: 0.3750480689518566\n",
      "18000/49000 loss: 0.3796366854538462\n",
      "24000/49000 loss: 0.3393178181545922\n",
      "30000/49000 loss: 0.35028380290448585\n",
      "36000/49000 loss: 0.39223295171776884\n",
      "42000/49000 loss: 0.39131662205485523\n",
      "48000/49000 loss: 0.3210017621529785\n",
      "epoch 46: valid acc = 0.866, new learning rate = 4.723412206886878e-05\n",
      "6000/49000 loss: 0.334600479600229\n",
      "12000/49000 loss: 0.3175230130515418\n",
      "18000/49000 loss: 0.3486528410900762\n",
      "24000/49000 loss: 0.3630268830367058\n",
      "30000/49000 loss: 0.4048377131051528\n",
      "36000/49000 loss: 0.313782931813429\n",
      "42000/49000 loss: 0.34616062271012177\n",
      "48000/49000 loss: 0.3653016517486719\n",
      "epoch 47: valid acc = 0.864, new learning rate = 4.487241596542534e-05\n",
      "6000/49000 loss: 0.3730585909843818\n",
      "12000/49000 loss: 0.35107943707714934\n",
      "18000/49000 loss: 0.38675198714954473\n",
      "24000/49000 loss: 0.33827555305564727\n",
      "30000/49000 loss: 0.343280281097033\n",
      "36000/49000 loss: 0.3508150601409596\n",
      "42000/49000 loss: 0.3553831242637862\n",
      "48000/49000 loss: 0.36987082617859346\n",
      "epoch 48: valid acc = 0.865, new learning rate = 4.262879516715407e-05\n",
      "6000/49000 loss: 0.31443708900533135\n",
      "12000/49000 loss: 0.41186332181806035\n",
      "18000/49000 loss: 0.3360745933351321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000/49000 loss: 0.3425724544483937\n",
      "30000/49000 loss: 0.3762446329079355\n",
      "36000/49000 loss: 0.35669835515939646\n",
      "42000/49000 loss: 0.34882199022919325\n",
      "48000/49000 loss: 0.37064053800012575\n",
      "epoch 49: valid acc = 0.863, new learning rate = 4.049735540879637e-05\n",
      "6000/49000 loss: 0.3655389836412504\n",
      "12000/49000 loss: 0.3428398738808444\n",
      "18000/49000 loss: 0.35394313629981605\n",
      "24000/49000 loss: 0.3233525338557808\n",
      "30000/49000 loss: 0.3296820905150604\n",
      "36000/49000 loss: 0.34044307679424823\n",
      "42000/49000 loss: 0.3454983935756533\n",
      "48000/49000 loss: 0.3427447019957749\n",
      "epoch 50: valid acc = 0.864, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.8691428571428571\n",
      "test acc: 0.864\n",
      "test acc: 0.8491\n",
      "number of batches for training: 81\n",
      "epoch 1: valid acc = 0.542, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.66, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.738, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.751, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.773, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.799, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.805, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.814, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.825, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.827, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.828, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.832, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.838, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.842, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.846, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.849, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.85, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.85, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.853, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.853, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.855, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.853, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.854, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.855, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.858, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.855, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.857, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.856, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.859, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.858, new learning rate = 0.00010731938197146858\n",
      "epoch 31: valid acc = 0.863, new learning rate = 0.00010195341287289515\n",
      "epoch 32: valid acc = 0.86, new learning rate = 9.685574222925039e-05\n",
      "epoch 33: valid acc = 0.863, new learning rate = 9.201295511778786e-05\n",
      "epoch 34: valid acc = 0.859, new learning rate = 8.741230736189846e-05\n",
      "epoch 35: valid acc = 0.864, new learning rate = 8.304169199380353e-05\n",
      "epoch 36: valid acc = 0.864, new learning rate = 7.888960739411335e-05\n",
      "epoch 37: valid acc = 0.86, new learning rate = 7.494512702440768e-05\n",
      "epoch 38: valid acc = 0.861, new learning rate = 7.119787067318729e-05\n",
      "epoch 39: valid acc = 0.862, new learning rate = 6.763797713952792e-05\n",
      "epoch 40: valid acc = 0.862, new learning rate = 6.425607828255152e-05\n",
      "epoch 41: valid acc = 0.863, new learning rate = 6.104327436842394e-05\n",
      "epoch 42: valid acc = 0.864, new learning rate = 5.799111065000274e-05\n",
      "epoch 43: valid acc = 0.862, new learning rate = 5.5091555117502596e-05\n",
      "epoch 44: valid acc = 0.864, new learning rate = 5.2336977361627463e-05\n",
      "epoch 45: valid acc = 0.862, new learning rate = 4.972012849354609e-05\n",
      "epoch 46: valid acc = 0.865, new learning rate = 4.723412206886878e-05\n",
      "epoch 47: valid acc = 0.864, new learning rate = 4.487241596542534e-05\n",
      "epoch 48: valid acc = 0.86, new learning rate = 4.262879516715407e-05\n",
      "epoch 49: valid acc = 0.864, new learning rate = 4.049735540879637e-05\n",
      "epoch 50: valid acc = 0.86, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.8678571428571429\n",
      "test acc: 0.86\n",
      "test acc: 0.8488\n",
      "number of batches for training: 81\n",
      "6000/49000 loss: 4.3962727925512635\n",
      "12000/49000 loss: 3.420680862518347\n",
      "18000/49000 loss: 2.5138261271925786\n",
      "24000/49000 loss: 2.642917925988552\n",
      "30000/49000 loss: 2.196073700580415\n",
      "36000/49000 loss: 2.1462149690927266\n",
      "42000/49000 loss: 1.8157916941596226\n",
      "48000/49000 loss: 1.499467482690932\n",
      "epoch 1: valid acc = 0.539, new learning rate = 0.000475\n",
      "6000/49000 loss: 1.3478130946522937\n",
      "12000/49000 loss: 1.2979347258494027\n",
      "18000/49000 loss: 1.3008051747232907\n",
      "24000/49000 loss: 1.1595632552502375\n",
      "30000/49000 loss: 1.0963119125417067\n",
      "36000/49000 loss: 1.0428299954076639\n",
      "42000/49000 loss: 1.0157980621996965\n",
      "48000/49000 loss: 1.063364867689262\n",
      "epoch 2: valid acc = 0.66, new learning rate = 0.00045125\n",
      "6000/49000 loss: 0.9334164331777828\n",
      "12000/49000 loss: 0.8846169766525795\n",
      "18000/49000 loss: 0.9063506801166457\n",
      "24000/49000 loss: 0.8576258177023502\n",
      "30000/49000 loss: 0.8542100105626093\n",
      "36000/49000 loss: 0.7861073697223073\n",
      "42000/49000 loss: 0.7843551785470915\n",
      "48000/49000 loss: 0.704377395309237\n",
      "epoch 3: valid acc = 0.736, new learning rate = 0.0004286875\n",
      "6000/49000 loss: 0.7046850760581201\n",
      "12000/49000 loss: 0.7009617379582381\n",
      "18000/49000 loss: 0.6783735961165033\n",
      "24000/49000 loss: 0.7110540561799132\n",
      "30000/49000 loss: 0.6365056730967079\n",
      "36000/49000 loss: 0.7149406116284429\n",
      "42000/49000 loss: 0.6503104072478708\n",
      "48000/49000 loss: 0.6393297025826332\n",
      "epoch 4: valid acc = 0.759, new learning rate = 0.00040725312499999993\n",
      "6000/49000 loss: 0.6462475203564559\n",
      "12000/49000 loss: 0.6164357726574318\n",
      "18000/49000 loss: 0.6133655929173133\n",
      "24000/49000 loss: 0.5760201772814675\n",
      "30000/49000 loss: 0.6196466038101428\n",
      "36000/49000 loss: 0.6331127517946595\n",
      "42000/49000 loss: 0.6038932175720103\n",
      "48000/49000 loss: 0.5990980148860656\n",
      "epoch 5: valid acc = 0.785, new learning rate = 0.0003868904687499999\n",
      "6000/49000 loss: 0.5659788049783301\n",
      "12000/49000 loss: 0.5606713815285614\n",
      "18000/49000 loss: 0.5335027646642446\n",
      "24000/49000 loss: 0.5640834665270008\n",
      "30000/49000 loss: 0.588138622356503\n",
      "36000/49000 loss: 0.5482293081854479\n",
      "42000/49000 loss: 0.5242463483208699\n",
      "48000/49000 loss: 0.500206258367502\n",
      "epoch 6: valid acc = 0.799, new learning rate = 0.0003675459453124999\n",
      "6000/49000 loss: 0.5866862533233123\n",
      "12000/49000 loss: 0.5487523327564223\n",
      "18000/49000 loss: 0.5382863071333854\n",
      "24000/49000 loss: 0.536732663770001\n",
      "30000/49000 loss: 0.5467792145073839\n",
      "36000/49000 loss: 0.5745723354907385\n",
      "42000/49000 loss: 0.5096817193833926\n",
      "48000/49000 loss: 0.5193357867965888\n",
      "epoch 7: valid acc = 0.812, new learning rate = 0.00034916864804687486\n",
      "6000/49000 loss: 0.5069980707046744\n",
      "12000/49000 loss: 0.48177053864429686\n",
      "18000/49000 loss: 0.5220046870071137\n",
      "24000/49000 loss: 0.5531837188715371\n",
      "30000/49000 loss: 0.4812891716250783\n",
      "36000/49000 loss: 0.5096051590402255\n",
      "42000/49000 loss: 0.5123669341181583\n",
      "48000/49000 loss: 0.536735109103797\n",
      "epoch 8: valid acc = 0.819, new learning rate = 0.0003317102156445311\n",
      "6000/49000 loss: 0.461179072402972\n",
      "12000/49000 loss: 0.4589178896671128\n",
      "18000/49000 loss: 0.4549744651075758\n",
      "24000/49000 loss: 0.5097993536705749\n",
      "30000/49000 loss: 0.5109597507537152\n",
      "36000/49000 loss: 0.42935803424407964\n",
      "42000/49000 loss: 0.4889529389662569\n",
      "48000/49000 loss: 0.4690068417540194\n",
      "epoch 9: valid acc = 0.823, new learning rate = 0.0003151247048623045\n",
      "6000/49000 loss: 0.4817510126830069\n",
      "12000/49000 loss: 0.43985100722362136\n",
      "18000/49000 loss: 0.48702106643870585\n",
      "24000/49000 loss: 0.4599416443865873\n",
      "30000/49000 loss: 0.5237356873059449\n",
      "36000/49000 loss: 0.44465567455486776\n",
      "42000/49000 loss: 0.44204729881751126\n",
      "48000/49000 loss: 0.47034243235177753\n",
      "epoch 10: valid acc = 0.824, new learning rate = 0.00029936846961918924\n",
      "6000/49000 loss: 0.43951460112037993\n",
      "12000/49000 loss: 0.46773285624058997\n",
      "18000/49000 loss: 0.4947338170937162\n",
      "24000/49000 loss: 0.4651275758720361\n",
      "30000/49000 loss: 0.5265524957699055\n",
      "36000/49000 loss: 0.40932163385837134\n",
      "42000/49000 loss: 0.5194976825666139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/49000 loss: 0.44016330383460517\n",
      "epoch 11: valid acc = 0.832, new learning rate = 0.00028440004613822977\n",
      "6000/49000 loss: 0.518543140009664\n",
      "12000/49000 loss: 0.4342508539110844\n",
      "18000/49000 loss: 0.45057457828545394\n",
      "24000/49000 loss: 0.46083683538066406\n",
      "30000/49000 loss: 0.4460883058740961\n",
      "36000/49000 loss: 0.483154060079228\n",
      "42000/49000 loss: 0.46772766278493877\n",
      "48000/49000 loss: 0.41195363429072546\n",
      "epoch 12: valid acc = 0.835, new learning rate = 0.00027018004383131826\n",
      "6000/49000 loss: 0.4395946307528439\n",
      "12000/49000 loss: 0.39881752777894597\n",
      "18000/49000 loss: 0.40444424752432845\n",
      "24000/49000 loss: 0.45475670481399527\n",
      "30000/49000 loss: 0.424958889339386\n",
      "36000/49000 loss: 0.4876621757237828\n",
      "42000/49000 loss: 0.48118392676932564\n",
      "48000/49000 loss: 0.44530525936955523\n",
      "epoch 13: valid acc = 0.834, new learning rate = 0.00025667104163975234\n",
      "6000/49000 loss: 0.3515115260402663\n",
      "12000/49000 loss: 0.48305411300426626\n",
      "18000/49000 loss: 0.3802618205492967\n",
      "24000/49000 loss: 0.44485668064870787\n",
      "30000/49000 loss: 0.4516914976330938\n",
      "36000/49000 loss: 0.42518862511389227\n",
      "42000/49000 loss: 0.45848262670665246\n",
      "48000/49000 loss: 0.42573127953304324\n",
      "epoch 14: valid acc = 0.836, new learning rate = 0.00024383748955776472\n",
      "6000/49000 loss: 0.494556982240871\n",
      "12000/49000 loss: 0.439270075569351\n",
      "18000/49000 loss: 0.4277793065613626\n",
      "24000/49000 loss: 0.43793594093976623\n",
      "30000/49000 loss: 0.4132185327091244\n",
      "36000/49000 loss: 0.481975731919241\n",
      "42000/49000 loss: 0.46257444209296633\n",
      "48000/49000 loss: 0.38365243066460636\n",
      "epoch 15: valid acc = 0.842, new learning rate = 0.00023164561507987649\n",
      "6000/49000 loss: 0.4487487718313689\n",
      "12000/49000 loss: 0.4798533926061891\n",
      "18000/49000 loss: 0.4436226729646525\n",
      "24000/49000 loss: 0.4467951180444798\n",
      "30000/49000 loss: 0.4577825241545036\n",
      "36000/49000 loss: 0.4452109934051299\n",
      "42000/49000 loss: 0.4125011223680082\n",
      "48000/49000 loss: 0.47799649092257\n",
      "epoch 16: valid acc = 0.842, new learning rate = 0.00022006333432588265\n",
      "6000/49000 loss: 0.48906163922110263\n",
      "12000/49000 loss: 0.4361841578086304\n",
      "18000/49000 loss: 0.3998586197588124\n",
      "24000/49000 loss: 0.40138896322520107\n",
      "30000/49000 loss: 0.4421101093503754\n",
      "36000/49000 loss: 0.42586229290597155\n",
      "42000/49000 loss: 0.3855357859527943\n",
      "48000/49000 loss: 0.42164061667703473\n",
      "epoch 17: valid acc = 0.847, new learning rate = 0.00020906016760958852\n",
      "6000/49000 loss: 0.44594694387559136\n",
      "12000/49000 loss: 0.4353157482232585\n",
      "18000/49000 loss: 0.4278879094131702\n",
      "24000/49000 loss: 0.4510483457296984\n",
      "30000/49000 loss: 0.3508117844295139\n",
      "36000/49000 loss: 0.42629097985882813\n",
      "42000/49000 loss: 0.3824071685774739\n",
      "48000/49000 loss: 0.4157317274117385\n",
      "epoch 18: valid acc = 0.849, new learning rate = 0.00019860715922910907\n",
      "6000/49000 loss: 0.40773075663643593\n",
      "12000/49000 loss: 0.3714883502501765\n",
      "18000/49000 loss: 0.41436803659934396\n",
      "24000/49000 loss: 0.46339823742700487\n",
      "30000/49000 loss: 0.48891319835031577\n",
      "36000/49000 loss: 0.4336711779967065\n",
      "42000/49000 loss: 0.4531175131258505\n",
      "48000/49000 loss: 0.40130476965073353\n",
      "epoch 19: valid acc = 0.849, new learning rate = 0.0001886768012676536\n",
      "6000/49000 loss: 0.393463351172856\n",
      "12000/49000 loss: 0.4308630612163819\n",
      "18000/49000 loss: 0.4158343482625119\n",
      "24000/49000 loss: 0.39226485101922487\n",
      "30000/49000 loss: 0.3283809280992372\n",
      "36000/49000 loss: 0.4158236217274437\n",
      "42000/49000 loss: 0.39190905811501453\n",
      "48000/49000 loss: 0.4092916386675498\n",
      "epoch 20: valid acc = 0.85, new learning rate = 0.0001792429612042709\n",
      "6000/49000 loss: 0.40998009572062566\n",
      "12000/49000 loss: 0.4005153266954819\n",
      "18000/49000 loss: 0.4013905728241207\n",
      "24000/49000 loss: 0.4292394494890372\n",
      "30000/49000 loss: 0.3298326917843033\n",
      "36000/49000 loss: 0.44110936018940144\n",
      "42000/49000 loss: 0.34370482404615005\n",
      "48000/49000 loss: 0.41795969347459533\n",
      "epoch 21: valid acc = 0.855, new learning rate = 0.00017028081314405735\n",
      "6000/49000 loss: 0.362856991511505\n",
      "12000/49000 loss: 0.37517038353228355\n",
      "18000/49000 loss: 0.42526666218920245\n",
      "24000/49000 loss: 0.3665255315689074\n",
      "30000/49000 loss: 0.411926695033338\n",
      "36000/49000 loss: 0.38551452587071156\n",
      "42000/49000 loss: 0.4624657412866593\n",
      "48000/49000 loss: 0.40305146842099043\n",
      "epoch 22: valid acc = 0.854, new learning rate = 0.00016176677248685447\n",
      "6000/49000 loss: 0.4551183261644183\n",
      "12000/49000 loss: 0.45875270493424986\n",
      "18000/49000 loss: 0.39738119234009933\n",
      "24000/49000 loss: 0.39734395878373824\n",
      "30000/49000 loss: 0.4343307500081229\n",
      "36000/49000 loss: 0.4385105423388481\n",
      "42000/49000 loss: 0.376308165094108\n",
      "48000/49000 loss: 0.42741030744979314\n",
      "epoch 23: valid acc = 0.855, new learning rate = 0.00015367843386251173\n",
      "6000/49000 loss: 0.39884916244617796\n",
      "12000/49000 loss: 0.38385948403593423\n",
      "18000/49000 loss: 0.3752951490570943\n",
      "24000/49000 loss: 0.37412397631827937\n",
      "30000/49000 loss: 0.43356587748789055\n",
      "36000/49000 loss: 0.3854614594193404\n",
      "42000/49000 loss: 0.33793329984021675\n",
      "48000/49000 loss: 0.390258606409132\n",
      "epoch 24: valid acc = 0.853, new learning rate = 0.00014599451216938612\n",
      "6000/49000 loss: 0.3559260595556702\n",
      "12000/49000 loss: 0.38312184342836836\n",
      "18000/49000 loss: 0.4106521892499398\n",
      "24000/49000 loss: 0.3705820856621892\n",
      "30000/49000 loss: 0.4831205689037352\n",
      "36000/49000 loss: 0.3999601844524347\n",
      "42000/49000 loss: 0.372358361277295\n",
      "48000/49000 loss: 0.4044741503316942\n",
      "epoch 25: valid acc = 0.857, new learning rate = 0.00013869478656091682\n",
      "6000/49000 loss: 0.4652333221421625\n",
      "12000/49000 loss: 0.441013344085447\n",
      "18000/49000 loss: 0.3785116292464247\n",
      "24000/49000 loss: 0.4213181054028909\n",
      "30000/49000 loss: 0.42849280796706585\n",
      "36000/49000 loss: 0.38545401725684675\n",
      "42000/49000 loss: 0.393228850656252\n",
      "48000/49000 loss: 0.379923540050611\n",
      "epoch 26: valid acc = 0.855, new learning rate = 0.00013176004723287096\n",
      "6000/49000 loss: 0.42535254826115837\n",
      "12000/49000 loss: 0.373815497316736\n",
      "18000/49000 loss: 0.3777513075517063\n",
      "24000/49000 loss: 0.40459579927304085\n",
      "30000/49000 loss: 0.4092159151126189\n",
      "36000/49000 loss: 0.3920698054616278\n",
      "42000/49000 loss: 0.3665096856216095\n",
      "48000/49000 loss: 0.3710799063698422\n",
      "epoch 27: valid acc = 0.861, new learning rate = 0.0001251720448712274\n",
      "6000/49000 loss: 0.40802306347635753\n",
      "12000/49000 loss: 0.41322087126933377\n",
      "18000/49000 loss: 0.44006568084244313\n",
      "24000/49000 loss: 0.3849497067423249\n",
      "30000/49000 loss: 0.3379908003089746\n",
      "36000/49000 loss: 0.4008333477066616\n",
      "42000/49000 loss: 0.3502816447049939\n",
      "48000/49000 loss: 0.3488097307078726\n",
      "epoch 28: valid acc = 0.862, new learning rate = 0.00011891344262766602\n",
      "6000/49000 loss: 0.37334586514168033\n",
      "12000/49000 loss: 0.42313830549622866\n",
      "18000/49000 loss: 0.3718370637852586\n",
      "24000/49000 loss: 0.43081657204182755\n",
      "30000/49000 loss: 0.3509545447856583\n",
      "36000/49000 loss: 0.3335250371082315\n",
      "42000/49000 loss: 0.3351990948827519\n",
      "48000/49000 loss: 0.37594337764752095\n",
      "epoch 29: valid acc = 0.858, new learning rate = 0.00011296777049628272\n",
      "6000/49000 loss: 0.38319235137939717\n",
      "12000/49000 loss: 0.39178612219525727\n",
      "18000/49000 loss: 0.38805463048854943\n",
      "24000/49000 loss: 0.3701159523971604\n",
      "30000/49000 loss: 0.42298676478661235\n",
      "36000/49000 loss: 0.38503603080637955\n",
      "42000/49000 loss: 0.3363667005211988\n",
      "48000/49000 loss: 0.37571658775975114\n",
      "epoch 30: valid acc = 0.863, new learning rate = 0.00010731938197146858\n",
      "6000/49000 loss: 0.4185990013434087\n",
      "12000/49000 loss: 0.3974535714499449\n",
      "18000/49000 loss: 0.3894252401228292\n",
      "24000/49000 loss: 0.3944168750720631\n",
      "30000/49000 loss: 0.37959748869327464\n",
      "36000/49000 loss: 0.40443290077848326\n",
      "42000/49000 loss: 0.36232326166619994\n",
      "48000/49000 loss: 0.4014973965147471\n",
      "epoch 31: valid acc = 0.861, new learning rate = 0.00010195341287289515\n",
      "6000/49000 loss: 0.4469946152411266\n",
      "12000/49000 loss: 0.3970263880222158\n",
      "18000/49000 loss: 0.41446189118050475\n",
      "24000/49000 loss: 0.36760603715079165\n",
      "30000/49000 loss: 0.4040176280386499\n",
      "36000/49000 loss: 0.36635088774259794\n",
      "42000/49000 loss: 0.3277711492416339\n",
      "48000/49000 loss: 0.36060159209416665\n",
      "epoch 32: valid acc = 0.862, new learning rate = 9.685574222925039e-05\n",
      "6000/49000 loss: 0.4522998233435548\n",
      "12000/49000 loss: 0.3798491365381637\n",
      "18000/49000 loss: 0.37495694785267325\n",
      "24000/49000 loss: 0.3204891837529481\n",
      "30000/49000 loss: 0.34853789123572054\n",
      "36000/49000 loss: 0.38744495427430614\n",
      "42000/49000 loss: 0.35294689753119524\n",
      "48000/49000 loss: 0.4051683582352814\n",
      "epoch 33: valid acc = 0.861, new learning rate = 9.201295511778786e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/49000 loss: 0.3807428893159939\n",
      "12000/49000 loss: 0.42802090233648243\n",
      "18000/49000 loss: 0.35873141821854054\n",
      "24000/49000 loss: 0.3767197535098981\n",
      "30000/49000 loss: 0.3869062182377599\n",
      "36000/49000 loss: 0.37960372014316696\n",
      "42000/49000 loss: 0.3634062548143144\n",
      "48000/49000 loss: 0.3674805692771591\n",
      "epoch 34: valid acc = 0.862, new learning rate = 8.741230736189846e-05\n",
      "6000/49000 loss: 0.37026033044399853\n",
      "12000/49000 loss: 0.4149436037880813\n",
      "18000/49000 loss: 0.37205262482509394\n",
      "24000/49000 loss: 0.3596432119136793\n",
      "30000/49000 loss: 0.37739608752885\n",
      "36000/49000 loss: 0.3573801903705404\n",
      "42000/49000 loss: 0.4139904065283891\n",
      "48000/49000 loss: 0.40018678876967123\n",
      "epoch 35: valid acc = 0.861, new learning rate = 8.304169199380353e-05\n",
      "6000/49000 loss: 0.43334962719842046\n",
      "12000/49000 loss: 0.33768461622313045\n",
      "18000/49000 loss: 0.3526985269390751\n",
      "24000/49000 loss: 0.37473233760110064\n",
      "30000/49000 loss: 0.3924650659737734\n",
      "36000/49000 loss: 0.3742606668186023\n",
      "42000/49000 loss: 0.3753025609048129\n",
      "48000/49000 loss: 0.3872856230905344\n",
      "epoch 36: valid acc = 0.862, new learning rate = 7.888960739411335e-05\n",
      "6000/49000 loss: 0.3671364748973788\n",
      "12000/49000 loss: 0.33456521126089717\n",
      "18000/49000 loss: 0.3219195632440308\n",
      "24000/49000 loss: 0.37780735318905734\n",
      "30000/49000 loss: 0.3751007576932578\n",
      "36000/49000 loss: 0.3960202333188143\n",
      "42000/49000 loss: 0.33631300433613776\n",
      "48000/49000 loss: 0.3232496037484245\n",
      "epoch 37: valid acc = 0.863, new learning rate = 7.494512702440768e-05\n",
      "6000/49000 loss: 0.3312010904927843\n",
      "12000/49000 loss: 0.3282608070500356\n",
      "18000/49000 loss: 0.3922791998645044\n",
      "24000/49000 loss: 0.44604819806149343\n",
      "30000/49000 loss: 0.3801239219798057\n",
      "36000/49000 loss: 0.39534802812761344\n",
      "42000/49000 loss: 0.3848319447911049\n",
      "48000/49000 loss: 0.4034336051873386\n",
      "epoch 38: valid acc = 0.863, new learning rate = 7.119787067318729e-05\n",
      "6000/49000 loss: 0.36904341778253164\n",
      "12000/49000 loss: 0.40378977608358435\n",
      "18000/49000 loss: 0.44457043617450087\n",
      "24000/49000 loss: 0.3367147753295948\n",
      "30000/49000 loss: 0.3914264812689798\n",
      "36000/49000 loss: 0.3776968101190463\n",
      "42000/49000 loss: 0.4159292460654956\n",
      "48000/49000 loss: 0.42305887883326787\n",
      "epoch 39: valid acc = 0.863, new learning rate = 6.763797713952792e-05\n",
      "6000/49000 loss: 0.38384476041406657\n",
      "12000/49000 loss: 0.37338898477240384\n",
      "18000/49000 loss: 0.3658624018807762\n",
      "24000/49000 loss: 0.3479170740719042\n",
      "30000/49000 loss: 0.4265822799082169\n",
      "36000/49000 loss: 0.3844884856841102\n",
      "42000/49000 loss: 0.3708847366788013\n",
      "48000/49000 loss: 0.35053679692170303\n",
      "epoch 40: valid acc = 0.866, new learning rate = 6.425607828255152e-05\n",
      "6000/49000 loss: 0.2985519327495695\n",
      "12000/49000 loss: 0.34596703483126484\n",
      "18000/49000 loss: 0.3665365504655123\n",
      "24000/49000 loss: 0.4039847572805753\n",
      "30000/49000 loss: 0.36223546839360604\n",
      "36000/49000 loss: 0.3675477914257624\n",
      "42000/49000 loss: 0.40431108780620983\n",
      "48000/49000 loss: 0.3930205801319788\n",
      "epoch 41: valid acc = 0.865, new learning rate = 6.104327436842394e-05\n",
      "6000/49000 loss: 0.380688550773556\n",
      "12000/49000 loss: 0.3520962739110728\n",
      "18000/49000 loss: 0.35863022605823747\n",
      "24000/49000 loss: 0.3940452708514869\n",
      "30000/49000 loss: 0.32483064155529057\n",
      "36000/49000 loss: 0.3309253401039296\n",
      "42000/49000 loss: 0.3642335346630754\n",
      "48000/49000 loss: 0.3631714650243503\n",
      "epoch 42: valid acc = 0.864, new learning rate = 5.799111065000274e-05\n",
      "6000/49000 loss: 0.41624393003355126\n",
      "12000/49000 loss: 0.3615174603734043\n",
      "18000/49000 loss: 0.386166439996774\n",
      "24000/49000 loss: 0.3753959565218665\n",
      "30000/49000 loss: 0.35167864163577317\n",
      "36000/49000 loss: 0.3667880574341978\n",
      "42000/49000 loss: 0.39835530777182554\n",
      "48000/49000 loss: 0.4026481420325621\n",
      "epoch 43: valid acc = 0.864, new learning rate = 5.5091555117502596e-05\n",
      "6000/49000 loss: 0.3807535884808739\n",
      "12000/49000 loss: 0.3795936612704894\n",
      "18000/49000 loss: 0.40535833796671017\n",
      "24000/49000 loss: 0.3906099367122067\n",
      "30000/49000 loss: 0.4115377860601236\n",
      "36000/49000 loss: 0.438490588057265\n",
      "42000/49000 loss: 0.32379176120780423\n",
      "48000/49000 loss: 0.3892263361345783\n",
      "epoch 44: valid acc = 0.863, new learning rate = 5.2336977361627463e-05\n",
      "6000/49000 loss: 0.3890627455980736\n",
      "12000/49000 loss: 0.40398961889536\n",
      "18000/49000 loss: 0.3336394709854696\n",
      "24000/49000 loss: 0.3508596806184931\n",
      "30000/49000 loss: 0.46628622958270016\n",
      "36000/49000 loss: 0.437546488930533\n",
      "42000/49000 loss: 0.33887512932245833\n",
      "48000/49000 loss: 0.35601367916708104\n",
      "epoch 45: valid acc = 0.866, new learning rate = 4.972012849354609e-05\n",
      "6000/49000 loss: 0.36900396833960547\n",
      "12000/49000 loss: 0.43436539305024885\n",
      "18000/49000 loss: 0.3715805330102224\n",
      "24000/49000 loss: 0.3809293718792562\n",
      "30000/49000 loss: 0.4395539033181019\n",
      "36000/49000 loss: 0.35741307768314773\n",
      "42000/49000 loss: 0.3755740858448189\n",
      "48000/49000 loss: 0.360122209854822\n",
      "epoch 46: valid acc = 0.862, new learning rate = 4.723412206886878e-05\n",
      "6000/49000 loss: 0.34853091317615387\n",
      "12000/49000 loss: 0.33672503791573477\n",
      "18000/49000 loss: 0.33042088265217134\n",
      "24000/49000 loss: 0.3916931018511363\n",
      "30000/49000 loss: 0.3717690559919081\n",
      "36000/49000 loss: 0.35569090357904104\n",
      "42000/49000 loss: 0.40499538741211427\n",
      "48000/49000 loss: 0.35493168751610915\n",
      "epoch 47: valid acc = 0.862, new learning rate = 4.487241596542534e-05\n",
      "6000/49000 loss: 0.4002946381706464\n",
      "12000/49000 loss: 0.3422171021146445\n",
      "18000/49000 loss: 0.4265448738849417\n",
      "24000/49000 loss: 0.39068032224786226\n",
      "30000/49000 loss: 0.3757405533509394\n",
      "36000/49000 loss: 0.32152135699714185\n",
      "42000/49000 loss: 0.37336428245156505\n",
      "48000/49000 loss: 0.33984809858198883\n",
      "epoch 48: valid acc = 0.864, new learning rate = 4.262879516715407e-05\n",
      "6000/49000 loss: 0.42414225074396267\n",
      "12000/49000 loss: 0.37121042114448605\n",
      "18000/49000 loss: 0.33922085554346915\n",
      "24000/49000 loss: 0.3656222899135941\n",
      "30000/49000 loss: 0.325815217707347\n",
      "36000/49000 loss: 0.3988726766291699\n",
      "42000/49000 loss: 0.3803208307926731\n",
      "48000/49000 loss: 0.32151086951808394\n",
      "epoch 49: valid acc = 0.866, new learning rate = 4.049735540879637e-05\n",
      "6000/49000 loss: 0.3933466345156507\n",
      "12000/49000 loss: 0.4029965768087949\n",
      "18000/49000 loss: 0.42226242387767127\n",
      "24000/49000 loss: 0.3916046516240453\n",
      "30000/49000 loss: 0.3631018280546377\n",
      "36000/49000 loss: 0.4285586726769384\n",
      "42000/49000 loss: 0.38642650061305567\n",
      "48000/49000 loss: 0.32182472431871056\n",
      "epoch 50: valid acc = 0.865, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.868938775510204\n",
      "test acc: 0.865\n",
      "test acc: 0.8494\n",
      "number of batches for training: 81\n",
      "epoch 1: valid acc = 0.531, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.671, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.731, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.759, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.783, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.804, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.806, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.814, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.823, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.822, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.828, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.832, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.834, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.839, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.845, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.84, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.845, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.849, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.849, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.851, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.859, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.851, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.855, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.854, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.858, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.853, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.856, new learning rate = 0.0001251720448712274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 28: valid acc = 0.858, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.86, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.855, new learning rate = 0.00010731938197146858\n",
      "epoch 31: valid acc = 0.863, new learning rate = 0.00010195341287289515\n",
      "epoch 32: valid acc = 0.863, new learning rate = 9.685574222925039e-05\n",
      "epoch 33: valid acc = 0.858, new learning rate = 9.201295511778786e-05\n",
      "epoch 34: valid acc = 0.86, new learning rate = 8.741230736189846e-05\n",
      "epoch 35: valid acc = 0.863, new learning rate = 8.304169199380353e-05\n",
      "epoch 36: valid acc = 0.861, new learning rate = 7.888960739411335e-05\n",
      "epoch 37: valid acc = 0.862, new learning rate = 7.494512702440768e-05\n",
      "epoch 38: valid acc = 0.858, new learning rate = 7.119787067318729e-05\n",
      "epoch 39: valid acc = 0.86, new learning rate = 6.763797713952792e-05\n",
      "epoch 40: valid acc = 0.858, new learning rate = 6.425607828255152e-05\n",
      "epoch 41: valid acc = 0.86, new learning rate = 6.104327436842394e-05\n",
      "epoch 42: valid acc = 0.859, new learning rate = 5.799111065000274e-05\n",
      "epoch 43: valid acc = 0.861, new learning rate = 5.5091555117502596e-05\n",
      "epoch 44: valid acc = 0.861, new learning rate = 5.2336977361627463e-05\n",
      "epoch 45: valid acc = 0.86, new learning rate = 4.972012849354609e-05\n",
      "epoch 46: valid acc = 0.861, new learning rate = 4.723412206886878e-05\n",
      "epoch 47: valid acc = 0.861, new learning rate = 4.487241596542534e-05\n",
      "epoch 48: valid acc = 0.861, new learning rate = 4.262879516715407e-05\n",
      "epoch 49: valid acc = 0.863, new learning rate = 4.049735540879637e-05\n",
      "epoch 50: valid acc = 0.861, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.8680408163265306\n",
      "test acc: 0.861\n",
      "test acc: 0.8467\n",
      "number of batches for training: 49\n",
      "10000/49000 loss: 3.7210222010108565\n",
      "20000/49000 loss: 2.947444443239837\n",
      "30000/49000 loss: 2.9782275697388085\n",
      "40000/49000 loss: 2.4845958319852848\n",
      "epoch 1: valid acc = 0.39, new learning rate = 0.000475\n",
      "10000/49000 loss: 2.1521214303466563\n",
      "20000/49000 loss: 1.9979810464573797\n",
      "30000/49000 loss: 1.747817623086728\n",
      "40000/49000 loss: 1.422776995203589\n",
      "epoch 2: valid acc = 0.528, new learning rate = 0.00045125\n",
      "10000/49000 loss: 1.2814260993351336\n",
      "20000/49000 loss: 1.1873166127228445\n",
      "30000/49000 loss: 1.1545226842444059\n",
      "40000/49000 loss: 1.1571450011353026\n",
      "epoch 3: valid acc = 0.622, new learning rate = 0.0004286875\n",
      "10000/49000 loss: 0.970879801779998\n",
      "20000/49000 loss: 0.9951390561089001\n",
      "30000/49000 loss: 0.9942401860101332\n",
      "40000/49000 loss: 0.9265607874124483\n",
      "epoch 4: valid acc = 0.707, new learning rate = 0.00040725312499999993\n",
      "10000/49000 loss: 0.8893624450629241\n",
      "20000/49000 loss: 0.8561255940220096\n",
      "30000/49000 loss: 0.7830765251338829\n",
      "40000/49000 loss: 0.7639933901170681\n",
      "epoch 5: valid acc = 0.736, new learning rate = 0.0003868904687499999\n",
      "10000/49000 loss: 0.7616175736744842\n",
      "20000/49000 loss: 0.7763482888924632\n",
      "30000/49000 loss: 0.7063278878163766\n",
      "40000/49000 loss: 0.7538573082853021\n",
      "epoch 6: valid acc = 0.745, new learning rate = 0.0003675459453124999\n",
      "10000/49000 loss: 0.6127957228910159\n",
      "20000/49000 loss: 0.6566410415203996\n",
      "30000/49000 loss: 0.6734247863628857\n",
      "40000/49000 loss: 0.6044766401106574\n",
      "epoch 7: valid acc = 0.763, new learning rate = 0.00034916864804687486\n",
      "10000/49000 loss: 0.6481859037403372\n",
      "20000/49000 loss: 0.6309456599480706\n",
      "30000/49000 loss: 0.6235686376854523\n",
      "40000/49000 loss: 0.5917511393628716\n",
      "epoch 8: valid acc = 0.768, new learning rate = 0.0003317102156445311\n",
      "10000/49000 loss: 0.5702278692068884\n",
      "20000/49000 loss: 0.5990507372289157\n",
      "30000/49000 loss: 0.6303654515358802\n",
      "40000/49000 loss: 0.5432946714479884\n",
      "epoch 9: valid acc = 0.784, new learning rate = 0.0003151247048623045\n",
      "10000/49000 loss: 0.5887534541456847\n",
      "20000/49000 loss: 0.5876357117357615\n",
      "30000/49000 loss: 0.5842662993450374\n",
      "40000/49000 loss: 0.5194742315918328\n",
      "epoch 10: valid acc = 0.786, new learning rate = 0.00029936846961918924\n",
      "10000/49000 loss: 0.5601688450679703\n",
      "20000/49000 loss: 0.5606836585936319\n",
      "30000/49000 loss: 0.5578299612314274\n",
      "40000/49000 loss: 0.5174352308413122\n",
      "epoch 11: valid acc = 0.789, new learning rate = 0.00028440004613822977\n",
      "10000/49000 loss: 0.5665648896903996\n",
      "20000/49000 loss: 0.5200227403236072\n",
      "30000/49000 loss: 0.5479689845040101\n",
      "40000/49000 loss: 0.513567587483673\n",
      "epoch 12: valid acc = 0.802, new learning rate = 0.00027018004383131826\n",
      "10000/49000 loss: 0.5463078582785871\n",
      "20000/49000 loss: 0.5541438652593911\n",
      "30000/49000 loss: 0.5388885094785529\n",
      "40000/49000 loss: 0.5366184400581552\n",
      "epoch 13: valid acc = 0.808, new learning rate = 0.00025667104163975234\n",
      "10000/49000 loss: 0.5630820196614914\n",
      "20000/49000 loss: 0.5313298720313826\n",
      "30000/49000 loss: 0.5069410192524748\n",
      "40000/49000 loss: 0.5365903907231809\n",
      "epoch 14: valid acc = 0.809, new learning rate = 0.00024383748955776472\n",
      "10000/49000 loss: 0.5260606277282064\n",
      "20000/49000 loss: 0.5422887378137894\n",
      "30000/49000 loss: 0.4917189449360149\n",
      "40000/49000 loss: 0.5360247133869767\n",
      "epoch 15: valid acc = 0.811, new learning rate = 0.00023164561507987649\n",
      "10000/49000 loss: 0.5100362712537293\n",
      "20000/49000 loss: 0.4965916799087554\n",
      "30000/49000 loss: 0.5003881342421803\n",
      "40000/49000 loss: 0.45037724115200295\n",
      "epoch 16: valid acc = 0.812, new learning rate = 0.00022006333432588265\n",
      "10000/49000 loss: 0.49517207465693336\n",
      "20000/49000 loss: 0.49637758397645465\n",
      "30000/49000 loss: 0.49983914200555396\n",
      "40000/49000 loss: 0.5087123846527009\n",
      "epoch 17: valid acc = 0.815, new learning rate = 0.00020906016760958852\n",
      "10000/49000 loss: 0.48498726169575573\n",
      "20000/49000 loss: 0.46175148301676894\n",
      "30000/49000 loss: 0.48541980893069936\n",
      "40000/49000 loss: 0.48916597265397\n",
      "epoch 18: valid acc = 0.823, new learning rate = 0.00019860715922910907\n",
      "10000/49000 loss: 0.4520219808769787\n",
      "20000/49000 loss: 0.4917540381598982\n",
      "30000/49000 loss: 0.48391814654022547\n",
      "40000/49000 loss: 0.4351681271503139\n",
      "epoch 19: valid acc = 0.826, new learning rate = 0.0001886768012676536\n",
      "10000/49000 loss: 0.4732354820947346\n",
      "20000/49000 loss: 0.5145425426759246\n",
      "30000/49000 loss: 0.4764750971694452\n",
      "40000/49000 loss: 0.4525406840103985\n",
      "epoch 20: valid acc = 0.828, new learning rate = 0.0001792429612042709\n",
      "10000/49000 loss: 0.4613630547839201\n",
      "20000/49000 loss: 0.48410504646441566\n",
      "30000/49000 loss: 0.4694902157049898\n",
      "40000/49000 loss: 0.42113932197548887\n",
      "epoch 21: valid acc = 0.83, new learning rate = 0.00017028081314405735\n",
      "10000/49000 loss: 0.4460490774307351\n",
      "20000/49000 loss: 0.475306264122255\n",
      "30000/49000 loss: 0.4576130400484542\n",
      "40000/49000 loss: 0.43727681461827594\n",
      "epoch 22: valid acc = 0.829, new learning rate = 0.00016176677248685447\n",
      "10000/49000 loss: 0.46804693120016344\n",
      "20000/49000 loss: 0.47451520212837883\n",
      "30000/49000 loss: 0.43729631590873974\n",
      "40000/49000 loss: 0.4543036971516514\n",
      "epoch 23: valid acc = 0.825, new learning rate = 0.00015367843386251173\n",
      "10000/49000 loss: 0.4651028659480395\n",
      "20000/49000 loss: 0.4535536668781252\n",
      "30000/49000 loss: 0.5019196754592757\n",
      "40000/49000 loss: 0.4312272389134076\n",
      "epoch 24: valid acc = 0.828, new learning rate = 0.00014599451216938612\n",
      "10000/49000 loss: 0.4156435255417394\n",
      "20000/49000 loss: 0.489390451672578\n",
      "30000/49000 loss: 0.47831133494866374\n",
      "40000/49000 loss: 0.48707170564369695\n",
      "epoch 25: valid acc = 0.83, new learning rate = 0.00013869478656091682\n",
      "10000/49000 loss: 0.4779294624761275\n",
      "20000/49000 loss: 0.4249231746144236\n",
      "30000/49000 loss: 0.49488599867255606\n",
      "40000/49000 loss: 0.4616401752593553\n",
      "epoch 26: valid acc = 0.832, new learning rate = 0.00013176004723287096\n",
      "10000/49000 loss: 0.4608801014817881\n",
      "20000/49000 loss: 0.4540934597037473\n",
      "30000/49000 loss: 0.4256301437316687\n",
      "40000/49000 loss: 0.3870839203442434\n",
      "epoch 27: valid acc = 0.831, new learning rate = 0.0001251720448712274\n",
      "10000/49000 loss: 0.46852477252710745\n",
      "20000/49000 loss: 0.38380847072017815\n",
      "30000/49000 loss: 0.4282381332855744\n",
      "40000/49000 loss: 0.42650931817906046\n",
      "epoch 28: valid acc = 0.832, new learning rate = 0.00011891344262766602\n",
      "10000/49000 loss: 0.4405780457164051\n",
      "20000/49000 loss: 0.4672297538682222\n",
      "30000/49000 loss: 0.4447643840102438\n",
      "40000/49000 loss: 0.4349932749754239\n",
      "epoch 29: valid acc = 0.834, new learning rate = 0.00011296777049628272\n",
      "10000/49000 loss: 0.45011826298053587\n",
      "20000/49000 loss: 0.4093873729601031\n",
      "30000/49000 loss: 0.43821592032308465\n",
      "40000/49000 loss: 0.485953788922992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30: valid acc = 0.836, new learning rate = 0.00010731938197146858\n",
      "10000/49000 loss: 0.4556315287049649\n",
      "20000/49000 loss: 0.4637276126372323\n",
      "30000/49000 loss: 0.46502065793357295\n",
      "40000/49000 loss: 0.4372378265403183\n",
      "epoch 31: valid acc = 0.833, new learning rate = 0.00010195341287289515\n",
      "10000/49000 loss: 0.44049183341215997\n",
      "20000/49000 loss: 0.42457631847458616\n",
      "30000/49000 loss: 0.44395835417485197\n",
      "40000/49000 loss: 0.4268012119959678\n",
      "epoch 32: valid acc = 0.835, new learning rate = 9.685574222925039e-05\n",
      "10000/49000 loss: 0.4349653003265842\n",
      "20000/49000 loss: 0.43857664986763584\n",
      "30000/49000 loss: 0.4343778751604105\n",
      "40000/49000 loss: 0.4514216121168525\n",
      "epoch 33: valid acc = 0.837, new learning rate = 9.201295511778786e-05\n",
      "10000/49000 loss: 0.43472358768346575\n",
      "20000/49000 loss: 0.46386705718900034\n",
      "30000/49000 loss: 0.3834621811156928\n",
      "40000/49000 loss: 0.4327204235054705\n",
      "epoch 34: valid acc = 0.837, new learning rate = 8.741230736189846e-05\n",
      "10000/49000 loss: 0.38697564142479396\n",
      "20000/49000 loss: 0.431134376249658\n",
      "30000/49000 loss: 0.48757617950503046\n",
      "40000/49000 loss: 0.4304895454310919\n",
      "epoch 35: valid acc = 0.839, new learning rate = 8.304169199380353e-05\n",
      "10000/49000 loss: 0.48179549547811806\n",
      "20000/49000 loss: 0.44604558667508976\n",
      "30000/49000 loss: 0.4433927489627374\n",
      "40000/49000 loss: 0.4313688581357986\n",
      "epoch 36: valid acc = 0.838, new learning rate = 7.888960739411335e-05\n",
      "10000/49000 loss: 0.44336904364181373\n",
      "20000/49000 loss: 0.4164512258125154\n",
      "30000/49000 loss: 0.4284945323019283\n",
      "40000/49000 loss: 0.43449262500119046\n",
      "epoch 37: valid acc = 0.84, new learning rate = 7.494512702440768e-05\n",
      "10000/49000 loss: 0.3917795886875381\n",
      "20000/49000 loss: 0.4378647434960332\n",
      "30000/49000 loss: 0.46367964768801545\n",
      "40000/49000 loss: 0.44670907783206043\n",
      "epoch 38: valid acc = 0.842, new learning rate = 7.119787067318729e-05\n",
      "10000/49000 loss: 0.4202958225245274\n",
      "20000/49000 loss: 0.4235502188267878\n",
      "30000/49000 loss: 0.42626359493507526\n",
      "40000/49000 loss: 0.44082963554027593\n",
      "epoch 39: valid acc = 0.84, new learning rate = 6.763797713952792e-05\n",
      "10000/49000 loss: 0.4706143605009508\n",
      "20000/49000 loss: 0.4202969446993256\n",
      "30000/49000 loss: 0.46897162369371426\n",
      "40000/49000 loss: 0.38072719108311104\n",
      "epoch 40: valid acc = 0.842, new learning rate = 6.425607828255152e-05\n",
      "10000/49000 loss: 0.44585967446285407\n",
      "20000/49000 loss: 0.45024977999281446\n",
      "30000/49000 loss: 0.4145035894507426\n",
      "40000/49000 loss: 0.4024959562312362\n",
      "epoch 41: valid acc = 0.841, new learning rate = 6.104327436842394e-05\n",
      "10000/49000 loss: 0.44698140688737936\n",
      "20000/49000 loss: 0.4238075795943311\n",
      "30000/49000 loss: 0.4247065530255472\n",
      "40000/49000 loss: 0.43496821438390415\n",
      "epoch 42: valid acc = 0.84, new learning rate = 5.799111065000274e-05\n",
      "10000/49000 loss: 0.42917636273187965\n",
      "20000/49000 loss: 0.4678969590661395\n",
      "30000/49000 loss: 0.4235336920944009\n",
      "40000/49000 loss: 0.4296453579403801\n",
      "epoch 43: valid acc = 0.841, new learning rate = 5.5091555117502596e-05\n",
      "10000/49000 loss: 0.3906382114238404\n",
      "20000/49000 loss: 0.4977594123461104\n",
      "30000/49000 loss: 0.4329664699787493\n",
      "40000/49000 loss: 0.4256959366343403\n",
      "epoch 44: valid acc = 0.843, new learning rate = 5.2336977361627463e-05\n",
      "10000/49000 loss: 0.4706954204874367\n",
      "20000/49000 loss: 0.4002655596859911\n",
      "30000/49000 loss: 0.4380733489762795\n",
      "40000/49000 loss: 0.4340593228917023\n",
      "epoch 45: valid acc = 0.844, new learning rate = 4.972012849354609e-05\n",
      "10000/49000 loss: 0.4493003381755663\n",
      "20000/49000 loss: 0.44431750571761974\n",
      "30000/49000 loss: 0.3641348618520549\n",
      "40000/49000 loss: 0.45297649254738376\n",
      "epoch 46: valid acc = 0.842, new learning rate = 4.723412206886878e-05\n",
      "10000/49000 loss: 0.42190161898867057\n",
      "20000/49000 loss: 0.45107334898378576\n",
      "30000/49000 loss: 0.4344801071052971\n",
      "40000/49000 loss: 0.44216832802356315\n",
      "epoch 47: valid acc = 0.841, new learning rate = 4.487241596542534e-05\n",
      "10000/49000 loss: 0.41699360295178756\n",
      "20000/49000 loss: 0.42657692842925793\n",
      "30000/49000 loss: 0.41710180388883705\n",
      "40000/49000 loss: 0.47338513258512904\n",
      "epoch 48: valid acc = 0.84, new learning rate = 4.262879516715407e-05\n",
      "10000/49000 loss: 0.4483407870869772\n",
      "20000/49000 loss: 0.4590892322422678\n",
      "30000/49000 loss: 0.4400612277586323\n",
      "40000/49000 loss: 0.3874421780526712\n",
      "epoch 49: valid acc = 0.842, new learning rate = 4.049735540879637e-05\n",
      "10000/49000 loss: 0.4222235185235982\n",
      "20000/49000 loss: 0.38450246040133906\n",
      "30000/49000 loss: 0.4585287039912373\n",
      "40000/49000 loss: 0.4012003305733147\n",
      "epoch 50: valid acc = 0.841, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.8503673469387755\n",
      "test acc: 0.841\n",
      "test acc: 0.8365\n",
      "number of batches for training: 49\n",
      "epoch 1: valid acc = 0.4, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.51, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.641, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.703, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.73, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.746, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.759, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.773, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.78, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.791, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.796, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.797, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.803, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.807, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.811, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.813, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.812, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.82, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.82, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.819, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.821, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.828, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.827, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.83, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.831, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.832, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.832, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.834, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.828, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.83, new learning rate = 0.00010731938197146858\n",
      "epoch 31: valid acc = 0.832, new learning rate = 0.00010195341287289515\n",
      "epoch 32: valid acc = 0.829, new learning rate = 9.685574222925039e-05\n",
      "epoch 33: valid acc = 0.836, new learning rate = 9.201295511778786e-05\n",
      "epoch 34: valid acc = 0.832, new learning rate = 8.741230736189846e-05\n",
      "epoch 35: valid acc = 0.832, new learning rate = 8.304169199380353e-05\n",
      "epoch 36: valid acc = 0.832, new learning rate = 7.888960739411335e-05\n",
      "epoch 37: valid acc = 0.834, new learning rate = 7.494512702440768e-05\n",
      "epoch 38: valid acc = 0.835, new learning rate = 7.119787067318729e-05\n",
      "epoch 39: valid acc = 0.835, new learning rate = 6.763797713952792e-05\n",
      "epoch 40: valid acc = 0.836, new learning rate = 6.425607828255152e-05\n",
      "epoch 41: valid acc = 0.839, new learning rate = 6.104327436842394e-05\n",
      "epoch 42: valid acc = 0.84, new learning rate = 5.799111065000274e-05\n",
      "epoch 43: valid acc = 0.841, new learning rate = 5.5091555117502596e-05\n",
      "epoch 44: valid acc = 0.839, new learning rate = 5.2336977361627463e-05\n",
      "epoch 45: valid acc = 0.842, new learning rate = 4.972012849354609e-05\n",
      "epoch 46: valid acc = 0.841, new learning rate = 4.723412206886878e-05\n",
      "epoch 47: valid acc = 0.843, new learning rate = 4.487241596542534e-05\n",
      "epoch 48: valid acc = 0.841, new learning rate = 4.262879516715407e-05\n",
      "epoch 49: valid acc = 0.841, new learning rate = 4.049735540879637e-05\n",
      "epoch 50: valid acc = 0.84, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.8505918367346939\n",
      "test acc: 0.84\n",
      "test acc: 0.8343\n",
      "number of batches for training: 49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/49000 loss: 3.301243332043481\n",
      "20000/49000 loss: 3.027595239119184\n",
      "30000/49000 loss: 2.820359451145458\n",
      "40000/49000 loss: 2.7673057502642586\n",
      "epoch 1: valid acc = 0.348, new learning rate = 0.000475\n",
      "10000/49000 loss: 2.2803736820256004\n",
      "20000/49000 loss: 2.013656334861809\n",
      "30000/49000 loss: 1.725552169357722\n",
      "40000/49000 loss: 1.3349580329057453\n",
      "epoch 2: valid acc = 0.531, new learning rate = 0.00045125\n",
      "10000/49000 loss: 1.2175750949344766\n",
      "20000/49000 loss: 1.1626048267756315\n",
      "30000/49000 loss: 1.1153424425109635\n",
      "40000/49000 loss: 1.1387825582324473\n",
      "epoch 3: valid acc = 0.635, new learning rate = 0.0004286875\n",
      "10000/49000 loss: 1.1018330261394154\n",
      "20000/49000 loss: 0.9654340614805383\n",
      "30000/49000 loss: 1.0041839222297804\n",
      "40000/49000 loss: 0.9407656793468354\n",
      "epoch 4: valid acc = 0.695, new learning rate = 0.00040725312499999993\n",
      "10000/49000 loss: 0.9084850137600808\n",
      "20000/49000 loss: 0.8594077616885807\n",
      "30000/49000 loss: 0.8262931083885192\n",
      "40000/49000 loss: 0.7621305182139679\n",
      "epoch 5: valid acc = 0.735, new learning rate = 0.0003868904687499999\n",
      "10000/49000 loss: 0.7810533120457149\n",
      "20000/49000 loss: 0.7815320386542067\n",
      "30000/49000 loss: 0.7746124545908576\n",
      "40000/49000 loss: 0.7635320826603609\n",
      "epoch 6: valid acc = 0.74, new learning rate = 0.0003675459453124999\n",
      "10000/49000 loss: 0.7388197649246283\n",
      "20000/49000 loss: 0.6418068893698983\n",
      "30000/49000 loss: 0.6680138066174298\n",
      "40000/49000 loss: 0.6756038478653712\n",
      "epoch 7: valid acc = 0.756, new learning rate = 0.00034916864804687486\n",
      "10000/49000 loss: 0.6133290984029276\n",
      "20000/49000 loss: 0.6680589825266072\n",
      "30000/49000 loss: 0.5862403018299799\n",
      "40000/49000 loss: 0.6553240965557661\n",
      "epoch 8: valid acc = 0.778, new learning rate = 0.0003317102156445311\n",
      "10000/49000 loss: 0.5685972719248105\n",
      "20000/49000 loss: 0.6156327301034119\n",
      "30000/49000 loss: 0.6062228279491699\n",
      "40000/49000 loss: 0.5735225650455716\n",
      "epoch 9: valid acc = 0.789, new learning rate = 0.0003151247048623045\n",
      "10000/49000 loss: 0.5518898249309206\n",
      "20000/49000 loss: 0.5490193984296674\n",
      "30000/49000 loss: 0.5572404574174249\n",
      "40000/49000 loss: 0.5974778215004722\n",
      "epoch 10: valid acc = 0.796, new learning rate = 0.00029936846961918924\n",
      "10000/49000 loss: 0.5711845105514192\n",
      "20000/49000 loss: 0.5387569568598266\n",
      "30000/49000 loss: 0.5667890235119045\n",
      "40000/49000 loss: 0.5543581396138293\n",
      "epoch 11: valid acc = 0.803, new learning rate = 0.00028440004613822977\n",
      "10000/49000 loss: 0.4958040144002672\n",
      "20000/49000 loss: 0.5726049190150924\n",
      "30000/49000 loss: 0.5261945747706802\n",
      "40000/49000 loss: 0.5476387900759554\n",
      "epoch 12: valid acc = 0.8, new learning rate = 0.00027018004383131826\n",
      "10000/49000 loss: 0.5683780512944333\n",
      "20000/49000 loss: 0.49105603544939447\n",
      "30000/49000 loss: 0.5278854863011918\n",
      "40000/49000 loss: 0.5567715813555939\n",
      "epoch 13: valid acc = 0.807, new learning rate = 0.00025667104163975234\n",
      "10000/49000 loss: 0.5497321768495395\n",
      "20000/49000 loss: 0.527099395947045\n",
      "30000/49000 loss: 0.5278374762025654\n",
      "40000/49000 loss: 0.5201404139549242\n",
      "epoch 14: valid acc = 0.81, new learning rate = 0.00024383748955776472\n",
      "10000/49000 loss: 0.5000382933049401\n",
      "20000/49000 loss: 0.49673223596237814\n",
      "30000/49000 loss: 0.5323922927969691\n",
      "40000/49000 loss: 0.5067344447451303\n",
      "epoch 15: valid acc = 0.813, new learning rate = 0.00023164561507987649\n",
      "10000/49000 loss: 0.4564881595053559\n",
      "20000/49000 loss: 0.5008363559191793\n",
      "30000/49000 loss: 0.5165834879712606\n",
      "40000/49000 loss: 0.48001052318947013\n",
      "epoch 16: valid acc = 0.819, new learning rate = 0.00022006333432588265\n",
      "10000/49000 loss: 0.5154709141033754\n",
      "20000/49000 loss: 0.48562757786201366\n",
      "30000/49000 loss: 0.5066249591005576\n",
      "40000/49000 loss: 0.5028864932330367\n",
      "epoch 17: valid acc = 0.816, new learning rate = 0.00020906016760958852\n",
      "10000/49000 loss: 0.4575872127605096\n",
      "20000/49000 loss: 0.46556128738636376\n",
      "30000/49000 loss: 0.48725412734220785\n",
      "40000/49000 loss: 0.5068217844456144\n",
      "epoch 18: valid acc = 0.817, new learning rate = 0.00019860715922910907\n",
      "10000/49000 loss: 0.5186967473159103\n",
      "20000/49000 loss: 0.5117604008481987\n",
      "30000/49000 loss: 0.4685131267256846\n",
      "40000/49000 loss: 0.5081908949370849\n",
      "epoch 19: valid acc = 0.82, new learning rate = 0.0001886768012676536\n",
      "10000/49000 loss: 0.4714225254255148\n",
      "20000/49000 loss: 0.4858218772370195\n",
      "30000/49000 loss: 0.4732146163859905\n",
      "40000/49000 loss: 0.46063778861939053\n",
      "epoch 20: valid acc = 0.823, new learning rate = 0.0001792429612042709\n",
      "10000/49000 loss: 0.49997161294240794\n",
      "20000/49000 loss: 0.43798215502499943\n",
      "30000/49000 loss: 0.43486948918323065\n",
      "40000/49000 loss: 0.4510318867374345\n",
      "epoch 21: valid acc = 0.826, new learning rate = 0.00017028081314405735\n",
      "10000/49000 loss: 0.4582886017414287\n",
      "20000/49000 loss: 0.4583581828569266\n",
      "30000/49000 loss: 0.459530757217669\n",
      "40000/49000 loss: 0.4536437005574589\n",
      "epoch 22: valid acc = 0.829, new learning rate = 0.00016176677248685447\n",
      "10000/49000 loss: 0.48017921574499955\n",
      "20000/49000 loss: 0.4681791611265838\n",
      "30000/49000 loss: 0.4265988286925207\n",
      "40000/49000 loss: 0.4937187869506759\n",
      "epoch 23: valid acc = 0.832, new learning rate = 0.00015367843386251173\n",
      "10000/49000 loss: 0.4417348732398987\n",
      "20000/49000 loss: 0.48782983011737824\n",
      "30000/49000 loss: 0.539525336084618\n",
      "40000/49000 loss: 0.459237593462064\n",
      "epoch 24: valid acc = 0.832, new learning rate = 0.00014599451216938612\n",
      "10000/49000 loss: 0.46116429200896886\n",
      "20000/49000 loss: 0.4630530974431802\n",
      "30000/49000 loss: 0.4440982051824792\n",
      "40000/49000 loss: 0.41705338995247077\n",
      "epoch 25: valid acc = 0.828, new learning rate = 0.00013869478656091682\n",
      "10000/49000 loss: 0.4244529702509165\n",
      "20000/49000 loss: 0.48766438277407714\n",
      "30000/49000 loss: 0.4213401776261081\n",
      "40000/49000 loss: 0.46010472915366984\n",
      "epoch 26: valid acc = 0.827, new learning rate = 0.00013176004723287096\n",
      "10000/49000 loss: 0.4599724356514865\n",
      "20000/49000 loss: 0.48074933310479245\n",
      "30000/49000 loss: 0.44100993040618713\n",
      "40000/49000 loss: 0.42446104275792823\n",
      "epoch 27: valid acc = 0.827, new learning rate = 0.0001251720448712274\n",
      "10000/49000 loss: 0.4092911039473689\n",
      "20000/49000 loss: 0.42796463083603975\n",
      "30000/49000 loss: 0.4619490367916847\n",
      "40000/49000 loss: 0.5034698408818927\n",
      "epoch 28: valid acc = 0.823, new learning rate = 0.00011891344262766602\n",
      "10000/49000 loss: 0.4469320730295732\n",
      "20000/49000 loss: 0.44883251297339266\n",
      "30000/49000 loss: 0.4612352230619663\n",
      "40000/49000 loss: 0.4727015818528106\n",
      "epoch 29: valid acc = 0.833, new learning rate = 0.00011296777049628272\n",
      "10000/49000 loss: 0.44293560768312323\n",
      "20000/49000 loss: 0.4442402279108689\n",
      "30000/49000 loss: 0.4195154992515398\n",
      "40000/49000 loss: 0.4939153395628545\n",
      "epoch 30: valid acc = 0.835, new learning rate = 0.00010731938197146858\n",
      "10000/49000 loss: 0.44489822321163647\n",
      "20000/49000 loss: 0.45760863374141164\n",
      "30000/49000 loss: 0.4259840420768277\n",
      "40000/49000 loss: 0.4359216791612441\n",
      "epoch 31: valid acc = 0.834, new learning rate = 0.00010195341287289515\n",
      "10000/49000 loss: 0.47123869538143054\n",
      "20000/49000 loss: 0.43408319603652024\n",
      "30000/49000 loss: 0.4404444822797359\n",
      "40000/49000 loss: 0.43519624735570045\n",
      "epoch 32: valid acc = 0.831, new learning rate = 9.685574222925039e-05\n",
      "10000/49000 loss: 0.4435200844012775\n",
      "20000/49000 loss: 0.4379406202678161\n",
      "30000/49000 loss: 0.45039805385406845\n",
      "40000/49000 loss: 0.48041940794263804\n",
      "epoch 33: valid acc = 0.83, new learning rate = 9.201295511778786e-05\n",
      "10000/49000 loss: 0.5173536144067602\n",
      "20000/49000 loss: 0.4573158086803403\n",
      "30000/49000 loss: 0.41746238450685413\n",
      "40000/49000 loss: 0.432962773801015\n",
      "epoch 34: valid acc = 0.837, new learning rate = 8.741230736189846e-05\n",
      "10000/49000 loss: 0.4357780660027344\n",
      "20000/49000 loss: 0.4444921391535267\n",
      "30000/49000 loss: 0.44308863040792795\n",
      "40000/49000 loss: 0.4726214059908965\n",
      "epoch 35: valid acc = 0.836, new learning rate = 8.304169199380353e-05\n",
      "10000/49000 loss: 0.4303448296027842\n",
      "20000/49000 loss: 0.40396539123930963\n",
      "30000/49000 loss: 0.4122555563655575\n",
      "40000/49000 loss: 0.4396582993805988\n",
      "epoch 36: valid acc = 0.84, new learning rate = 7.888960739411335e-05\n",
      "10000/49000 loss: 0.4399964387709587\n",
      "20000/49000 loss: 0.42290431300673526\n",
      "30000/49000 loss: 0.4314131817344224\n",
      "40000/49000 loss: 0.44937577835465553\n",
      "epoch 37: valid acc = 0.841, new learning rate = 7.494512702440768e-05\n",
      "10000/49000 loss: 0.4767550193936723\n",
      "20000/49000 loss: 0.4542605240237968\n",
      "30000/49000 loss: 0.39041854750115074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/49000 loss: 0.4691142605837237\n",
      "epoch 38: valid acc = 0.842, new learning rate = 7.119787067318729e-05\n",
      "10000/49000 loss: 0.43482320491161414\n",
      "20000/49000 loss: 0.47423481966166975\n",
      "30000/49000 loss: 0.4166128191508526\n",
      "40000/49000 loss: 0.39679102116913\n",
      "epoch 39: valid acc = 0.843, new learning rate = 6.763797713952792e-05\n",
      "10000/49000 loss: 0.47929414666933096\n",
      "20000/49000 loss: 0.45060870538479425\n",
      "30000/49000 loss: 0.4522495253821542\n",
      "40000/49000 loss: 0.4261543683708734\n",
      "epoch 40: valid acc = 0.843, new learning rate = 6.425607828255152e-05\n",
      "10000/49000 loss: 0.4096776499933472\n",
      "20000/49000 loss: 0.4324666220329881\n",
      "30000/49000 loss: 0.4355072798578257\n",
      "40000/49000 loss: 0.38359486386213876\n",
      "epoch 41: valid acc = 0.843, new learning rate = 6.104327436842394e-05\n",
      "10000/49000 loss: 0.4414777444504508\n",
      "20000/49000 loss: 0.3783267816251373\n",
      "30000/49000 loss: 0.4676444983259037\n",
      "40000/49000 loss: 0.4272416894820169\n",
      "epoch 42: valid acc = 0.845, new learning rate = 5.799111065000274e-05\n",
      "10000/49000 loss: 0.4383649132861006\n",
      "20000/49000 loss: 0.4218181270312825\n",
      "30000/49000 loss: 0.43113154135069554\n",
      "40000/49000 loss: 0.4037064271406906\n",
      "epoch 43: valid acc = 0.843, new learning rate = 5.5091555117502596e-05\n",
      "10000/49000 loss: 0.4498410078707239\n",
      "20000/49000 loss: 0.44314884010975275\n",
      "30000/49000 loss: 0.4390220796250594\n",
      "40000/49000 loss: 0.42650558799082966\n",
      "epoch 44: valid acc = 0.845, new learning rate = 5.2336977361627463e-05\n",
      "10000/49000 loss: 0.44439389287932685\n",
      "20000/49000 loss: 0.4370054671696015\n",
      "30000/49000 loss: 0.4286136698251702\n",
      "40000/49000 loss: 0.4269880064960178\n",
      "epoch 45: valid acc = 0.844, new learning rate = 4.972012849354609e-05\n",
      "10000/49000 loss: 0.42600921544258585\n",
      "20000/49000 loss: 0.45449727235001985\n",
      "30000/49000 loss: 0.4070014894899342\n",
      "40000/49000 loss: 0.4307080195419722\n",
      "epoch 46: valid acc = 0.845, new learning rate = 4.723412206886878e-05\n",
      "10000/49000 loss: 0.4371648065299465\n",
      "20000/49000 loss: 0.43986745170164493\n",
      "30000/49000 loss: 0.42722386567403914\n",
      "40000/49000 loss: 0.4511601479288085\n",
      "epoch 47: valid acc = 0.846, new learning rate = 4.487241596542534e-05\n",
      "10000/49000 loss: 0.4174722112487398\n",
      "20000/49000 loss: 0.38515835865115267\n",
      "30000/49000 loss: 0.36517732020133487\n",
      "40000/49000 loss: 0.4577154871182827\n",
      "epoch 48: valid acc = 0.845, new learning rate = 4.262879516715407e-05\n",
      "10000/49000 loss: 0.4096785569440372\n",
      "20000/49000 loss: 0.4127322599045202\n",
      "30000/49000 loss: 0.39714070525936224\n",
      "40000/49000 loss: 0.3936148048215429\n",
      "epoch 49: valid acc = 0.845, new learning rate = 4.049735540879637e-05\n",
      "10000/49000 loss: 0.4596098317835875\n",
      "20000/49000 loss: 0.4364121013836734\n",
      "30000/49000 loss: 0.42879355709282696\n",
      "40000/49000 loss: 0.38258313904895397\n",
      "epoch 50: valid acc = 0.845, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.8499591836734693\n",
      "test acc: 0.845\n",
      "test acc: 0.8348\n",
      "number of batches for training: 49\n",
      "epoch 1: valid acc = 0.392, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.524, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.629, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.694, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.719, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.743, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.753, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.77, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.772, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.784, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.792, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.806, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.807, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.804, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.809, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.809, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.814, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.816, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.819, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.824, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.828, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.83, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.833, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.836, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.835, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.835, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.835, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.836, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.837, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.835, new learning rate = 0.00010731938197146858\n",
      "epoch 31: valid acc = 0.839, new learning rate = 0.00010195341287289515\n",
      "epoch 32: valid acc = 0.836, new learning rate = 9.685574222925039e-05\n",
      "epoch 33: valid acc = 0.835, new learning rate = 9.201295511778786e-05\n",
      "epoch 34: valid acc = 0.834, new learning rate = 8.741230736189846e-05\n",
      "epoch 35: valid acc = 0.841, new learning rate = 8.304169199380353e-05\n",
      "epoch 36: valid acc = 0.839, new learning rate = 7.888960739411335e-05\n",
      "epoch 37: valid acc = 0.84, new learning rate = 7.494512702440768e-05\n",
      "epoch 38: valid acc = 0.843, new learning rate = 7.119787067318729e-05\n",
      "epoch 39: valid acc = 0.842, new learning rate = 6.763797713952792e-05\n",
      "epoch 40: valid acc = 0.843, new learning rate = 6.425607828255152e-05\n",
      "epoch 41: valid acc = 0.842, new learning rate = 6.104327436842394e-05\n",
      "epoch 42: valid acc = 0.843, new learning rate = 5.799111065000274e-05\n",
      "epoch 43: valid acc = 0.846, new learning rate = 5.5091555117502596e-05\n",
      "epoch 44: valid acc = 0.845, new learning rate = 5.2336977361627463e-05\n",
      "epoch 45: valid acc = 0.843, new learning rate = 4.972012849354609e-05\n",
      "epoch 46: valid acc = 0.845, new learning rate = 4.723412206886878e-05\n",
      "epoch 47: valid acc = 0.845, new learning rate = 4.487241596542534e-05\n",
      "epoch 48: valid acc = 0.844, new learning rate = 4.262879516715407e-05\n",
      "epoch 49: valid acc = 0.845, new learning rate = 4.049735540879637e-05\n",
      "epoch 50: valid acc = 0.847, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.8503061224489796\n",
      "test acc: 0.847\n",
      "test acc: 0.8354\n",
      "number of batches for training: 49\n",
      "10000/49000 loss: 3.703472601018249\n",
      "20000/49000 loss: 3.555976191267863\n",
      "30000/49000 loss: 2.5374061250460476\n",
      "40000/49000 loss: 2.71042775935384\n",
      "epoch 1: valid acc = 0.392, new learning rate = 0.000475\n",
      "10000/49000 loss: 2.002082250452467\n",
      "20000/49000 loss: 1.8044529646521317\n",
      "30000/49000 loss: 1.6178923908476561\n",
      "40000/49000 loss: 1.3305172180024751\n",
      "epoch 2: valid acc = 0.544, new learning rate = 0.00045125\n",
      "10000/49000 loss: 1.3009340924757156\n",
      "20000/49000 loss: 1.1782153020463177\n",
      "30000/49000 loss: 1.0902120215253588\n",
      "40000/49000 loss: 1.1085486041618164\n",
      "epoch 3: valid acc = 0.651, new learning rate = 0.0004286875\n",
      "10000/49000 loss: 0.9716189684830874\n",
      "20000/49000 loss: 0.940459020519951\n",
      "30000/49000 loss: 0.9745593654658935\n",
      "40000/49000 loss: 0.9188111505507965\n",
      "epoch 4: valid acc = 0.7, new learning rate = 0.00040725312499999993\n",
      "10000/49000 loss: 0.9022199477228281\n",
      "20000/49000 loss: 0.8589740629166229\n",
      "30000/49000 loss: 0.8313008593672325\n",
      "40000/49000 loss: 0.8107494250845627\n",
      "epoch 5: valid acc = 0.731, new learning rate = 0.0003868904687499999\n",
      "10000/49000 loss: 0.7316432075876683\n",
      "20000/49000 loss: 0.7645560470925562\n",
      "30000/49000 loss: 0.7326046155472627\n",
      "40000/49000 loss: 0.7181584207881688\n",
      "epoch 6: valid acc = 0.753, new learning rate = 0.0003675459453124999\n",
      "10000/49000 loss: 0.6904077251701634\n",
      "20000/49000 loss: 0.6680770897504065\n",
      "30000/49000 loss: 0.7366028833289533\n",
      "40000/49000 loss: 0.6361230873690468\n",
      "epoch 7: valid acc = 0.753, new learning rate = 0.00034916864804687486\n",
      "10000/49000 loss: 0.6042172342522378\n",
      "20000/49000 loss: 0.6518964441400956\n",
      "30000/49000 loss: 0.6147103544842225\n",
      "40000/49000 loss: 0.6484295006994635\n",
      "epoch 8: valid acc = 0.776, new learning rate = 0.0003317102156445311\n",
      "10000/49000 loss: 0.6645275854692582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/49000 loss: 0.6205237126925781\n",
      "30000/49000 loss: 0.6393270532068709\n",
      "40000/49000 loss: 0.5692530780209369\n",
      "epoch 9: valid acc = 0.78, new learning rate = 0.0003151247048623045\n",
      "10000/49000 loss: 0.5857340776044894\n",
      "20000/49000 loss: 0.6023333634485671\n",
      "30000/49000 loss: 0.6268250615564837\n",
      "40000/49000 loss: 0.5528895407523362\n",
      "epoch 10: valid acc = 0.795, new learning rate = 0.00029936846961918924\n",
      "10000/49000 loss: 0.5642082374193572\n",
      "20000/49000 loss: 0.5357793095344348\n",
      "30000/49000 loss: 0.568620846285231\n",
      "40000/49000 loss: 0.5308746399253856\n",
      "epoch 11: valid acc = 0.798, new learning rate = 0.00028440004613822977\n",
      "10000/49000 loss: 0.5096739610868934\n",
      "20000/49000 loss: 0.4649919050499861\n",
      "30000/49000 loss: 0.5593268167587918\n",
      "40000/49000 loss: 0.5875172342347775\n",
      "epoch 12: valid acc = 0.804, new learning rate = 0.00027018004383131826\n",
      "10000/49000 loss: 0.538995065194844\n",
      "20000/49000 loss: 0.47822169634158884\n",
      "30000/49000 loss: 0.5335197323137104\n",
      "40000/49000 loss: 0.5432538279326206\n",
      "epoch 13: valid acc = 0.805, new learning rate = 0.00025667104163975234\n",
      "10000/49000 loss: 0.5363032695415472\n",
      "20000/49000 loss: 0.5387448017940263\n",
      "30000/49000 loss: 0.5112951171277201\n",
      "40000/49000 loss: 0.5376508992424631\n",
      "epoch 14: valid acc = 0.805, new learning rate = 0.00024383748955776472\n",
      "10000/49000 loss: 0.5085768608894671\n",
      "20000/49000 loss: 0.4872881714399749\n",
      "30000/49000 loss: 0.5289157513196385\n",
      "40000/49000 loss: 0.5024936492732343\n",
      "epoch 15: valid acc = 0.814, new learning rate = 0.00023164561507987649\n",
      "10000/49000 loss: 0.529862156507088\n",
      "20000/49000 loss: 0.5530698261647101\n",
      "30000/49000 loss: 0.5024821466769973\n",
      "40000/49000 loss: 0.514085619598457\n",
      "epoch 16: valid acc = 0.813, new learning rate = 0.00022006333432588265\n",
      "10000/49000 loss: 0.5102757725082689\n",
      "20000/49000 loss: 0.5376613492577736\n",
      "30000/49000 loss: 0.5360763320386093\n",
      "40000/49000 loss: 0.49678363101651485\n",
      "epoch 17: valid acc = 0.815, new learning rate = 0.00020906016760958852\n",
      "10000/49000 loss: 0.49786996462422617\n",
      "20000/49000 loss: 0.47015979718913453\n",
      "30000/49000 loss: 0.4904359999532144\n",
      "40000/49000 loss: 0.4747098454193425\n",
      "epoch 18: valid acc = 0.819, new learning rate = 0.00019860715922910907\n",
      "10000/49000 loss: 0.44106411911198096\n",
      "20000/49000 loss: 0.4403695063022433\n",
      "30000/49000 loss: 0.47642322644780266\n",
      "40000/49000 loss: 0.48334417802745805\n",
      "epoch 19: valid acc = 0.819, new learning rate = 0.0001886768012676536\n",
      "10000/49000 loss: 0.47825199187286765\n",
      "20000/49000 loss: 0.47312487123505315\n",
      "30000/49000 loss: 0.4580977547391945\n",
      "40000/49000 loss: 0.45086887328205155\n",
      "epoch 20: valid acc = 0.822, new learning rate = 0.0001792429612042709\n",
      "10000/49000 loss: 0.4696294912450071\n",
      "20000/49000 loss: 0.4287586377423135\n",
      "30000/49000 loss: 0.4504176609142278\n",
      "40000/49000 loss: 0.44920268849822265\n",
      "epoch 21: valid acc = 0.825, new learning rate = 0.00017028081314405735\n",
      "10000/49000 loss: 0.4432857238203246\n",
      "20000/49000 loss: 0.4486449037601244\n",
      "30000/49000 loss: 0.4712711447541109\n",
      "40000/49000 loss: 0.48085120714046226\n",
      "epoch 22: valid acc = 0.828, new learning rate = 0.00016176677248685447\n",
      "10000/49000 loss: 0.49515967868299365\n",
      "20000/49000 loss: 0.46397039940906254\n",
      "30000/49000 loss: 0.45702598007439316\n",
      "40000/49000 loss: 0.5163528978808407\n",
      "epoch 23: valid acc = 0.828, new learning rate = 0.00015367843386251173\n",
      "10000/49000 loss: 0.4621471502452164\n",
      "20000/49000 loss: 0.4693877109542876\n",
      "30000/49000 loss: 0.45472588520623836\n",
      "40000/49000 loss: 0.406708229208317\n",
      "epoch 24: valid acc = 0.831, new learning rate = 0.00014599451216938612\n",
      "10000/49000 loss: 0.45474085734122843\n",
      "20000/49000 loss: 0.4371421270947276\n",
      "30000/49000 loss: 0.48473906330639954\n",
      "40000/49000 loss: 0.44962359666345136\n",
      "epoch 25: valid acc = 0.83, new learning rate = 0.00013869478656091682\n",
      "10000/49000 loss: 0.43707985977285224\n",
      "20000/49000 loss: 0.4737159295609303\n",
      "30000/49000 loss: 0.4531491412813236\n",
      "40000/49000 loss: 0.4554368700492198\n",
      "epoch 26: valid acc = 0.832, new learning rate = 0.00013176004723287096\n",
      "10000/49000 loss: 0.4118931183334543\n",
      "20000/49000 loss: 0.4412772808107969\n",
      "30000/49000 loss: 0.4376733583061464\n",
      "40000/49000 loss: 0.4105454731632298\n",
      "epoch 27: valid acc = 0.834, new learning rate = 0.0001251720448712274\n",
      "10000/49000 loss: 0.42197488359232843\n",
      "20000/49000 loss: 0.4877025958380404\n",
      "30000/49000 loss: 0.46070286537503263\n",
      "40000/49000 loss: 0.4389313466852601\n",
      "epoch 28: valid acc = 0.832, new learning rate = 0.00011891344262766602\n",
      "10000/49000 loss: 0.47582293275697113\n",
      "20000/49000 loss: 0.47090218949386703\n",
      "30000/49000 loss: 0.44982433198970384\n",
      "40000/49000 loss: 0.4797987513304376\n",
      "epoch 29: valid acc = 0.828, new learning rate = 0.00011296777049628272\n",
      "10000/49000 loss: 0.44885238131854904\n",
      "20000/49000 loss: 0.47338135419415517\n",
      "30000/49000 loss: 0.4370567329843569\n",
      "40000/49000 loss: 0.47053646301399304\n",
      "epoch 30: valid acc = 0.836, new learning rate = 0.00010731938197146858\n",
      "10000/49000 loss: 0.4901429740778715\n",
      "20000/49000 loss: 0.4304719651506865\n",
      "30000/49000 loss: 0.4462873643529724\n",
      "40000/49000 loss: 0.4237739182308403\n",
      "epoch 31: valid acc = 0.836, new learning rate = 0.00010195341287289515\n",
      "10000/49000 loss: 0.4458530485665951\n",
      "20000/49000 loss: 0.4203575999892891\n",
      "30000/49000 loss: 0.3945977624594632\n",
      "40000/49000 loss: 0.44291237746549916\n",
      "epoch 32: valid acc = 0.836, new learning rate = 9.685574222925039e-05\n",
      "10000/49000 loss: 0.4776529287563575\n",
      "20000/49000 loss: 0.4049672242538394\n",
      "30000/49000 loss: 0.436822573070826\n",
      "40000/49000 loss: 0.4249852476395032\n",
      "epoch 33: valid acc = 0.839, new learning rate = 9.201295511778786e-05\n",
      "10000/49000 loss: 0.4281998595292767\n",
      "20000/49000 loss: 0.42363783068235683\n",
      "30000/49000 loss: 0.42332092784587366\n",
      "40000/49000 loss: 0.4599920105312579\n",
      "epoch 34: valid acc = 0.843, new learning rate = 8.741230736189846e-05\n",
      "10000/49000 loss: 0.42023765457697365\n",
      "20000/49000 loss: 0.41555773025678894\n",
      "30000/49000 loss: 0.3992597551908887\n",
      "40000/49000 loss: 0.4383724137894465\n",
      "epoch 35: valid acc = 0.843, new learning rate = 8.304169199380353e-05\n",
      "10000/49000 loss: 0.4763437774294752\n",
      "20000/49000 loss: 0.4426790065651124\n",
      "30000/49000 loss: 0.4249236648521224\n",
      "40000/49000 loss: 0.41500939146169347\n",
      "epoch 36: valid acc = 0.841, new learning rate = 7.888960739411335e-05\n",
      "10000/49000 loss: 0.4556616856763848\n",
      "20000/49000 loss: 0.4635645274850006\n",
      "30000/49000 loss: 0.4391422222193667\n",
      "40000/49000 loss: 0.39785705086038325\n",
      "epoch 37: valid acc = 0.839, new learning rate = 7.494512702440768e-05\n",
      "10000/49000 loss: 0.4500150471583422\n",
      "20000/49000 loss: 0.41835730887333966\n",
      "30000/49000 loss: 0.44477263082530566\n",
      "40000/49000 loss: 0.4842100890952247\n",
      "epoch 38: valid acc = 0.839, new learning rate = 7.119787067318729e-05\n",
      "10000/49000 loss: 0.4073282848619465\n",
      "20000/49000 loss: 0.4247510387498241\n",
      "30000/49000 loss: 0.3828421946334323\n",
      "40000/49000 loss: 0.4543110045440932\n",
      "epoch 39: valid acc = 0.843, new learning rate = 6.763797713952792e-05\n",
      "10000/49000 loss: 0.439463878847769\n",
      "20000/49000 loss: 0.48957540397196136\n",
      "30000/49000 loss: 0.42278684698655705\n",
      "40000/49000 loss: 0.39460520879720634\n",
      "epoch 40: valid acc = 0.843, new learning rate = 6.425607828255152e-05\n",
      "10000/49000 loss: 0.44460942817499255\n",
      "20000/49000 loss: 0.3924410996976897\n",
      "30000/49000 loss: 0.41749488080800035\n",
      "40000/49000 loss: 0.4313377958261327\n",
      "epoch 41: valid acc = 0.842, new learning rate = 6.104327436842394e-05\n",
      "10000/49000 loss: 0.40673419905415975\n",
      "20000/49000 loss: 0.42213332123297914\n",
      "30000/49000 loss: 0.4420118882052085\n",
      "40000/49000 loss: 0.464584012395934\n",
      "epoch 42: valid acc = 0.842, new learning rate = 5.799111065000274e-05\n",
      "10000/49000 loss: 0.39635688624003085\n",
      "20000/49000 loss: 0.4090161358529666\n",
      "30000/49000 loss: 0.40028951770055715\n",
      "40000/49000 loss: 0.4036595924425596\n",
      "epoch 43: valid acc = 0.846, new learning rate = 5.5091555117502596e-05\n",
      "10000/49000 loss: 0.4037906051016263\n",
      "20000/49000 loss: 0.42476403363465176\n",
      "30000/49000 loss: 0.45243816559024025\n",
      "40000/49000 loss: 0.4268477509461058\n",
      "epoch 44: valid acc = 0.842, new learning rate = 5.2336977361627463e-05\n",
      "10000/49000 loss: 0.4504148611259019\n",
      "20000/49000 loss: 0.4336856335895575\n",
      "30000/49000 loss: 0.4627382434961313\n",
      "40000/49000 loss: 0.4208971581935416\n",
      "epoch 45: valid acc = 0.843, new learning rate = 4.972012849354609e-05\n",
      "10000/49000 loss: 0.43918349718467425\n",
      "20000/49000 loss: 0.433116403609025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000/49000 loss: 0.4283179097970101\n",
      "40000/49000 loss: 0.426827830614675\n",
      "epoch 46: valid acc = 0.846, new learning rate = 4.723412206886878e-05\n",
      "10000/49000 loss: 0.43370547237157453\n",
      "20000/49000 loss: 0.40319278242009815\n",
      "30000/49000 loss: 0.37551572569987196\n",
      "40000/49000 loss: 0.4296929379819474\n",
      "epoch 47: valid acc = 0.848, new learning rate = 4.487241596542534e-05\n",
      "10000/49000 loss: 0.40907527162570173\n",
      "20000/49000 loss: 0.44132376143469026\n",
      "30000/49000 loss: 0.46129807373370524\n",
      "40000/49000 loss: 0.40900179240097834\n",
      "epoch 48: valid acc = 0.845, new learning rate = 4.262879516715407e-05\n",
      "10000/49000 loss: 0.4427059820336604\n",
      "20000/49000 loss: 0.4489191617739151\n",
      "30000/49000 loss: 0.3935847535593124\n",
      "40000/49000 loss: 0.4399455037047636\n",
      "epoch 49: valid acc = 0.846, new learning rate = 4.049735540879637e-05\n",
      "10000/49000 loss: 0.43243348282671795\n",
      "20000/49000 loss: 0.39833754591818216\n",
      "30000/49000 loss: 0.39229790807571774\n",
      "40000/49000 loss: 0.44111152760902944\n",
      "epoch 50: valid acc = 0.847, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.8511632653061224\n",
      "test acc: 0.847\n",
      "test acc: 0.8331\n",
      "number of batches for training: 49\n",
      "epoch 1: valid acc = 0.392, new learning rate = 0.000475\n",
      "epoch 2: valid acc = 0.514, new learning rate = 0.00045125\n",
      "epoch 3: valid acc = 0.643, new learning rate = 0.0004286875\n",
      "epoch 4: valid acc = 0.694, new learning rate = 0.00040725312499999993\n",
      "epoch 5: valid acc = 0.734, new learning rate = 0.0003868904687499999\n",
      "epoch 6: valid acc = 0.746, new learning rate = 0.0003675459453124999\n",
      "epoch 7: valid acc = 0.756, new learning rate = 0.00034916864804687486\n",
      "epoch 8: valid acc = 0.777, new learning rate = 0.0003317102156445311\n",
      "epoch 9: valid acc = 0.785, new learning rate = 0.0003151247048623045\n",
      "epoch 10: valid acc = 0.792, new learning rate = 0.00029936846961918924\n",
      "epoch 11: valid acc = 0.793, new learning rate = 0.00028440004613822977\n",
      "epoch 12: valid acc = 0.805, new learning rate = 0.00027018004383131826\n",
      "epoch 13: valid acc = 0.806, new learning rate = 0.00025667104163975234\n",
      "epoch 14: valid acc = 0.807, new learning rate = 0.00024383748955776472\n",
      "epoch 15: valid acc = 0.811, new learning rate = 0.00023164561507987649\n",
      "epoch 16: valid acc = 0.816, new learning rate = 0.00022006333432588265\n",
      "epoch 17: valid acc = 0.815, new learning rate = 0.00020906016760958852\n",
      "epoch 18: valid acc = 0.82, new learning rate = 0.00019860715922910907\n",
      "epoch 19: valid acc = 0.825, new learning rate = 0.0001886768012676536\n",
      "epoch 20: valid acc = 0.825, new learning rate = 0.0001792429612042709\n",
      "epoch 21: valid acc = 0.826, new learning rate = 0.00017028081314405735\n",
      "epoch 22: valid acc = 0.823, new learning rate = 0.00016176677248685447\n",
      "epoch 23: valid acc = 0.827, new learning rate = 0.00015367843386251173\n",
      "epoch 24: valid acc = 0.829, new learning rate = 0.00014599451216938612\n",
      "epoch 25: valid acc = 0.829, new learning rate = 0.00013869478656091682\n",
      "epoch 26: valid acc = 0.828, new learning rate = 0.00013176004723287096\n",
      "epoch 27: valid acc = 0.828, new learning rate = 0.0001251720448712274\n",
      "epoch 28: valid acc = 0.831, new learning rate = 0.00011891344262766602\n",
      "epoch 29: valid acc = 0.83, new learning rate = 0.00011296777049628272\n",
      "epoch 30: valid acc = 0.833, new learning rate = 0.00010731938197146858\n",
      "epoch 31: valid acc = 0.834, new learning rate = 0.00010195341287289515\n",
      "epoch 32: valid acc = 0.836, new learning rate = 9.685574222925039e-05\n",
      "epoch 33: valid acc = 0.832, new learning rate = 9.201295511778786e-05\n",
      "epoch 34: valid acc = 0.832, new learning rate = 8.741230736189846e-05\n",
      "epoch 35: valid acc = 0.834, new learning rate = 8.304169199380353e-05\n",
      "epoch 36: valid acc = 0.836, new learning rate = 7.888960739411335e-05\n",
      "epoch 37: valid acc = 0.836, new learning rate = 7.494512702440768e-05\n",
      "epoch 38: valid acc = 0.839, new learning rate = 7.119787067318729e-05\n",
      "epoch 39: valid acc = 0.839, new learning rate = 6.763797713952792e-05\n",
      "epoch 40: valid acc = 0.837, new learning rate = 6.425607828255152e-05\n",
      "epoch 41: valid acc = 0.837, new learning rate = 6.104327436842394e-05\n",
      "epoch 42: valid acc = 0.841, new learning rate = 5.799111065000274e-05\n",
      "epoch 43: valid acc = 0.839, new learning rate = 5.5091555117502596e-05\n",
      "epoch 44: valid acc = 0.839, new learning rate = 5.2336977361627463e-05\n",
      "epoch 45: valid acc = 0.845, new learning rate = 4.972012849354609e-05\n",
      "epoch 46: valid acc = 0.845, new learning rate = 4.723412206886878e-05\n",
      "epoch 47: valid acc = 0.843, new learning rate = 4.487241596542534e-05\n",
      "epoch 48: valid acc = 0.846, new learning rate = 4.262879516715407e-05\n",
      "epoch 49: valid acc = 0.846, new learning rate = 4.049735540879637e-05\n",
      "epoch 50: valid acc = 0.846, new learning rate = 3.847248763835655e-05\n",
      "test acc: 0.8495714285714285\n",
      "test acc: 0.846\n",
      "test acc: 0.8348\n"
     ]
    }
   ],
   "source": [
    "from utils.classifiers.twolayernet import TwoLayerNet\n",
    "\n",
    "num_epochs = [10,30,50]\n",
    "batch_sizes = [200,600,1000] \n",
    "hidden_dims = [100,400,700]\n",
    "lr_rates = [0.01,0.001,0.0001]\n",
    "verboses = [True,False]\n",
    "\n",
    "results = []\n",
    "\n",
    "for hidden_dim in hidden_dims:\n",
    "    for num_epoch in num_epochs:\n",
    "        for batch_size in batch_sizes:\n",
    "            for lr_rate in lr_rates:\n",
    "                for verbose in verboses:\n",
    "                    \n",
    "                    model = TwoLayerNet(input_dim=X_train.shape[1], hidden_dim=hidden_dim, num_classes=20, reg=1e-4, weight_scale=1e-3)\n",
    "\n",
    "                    train_acc_hist, val_acc_hist = train(model, X_train, y_train, X_val, y_val, \n",
    "                          num_epoch=num_epoch, batch_size=batch_size, learning_rate=lr, verbose=verbose)\n",
    "                    results.append([test(model, X_train, y_train), test(model, X_val, y_val), test(model, X_test, y_test)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 571
    },
    "colab_type": "code",
    "id": "9vR35FLUtrqG",
    "outputId": "78833fb4-1da7-48be-9c89-e886771d76d2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches for training: 245\n",
      "2000/49000 loss: 3.288060895982621\n",
      "4000/49000 loss: 2.6702630930782094\n",
      "6000/49000 loss: 2.8940291683684816\n",
      "8000/49000 loss: 2.7000556617448725\n",
      "10000/49000 loss: 2.1220280138298286\n",
      "12000/49000 loss: 1.8816655886346803\n",
      "14000/49000 loss: 1.8570191088043908\n",
      "16000/49000 loss: 1.4853561796050678\n",
      "18000/49000 loss: 1.4310730436419934\n",
      "20000/49000 loss: 1.442975092148322\n",
      "22000/49000 loss: 1.249827617189242\n",
      "24000/49000 loss: 1.0870482360194773\n",
      "26000/49000 loss: 1.080235585861987\n",
      "28000/49000 loss: 1.019006959533381\n",
      "30000/49000 loss: 1.106774331340137\n",
      "32000/49000 loss: 0.9604849722801755\n",
      "34000/49000 loss: 0.9253322310062634\n",
      "36000/49000 loss: 0.868975174859297\n",
      "38000/49000 loss: 0.9052577380353091\n",
      "40000/49000 loss: 0.8214029650046902\n",
      "42000/49000 loss: 0.7606157276050083\n",
      "44000/49000 loss: 0.7108942414755696\n",
      "46000/49000 loss: 0.7581694084814535\n",
      "48000/49000 loss: 0.6923153667547071\n",
      "epoch 1: valid acc = 0.742, new learning rate = 0.000475\n",
      "2000/49000 loss: 0.6104332690160599\n",
      "4000/49000 loss: 0.6718755749338403\n",
      "6000/49000 loss: 0.6343179347562493\n",
      "8000/49000 loss: 0.7287396847471964\n",
      "10000/49000 loss: 0.6966754420184902\n",
      "12000/49000 loss: 0.6916941680177594\n",
      "14000/49000 loss: 0.6251730021065933\n",
      "16000/49000 loss: 0.5660531489542798\n",
      "18000/49000 loss: 0.6331548427479059\n",
      "20000/49000 loss: 0.5863821155378321\n",
      "22000/49000 loss: 0.5613022438431514\n",
      "24000/49000 loss: 0.5379384332547328\n",
      "26000/49000 loss: 0.70719652461575\n",
      "28000/49000 loss: 0.5081850421782347\n",
      "30000/49000 loss: 0.5327610089217372\n",
      "32000/49000 loss: 0.6984943122153253\n",
      "34000/49000 loss: 0.5479796642133575\n",
      "36000/49000 loss: 0.5695335312958684\n",
      "38000/49000 loss: 0.5910703878652803\n",
      "40000/49000 loss: 0.7176312859635893\n",
      "42000/49000 loss: 0.4634424953099457\n",
      "44000/49000 loss: 0.5044483935011054\n",
      "46000/49000 loss: 0.5448261920349723\n",
      "48000/49000 loss: 0.5973793372141866\n",
      "epoch 2: valid acc = 0.811, new learning rate = 0.00045125\n",
      "2000/49000 loss: 0.5555037244338745\n",
      "4000/49000 loss: 0.4909558096622371\n",
      "6000/49000 loss: 0.4273384442142767\n",
      "8000/49000 loss: 0.561149243313039\n",
      "10000/49000 loss: 0.45383918236372156\n",
      "12000/49000 loss: 0.556659734488882\n",
      "14000/49000 loss: 0.4597893437662535\n",
      "16000/49000 loss: 0.42283937887427486\n",
      "18000/49000 loss: 0.6199206794433965\n",
      "20000/49000 loss: 0.4596853607573474\n",
      "22000/49000 loss: 0.44384711464288484\n",
      "24000/49000 loss: 0.42900848571690986\n",
      "26000/49000 loss: 0.49508408388629926\n",
      "28000/49000 loss: 0.4286551790373475\n",
      "30000/49000 loss: 0.506825953016316\n",
      "32000/49000 loss: 0.3999468177358801\n",
      "34000/49000 loss: 0.5486437834245443\n",
      "36000/49000 loss: 0.4143299720491666\n",
      "38000/49000 loss: 0.44941189801253123\n",
      "40000/49000 loss: 0.5255938105293003\n",
      "42000/49000 loss: 0.4719441864159521\n",
      "44000/49000 loss: 0.5189827622420453\n",
      "46000/49000 loss: 0.5604388222090122\n",
      "48000/49000 loss: 0.45278728516439753\n",
      "epoch 3: valid acc = 0.822, new learning rate = 0.0004286875\n",
      "2000/49000 loss: 0.4626678609545649\n",
      "4000/49000 loss: 0.48128417905700677\n",
      "6000/49000 loss: 0.5015287706934127\n",
      "8000/49000 loss: 0.5186646762546785\n",
      "10000/49000 loss: 0.37463708616272795\n",
      "12000/49000 loss: 0.48317087088301314\n",
      "14000/49000 loss: 0.5205949043267224\n",
      "16000/49000 loss: 0.3737979796665409\n",
      "18000/49000 loss: 0.41397280780203977\n",
      "20000/49000 loss: 0.4533846399681721\n",
      "22000/49000 loss: 0.40321801370899546\n",
      "24000/49000 loss: 0.32985719133529756\n",
      "26000/49000 loss: 0.3651321519138342\n",
      "28000/49000 loss: 0.4289189693566758\n",
      "30000/49000 loss: 0.33350201008299263\n",
      "32000/49000 loss: 0.5568330870898105\n",
      "34000/49000 loss: 0.39564182577909074\n",
      "36000/49000 loss: 0.4955059504827647\n",
      "38000/49000 loss: 0.38789639718409047\n",
      "40000/49000 loss: 0.49656890186893915\n",
      "42000/49000 loss: 0.44350159879539075\n",
      "44000/49000 loss: 0.3648817118072691\n",
      "46000/49000 loss: 0.419917700240078\n",
      "48000/49000 loss: 0.5113765154192338\n",
      "epoch 4: valid acc = 0.843, new learning rate = 0.00040725312499999993\n",
      "2000/49000 loss: 0.37131573262335543\n",
      "4000/49000 loss: 0.45974960034458\n",
      "6000/49000 loss: 0.36762755853511986\n",
      "8000/49000 loss: 0.39157045289791315\n",
      "10000/49000 loss: 0.4467372800730103\n",
      "12000/49000 loss: 0.4205939315601517\n",
      "14000/49000 loss: 0.4797989257385182\n",
      "16000/49000 loss: 0.3107654289385628\n",
      "18000/49000 loss: 0.5056686338482937\n",
      "20000/49000 loss: 0.2899922644652245\n",
      "22000/49000 loss: 0.4749144366162923\n",
      "24000/49000 loss: 0.3910512727789426\n",
      "26000/49000 loss: 0.45098376443374477\n",
      "28000/49000 loss: 0.40110824818631086\n",
      "30000/49000 loss: 0.4296563540206571\n",
      "32000/49000 loss: 0.4304036615670055\n",
      "34000/49000 loss: 0.44049059111379496\n",
      "36000/49000 loss: 0.37575806852033117\n",
      "38000/49000 loss: 0.29808846187836147\n",
      "40000/49000 loss: 0.40640422050690356\n",
      "42000/49000 loss: 0.33442186751058484\n",
      "44000/49000 loss: 0.387894176972181\n",
      "46000/49000 loss: 0.389386588926194\n",
      "48000/49000 loss: 0.41938138788281604\n",
      "epoch 5: valid acc = 0.852, new learning rate = 0.0003868904687499999\n",
      "2000/49000 loss: 0.42139384283675485\n",
      "4000/49000 loss: 0.47181216779629515\n",
      "6000/49000 loss: 0.4605918887879455\n",
      "8000/49000 loss: 0.3101187026036782\n",
      "10000/49000 loss: 0.36589823551417144\n",
      "12000/49000 loss: 0.404853220588928\n",
      "14000/49000 loss: 0.36943744483213375\n",
      "16000/49000 loss: 0.4566668047698135\n",
      "18000/49000 loss: 0.4799581158186624\n",
      "20000/49000 loss: 0.38611730397995764\n",
      "22000/49000 loss: 0.41746690877860204\n",
      "24000/49000 loss: 0.4548109548880886\n",
      "26000/49000 loss: 0.46304958472696356\n",
      "28000/49000 loss: 0.4250499050091686\n",
      "30000/49000 loss: 0.4491125901107764\n",
      "32000/49000 loss: 0.3342648562015158\n",
      "34000/49000 loss: 0.40748605872693366\n",
      "36000/49000 loss: 0.5399208820724474\n",
      "38000/49000 loss: 0.4204779783530383\n",
      "40000/49000 loss: 0.44233995492780936\n",
      "42000/49000 loss: 0.32923506529617447\n",
      "44000/49000 loss: 0.36701462413430097\n",
      "46000/49000 loss: 0.44874216302966724\n",
      "48000/49000 loss: 0.4453978666089671\n",
      "epoch 6: valid acc = 0.861, new learning rate = 0.0003675459453124999\n",
      "2000/49000 loss: 0.41337518344751883\n",
      "4000/49000 loss: 0.33874291761150876\n",
      "6000/49000 loss: 0.4481482740974021\n",
      "8000/49000 loss: 0.39319117861300235\n",
      "10000/49000 loss: 0.39449111531077796\n",
      "12000/49000 loss: 0.4265598843094624\n",
      "14000/49000 loss: 0.28058026535511077\n",
      "16000/49000 loss: 0.39573868035483284\n",
      "18000/49000 loss: 0.3934689143062229\n",
      "20000/49000 loss: 0.36565277630826803\n",
      "22000/49000 loss: 0.3766178618694342\n",
      "24000/49000 loss: 0.404217913266721\n",
      "26000/49000 loss: 0.4331005657394919\n",
      "28000/49000 loss: 0.3276672229505897\n",
      "30000/49000 loss: 0.3235150329246476\n",
      "32000/49000 loss: 0.431683122189244\n",
      "34000/49000 loss: 0.32892520961299593\n",
      "36000/49000 loss: 0.42317109201868225\n",
      "38000/49000 loss: 0.42368789698747433\n",
      "40000/49000 loss: 0.3577282663639822\n",
      "42000/49000 loss: 0.3451880043437455\n",
      "44000/49000 loss: 0.33954565701889067\n",
      "46000/49000 loss: 0.24288454152577785\n",
      "48000/49000 loss: 0.3893876975850529\n",
      "epoch 7: valid acc = 0.855, new learning rate = 0.00034916864804687486\n",
      "2000/49000 loss: 0.3690915380714796\n",
      "4000/49000 loss: 0.4581983704149969\n",
      "6000/49000 loss: 0.3683634187274933\n",
      "8000/49000 loss: 0.4614891160172994\n",
      "10000/49000 loss: 0.33387552860625497\n",
      "12000/49000 loss: 0.43591944396252535\n",
      "14000/49000 loss: 0.351512363886901\n",
      "16000/49000 loss: 0.4115262342066714\n",
      "18000/49000 loss: 0.48182169474839\n",
      "20000/49000 loss: 0.483434119476503\n",
      "22000/49000 loss: 0.3882121728261669\n",
      "24000/49000 loss: 0.40579873785264003\n",
      "26000/49000 loss: 0.27003947801487704\n",
      "28000/49000 loss: 0.3831980908749735\n",
      "30000/49000 loss: 0.358938902850744\n",
      "32000/49000 loss: 0.48163251487797937\n",
      "34000/49000 loss: 0.3096316093666197\n",
      "36000/49000 loss: 0.4168957340020165\n",
      "38000/49000 loss: 0.27074478662356216\n",
      "40000/49000 loss: 0.34699773183637145\n",
      "42000/49000 loss: 0.512917397983772\n",
      "44000/49000 loss: 0.32680769140826893\n",
      "46000/49000 loss: 0.46942881825853705\n",
      "48000/49000 loss: 0.36417244193469406\n",
      "epoch 8: valid acc = 0.867, new learning rate = 0.0003317102156445311\n",
      "2000/49000 loss: 0.4245150550318328\n",
      "4000/49000 loss: 0.2951423712977388\n",
      "6000/49000 loss: 0.2967941956280119\n",
      "8000/49000 loss: 0.4254484394682033\n",
      "10000/49000 loss: 0.4528670665468537\n",
      "12000/49000 loss: 0.433444396943374\n",
      "14000/49000 loss: 0.34656663933554765\n",
      "16000/49000 loss: 0.3704336526289854\n",
      "18000/49000 loss: 0.33668221763203615\n",
      "20000/49000 loss: 0.29605999985901743\n",
      "22000/49000 loss: 0.4113816244797981\n",
      "24000/49000 loss: 0.35545129181486795\n",
      "26000/49000 loss: 0.31665994943799536\n",
      "28000/49000 loss: 0.38204473816969015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000/49000 loss: 0.4242586959930032\n",
      "32000/49000 loss: 0.42034559306982694\n",
      "34000/49000 loss: 0.4068103536681803\n",
      "36000/49000 loss: 0.2939000645439337\n",
      "38000/49000 loss: 0.3603893462939608\n",
      "40000/49000 loss: 0.37527086270571725\n",
      "42000/49000 loss: 0.45808592771386103\n",
      "44000/49000 loss: 0.4647270897561837\n",
      "46000/49000 loss: 0.34980212050415554\n",
      "48000/49000 loss: 0.48230117765216707\n",
      "epoch 9: valid acc = 0.862, new learning rate = 0.0003151247048623045\n",
      "2000/49000 loss: 0.34892915638747757\n",
      "4000/49000 loss: 0.4145918839748189\n",
      "6000/49000 loss: 0.3494525970858285\n",
      "8000/49000 loss: 0.2983515777201235\n",
      "10000/49000 loss: 0.2770085055664492\n",
      "12000/49000 loss: 0.377810245022952\n",
      "14000/49000 loss: 0.33226299822542116\n",
      "16000/49000 loss: 0.39251179643435863\n",
      "18000/49000 loss: 0.36530830133105735\n",
      "20000/49000 loss: 0.45320616251950757\n",
      "22000/49000 loss: 0.3627812468294922\n",
      "24000/49000 loss: 0.31037112894434254\n",
      "26000/49000 loss: 0.3051540192403422\n",
      "28000/49000 loss: 0.32845303976693935\n",
      "30000/49000 loss: 0.3229301781574068\n",
      "32000/49000 loss: 0.34491902462352125\n",
      "34000/49000 loss: 0.42302906902734105\n",
      "36000/49000 loss: 0.4487680381175956\n",
      "38000/49000 loss: 0.3701642548220832\n",
      "40000/49000 loss: 0.48455112352613827\n",
      "42000/49000 loss: 0.30114928597952273\n",
      "44000/49000 loss: 0.36227682105327325\n",
      "46000/49000 loss: 0.4279510930802787\n",
      "48000/49000 loss: 0.24991317116845557\n",
      "epoch 10: valid acc = 0.871, new learning rate = 0.00029936846961918924\n",
      "number of batches for training: 245\n",
      "2000/49000 loss: 0.3615791686194451\n",
      "4000/49000 loss: 0.41500850754148694\n",
      "6000/49000 loss: 0.29928638609774166\n",
      "8000/49000 loss: 0.3017995878751091\n",
      "10000/49000 loss: 0.32169075954738396\n",
      "12000/49000 loss: 0.3443665984840022\n",
      "14000/49000 loss: 0.3903175712840343\n",
      "16000/49000 loss: 0.33710490716345504\n",
      "18000/49000 loss: 0.4165698071961657\n",
      "20000/49000 loss: 0.3055532833277197\n",
      "22000/49000 loss: 0.2935313745566719\n",
      "24000/49000 loss: 0.3418671946598802\n",
      "26000/49000 loss: 0.3884757314309092\n",
      "28000/49000 loss: 0.29297980487309916\n",
      "30000/49000 loss: 0.4173762091563707\n",
      "32000/49000 loss: 0.39479351468157553\n",
      "34000/49000 loss: 0.2567535831229334\n",
      "36000/49000 loss: 0.35249887652030276\n",
      "38000/49000 loss: 0.3481989451957124\n",
      "40000/49000 loss: 0.31088056284355153\n",
      "42000/49000 loss: 0.31891773045383576\n",
      "44000/49000 loss: 0.3662872251913493\n",
      "46000/49000 loss: 0.31768334057191416\n",
      "48000/49000 loss: 0.3705418933385196\n",
      "epoch 1: valid acc = 0.865, new learning rate = 0.000475\n",
      "2000/49000 loss: 0.3783294465045689\n",
      "4000/49000 loss: 0.4278453640805105\n",
      "6000/49000 loss: 0.34532652396101965\n",
      "8000/49000 loss: 0.4395373261038524\n",
      "10000/49000 loss: 0.3706476293224856\n",
      "12000/49000 loss: 0.39758787450297833\n",
      "14000/49000 loss: 0.30784874787312355\n",
      "16000/49000 loss: 0.3221060716670364\n",
      "18000/49000 loss: 0.35459119384460847\n",
      "20000/49000 loss: 0.37706198610807423\n",
      "22000/49000 loss: 0.3396381763597161\n",
      "24000/49000 loss: 0.38061929064513755\n",
      "26000/49000 loss: 0.3366245122953347\n",
      "28000/49000 loss: 0.4545705262518383\n",
      "30000/49000 loss: 0.3965941297724288\n",
      "32000/49000 loss: 0.2877443998284883\n",
      "34000/49000 loss: 0.2919641805368974\n",
      "36000/49000 loss: 0.3367375867029313\n",
      "38000/49000 loss: 0.3418851679571014\n",
      "40000/49000 loss: 0.4087710280662345\n",
      "42000/49000 loss: 0.3425112560488843\n",
      "44000/49000 loss: 0.34136802494493024\n",
      "46000/49000 loss: 0.270112621345847\n",
      "48000/49000 loss: 0.3328110874718292\n",
      "epoch 2: valid acc = 0.879, new learning rate = 0.00045125\n",
      "2000/49000 loss: 0.2566366451562176\n",
      "4000/49000 loss: 0.2992587320251722\n",
      "6000/49000 loss: 0.3636452576816913\n",
      "8000/49000 loss: 0.39350004099072394\n",
      "10000/49000 loss: 0.44650988972856265\n",
      "12000/49000 loss: 0.4051463301495592\n",
      "14000/49000 loss: 0.36133103210676265\n",
      "16000/49000 loss: 0.2757430629257411\n",
      "18000/49000 loss: 0.38070229981277137\n",
      "20000/49000 loss: 0.24853757664529486\n",
      "22000/49000 loss: 0.3634198543277355\n",
      "24000/49000 loss: 0.3961091141372336\n",
      "26000/49000 loss: 0.2541454773969571\n",
      "28000/49000 loss: 0.3405480228305735\n",
      "30000/49000 loss: 0.21973015547008584\n",
      "32000/49000 loss: 0.3191263501898083\n",
      "34000/49000 loss: 0.3337077669798237\n",
      "36000/49000 loss: 0.3809462328042457\n",
      "38000/49000 loss: 0.3526253704128089\n",
      "40000/49000 loss: 0.3089189191985039\n",
      "42000/49000 loss: 0.33836204849559937\n",
      "44000/49000 loss: 0.27955753468291283\n",
      "46000/49000 loss: 0.3234335460917011\n",
      "48000/49000 loss: 0.3504613546906158\n",
      "epoch 3: valid acc = 0.877, new learning rate = 0.0004286875\n",
      "2000/49000 loss: 0.29789577093547276\n",
      "4000/49000 loss: 0.3494425017309873\n",
      "6000/49000 loss: 0.42137710658031907\n",
      "8000/49000 loss: 0.2990877953970641\n",
      "10000/49000 loss: 0.33464692616822594\n",
      "12000/49000 loss: 0.3335337135108684\n",
      "14000/49000 loss: 0.24320660050607396\n",
      "16000/49000 loss: 0.369792701906346\n",
      "18000/49000 loss: 0.36015638393348387\n",
      "20000/49000 loss: 0.24684851275188968\n",
      "22000/49000 loss: 0.23843085853326276\n",
      "24000/49000 loss: 0.3443036481871014\n",
      "26000/49000 loss: 0.30802730361861747\n",
      "28000/49000 loss: 0.2902421305976628\n",
      "30000/49000 loss: 0.33159815472286824\n",
      "32000/49000 loss: 0.324831442496669\n",
      "34000/49000 loss: 0.30661230402977124\n",
      "36000/49000 loss: 0.3595258306952199\n",
      "38000/49000 loss: 0.46697605486407834\n",
      "40000/49000 loss: 0.40094105807404823\n",
      "42000/49000 loss: 0.2968220825311297\n",
      "44000/49000 loss: 0.3509204905615931\n",
      "46000/49000 loss: 0.3397610684721065\n",
      "48000/49000 loss: 0.3121193044649542\n",
      "epoch 4: valid acc = 0.878, new learning rate = 0.00040725312499999993\n",
      "2000/49000 loss: 0.3394182793836555\n",
      "4000/49000 loss: 0.30870486084404647\n",
      "6000/49000 loss: 0.2858064465870262\n",
      "8000/49000 loss: 0.3255917079767384\n",
      "10000/49000 loss: 0.38734642098168265\n",
      "12000/49000 loss: 0.3650348043768073\n",
      "14000/49000 loss: 0.3619212886079211\n",
      "16000/49000 loss: 0.411951772755599\n",
      "18000/49000 loss: 0.29615419277149957\n",
      "20000/49000 loss: 0.29443428263527727\n",
      "22000/49000 loss: 0.32634181732545603\n",
      "24000/49000 loss: 0.3929017004766182\n",
      "26000/49000 loss: 0.3159887320763382\n",
      "28000/49000 loss: 0.4080214520499482\n",
      "30000/49000 loss: 0.21448059796737914\n",
      "32000/49000 loss: 0.25629963586831656\n",
      "34000/49000 loss: 0.2633194603568997\n",
      "36000/49000 loss: 0.32870508881088734\n",
      "38000/49000 loss: 0.2961406869378838\n",
      "40000/49000 loss: 0.3691613884835207\n",
      "42000/49000 loss: 0.21153750768704646\n",
      "44000/49000 loss: 0.25426177449700027\n",
      "46000/49000 loss: 0.3037473023602865\n",
      "48000/49000 loss: 0.2350353890420385\n",
      "epoch 5: valid acc = 0.881, new learning rate = 0.0003868904687499999\n",
      "2000/49000 loss: 0.3112951247974625\n",
      "4000/49000 loss: 0.2422785465289426\n",
      "6000/49000 loss: 0.35787709603243917\n",
      "8000/49000 loss: 0.370469815250154\n",
      "10000/49000 loss: 0.39111129146634593\n",
      "12000/49000 loss: 0.2752466002855315\n",
      "14000/49000 loss: 0.24904765834512463\n",
      "16000/49000 loss: 0.3634790394643905\n",
      "18000/49000 loss: 0.32647635462388364\n",
      "20000/49000 loss: 0.2939032583792534\n",
      "22000/49000 loss: 0.37562696365243\n",
      "24000/49000 loss: 0.45458628339347634\n",
      "26000/49000 loss: 0.35325544356428124\n",
      "28000/49000 loss: 0.27523274078778087\n",
      "30000/49000 loss: 0.39313910529752794\n",
      "32000/49000 loss: 0.2662288750298871\n",
      "34000/49000 loss: 0.3198672564818651\n",
      "36000/49000 loss: 0.2545032896933366\n",
      "38000/49000 loss: 0.3613879597275791\n",
      "40000/49000 loss: 0.2995290666496939\n",
      "42000/49000 loss: 0.3760660171896929\n",
      "44000/49000 loss: 0.22353140616581194\n",
      "46000/49000 loss: 0.35611231643750685\n",
      "48000/49000 loss: 0.3513198772604487\n",
      "epoch 6: valid acc = 0.88, new learning rate = 0.0003675459453124999\n",
      "2000/49000 loss: 0.24307086692419252\n",
      "4000/49000 loss: 0.3026592501189738\n",
      "6000/49000 loss: 0.3254537049835829\n",
      "8000/49000 loss: 0.3509607498289016\n",
      "10000/49000 loss: 0.31022378533166634\n",
      "12000/49000 loss: 0.37203961255388557\n",
      "14000/49000 loss: 0.32855784395006693\n",
      "16000/49000 loss: 0.3270528842182615\n",
      "18000/49000 loss: 0.32750053158223824\n",
      "20000/49000 loss: 0.3977633645876074\n",
      "22000/49000 loss: 0.35906932632301664\n",
      "24000/49000 loss: 0.27519893603888346\n",
      "26000/49000 loss: 0.2825736516512363\n",
      "28000/49000 loss: 0.3665855581902209\n",
      "30000/49000 loss: 0.3224214993233458\n",
      "32000/49000 loss: 0.3851091252626616\n",
      "34000/49000 loss: 0.37038123127352396\n",
      "36000/49000 loss: 0.40768913352258784\n",
      "38000/49000 loss: 0.3489076886468927\n",
      "40000/49000 loss: 0.30601657075447325\n",
      "42000/49000 loss: 0.2745797646810449\n",
      "44000/49000 loss: 0.28799503802118814\n",
      "46000/49000 loss: 0.2811578745465602\n",
      "48000/49000 loss: 0.2921829954194074\n",
      "epoch 7: valid acc = 0.882, new learning rate = 0.00034916864804687486\n",
      "2000/49000 loss: 0.3217331913781191\n",
      "4000/49000 loss: 0.4183579801987354\n",
      "6000/49000 loss: 0.25389811484930314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/49000 loss: 0.3337763651323417\n",
      "10000/49000 loss: 0.22630871336953465\n",
      "12000/49000 loss: 0.3799395604620549\n",
      "14000/49000 loss: 0.38559136534586064\n",
      "16000/49000 loss: 0.35762698804021714\n",
      "18000/49000 loss: 0.352599974673304\n",
      "20000/49000 loss: 0.19955754330089406\n",
      "22000/49000 loss: 0.26919602064300935\n",
      "24000/49000 loss: 0.22113994951618668\n",
      "26000/49000 loss: 0.2713588339846217\n",
      "28000/49000 loss: 0.29299447548945623\n",
      "30000/49000 loss: 0.28689271855981624\n",
      "32000/49000 loss: 0.2712509283206411\n",
      "34000/49000 loss: 0.3089605468391063\n",
      "36000/49000 loss: 0.3478946826756194\n",
      "38000/49000 loss: 0.32319003389527906\n",
      "40000/49000 loss: 0.22452010120288082\n",
      "42000/49000 loss: 0.3410256595566983\n",
      "44000/49000 loss: 0.3164972097128844\n",
      "46000/49000 loss: 0.29793677804044794\n",
      "48000/49000 loss: 0.3459839892587176\n",
      "epoch 8: valid acc = 0.883, new learning rate = 0.0003317102156445311\n",
      "2000/49000 loss: 0.4088702887791349\n",
      "4000/49000 loss: 0.3227135923572127\n",
      "6000/49000 loss: 0.23504983715213149\n",
      "8000/49000 loss: 0.3072079459572096\n",
      "10000/49000 loss: 0.2559087813587054\n",
      "12000/49000 loss: 0.26319732236843685\n",
      "14000/49000 loss: 0.21911253375401332\n",
      "16000/49000 loss: 0.22736504482802938\n",
      "18000/49000 loss: 0.3044082870657449\n",
      "20000/49000 loss: 0.23621193983716404\n",
      "22000/49000 loss: 0.3423701181702358\n",
      "24000/49000 loss: 0.25059007230477387\n",
      "26000/49000 loss: 0.200265920413283\n",
      "28000/49000 loss: 0.3483451664569918\n",
      "30000/49000 loss: 0.3053835287583405\n",
      "32000/49000 loss: 0.2067877025499107\n",
      "34000/49000 loss: 0.33797552528313496\n",
      "36000/49000 loss: 0.2447099801013419\n",
      "38000/49000 loss: 0.2664805250704685\n",
      "40000/49000 loss: 0.24796107671557832\n",
      "42000/49000 loss: 0.3415994625594749\n",
      "44000/49000 loss: 0.24943223423741745\n",
      "46000/49000 loss: 0.3445793971584615\n",
      "48000/49000 loss: 0.319027370389868\n",
      "epoch 9: valid acc = 0.886, new learning rate = 0.0003151247048623045\n",
      "2000/49000 loss: 0.2771902968203448\n",
      "4000/49000 loss: 0.24719713369157992\n",
      "6000/49000 loss: 0.30489622233437796\n",
      "8000/49000 loss: 0.30905320802883135\n",
      "10000/49000 loss: 0.35011499135229773\n",
      "12000/49000 loss: 0.28714796629217737\n",
      "14000/49000 loss: 0.2965823507034751\n",
      "16000/49000 loss: 0.21504096570414072\n",
      "18000/49000 loss: 0.29625231457401346\n",
      "20000/49000 loss: 0.20781101856862097\n",
      "22000/49000 loss: 0.2619750780086905\n",
      "24000/49000 loss: 0.2768232974195603\n",
      "26000/49000 loss: 0.35880260947910675\n",
      "28000/49000 loss: 0.2927204805843783\n",
      "30000/49000 loss: 0.30762166388411427\n",
      "32000/49000 loss: 0.34403999841627303\n",
      "34000/49000 loss: 0.26537575265027136\n",
      "36000/49000 loss: 0.305089297090765\n",
      "38000/49000 loss: 0.2589860479381253\n",
      "40000/49000 loss: 0.3306872758742447\n",
      "42000/49000 loss: 0.23950492117729658\n",
      "44000/49000 loss: 0.29259820710823986\n",
      "46000/49000 loss: 0.26405856074325823\n",
      "48000/49000 loss: 0.35279144293549686\n",
      "epoch 10: valid acc = 0.887, new learning rate = 0.00029936846961918924\n",
      "number of batches for training: 245\n",
      "2000/49000 loss: 0.2689698790460087\n",
      "4000/49000 loss: 0.22702874603069656\n",
      "6000/49000 loss: 0.20772969986864304\n",
      "8000/49000 loss: 0.23817710148427554\n",
      "10000/49000 loss: 0.2690920614527236\n",
      "12000/49000 loss: 0.3031615160103912\n",
      "14000/49000 loss: 0.3140496217138612\n",
      "16000/49000 loss: 0.2970400115521682\n",
      "18000/49000 loss: 0.24695574703569087\n",
      "20000/49000 loss: 0.34744416307771575\n",
      "22000/49000 loss: 0.2222565246337556\n",
      "24000/49000 loss: 0.30320948917709034\n",
      "26000/49000 loss: 0.2572090048795409\n",
      "28000/49000 loss: 0.29014875004775553\n",
      "30000/49000 loss: 0.2881712252487877\n",
      "32000/49000 loss: 0.3616255940762456\n",
      "34000/49000 loss: 0.24256597566383525\n",
      "36000/49000 loss: 0.23844403989996038\n",
      "38000/49000 loss: 0.26875018445677507\n",
      "40000/49000 loss: 0.4549013638150157\n",
      "42000/49000 loss: 0.26578657150340845\n",
      "44000/49000 loss: 0.35369113753297676\n",
      "46000/49000 loss: 0.2861604493659578\n",
      "48000/49000 loss: 0.3182438559380028\n",
      "epoch 1: valid acc = 0.879, new learning rate = 0.000475\n",
      "2000/49000 loss: 0.2420015297546193\n",
      "4000/49000 loss: 0.3611297298759455\n",
      "6000/49000 loss: 0.2695583360864113\n",
      "8000/49000 loss: 0.253130204483154\n",
      "10000/49000 loss: 0.31147253919287227\n",
      "12000/49000 loss: 0.33397505752986784\n",
      "14000/49000 loss: 0.36454530165056165\n",
      "16000/49000 loss: 0.3534924157230305\n",
      "18000/49000 loss: 0.26890652970617945\n",
      "20000/49000 loss: 0.3291992282995311\n",
      "22000/49000 loss: 0.3215136991341809\n",
      "24000/49000 loss: 0.3043464821284322\n",
      "26000/49000 loss: 0.30966665495158574\n",
      "28000/49000 loss: 0.32335872876680744\n",
      "30000/49000 loss: 0.3283470251676053\n",
      "32000/49000 loss: 0.309113466017323\n",
      "34000/49000 loss: 0.25993464956075896\n",
      "36000/49000 loss: 0.28430223307823804\n",
      "38000/49000 loss: 0.2695355514675389\n",
      "40000/49000 loss: 0.21557618530728015\n",
      "42000/49000 loss: 0.23798953538239967\n",
      "44000/49000 loss: 0.2935375587929214\n",
      "46000/49000 loss: 0.20662446237244772\n",
      "48000/49000 loss: 0.29040307659417003\n",
      "epoch 2: valid acc = 0.884, new learning rate = 0.00045125\n",
      "2000/49000 loss: 0.32245933476922306\n",
      "4000/49000 loss: 0.3860135705370295\n",
      "6000/49000 loss: 0.26951088180779703\n",
      "8000/49000 loss: 0.31890359666234824\n",
      "10000/49000 loss: 0.22282284108153247\n",
      "12000/49000 loss: 0.3217604960246269\n",
      "14000/49000 loss: 0.32057764112918957\n",
      "16000/49000 loss: 0.25542704013390916\n",
      "18000/49000 loss: 0.28369778234289916\n",
      "20000/49000 loss: 0.21653047863554942\n",
      "22000/49000 loss: 0.2312548004143231\n",
      "24000/49000 loss: 0.20863814840158837\n",
      "26000/49000 loss: 0.18866938828724958\n",
      "28000/49000 loss: 0.2668880550299178\n",
      "30000/49000 loss: 0.2528935439976514\n",
      "32000/49000 loss: 0.1860011647949693\n",
      "34000/49000 loss: 0.32126328374808677\n",
      "36000/49000 loss: 0.1898097390610154\n",
      "38000/49000 loss: 0.2244666307551102\n",
      "40000/49000 loss: 0.3441217152965049\n",
      "42000/49000 loss: 0.28067819332666594\n",
      "44000/49000 loss: 0.3075290407733625\n",
      "46000/49000 loss: 0.28897073584209343\n",
      "48000/49000 loss: 0.30627024054818874\n",
      "epoch 3: valid acc = 0.885, new learning rate = 0.0004286875\n",
      "2000/49000 loss: 0.2528736683962664\n",
      "4000/49000 loss: 0.23001510848569715\n",
      "6000/49000 loss: 0.30852889334887085\n",
      "8000/49000 loss: 0.27195366293211226\n",
      "10000/49000 loss: 0.23498150579690436\n",
      "12000/49000 loss: 0.28096258529129375\n",
      "14000/49000 loss: 0.2802865233166218\n",
      "16000/49000 loss: 0.3470165961837985\n",
      "18000/49000 loss: 0.1998109858445361\n",
      "20000/49000 loss: 0.3095861571019938\n",
      "22000/49000 loss: 0.273612874595598\n",
      "24000/49000 loss: 0.2455219238728913\n",
      "26000/49000 loss: 0.2919791425997912\n",
      "28000/49000 loss: 0.3390676489098029\n",
      "30000/49000 loss: 0.2700715894664938\n",
      "32000/49000 loss: 0.2599783854197414\n",
      "34000/49000 loss: 0.28764934572107864\n",
      "36000/49000 loss: 0.27047060386618416\n",
      "38000/49000 loss: 0.2878044753797709\n",
      "40000/49000 loss: 0.3120411377828437\n",
      "42000/49000 loss: 0.2915677538387973\n",
      "44000/49000 loss: 0.22764310592597753\n",
      "46000/49000 loss: 0.2808976467721266\n",
      "48000/49000 loss: 0.3302477135795423\n",
      "epoch 4: valid acc = 0.879, new learning rate = 0.00040725312499999993\n",
      "2000/49000 loss: 0.28294901350019097\n",
      "4000/49000 loss: 0.2057784482457736\n",
      "6000/49000 loss: 0.21819284430225971\n",
      "8000/49000 loss: 0.35781217693771755\n",
      "10000/49000 loss: 0.35178984201164465\n",
      "12000/49000 loss: 0.3358485520289727\n",
      "14000/49000 loss: 0.29840082588213007\n",
      "16000/49000 loss: 0.3085372016854824\n",
      "18000/49000 loss: 0.29999562633544197\n",
      "20000/49000 loss: 0.29430256302086494\n",
      "22000/49000 loss: 0.2985667545062248\n",
      "24000/49000 loss: 0.3331164245626515\n",
      "26000/49000 loss: 0.36687216930924627\n",
      "28000/49000 loss: 0.3355415914387021\n",
      "30000/49000 loss: 0.20991071434723585\n",
      "32000/49000 loss: 0.3336885002110113\n",
      "34000/49000 loss: 0.3547806085445923\n",
      "36000/49000 loss: 0.2394083627947021\n",
      "38000/49000 loss: 0.3207615162832912\n",
      "40000/49000 loss: 0.2480275701837073\n",
      "42000/49000 loss: 0.23719202843477205\n",
      "44000/49000 loss: 0.2938056620060404\n",
      "46000/49000 loss: 0.2423148495630363\n",
      "48000/49000 loss: 0.31378220123423956\n",
      "epoch 5: valid acc = 0.888, new learning rate = 0.0003868904687499999\n",
      "2000/49000 loss: 0.268920504196466\n",
      "4000/49000 loss: 0.283314447804841\n",
      "6000/49000 loss: 0.23098848416963536\n",
      "8000/49000 loss: 0.25339683319132217\n",
      "10000/49000 loss: 0.36581989046354535\n",
      "12000/49000 loss: 0.28133237187927135\n",
      "14000/49000 loss: 0.26238318057713683\n",
      "16000/49000 loss: 0.23494296858278213\n",
      "18000/49000 loss: 0.33802017166025056\n",
      "20000/49000 loss: 0.39298011615239803\n",
      "22000/49000 loss: 0.2911004732112546\n",
      "24000/49000 loss: 0.36287308754629033\n",
      "26000/49000 loss: 0.32362030535055897\n",
      "28000/49000 loss: 0.21587467277887407\n",
      "30000/49000 loss: 0.28459161879799216\n",
      "32000/49000 loss: 0.23919115981651695\n",
      "34000/49000 loss: 0.28424022067296345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36000/49000 loss: 0.2961674474809176\n",
      "38000/49000 loss: 0.23584824627615467\n",
      "40000/49000 loss: 0.2558637479970685\n",
      "42000/49000 loss: 0.27801006867657013\n",
      "44000/49000 loss: 0.2321882386093667\n",
      "46000/49000 loss: 0.368702512449367\n",
      "48000/49000 loss: 0.2835022929835952\n",
      "epoch 6: valid acc = 0.883, new learning rate = 0.0003675459453124999\n",
      "2000/49000 loss: 0.21893380919222613\n",
      "4000/49000 loss: 0.21355458364308968\n",
      "6000/49000 loss: 0.30608093399343306\n",
      "8000/49000 loss: 0.18967722968964817\n",
      "10000/49000 loss: 0.209170003886025\n",
      "12000/49000 loss: 0.31745905341428277\n",
      "14000/49000 loss: 0.3065649226096818\n",
      "16000/49000 loss: 0.3387975695032509\n",
      "18000/49000 loss: 0.23025019025584426\n",
      "20000/49000 loss: 0.2397348555177352\n",
      "22000/49000 loss: 0.2921901339575023\n",
      "24000/49000 loss: 0.295120420714568\n",
      "26000/49000 loss: 0.27662901879678586\n",
      "28000/49000 loss: 0.273510910691693\n",
      "30000/49000 loss: 0.21520576281474657\n",
      "32000/49000 loss: 0.3083451048524307\n",
      "34000/49000 loss: 0.2533032763102843\n",
      "36000/49000 loss: 0.38435310493672437\n",
      "38000/49000 loss: 0.20572226347788844\n",
      "40000/49000 loss: 0.2398037437720857\n",
      "42000/49000 loss: 0.22790662812252577\n",
      "44000/49000 loss: 0.23428031277204137\n",
      "46000/49000 loss: 0.2565143954667132\n",
      "48000/49000 loss: 0.2528003031666016\n",
      "epoch 7: valid acc = 0.877, new learning rate = 0.00034916864804687486\n",
      "2000/49000 loss: 0.23862804750295227\n",
      "4000/49000 loss: 0.23982525125442536\n",
      "6000/49000 loss: 0.33501802470182857\n",
      "8000/49000 loss: 0.23772817504187896\n",
      "10000/49000 loss: 0.24729316337245646\n",
      "12000/49000 loss: 0.21743895709189684\n",
      "14000/49000 loss: 0.2516086874069245\n",
      "16000/49000 loss: 0.24715954628105719\n",
      "18000/49000 loss: 0.2454179006839084\n",
      "20000/49000 loss: 0.2592894847781421\n",
      "22000/49000 loss: 0.23920326160457447\n",
      "24000/49000 loss: 0.31828651239406974\n",
      "26000/49000 loss: 0.2889093114017377\n",
      "28000/49000 loss: 0.2871605992969018\n",
      "30000/49000 loss: 0.33575480967991356\n",
      "32000/49000 loss: 0.2667322810390695\n",
      "34000/49000 loss: 0.24102839586204142\n",
      "36000/49000 loss: 0.259559857735692\n",
      "38000/49000 loss: 0.28680954389558944\n",
      "40000/49000 loss: 0.24283307128454926\n",
      "42000/49000 loss: 0.2011587164404993\n",
      "44000/49000 loss: 0.20131998033686277\n",
      "46000/49000 loss: 0.31559591435134376\n",
      "48000/49000 loss: 0.3110816676262288\n",
      "epoch 8: valid acc = 0.89, new learning rate = 0.0003317102156445311\n",
      "2000/49000 loss: 0.34600429998891574\n",
      "4000/49000 loss: 0.1328008190585152\n",
      "6000/49000 loss: 0.22870545459600855\n",
      "8000/49000 loss: 0.3572185367299257\n",
      "10000/49000 loss: 0.2628835475266202\n",
      "12000/49000 loss: 0.21764776958814286\n",
      "14000/49000 loss: 0.2252319547493853\n",
      "16000/49000 loss: 0.3734299783795605\n",
      "18000/49000 loss: 0.36817300737781616\n",
      "20000/49000 loss: 0.32631086096161455\n",
      "22000/49000 loss: 0.21954766419931393\n",
      "24000/49000 loss: 0.309726275659525\n",
      "26000/49000 loss: 0.2787572609782558\n",
      "28000/49000 loss: 0.28080265512138775\n",
      "30000/49000 loss: 0.2805403628837368\n",
      "32000/49000 loss: 0.28559953240693614\n",
      "34000/49000 loss: 0.28973493244608045\n",
      "36000/49000 loss: 0.2290257637591823\n",
      "38000/49000 loss: 0.2178947108009464\n",
      "40000/49000 loss: 0.23878450362899792\n",
      "42000/49000 loss: 0.28952990541063084\n",
      "44000/49000 loss: 0.32017709044526577\n",
      "46000/49000 loss: 0.30200150336067816\n",
      "48000/49000 loss: 0.20440536633378834\n",
      "epoch 9: valid acc = 0.885, new learning rate = 0.0003151247048623045\n",
      "2000/49000 loss: 0.16754352341610262\n",
      "4000/49000 loss: 0.20682687912189449\n",
      "6000/49000 loss: 0.2334530636065179\n",
      "8000/49000 loss: 0.26366474123364064\n",
      "10000/49000 loss: 0.39962725232457286\n",
      "12000/49000 loss: 0.16118344657300637\n",
      "14000/49000 loss: 0.220845513284198\n",
      "16000/49000 loss: 0.26495938860233464\n",
      "18000/49000 loss: 0.2810074543329689\n",
      "20000/49000 loss: 0.21544618579735375\n",
      "22000/49000 loss: 0.3246581785763442\n",
      "24000/49000 loss: 0.22551810027572436\n",
      "26000/49000 loss: 0.18744085506261687\n",
      "28000/49000 loss: 0.277031621985171\n",
      "30000/49000 loss: 0.2862429872605035\n",
      "32000/49000 loss: 0.22054357408071995\n",
      "34000/49000 loss: 0.3279875798209005\n",
      "36000/49000 loss: 0.23403788057230576\n",
      "38000/49000 loss: 0.26093427382731926\n",
      "40000/49000 loss: 0.230338574464348\n",
      "42000/49000 loss: 0.22831661078691165\n",
      "44000/49000 loss: 0.2629078039853404\n",
      "46000/49000 loss: 0.27681805639534574\n",
      "48000/49000 loss: 0.20593070764189425\n",
      "epoch 10: valid acc = 0.885, new learning rate = 0.00029936846961918924\n",
      "number of batches for training: 245\n",
      "2000/49000 loss: 0.3272646314750539\n",
      "4000/49000 loss: 0.19789227438465232\n",
      "6000/49000 loss: 0.2848560694522742\n",
      "8000/49000 loss: 0.24950078708916734\n",
      "10000/49000 loss: 0.2483074399044647\n",
      "12000/49000 loss: 0.32614639322885597\n",
      "14000/49000 loss: 0.1842143730896393\n",
      "16000/49000 loss: 0.22635380254651657\n",
      "18000/49000 loss: 0.284390319818419\n",
      "20000/49000 loss: 0.24469285323109544\n",
      "22000/49000 loss: 0.23361812847805274\n",
      "24000/49000 loss: 0.2661447972018449\n",
      "26000/49000 loss: 0.18055419069181858\n",
      "28000/49000 loss: 0.29341928504514175\n",
      "30000/49000 loss: 0.27935873728869565\n",
      "32000/49000 loss: 0.29539331647156064\n",
      "34000/49000 loss: 0.23883900438352995\n",
      "36000/49000 loss: 0.23301802723911355\n",
      "38000/49000 loss: 0.25916944511050793\n",
      "40000/49000 loss: 0.1979407661759395\n",
      "42000/49000 loss: 0.21735322278077604\n",
      "44000/49000 loss: 0.32318278720026705\n",
      "46000/49000 loss: 0.29776906813969023\n",
      "48000/49000 loss: 0.19573684746462594\n",
      "epoch 1: valid acc = 0.882, new learning rate = 0.000475\n",
      "2000/49000 loss: 0.20036812576178045\n",
      "4000/49000 loss: 0.3053735675469021\n",
      "6000/49000 loss: 0.30553619410518457\n",
      "8000/49000 loss: 0.26302274346861265\n",
      "10000/49000 loss: 0.2185069752851607\n",
      "12000/49000 loss: 0.26505016284402955\n",
      "14000/49000 loss: 0.2185495339456816\n",
      "16000/49000 loss: 0.24783660730802154\n",
      "18000/49000 loss: 0.26756694847128226\n",
      "20000/49000 loss: 0.23378084572433064\n",
      "22000/49000 loss: 0.33551922053292427\n",
      "24000/49000 loss: 0.28217501354761604\n",
      "26000/49000 loss: 0.2153528272280083\n",
      "28000/49000 loss: 0.17617867471696508\n",
      "30000/49000 loss: 0.21965503040093942\n",
      "32000/49000 loss: 0.1886736838425874\n",
      "34000/49000 loss: 0.26446153259406663\n",
      "36000/49000 loss: 0.2246316092486873\n",
      "38000/49000 loss: 0.2629540086441174\n",
      "40000/49000 loss: 0.24621377080768717\n",
      "42000/49000 loss: 0.21480199110847165\n",
      "44000/49000 loss: 0.19137078871244445\n",
      "46000/49000 loss: 0.19148465046892865\n",
      "48000/49000 loss: 0.17609969718626456\n",
      "epoch 2: valid acc = 0.892, new learning rate = 0.00045125\n",
      "2000/49000 loss: 0.2117724552902334\n",
      "4000/49000 loss: 0.2712604246061287\n",
      "6000/49000 loss: 0.2801044045731574\n",
      "8000/49000 loss: 0.2552477655960724\n",
      "10000/49000 loss: 0.19561599760400009\n",
      "12000/49000 loss: 0.2093042248322275\n",
      "14000/49000 loss: 0.2817925672098645\n",
      "16000/49000 loss: 0.26501089492765706\n",
      "18000/49000 loss: 0.2721059972876259\n",
      "20000/49000 loss: 0.26693974469685156\n",
      "22000/49000 loss: 0.2063211015663825\n",
      "24000/49000 loss: 0.29668511654602986\n",
      "26000/49000 loss: 0.23736500430142568\n",
      "28000/49000 loss: 0.24928746838788582\n",
      "30000/49000 loss: 0.22723061440318781\n",
      "32000/49000 loss: 0.30416703698954406\n",
      "34000/49000 loss: 0.24611380578147146\n",
      "36000/49000 loss: 0.23721773376860925\n",
      "38000/49000 loss: 0.2203379520568368\n",
      "40000/49000 loss: 0.20124193108408991\n",
      "42000/49000 loss: 0.22122752836217446\n",
      "44000/49000 loss: 0.2879824183935171\n",
      "46000/49000 loss: 0.26104457308646345\n",
      "48000/49000 loss: 0.2485046831078884\n",
      "epoch 3: valid acc = 0.892, new learning rate = 0.0004286875\n",
      "2000/49000 loss: 0.26997201627328665\n",
      "4000/49000 loss: 0.24485131787441414\n",
      "6000/49000 loss: 0.27561611054643903\n",
      "8000/49000 loss: 0.18706264542257597\n",
      "10000/49000 loss: 0.23283542787016\n",
      "12000/49000 loss: 0.18339422149618806\n",
      "14000/49000 loss: 0.2125603064446804\n",
      "16000/49000 loss: 0.2609218723312325\n",
      "18000/49000 loss: 0.3075228059541905\n",
      "20000/49000 loss: 0.22959780939819147\n",
      "22000/49000 loss: 0.2297284160395382\n",
      "24000/49000 loss: 0.2109126320858884\n",
      "26000/49000 loss: 0.2283527476194181\n",
      "28000/49000 loss: 0.27420368664142103\n",
      "30000/49000 loss: 0.2659036603793691\n",
      "32000/49000 loss: 0.4231165811212382\n",
      "34000/49000 loss: 0.20790183033945012\n",
      "36000/49000 loss: 0.22481924604551343\n",
      "38000/49000 loss: 0.266018625750485\n",
      "40000/49000 loss: 0.2495840959312278\n",
      "42000/49000 loss: 0.17681520995215683\n",
      "44000/49000 loss: 0.32177786640919637\n",
      "46000/49000 loss: 0.19230295510107392\n",
      "48000/49000 loss: 0.19552935032900695\n",
      "epoch 4: valid acc = 0.883, new learning rate = 0.00040725312499999993\n",
      "2000/49000 loss: 0.2376988137326645\n",
      "4000/49000 loss: 0.26639070358179595\n",
      "6000/49000 loss: 0.20054278061865335\n",
      "8000/49000 loss: 0.2134526100012226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/49000 loss: 0.22162180957350527\n",
      "12000/49000 loss: 0.23950126388581083\n",
      "14000/49000 loss: 0.27104057787657626\n",
      "16000/49000 loss: 0.21756005141080811\n",
      "18000/49000 loss: 0.15200340872882898\n",
      "20000/49000 loss: 0.2394700195144483\n",
      "22000/49000 loss: 0.3469429793606633\n",
      "24000/49000 loss: 0.2552588465089573\n",
      "26000/49000 loss: 0.23882428979663775\n",
      "28000/49000 loss: 0.26596237515466864\n",
      "30000/49000 loss: 0.25492599161112744\n",
      "32000/49000 loss: 0.28014158788364335\n",
      "34000/49000 loss: 0.24210560345146032\n",
      "36000/49000 loss: 0.2436929344082135\n",
      "38000/49000 loss: 0.17458002932534641\n",
      "40000/49000 loss: 0.23016462434569263\n",
      "42000/49000 loss: 0.2960719520415531\n",
      "44000/49000 loss: 0.18883282352945407\n",
      "46000/49000 loss: 0.23830224241877038\n",
      "48000/49000 loss: 0.21248099929561046\n",
      "epoch 5: valid acc = 0.892, new learning rate = 0.0003868904687499999\n",
      "2000/49000 loss: 0.2551140460366045\n",
      "4000/49000 loss: 0.17982528117101396\n",
      "6000/49000 loss: 0.30476845518120405\n",
      "8000/49000 loss: 0.22695860825207534\n",
      "10000/49000 loss: 0.22756931370911082\n",
      "12000/49000 loss: 0.3338313083342618\n",
      "14000/49000 loss: 0.19947637437856391\n",
      "16000/49000 loss: 0.1927947680123214\n",
      "18000/49000 loss: 0.2458266284373549\n",
      "20000/49000 loss: 0.18611890643214862\n",
      "22000/49000 loss: 0.2924467274830158\n",
      "24000/49000 loss: 0.20817655601899526\n",
      "26000/49000 loss: 0.25093045668109804\n",
      "28000/49000 loss: 0.2775570862192338\n",
      "30000/49000 loss: 0.29453779748832537\n",
      "32000/49000 loss: 0.21898151176573322\n",
      "34000/49000 loss: 0.1935919978557851\n",
      "36000/49000 loss: 0.2572470213642499\n",
      "38000/49000 loss: 0.2170498257286779\n",
      "40000/49000 loss: 0.26882015841807455\n",
      "42000/49000 loss: 0.2660268838929121\n",
      "44000/49000 loss: 0.17031335540742285\n",
      "46000/49000 loss: 0.34649151092337627\n",
      "48000/49000 loss: 0.19950140993208393\n",
      "epoch 6: valid acc = 0.891, new learning rate = 0.0003675459453124999\n",
      "2000/49000 loss: 0.20545685475700828\n",
      "4000/49000 loss: 0.18222214426024602\n",
      "6000/49000 loss: 0.3058857402602176\n",
      "8000/49000 loss: 0.22603017502241568\n",
      "10000/49000 loss: 0.2677502789121699\n",
      "12000/49000 loss: 0.174873047813352\n",
      "14000/49000 loss: 0.18685647479260026\n",
      "16000/49000 loss: 0.3140580878900301\n",
      "18000/49000 loss: 0.22094902762886837\n",
      "20000/49000 loss: 0.26290272193843317\n",
      "22000/49000 loss: 0.20491722170034973\n",
      "24000/49000 loss: 0.2696415358114834\n",
      "26000/49000 loss: 0.17925955854754205\n",
      "28000/49000 loss: 0.17109526417813278\n",
      "30000/49000 loss: 0.2145872254420713\n",
      "32000/49000 loss: 0.2821273522903861\n",
      "34000/49000 loss: 0.24510303487298205\n",
      "36000/49000 loss: 0.29103377673215974\n",
      "38000/49000 loss: 0.2649406929171996\n",
      "40000/49000 loss: 0.27623053319736524\n",
      "42000/49000 loss: 0.24501414514105635\n",
      "44000/49000 loss: 0.20108284226384537\n",
      "46000/49000 loss: 0.2933140225071984\n",
      "48000/49000 loss: 0.18912362093083612\n",
      "epoch 7: valid acc = 0.89, new learning rate = 0.00034916864804687486\n",
      "2000/49000 loss: 0.26952788736826194\n",
      "4000/49000 loss: 0.23305537773816123\n",
      "6000/49000 loss: 0.23372319791247428\n",
      "8000/49000 loss: 0.22747300099580137\n",
      "10000/49000 loss: 0.17761872284487362\n",
      "12000/49000 loss: 0.2434362057190987\n",
      "14000/49000 loss: 0.25135110479008127\n",
      "16000/49000 loss: 0.16200843587870783\n",
      "18000/49000 loss: 0.2538867999784601\n",
      "20000/49000 loss: 0.24286963022243935\n",
      "22000/49000 loss: 0.17361387397459516\n",
      "24000/49000 loss: 0.23577720761870624\n",
      "26000/49000 loss: 0.18314617385605206\n",
      "28000/49000 loss: 0.20020859476836042\n",
      "30000/49000 loss: 0.15385319296985675\n",
      "32000/49000 loss: 0.18141708886486932\n",
      "34000/49000 loss: 0.24897537136658043\n",
      "36000/49000 loss: 0.22621544695936324\n",
      "38000/49000 loss: 0.20460243407654555\n",
      "40000/49000 loss: 0.14995172287566808\n",
      "42000/49000 loss: 0.20458468547313804\n",
      "44000/49000 loss: 0.3306888564685797\n",
      "46000/49000 loss: 0.1994354883383236\n",
      "48000/49000 loss: 0.19322298114255812\n",
      "epoch 8: valid acc = 0.888, new learning rate = 0.0003317102156445311\n",
      "2000/49000 loss: 0.19235153690647244\n",
      "4000/49000 loss: 0.20689571577296093\n",
      "6000/49000 loss: 0.2313333002084749\n",
      "8000/49000 loss: 0.21331217071250996\n",
      "10000/49000 loss: 0.2284085906496007\n",
      "12000/49000 loss: 0.1808402295127986\n",
      "14000/49000 loss: 0.29330808105587436\n",
      "16000/49000 loss: 0.22178673736246246\n",
      "18000/49000 loss: 0.22748708258723566\n",
      "20000/49000 loss: 0.22964349398455802\n",
      "22000/49000 loss: 0.19219168670991127\n",
      "24000/49000 loss: 0.26192561264974945\n",
      "26000/49000 loss: 0.23374108452881284\n",
      "28000/49000 loss: 0.23984744518801668\n",
      "30000/49000 loss: 0.31870472038372144\n",
      "32000/49000 loss: 0.24659042006862447\n",
      "34000/49000 loss: 0.2530878620926696\n",
      "36000/49000 loss: 0.24303536027295683\n",
      "38000/49000 loss: 0.20103548171538108\n",
      "40000/49000 loss: 0.2705829571890866\n",
      "42000/49000 loss: 0.2116017156388679\n",
      "44000/49000 loss: 0.23947086650581453\n",
      "46000/49000 loss: 0.23276959732794117\n",
      "48000/49000 loss: 0.16427723664160526\n",
      "epoch 9: valid acc = 0.889, new learning rate = 0.0003151247048623045\n",
      "2000/49000 loss: 0.22023313101004047\n",
      "4000/49000 loss: 0.22402397419846856\n",
      "6000/49000 loss: 0.23615888000559507\n",
      "8000/49000 loss: 0.23581614584946411\n",
      "10000/49000 loss: 0.19402937921294422\n",
      "12000/49000 loss: 0.1983593840884314\n",
      "14000/49000 loss: 0.21982686872155457\n",
      "16000/49000 loss: 0.17852244523355087\n",
      "18000/49000 loss: 0.24950303653484543\n",
      "20000/49000 loss: 0.19716216602084305\n",
      "22000/49000 loss: 0.22232914136830761\n",
      "24000/49000 loss: 0.25687100924661715\n",
      "26000/49000 loss: 0.19198993804780937\n",
      "28000/49000 loss: 0.20151671000146112\n",
      "30000/49000 loss: 0.2382834011515313\n",
      "32000/49000 loss: 0.2197246629594812\n",
      "34000/49000 loss: 0.2027464042111966\n",
      "36000/49000 loss: 0.19422841411347358\n",
      "38000/49000 loss: 0.19190244180977667\n",
      "40000/49000 loss: 0.1922057880453278\n",
      "42000/49000 loss: 0.21106671583775666\n",
      "44000/49000 loss: 0.19005977885790365\n",
      "46000/49000 loss: 0.18187041283678468\n",
      "48000/49000 loss: 0.30010472018055356\n",
      "epoch 10: valid acc = 0.885, new learning rate = 0.00029936846961918924\n",
      "number of batches for training: 122\n",
      "4000/49000 loss: 0.2608680465635325\n",
      "8000/49000 loss: 0.2099993974366949\n",
      "12000/49000 loss: 0.2356339863523044\n",
      "16000/49000 loss: 0.25034306503628023\n",
      "20000/49000 loss: 0.2963674622848418\n",
      "24000/49000 loss: 0.200864289697909\n",
      "28000/49000 loss: 0.20317184814749506\n",
      "32000/49000 loss: 0.21373933882034352\n",
      "36000/49000 loss: 0.20354859314572304\n",
      "40000/49000 loss: 0.2274281893156135\n",
      "44000/49000 loss: 0.18188877793828984\n",
      "48000/49000 loss: 0.17870307596673696\n",
      "epoch 1: valid acc = 0.893, new learning rate = 0.000475\n",
      "4000/49000 loss: 0.24766952173376727\n",
      "8000/49000 loss: 0.33206124434083123\n",
      "12000/49000 loss: 0.27697676384905184\n",
      "16000/49000 loss: 0.19177298021178782\n",
      "20000/49000 loss: 0.2111496454864181\n",
      "24000/49000 loss: 0.2574911104582299\n",
      "28000/49000 loss: 0.18656264522740706\n",
      "32000/49000 loss: 0.19117150004371997\n",
      "36000/49000 loss: 0.21243213443744322\n",
      "40000/49000 loss: 0.23213555755845952\n",
      "44000/49000 loss: 0.21134031467815542\n",
      "48000/49000 loss: 0.23011317282371183\n",
      "epoch 2: valid acc = 0.895, new learning rate = 0.00045125\n",
      "4000/49000 loss: 0.20303262891289725\n",
      "8000/49000 loss: 0.22748839343017496\n",
      "12000/49000 loss: 0.16809105188213144\n",
      "16000/49000 loss: 0.1864827957306281\n",
      "20000/49000 loss: 0.20881218408138533\n",
      "24000/49000 loss: 0.21593280702098674\n",
      "28000/49000 loss: 0.19915763919145624\n",
      "32000/49000 loss: 0.2632865862778606\n",
      "36000/49000 loss: 0.22565125439497005\n",
      "40000/49000 loss: 0.15523900036037533\n",
      "44000/49000 loss: 0.2061365132062623\n",
      "48000/49000 loss: 0.23624460321056295\n",
      "epoch 3: valid acc = 0.889, new learning rate = 0.0004286875\n",
      "4000/49000 loss: 0.1849537611741127\n",
      "8000/49000 loss: 0.21801712813918211\n",
      "12000/49000 loss: 0.24185811334036922\n",
      "16000/49000 loss: 0.18335487586198132\n",
      "20000/49000 loss: 0.15650138737759361\n",
      "24000/49000 loss: 0.194344900056356\n",
      "28000/49000 loss: 0.16686789095807988\n",
      "32000/49000 loss: 0.2076381450530212\n",
      "36000/49000 loss: 0.20013151783376476\n",
      "40000/49000 loss: 0.23277351576488617\n",
      "44000/49000 loss: 0.22197935299513205\n",
      "48000/49000 loss: 0.2038079455368937\n",
      "epoch 4: valid acc = 0.887, new learning rate = 0.00040725312499999993\n",
      "4000/49000 loss: 0.19245302750556598\n",
      "8000/49000 loss: 0.20515520945305624\n",
      "12000/49000 loss: 0.24884547163326384\n",
      "16000/49000 loss: 0.19465626978314313\n",
      "20000/49000 loss: 0.21700215083896013\n",
      "24000/49000 loss: 0.21720891581664453\n",
      "28000/49000 loss: 0.2163435494291263\n",
      "32000/49000 loss: 0.21520145801952809\n",
      "36000/49000 loss: 0.1802600277293103\n",
      "40000/49000 loss: 0.19247871936159916\n",
      "44000/49000 loss: 0.23824549602191952\n",
      "48000/49000 loss: 0.23357151642257307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5: valid acc = 0.893, new learning rate = 0.0003868904687499999\n",
      "4000/49000 loss: 0.19448657790869636\n",
      "8000/49000 loss: 0.19977573366829313\n",
      "12000/49000 loss: 0.23617033681856234\n",
      "16000/49000 loss: 0.3163325698472879\n",
      "20000/49000 loss: 0.21228368108715903\n",
      "24000/49000 loss: 0.2236771726239648\n",
      "28000/49000 loss: 0.1897369862638834\n",
      "32000/49000 loss: 0.26314228255968114\n",
      "36000/49000 loss: 0.18146312535589454\n",
      "40000/49000 loss: 0.2864624626892887\n",
      "44000/49000 loss: 0.20031497945511098\n",
      "48000/49000 loss: 0.20694305325620854\n",
      "epoch 6: valid acc = 0.892, new learning rate = 0.0003675459453124999\n",
      "4000/49000 loss: 0.3005802965548855\n",
      "8000/49000 loss: 0.17310804765340387\n",
      "12000/49000 loss: 0.22043713600258047\n",
      "16000/49000 loss: 0.17462393563482242\n",
      "20000/49000 loss: 0.2638928993257687\n",
      "24000/49000 loss: 0.21772038894834297\n",
      "28000/49000 loss: 0.2253628095546645\n",
      "32000/49000 loss: 0.21029323597475852\n",
      "36000/49000 loss: 0.24315416024725875\n",
      "40000/49000 loss: 0.21349664496488846\n",
      "44000/49000 loss: 0.2122143138236121\n",
      "48000/49000 loss: 0.2034653389073612\n",
      "epoch 7: valid acc = 0.894, new learning rate = 0.00034916864804687486\n",
      "4000/49000 loss: 0.24656356182088413\n",
      "8000/49000 loss: 0.17141343592581912\n",
      "12000/49000 loss: 0.1918928699083722\n",
      "16000/49000 loss: 0.22096889838241623\n",
      "20000/49000 loss: 0.2138059608444907\n",
      "24000/49000 loss: 0.1676262798709066\n",
      "28000/49000 loss: 0.22559625672099756\n",
      "32000/49000 loss: 0.2076475394023034\n",
      "36000/49000 loss: 0.19602217615500644\n",
      "40000/49000 loss: 0.22100027536147346\n",
      "44000/49000 loss: 0.1995132728399253\n",
      "48000/49000 loss: 0.24386519919644414\n",
      "epoch 8: valid acc = 0.89, new learning rate = 0.0003317102156445311\n",
      "4000/49000 loss: 0.20152891051859056\n",
      "8000/49000 loss: 0.17801271539876468\n",
      "12000/49000 loss: 0.15929364411299266\n",
      "16000/49000 loss: 0.20018255044238048\n",
      "20000/49000 loss: 0.2192320118574764\n",
      "24000/49000 loss: 0.20331811022592505\n",
      "28000/49000 loss: 0.24722020994400687\n",
      "32000/49000 loss: 0.2070085926508743\n",
      "36000/49000 loss: 0.19811775636963377\n",
      "40000/49000 loss: 0.21369481093566364\n",
      "44000/49000 loss: 0.24180151841242584\n",
      "48000/49000 loss: 0.21484638075955315\n",
      "epoch 9: valid acc = 0.888, new learning rate = 0.0003151247048623045\n",
      "4000/49000 loss: 0.250908115088721\n",
      "8000/49000 loss: 0.20971877915444204\n",
      "12000/49000 loss: 0.19518156497731481\n",
      "16000/49000 loss: 0.18201107067118444\n",
      "20000/49000 loss: 0.19161098033185792\n",
      "24000/49000 loss: 0.19315560177370425\n",
      "28000/49000 loss: 0.23243927001016748\n",
      "32000/49000 loss: 0.21055443286716807\n",
      "36000/49000 loss: 0.21033926219117038\n",
      "40000/49000 loss: 0.24405866451495778\n",
      "44000/49000 loss: 0.17620216444954476\n",
      "48000/49000 loss: 0.26112720880545254\n",
      "epoch 10: valid acc = 0.888, new learning rate = 0.00029936846961918924\n",
      "number of batches for training: 122\n",
      "4000/49000 loss: 0.1740714133039626\n",
      "8000/49000 loss: 0.21619334789592326\n",
      "12000/49000 loss: 0.19911912708023552\n",
      "16000/49000 loss: 0.2341332483851966\n",
      "20000/49000 loss: 0.2028894726578248\n",
      "24000/49000 loss: 0.19815951453394906\n",
      "28000/49000 loss: 0.20433998962580163\n",
      "32000/49000 loss: 0.25443322751110026\n",
      "36000/49000 loss: 0.20937392856285278\n",
      "40000/49000 loss: 0.20221487812695402\n",
      "44000/49000 loss: 0.18502324713585025\n",
      "48000/49000 loss: 0.14702464717374605\n",
      "epoch 1: valid acc = 0.886, new learning rate = 0.000475\n",
      "4000/49000 loss: 0.16719926493312665\n",
      "8000/49000 loss: 0.24453478747459823\n",
      "12000/49000 loss: 0.21080426989006507\n",
      "16000/49000 loss: 0.24506796730516683\n",
      "20000/49000 loss: 0.20147632499697846\n",
      "24000/49000 loss: 0.14797041357044394\n",
      "28000/49000 loss: 0.1749545879481633\n",
      "32000/49000 loss: 0.2379137517150105\n",
      "36000/49000 loss: 0.18506672261645302\n",
      "40000/49000 loss: 0.18929079346726352\n",
      "44000/49000 loss: 0.25074564167040836\n",
      "48000/49000 loss: 0.19033520542298515\n",
      "epoch 2: valid acc = 0.889, new learning rate = 0.00045125\n",
      "4000/49000 loss: 0.22423906994631995\n",
      "8000/49000 loss: 0.190154133916024\n",
      "12000/49000 loss: 0.19520077555560106\n",
      "16000/49000 loss: 0.15944056117297867\n",
      "20000/49000 loss: 0.15689175678312386\n",
      "24000/49000 loss: 0.2022572450407578\n",
      "28000/49000 loss: 0.2052001961630504\n",
      "32000/49000 loss: 0.21867474675255114\n",
      "36000/49000 loss: 0.2136420258167904\n",
      "40000/49000 loss: 0.1858422107220575\n",
      "44000/49000 loss: 0.20119923043874482\n",
      "48000/49000 loss: 0.2114964639433437\n",
      "epoch 3: valid acc = 0.894, new learning rate = 0.0004286875\n",
      "4000/49000 loss: 0.2759073114625274\n",
      "8000/49000 loss: 0.15459766653229529\n",
      "12000/49000 loss: 0.18569881011511635\n",
      "16000/49000 loss: 0.18589345228088508\n",
      "20000/49000 loss: 0.20139573398984728\n",
      "24000/49000 loss: 0.20393372416623792\n",
      "28000/49000 loss: 0.20789845909318072\n",
      "32000/49000 loss: 0.25028010311270876\n",
      "36000/49000 loss: 0.1776607401751727\n",
      "40000/49000 loss: 0.21462303156652426\n",
      "44000/49000 loss: 0.2254201042025676\n",
      "48000/49000 loss: 0.2334946948547167\n",
      "epoch 4: valid acc = 0.89, new learning rate = 0.00040725312499999993\n",
      "4000/49000 loss: 0.22254378183009887\n",
      "8000/49000 loss: 0.20461609048611568\n",
      "12000/49000 loss: 0.15492534269015792\n",
      "16000/49000 loss: 0.23298262994687416\n",
      "20000/49000 loss: 0.2484448078528878\n",
      "24000/49000 loss: 0.19157445533515308\n",
      "28000/49000 loss: 0.18269536436775913\n",
      "32000/49000 loss: 0.24468432077832214\n",
      "36000/49000 loss: 0.1932044422513656\n",
      "40000/49000 loss: 0.15626659795494213\n",
      "44000/49000 loss: 0.1621942943405744\n",
      "48000/49000 loss: 0.2218618002813557\n",
      "epoch 5: valid acc = 0.893, new learning rate = 0.0003868904687499999\n",
      "4000/49000 loss: 0.20609387066864202\n",
      "8000/49000 loss: 0.18070696258638166\n",
      "12000/49000 loss: 0.20736849097894083\n",
      "16000/49000 loss: 0.1955774295569632\n",
      "20000/49000 loss: 0.17853872826939388\n",
      "24000/49000 loss: 0.1866452386249675\n",
      "28000/49000 loss: 0.1656242083561198\n",
      "32000/49000 loss: 0.23371711631526498\n",
      "36000/49000 loss: 0.17754703671519834\n",
      "40000/49000 loss: 0.1470650987036518\n",
      "44000/49000 loss: 0.1788047340620993\n",
      "48000/49000 loss: 0.18411924840297808\n",
      "epoch 6: valid acc = 0.891, new learning rate = 0.0003675459453124999\n",
      "4000/49000 loss: 0.2390708838831923\n",
      "8000/49000 loss: 0.14615372942235347\n",
      "12000/49000 loss: 0.22266653101497919\n",
      "16000/49000 loss: 0.19726081666327258\n",
      "20000/49000 loss: 0.18464945324606302\n",
      "24000/49000 loss: 0.17252610231507903\n",
      "28000/49000 loss: 0.21815266176767611\n",
      "32000/49000 loss: 0.20187450261521334\n",
      "36000/49000 loss: 0.18985843425863566\n",
      "40000/49000 loss: 0.1867882746086005\n",
      "44000/49000 loss: 0.1916170248749962\n",
      "48000/49000 loss: 0.1982872754272753\n",
      "epoch 7: valid acc = 0.892, new learning rate = 0.00034916864804687486\n",
      "4000/49000 loss: 0.2003302024962043\n",
      "8000/49000 loss: 0.17322231223104678\n",
      "12000/49000 loss: 0.2097753975476974\n",
      "16000/49000 loss: 0.21956651266845217\n",
      "20000/49000 loss: 0.18542995963798745\n",
      "24000/49000 loss: 0.20064896535677187\n",
      "28000/49000 loss: 0.18915376358810934\n",
      "32000/49000 loss: 0.20429038301906827\n",
      "36000/49000 loss: 0.15065475163218067\n",
      "40000/49000 loss: 0.176934143007121\n",
      "44000/49000 loss: 0.21978875739108275\n",
      "48000/49000 loss: 0.17603009119595892\n",
      "epoch 8: valid acc = 0.893, new learning rate = 0.0003317102156445311\n",
      "4000/49000 loss: 0.17473364050215784\n",
      "8000/49000 loss: 0.1689904106277515\n",
      "12000/49000 loss: 0.21429046835640672\n",
      "16000/49000 loss: 0.17426492974142066\n",
      "20000/49000 loss: 0.24671472678513892\n",
      "24000/49000 loss: 0.24068965285991814\n",
      "28000/49000 loss: 0.19734324823346064\n",
      "32000/49000 loss: 0.15840666043160975\n",
      "36000/49000 loss: 0.16538530211148\n",
      "40000/49000 loss: 0.16455757288447204\n",
      "44000/49000 loss: 0.2369849362509408\n",
      "48000/49000 loss: 0.18706478116734845\n",
      "epoch 9: valid acc = 0.893, new learning rate = 0.0003151247048623045\n",
      "4000/49000 loss: 0.15769650932832374\n",
      "8000/49000 loss: 0.16995273207636608\n",
      "12000/49000 loss: 0.1877268678237004\n",
      "16000/49000 loss: 0.16973852034821416\n",
      "20000/49000 loss: 0.20506818500523352\n",
      "24000/49000 loss: 0.2230973175342609\n",
      "28000/49000 loss: 0.1446133167453181\n",
      "32000/49000 loss: 0.17246304348048577\n",
      "36000/49000 loss: 0.21162538480953072\n",
      "40000/49000 loss: 0.23120097000714743\n",
      "44000/49000 loss: 0.22405863991031244\n",
      "48000/49000 loss: 0.23731185671513141\n",
      "epoch 10: valid acc = 0.893, new learning rate = 0.00029936846961918924\n",
      "number of batches for training: 122\n",
      "4000/49000 loss: 0.17941749617002956\n",
      "8000/49000 loss: 0.19914487773677925\n",
      "12000/49000 loss: 0.16985370806269975\n",
      "16000/49000 loss: 0.15626159344750323\n",
      "20000/49000 loss: 0.23417163278860612\n",
      "24000/49000 loss: 0.16174213281196484\n",
      "28000/49000 loss: 0.2166308955552644\n",
      "32000/49000 loss: 0.1584674955550415\n",
      "36000/49000 loss: 0.18804904934144168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/49000 loss: 0.18917447892850078\n",
      "44000/49000 loss: 0.1697124815243356\n",
      "48000/49000 loss: 0.1735886494455414\n",
      "epoch 1: valid acc = 0.882, new learning rate = 0.000475\n",
      "4000/49000 loss: 0.21846269414712435\n",
      "8000/49000 loss: 0.23987540136543561\n",
      "12000/49000 loss: 0.10683887677370715\n",
      "16000/49000 loss: 0.1818286359299835\n",
      "20000/49000 loss: 0.18619549343264172\n",
      "24000/49000 loss: 0.19351144831419378\n",
      "28000/49000 loss: 0.2040106371881493\n",
      "32000/49000 loss: 0.23338466385478546\n",
      "36000/49000 loss: 0.2608787757866974\n",
      "40000/49000 loss: 0.1834654172919124\n",
      "44000/49000 loss: 0.18478005318871113\n",
      "48000/49000 loss: 0.1994572011068374\n",
      "epoch 2: valid acc = 0.889, new learning rate = 0.00045125\n",
      "4000/49000 loss: 0.19450247123741407\n",
      "8000/49000 loss: 0.17992093799431447\n",
      "12000/49000 loss: 0.17281677942764018\n",
      "16000/49000 loss: 0.1921006308649114\n",
      "20000/49000 loss: 0.1602013767164621\n",
      "24000/49000 loss: 0.18196745802350967\n",
      "28000/49000 loss: 0.1875220540112789\n",
      "32000/49000 loss: 0.227900840181938\n",
      "36000/49000 loss: 0.1605563289194604\n",
      "40000/49000 loss: 0.206982413437342\n",
      "44000/49000 loss: 0.1828000511409318\n",
      "48000/49000 loss: 0.15551355917147894\n",
      "epoch 3: valid acc = 0.896, new learning rate = 0.0004286875\n",
      "4000/49000 loss: 0.2108702024991754\n",
      "8000/49000 loss: 0.23664669232835625\n",
      "12000/49000 loss: 0.16899991256889227\n",
      "16000/49000 loss: 0.23944184611493852\n",
      "20000/49000 loss: 0.2133070407205974\n",
      "24000/49000 loss: 0.20703482438291654\n",
      "28000/49000 loss: 0.21442501061185895\n",
      "32000/49000 loss: 0.17797952121648072\n",
      "36000/49000 loss: 0.25330725673478566\n",
      "40000/49000 loss: 0.1951679033033704\n",
      "44000/49000 loss: 0.21398838611276644\n",
      "48000/49000 loss: 0.1763817003932154\n",
      "epoch 4: valid acc = 0.893, new learning rate = 0.00040725312499999993\n",
      "4000/49000 loss: 0.1686056469956776\n",
      "8000/49000 loss: 0.1805474568180763\n",
      "12000/49000 loss: 0.146858876795919\n",
      "16000/49000 loss: 0.19425437532801343\n",
      "20000/49000 loss: 0.14701204046204602\n",
      "24000/49000 loss: 0.17848387092068313\n",
      "28000/49000 loss: 0.21144911180512121\n",
      "32000/49000 loss: 0.2025596610429644\n",
      "36000/49000 loss: 0.19364223068922654\n",
      "40000/49000 loss: 0.1943150123967556\n",
      "44000/49000 loss: 0.18554044544307002\n",
      "48000/49000 loss: 0.17987744113365667\n",
      "epoch 5: valid acc = 0.888, new learning rate = 0.0003868904687499999\n",
      "4000/49000 loss: 0.2115415611834334\n",
      "8000/49000 loss: 0.1493537861317128\n",
      "12000/49000 loss: 0.1903085063793967\n",
      "16000/49000 loss: 0.1816633157091024\n",
      "20000/49000 loss: 0.1686597884700531\n",
      "24000/49000 loss: 0.15817124797555485\n",
      "28000/49000 loss: 0.1612469798461638\n",
      "32000/49000 loss: 0.2304784904035895\n",
      "36000/49000 loss: 0.18054018345582223\n",
      "40000/49000 loss: 0.15973370984236113\n",
      "44000/49000 loss: 0.19215167352966217\n",
      "48000/49000 loss: 0.17597744004625857\n",
      "epoch 6: valid acc = 0.892, new learning rate = 0.0003675459453124999\n",
      "4000/49000 loss: 0.1929113955890623\n",
      "8000/49000 loss: 0.1985298366432302\n",
      "12000/49000 loss: 0.1467904843195594\n",
      "16000/49000 loss: 0.16580854746221826\n",
      "20000/49000 loss: 0.15231224626085266\n",
      "24000/49000 loss: 0.23068491594248194\n",
      "28000/49000 loss: 0.20035222868562658\n",
      "32000/49000 loss: 0.18601731709025987\n",
      "36000/49000 loss: 0.19272355571769625\n",
      "40000/49000 loss: 0.1634441135519458\n",
      "44000/49000 loss: 0.16697581613773388\n",
      "48000/49000 loss: 0.18723802069547751\n",
      "epoch 7: valid acc = 0.896, new learning rate = 0.00034916864804687486\n",
      "4000/49000 loss: 0.16361302500167027\n",
      "8000/49000 loss: 0.19742777946777487\n",
      "12000/49000 loss: 0.18237807364495054\n",
      "16000/49000 loss: 0.13504267570921083\n",
      "20000/49000 loss: 0.1807902123880735\n",
      "24000/49000 loss: 0.18193447452080544\n",
      "28000/49000 loss: 0.22273040066359545\n",
      "32000/49000 loss: 0.18074198265040706\n",
      "36000/49000 loss: 0.16478838346801628\n",
      "40000/49000 loss: 0.1673711212587649\n",
      "44000/49000 loss: 0.19162384088016587\n",
      "48000/49000 loss: 0.17536048928746895\n",
      "epoch 8: valid acc = 0.891, new learning rate = 0.0003317102156445311\n",
      "4000/49000 loss: 0.19792996141247016\n",
      "8000/49000 loss: 0.18534000614908702\n",
      "12000/49000 loss: 0.18822101108094658\n",
      "16000/49000 loss: 0.1702542815408612\n",
      "20000/49000 loss: 0.1712494415972279\n",
      "24000/49000 loss: 0.236270506097744\n",
      "28000/49000 loss: 0.16704711845845774\n",
      "32000/49000 loss: 0.20011102316239357\n",
      "36000/49000 loss: 0.15269259827360007\n",
      "40000/49000 loss: 0.2120593036987636\n",
      "44000/49000 loss: 0.1408277038739716\n",
      "48000/49000 loss: 0.1953452409791688\n",
      "epoch 9: valid acc = 0.887, new learning rate = 0.0003151247048623045\n",
      "4000/49000 loss: 0.16691085342212675\n",
      "8000/49000 loss: 0.14701510123309083\n",
      "12000/49000 loss: 0.1540251058885467\n",
      "16000/49000 loss: 0.12451853739950099\n",
      "20000/49000 loss: 0.1940514165857725\n",
      "24000/49000 loss: 0.15366027206539135\n",
      "28000/49000 loss: 0.15448126615794242\n",
      "32000/49000 loss: 0.1795668905782267\n",
      "36000/49000 loss: 0.15801358781399433\n",
      "40000/49000 loss: 0.18682179489630976\n",
      "44000/49000 loss: 0.1970132493444087\n",
      "48000/49000 loss: 0.1542172341595408\n",
      "epoch 10: valid acc = 0.895, new learning rate = 0.00029936846961918924\n",
      "number of batches for training: 122\n",
      "4000/49000 loss: 0.18035631842251137\n",
      "8000/49000 loss: 0.18973188940487326\n",
      "12000/49000 loss: 0.20375476875410237\n",
      "16000/49000 loss: 0.18316950761096917\n",
      "20000/49000 loss: 0.16198593569410447\n",
      "24000/49000 loss: 0.24646982937657266\n",
      "28000/49000 loss: 0.20845546112377802\n",
      "32000/49000 loss: 0.1717422176736124\n",
      "36000/49000 loss: 0.1796403914088398\n",
      "40000/49000 loss: 0.18851868818554798\n",
      "44000/49000 loss: 0.16718446777538198\n",
      "48000/49000 loss: 0.20222034842617112\n",
      "epoch 1: valid acc = 0.89, new learning rate = 0.000475\n",
      "4000/49000 loss: 0.22752495691984903\n",
      "8000/49000 loss: 0.17204456174056246\n",
      "12000/49000 loss: 0.14007617522910312\n",
      "16000/49000 loss: 0.18514183783087426\n",
      "20000/49000 loss: 0.1791687604007921\n",
      "24000/49000 loss: 0.14895671813521635\n",
      "28000/49000 loss: 0.2258523838576524\n",
      "32000/49000 loss: 0.26686690666535146\n",
      "36000/49000 loss: 0.20776313787673625\n",
      "40000/49000 loss: 0.16813261564611245\n",
      "44000/49000 loss: 0.19272836051700923\n",
      "48000/49000 loss: 0.1693342869628215\n",
      "epoch 2: valid acc = 0.89, new learning rate = 0.00045125\n",
      "4000/49000 loss: 0.17162953985346932\n",
      "8000/49000 loss: 0.1801655163082669\n",
      "12000/49000 loss: 0.14478376366898352\n",
      "16000/49000 loss: 0.1682170041881308\n",
      "20000/49000 loss: 0.20480297286629381\n",
      "24000/49000 loss: 0.15926470782774438\n",
      "28000/49000 loss: 0.170158038125549\n",
      "32000/49000 loss: 0.18616821388436136\n",
      "36000/49000 loss: 0.1634319780698078\n",
      "40000/49000 loss: 0.15856288859019466\n",
      "44000/49000 loss: 0.20519295801326695\n",
      "48000/49000 loss: 0.15912935239163745\n",
      "epoch 3: valid acc = 0.89, new learning rate = 0.0004286875\n",
      "4000/49000 loss: 0.16031295979554308\n",
      "8000/49000 loss: 0.1394490062230067\n",
      "12000/49000 loss: 0.19494744991345903\n",
      "16000/49000 loss: 0.16694724648760056\n",
      "20000/49000 loss: 0.17553963463991307\n",
      "24000/49000 loss: 0.16262843009016256\n",
      "28000/49000 loss: 0.15643329169901446\n",
      "32000/49000 loss: 0.1556166505258476\n",
      "36000/49000 loss: 0.24133338947545677\n",
      "40000/49000 loss: 0.21718512396616055\n",
      "44000/49000 loss: 0.12734315618675845\n",
      "48000/49000 loss: 0.19707724859817066\n",
      "epoch 4: valid acc = 0.891, new learning rate = 0.00040725312499999993\n",
      "4000/49000 loss: 0.17925066477671808\n",
      "8000/49000 loss: 0.18299247483049133\n",
      "12000/49000 loss: 0.2037919189357109\n",
      "16000/49000 loss: 0.16969364830985312\n",
      "20000/49000 loss: 0.14257093491349004\n",
      "24000/49000 loss: 0.1588844077192022\n",
      "28000/49000 loss: 0.1789269892503215\n",
      "32000/49000 loss: 0.16157634967392837\n",
      "36000/49000 loss: 0.16232593491997938\n",
      "40000/49000 loss: 0.18416538046916014\n",
      "44000/49000 loss: 0.1727168195752151\n",
      "48000/49000 loss: 0.19184302637351314\n",
      "epoch 5: valid acc = 0.888, new learning rate = 0.0003868904687499999\n",
      "4000/49000 loss: 0.16713065418563797\n",
      "8000/49000 loss: 0.1806643820725932\n",
      "12000/49000 loss: 0.1541239087481313\n",
      "16000/49000 loss: 0.21609549767630762\n",
      "20000/49000 loss: 0.21115229098330315\n",
      "24000/49000 loss: 0.188823724399876\n",
      "28000/49000 loss: 0.17450571274389534\n",
      "32000/49000 loss: 0.16473055419328467\n",
      "36000/49000 loss: 0.16899169829755895\n",
      "40000/49000 loss: 0.18108496882989314\n",
      "44000/49000 loss: 0.16416376191496695\n",
      "48000/49000 loss: 0.15892750253500346\n",
      "epoch 6: valid acc = 0.896, new learning rate = 0.0003675459453124999\n",
      "4000/49000 loss: 0.16986650375648907\n",
      "8000/49000 loss: 0.1597335462994907\n",
      "12000/49000 loss: 0.2525454868992058\n",
      "16000/49000 loss: 0.1507995828611127\n",
      "20000/49000 loss: 0.1493263784827777\n",
      "24000/49000 loss: 0.1791293073021873\n",
      "28000/49000 loss: 0.15061275797059243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000/49000 loss: 0.17141424914065734\n",
      "36000/49000 loss: 0.16529824577968574\n",
      "40000/49000 loss: 0.1445771439652955\n",
      "44000/49000 loss: 0.16984853285170093\n",
      "48000/49000 loss: 0.173121630051207\n",
      "epoch 7: valid acc = 0.892, new learning rate = 0.00034916864804687486\n",
      "4000/49000 loss: 0.22070848942573107\n",
      "8000/49000 loss: 0.1288529218887462\n",
      "12000/49000 loss: 0.13349641444463872\n",
      "16000/49000 loss: 0.1334267811531169\n",
      "20000/49000 loss: 0.1935209541569457\n",
      "24000/49000 loss: 0.19089000506792544\n",
      "28000/49000 loss: 0.18232388916045222\n",
      "32000/49000 loss: 0.15556683942035887\n",
      "36000/49000 loss: 0.1531578378226372\n",
      "40000/49000 loss: 0.17744461753662344\n",
      "44000/49000 loss: 0.18253339525982434\n",
      "48000/49000 loss: 0.15201566523290408\n",
      "epoch 8: valid acc = 0.892, new learning rate = 0.0003317102156445311\n",
      "4000/49000 loss: 0.13495835263466177\n",
      "8000/49000 loss: 0.16483206590185587\n",
      "12000/49000 loss: 0.17208736528736113\n",
      "16000/49000 loss: 0.18337935633288063\n",
      "20000/49000 loss: 0.16493014752071813\n",
      "24000/49000 loss: 0.1432358366230915\n",
      "28000/49000 loss: 0.1558999901369031\n",
      "32000/49000 loss: 0.15908518128975263\n",
      "36000/49000 loss: 0.2160739255673148\n",
      "40000/49000 loss: 0.16739752124337184\n",
      "44000/49000 loss: 0.14763306509331797\n",
      "48000/49000 loss: 0.15580351780240934\n",
      "epoch 9: valid acc = 0.891, new learning rate = 0.0003151247048623045\n",
      "4000/49000 loss: 0.18955920357918515\n",
      "8000/49000 loss: 0.1456197609465404\n",
      "12000/49000 loss: 0.16654810011736984\n",
      "16000/49000 loss: 0.15346736570924133\n",
      "20000/49000 loss: 0.18501256518672465\n",
      "24000/49000 loss: 0.15747953670889234\n",
      "28000/49000 loss: 0.16452278833144723\n",
      "32000/49000 loss: 0.17921280863110975\n",
      "36000/49000 loss: 0.1467775782779824\n",
      "40000/49000 loss: 0.15537002946110082\n",
      "44000/49000 loss: 0.13897975037461946\n",
      "48000/49000 loss: 0.15575734228763755\n",
      "epoch 10: valid acc = 0.892, new learning rate = 0.00029936846961918924\n",
      "number of batches for training: 81\n",
      "6000/49000 loss: 0.1377174060543342\n",
      "12000/49000 loss: 0.22856391548097638\n",
      "18000/49000 loss: 0.13893032974860578\n",
      "24000/49000 loss: 0.16657592754931855\n",
      "30000/49000 loss: 0.17401702240743505\n",
      "36000/49000 loss: 0.16171362611584408\n",
      "42000/49000 loss: 0.16743025957091884\n",
      "48000/49000 loss: 0.15014619212975044\n",
      "epoch 1: valid acc = 0.888, new learning rate = 0.000475\n",
      "6000/49000 loss: 0.18312899431818605\n",
      "12000/49000 loss: 0.15662411195922965\n",
      "18000/49000 loss: 0.18951249900785966\n",
      "24000/49000 loss: 0.18387398690404133\n",
      "30000/49000 loss: 0.1817469632615024\n",
      "36000/49000 loss: 0.14501046720213032\n",
      "42000/49000 loss: 0.1584250904572429\n",
      "48000/49000 loss: 0.11754167074714367\n",
      "epoch 2: valid acc = 0.885, new learning rate = 0.00045125\n",
      "6000/49000 loss: 0.14827869117781808\n",
      "12000/49000 loss: 0.15802169629978144\n",
      "18000/49000 loss: 0.16831534834263565\n",
      "24000/49000 loss: 0.185568567012041\n",
      "30000/49000 loss: 0.1893511554986509\n",
      "36000/49000 loss: 0.17355184935545556\n",
      "42000/49000 loss: 0.1497544065830662\n",
      "48000/49000 loss: 0.15901669892907067\n",
      "epoch 3: valid acc = 0.885, new learning rate = 0.0004286875\n",
      "6000/49000 loss: 0.16365073095933821\n",
      "12000/49000 loss: 0.1560483717913011\n",
      "18000/49000 loss: 0.16253810880559297\n",
      "24000/49000 loss: 0.18039077313711155\n",
      "30000/49000 loss: 0.15712905637780103\n",
      "36000/49000 loss: 0.2053661502500829\n",
      "42000/49000 loss: 0.19437371476453266\n",
      "48000/49000 loss: 0.15735080849837718\n",
      "epoch 4: valid acc = 0.888, new learning rate = 0.00040725312499999993\n",
      "6000/49000 loss: 0.1712520484159931\n",
      "12000/49000 loss: 0.17377168756701536\n",
      "18000/49000 loss: 0.13822911015364817\n",
      "24000/49000 loss: 0.14148292936003795\n",
      "30000/49000 loss: 0.18508115661082578\n",
      "36000/49000 loss: 0.1786024455055091\n",
      "42000/49000 loss: 0.15629005840372534\n",
      "48000/49000 loss: 0.16543052630584582\n",
      "epoch 5: valid acc = 0.888, new learning rate = 0.0003868904687499999\n",
      "6000/49000 loss: 0.17558034271804338\n",
      "12000/49000 loss: 0.15941670955068205\n",
      "18000/49000 loss: 0.15409983998375365\n",
      "24000/49000 loss: 0.15255262088127908\n",
      "30000/49000 loss: 0.15627140254968563\n",
      "36000/49000 loss: 0.17426752034716692\n",
      "42000/49000 loss: 0.1716581618423204\n",
      "48000/49000 loss: 0.16397592261278854\n",
      "epoch 6: valid acc = 0.882, new learning rate = 0.0003675459453124999\n",
      "6000/49000 loss: 0.19395300623407452\n",
      "12000/49000 loss: 0.1915491305830001\n",
      "18000/49000 loss: 0.15279227056512257\n",
      "24000/49000 loss: 0.1789454573632944\n",
      "30000/49000 loss: 0.15886763514608387\n",
      "36000/49000 loss: 0.14007023206331537\n",
      "42000/49000 loss: 0.16642853040268918\n",
      "48000/49000 loss: 0.1342246396151319\n",
      "epoch 7: valid acc = 0.887, new learning rate = 0.00034916864804687486\n",
      "6000/49000 loss: 0.19660670559995796\n",
      "12000/49000 loss: 0.1546434801850069\n",
      "18000/49000 loss: 0.14213507502348327\n",
      "24000/49000 loss: 0.16565029262130693\n",
      "30000/49000 loss: 0.1629181839296833\n",
      "36000/49000 loss: 0.14606926549133897\n",
      "42000/49000 loss: 0.15273507966127137\n",
      "48000/49000 loss: 0.15992674422375597\n",
      "epoch 8: valid acc = 0.891, new learning rate = 0.0003317102156445311\n",
      "6000/49000 loss: 0.17336016850952768\n",
      "12000/49000 loss: 0.13916176728338175\n",
      "18000/49000 loss: 0.15059300825060137\n",
      "24000/49000 loss: 0.15297445172660556\n",
      "30000/49000 loss: 0.14539517895514714\n",
      "36000/49000 loss: 0.13345960776461374\n",
      "42000/49000 loss: 0.18685181714911656\n",
      "48000/49000 loss: 0.14881233047142056\n",
      "epoch 9: valid acc = 0.888, new learning rate = 0.0003151247048623045\n",
      "6000/49000 loss: 0.1762530735041705\n",
      "12000/49000 loss: 0.1805694943573198\n",
      "18000/49000 loss: 0.16210859881072578\n",
      "24000/49000 loss: 0.16349667723759537\n",
      "30000/49000 loss: 0.20586840614974744\n",
      "36000/49000 loss: 0.17101248411075395\n",
      "42000/49000 loss: 0.15089960769778113\n",
      "48000/49000 loss: 0.16209582296002711\n",
      "epoch 10: valid acc = 0.888, new learning rate = 0.00029936846961918924\n",
      "number of batches for training: 81\n",
      "6000/49000 loss: 0.13550637951325045\n",
      "12000/49000 loss: 0.19379085371455212\n",
      "18000/49000 loss: 0.15803346125772336\n",
      "24000/49000 loss: 0.1788984253309797\n",
      "30000/49000 loss: 0.13521369825489493\n",
      "36000/49000 loss: 0.13643225152928293\n",
      "42000/49000 loss: 0.14912639372797437\n",
      "48000/49000 loss: 0.15079996915061772\n",
      "epoch 1: valid acc = 0.894, new learning rate = 0.000475\n",
      "6000/49000 loss: 0.16752564502814168\n",
      "12000/49000 loss: 0.1463437954064157\n",
      "18000/49000 loss: 0.21420368306633572\n",
      "24000/49000 loss: 0.1540442753190888\n",
      "30000/49000 loss: 0.1961462029433583\n",
      "36000/49000 loss: 0.1791026803134712\n",
      "42000/49000 loss: 0.13909944193822252\n",
      "48000/49000 loss: 0.18002393203769115\n",
      "epoch 2: valid acc = 0.887, new learning rate = 0.00045125\n",
      "6000/49000 loss: 0.15949753794499585\n",
      "12000/49000 loss: 0.15462961607587622\n",
      "18000/49000 loss: 0.16705870109757598\n",
      "24000/49000 loss: 0.15723951566320016\n",
      "30000/49000 loss: 0.17067689644433118\n",
      "36000/49000 loss: 0.15931001249068003\n",
      "42000/49000 loss: 0.16750547140761257\n",
      "48000/49000 loss: 0.14087950978495845\n",
      "epoch 3: valid acc = 0.887, new learning rate = 0.0004286875\n",
      "6000/49000 loss: 0.1423436051509186\n",
      "12000/49000 loss: 0.16674765379878193\n",
      "18000/49000 loss: 0.15990348449592104\n",
      "24000/49000 loss: 0.1508309026122029\n",
      "30000/49000 loss: 0.15040081000017338\n",
      "36000/49000 loss: 0.15441270207058597\n",
      "42000/49000 loss: 0.17989077234214765\n",
      "48000/49000 loss: 0.13583643512931973\n",
      "epoch 4: valid acc = 0.885, new learning rate = 0.00040725312499999993\n",
      "6000/49000 loss: 0.12516924389212666\n",
      "12000/49000 loss: 0.1609522828937427\n",
      "18000/49000 loss: 0.17388493494322113\n",
      "24000/49000 loss: 0.17247831360941446\n",
      "30000/49000 loss: 0.15621986201155294\n",
      "36000/49000 loss: 0.16277653329412095\n",
      "42000/49000 loss: 0.1371613405084078\n",
      "48000/49000 loss: 0.15825601088535016\n",
      "epoch 5: valid acc = 0.89, new learning rate = 0.0003868904687499999\n",
      "6000/49000 loss: 0.14481452260876052\n",
      "12000/49000 loss: 0.16558163745561227\n",
      "18000/49000 loss: 0.14150861298858583\n",
      "24000/49000 loss: 0.1309069143245606\n",
      "30000/49000 loss: 0.17369032785759067\n",
      "36000/49000 loss: 0.15628396628352775\n",
      "42000/49000 loss: 0.12822315888149516\n",
      "48000/49000 loss: 0.15717121564628006\n",
      "epoch 6: valid acc = 0.894, new learning rate = 0.0003675459453124999\n",
      "6000/49000 loss: 0.1612609891552729\n",
      "12000/49000 loss: 0.17536686283061376\n",
      "18000/49000 loss: 0.14675927791619323\n",
      "24000/49000 loss: 0.13840982426384543\n",
      "30000/49000 loss: 0.1836280452540276\n",
      "36000/49000 loss: 0.1686948329081895\n",
      "42000/49000 loss: 0.16052747491696684\n",
      "48000/49000 loss: 0.15115459053715258\n",
      "epoch 7: valid acc = 0.892, new learning rate = 0.00034916864804687486\n",
      "6000/49000 loss: 0.18541786993465725\n",
      "12000/49000 loss: 0.12565451538598402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/49000 loss: 0.15593576401153733\n",
      "24000/49000 loss: 0.154697687082877\n",
      "30000/49000 loss: 0.1623126211329307\n",
      "36000/49000 loss: 0.1466605822171142\n",
      "42000/49000 loss: 0.18654676609406776\n",
      "48000/49000 loss: 0.11588824913894252\n",
      "epoch 8: valid acc = 0.89, new learning rate = 0.0003317102156445311\n",
      "6000/49000 loss: 0.12855829870122507\n",
      "12000/49000 loss: 0.1737949590580937\n",
      "18000/49000 loss: 0.16325400748349683\n",
      "24000/49000 loss: 0.14879705375209548\n",
      "30000/49000 loss: 0.15424907598637005\n",
      "36000/49000 loss: 0.1348152666603772\n",
      "42000/49000 loss: 0.1534185916936321\n",
      "48000/49000 loss: 0.16204843371483058\n",
      "epoch 9: valid acc = 0.893, new learning rate = 0.0003151247048623045\n",
      "6000/49000 loss: 0.15779403633063813\n",
      "12000/49000 loss: 0.17470185103326083\n",
      "18000/49000 loss: 0.17405049853923424\n",
      "24000/49000 loss: 0.13245676890681954\n",
      "30000/49000 loss: 0.1638003146044851\n",
      "36000/49000 loss: 0.14581427784910414\n",
      "42000/49000 loss: 0.16163893583379038\n",
      "48000/49000 loss: 0.14027977423721946\n",
      "epoch 10: valid acc = 0.889, new learning rate = 0.00029936846961918924\n",
      "number of batches for training: 81\n",
      "6000/49000 loss: 0.14282359785602883\n",
      "12000/49000 loss: 0.1616000315464151\n",
      "18000/49000 loss: 0.15042405063312453\n",
      "24000/49000 loss: 0.16926113275268262\n",
      "30000/49000 loss: 0.18612141608360128\n",
      "36000/49000 loss: 0.14618312582802787\n",
      "42000/49000 loss: 0.12139894000345\n",
      "48000/49000 loss: 0.1796705080732326\n",
      "epoch 1: valid acc = 0.887, new learning rate = 0.000475\n",
      "6000/49000 loss: 0.1779222319964645\n",
      "12000/49000 loss: 0.12192581261205238\n",
      "18000/49000 loss: 0.144884456148043\n",
      "24000/49000 loss: 0.13444548156704078\n",
      "30000/49000 loss: 0.18417719696724888\n",
      "36000/49000 loss: 0.1470977366635859\n",
      "42000/49000 loss: 0.1664904944118804\n",
      "48000/49000 loss: 0.14951373396961876\n",
      "epoch 2: valid acc = 0.891, new learning rate = 0.00045125\n",
      "6000/49000 loss: 0.13923613812866678\n",
      "12000/49000 loss: 0.17201189628489874\n",
      "18000/49000 loss: 0.13303156455773774\n",
      "24000/49000 loss: 0.12631845041187503\n",
      "30000/49000 loss: 0.13369352945949578\n",
      "36000/49000 loss: 0.17789204253255775\n",
      "42000/49000 loss: 0.14022984860424056\n",
      "48000/49000 loss: 0.14666555997480937\n",
      "epoch 3: valid acc = 0.888, new learning rate = 0.0004286875\n",
      "6000/49000 loss: 0.16558279419403868\n",
      "12000/49000 loss: 0.15710939736382631\n",
      "18000/49000 loss: 0.1515038712194294\n",
      "24000/49000 loss: 0.16489062835184973\n",
      "30000/49000 loss: 0.14373533301256503\n",
      "36000/49000 loss: 0.14142865787422262\n",
      "42000/49000 loss: 0.17836870134328242\n",
      "48000/49000 loss: 0.14383239208246093\n",
      "epoch 4: valid acc = 0.891, new learning rate = 0.00040725312499999993\n",
      "6000/49000 loss: 0.12813072169580467\n",
      "12000/49000 loss: 0.17739236214011783\n",
      "18000/49000 loss: 0.15779308114628818\n",
      "24000/49000 loss: 0.1399397608930978\n",
      "30000/49000 loss: 0.14383381788422034\n",
      "36000/49000 loss: 0.13415503824689565\n",
      "42000/49000 loss: 0.15630628210662265\n",
      "48000/49000 loss: 0.134224235300642\n",
      "epoch 5: valid acc = 0.892, new learning rate = 0.0003868904687499999\n",
      "6000/49000 loss: 0.16810057340821782\n",
      "12000/49000 loss: 0.1362238755248943\n",
      "18000/49000 loss: 0.14760452532624785\n",
      "24000/49000 loss: 0.12255243683032423\n",
      "30000/49000 loss: 0.15436698790377712\n",
      "36000/49000 loss: 0.15904849552309272\n",
      "42000/49000 loss: 0.11305056689333802\n",
      "48000/49000 loss: 0.12133395087820238\n",
      "epoch 6: valid acc = 0.884, new learning rate = 0.0003675459453124999\n",
      "6000/49000 loss: 0.17077767964042284\n",
      "12000/49000 loss: 0.1449987716704571\n",
      "18000/49000 loss: 0.15217906943323298\n",
      "24000/49000 loss: 0.1115680707708699\n",
      "30000/49000 loss: 0.1586056425159726\n",
      "36000/49000 loss: 0.1728081621479279\n",
      "42000/49000 loss: 0.14295373575648324\n",
      "48000/49000 loss: 0.12846057944946224\n",
      "epoch 7: valid acc = 0.888, new learning rate = 0.00034916864804687486\n",
      "6000/49000 loss: 0.16396735208085453\n",
      "12000/49000 loss: 0.11181584313647379\n",
      "18000/49000 loss: 0.1513751334764539\n",
      "24000/49000 loss: 0.1470307005600767\n",
      "30000/49000 loss: 0.13793548412569123\n",
      "36000/49000 loss: 0.13947988628510072\n",
      "42000/49000 loss: 0.1429691681114197\n",
      "48000/49000 loss: 0.1416634375945891\n",
      "epoch 8: valid acc = 0.888, new learning rate = 0.0003317102156445311\n",
      "6000/49000 loss: 0.11354865585077288\n",
      "12000/49000 loss: 0.1589420561388522\n",
      "18000/49000 loss: 0.1385553465550047\n",
      "24000/49000 loss: 0.13628115147477887\n",
      "30000/49000 loss: 0.1551337621323108\n",
      "36000/49000 loss: 0.13972396777999777\n",
      "42000/49000 loss: 0.15516120449093984\n",
      "48000/49000 loss: 0.13560926615634825\n",
      "epoch 9: valid acc = 0.886, new learning rate = 0.0003151247048623045\n",
      "6000/49000 loss: 0.15252901359158802\n",
      "12000/49000 loss: 0.13571789238792012\n",
      "18000/49000 loss: 0.12904253593352105\n",
      "24000/49000 loss: 0.1266109781666903\n",
      "30000/49000 loss: 0.12955703769032983\n",
      "36000/49000 loss: 0.1568765803040523\n",
      "42000/49000 loss: 0.14398493020952696\n",
      "48000/49000 loss: 0.11830879659872419\n",
      "epoch 10: valid acc = 0.891, new learning rate = 0.00029936846961918924\n",
      "number of batches for training: 81\n",
      "6000/49000 loss: 0.1484602159972362\n",
      "12000/49000 loss: 0.14930959160133495\n",
      "18000/49000 loss: 0.14566163470089158\n",
      "24000/49000 loss: 0.1901649182007365\n",
      "30000/49000 loss: 0.12463503397179711\n",
      "36000/49000 loss: 0.13086203819604403\n",
      "42000/49000 loss: 0.1670382287736452\n",
      "48000/49000 loss: 0.15745179655459238\n",
      "epoch 1: valid acc = 0.894, new learning rate = 0.000475\n",
      "6000/49000 loss: 0.1553004767291594\n",
      "12000/49000 loss: 0.14223388251465435\n",
      "18000/49000 loss: 0.13713952417874714\n",
      "24000/49000 loss: 0.1363061888153445\n",
      "30000/49000 loss: 0.1442916765765384\n",
      "36000/49000 loss: 0.15756877573559927\n",
      "42000/49000 loss: 0.1439561376480145\n",
      "48000/49000 loss: 0.12941569887860396\n",
      "epoch 2: valid acc = 0.891, new learning rate = 0.00045125\n",
      "6000/49000 loss: 0.14462530502306137\n",
      "12000/49000 loss: 0.12381451948358604\n",
      "18000/49000 loss: 0.1509487842358522\n",
      "24000/49000 loss: 0.17491098794204166\n",
      "30000/49000 loss: 0.13529033480233987\n",
      "36000/49000 loss: 0.15184464911613132\n",
      "42000/49000 loss: 0.12665207070881082\n",
      "48000/49000 loss: 0.1356224180531115\n",
      "epoch 3: valid acc = 0.889, new learning rate = 0.0004286875\n",
      "6000/49000 loss: 0.1599392680942021\n",
      "12000/49000 loss: 0.14344629112675503\n",
      "18000/49000 loss: 0.1980542907348582\n",
      "24000/49000 loss: 0.14710407311888354\n",
      "30000/49000 loss: 0.14162171694712314\n",
      "36000/49000 loss: 0.1515630813421713\n",
      "42000/49000 loss: 0.15568467595247762\n",
      "48000/49000 loss: 0.13380940727620647\n",
      "epoch 4: valid acc = 0.892, new learning rate = 0.00040725312499999993\n",
      "6000/49000 loss: 0.1421094649284752\n",
      "12000/49000 loss: 0.11753725500732953\n",
      "18000/49000 loss: 0.12648374290696687\n",
      "24000/49000 loss: 0.1362234923023526\n",
      "30000/49000 loss: 0.1225872789318537\n",
      "36000/49000 loss: 0.1306813224675673\n",
      "42000/49000 loss: 0.13329239899359777\n",
      "48000/49000 loss: 0.17290658373196033\n",
      "epoch 5: valid acc = 0.886, new learning rate = 0.0003868904687499999\n",
      "6000/49000 loss: 0.1410728740803987\n",
      "12000/49000 loss: 0.14111186439914827\n",
      "18000/49000 loss: 0.1502224625029583\n",
      "24000/49000 loss: 0.17913829251157845\n",
      "30000/49000 loss: 0.15281101993049526\n",
      "36000/49000 loss: 0.1294482136038252\n",
      "42000/49000 loss: 0.14045331531406155\n",
      "48000/49000 loss: 0.12240970026029348\n",
      "epoch 6: valid acc = 0.895, new learning rate = 0.0003675459453124999\n",
      "6000/49000 loss: 0.13791442962363606\n",
      "12000/49000 loss: 0.1782048675314646\n",
      "18000/49000 loss: 0.12593661878221424\n",
      "24000/49000 loss: 0.1520067076676439\n",
      "30000/49000 loss: 0.13048155831387734\n",
      "36000/49000 loss: 0.15759258171775173\n",
      "42000/49000 loss: 0.15290272687282902\n",
      "48000/49000 loss: 0.1628390737716939\n",
      "epoch 7: valid acc = 0.895, new learning rate = 0.00034916864804687486\n",
      "6000/49000 loss: 0.15619837341074777\n",
      "12000/49000 loss: 0.1509784292436892\n",
      "18000/49000 loss: 0.18126122652767737\n",
      "24000/49000 loss: 0.17158461023317245\n",
      "30000/49000 loss: 0.1429652069755015\n",
      "36000/49000 loss: 0.1382115491269527\n",
      "42000/49000 loss: 0.1326992966748469\n",
      "48000/49000 loss: 0.14708528064292406\n",
      "epoch 8: valid acc = 0.886, new learning rate = 0.0003317102156445311\n",
      "6000/49000 loss: 0.11578314414317858\n",
      "12000/49000 loss: 0.14810129873388073\n",
      "18000/49000 loss: 0.15667168326902775\n",
      "24000/49000 loss: 0.1325241006503299\n",
      "30000/49000 loss: 0.14396184051681973\n",
      "36000/49000 loss: 0.13030382510613833\n",
      "42000/49000 loss: 0.12103332512576147\n",
      "48000/49000 loss: 0.11989289775114591\n",
      "epoch 9: valid acc = 0.888, new learning rate = 0.0003151247048623045\n",
      "6000/49000 loss: 0.14045386776604182\n",
      "12000/49000 loss: 0.13456939173813978\n",
      "18000/49000 loss: 0.16012935221220806\n",
      "24000/49000 loss: 0.16602197603830285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000/49000 loss: 0.12107921735581725\n",
      "36000/49000 loss: 0.13990162522170516\n",
      "42000/49000 loss: 0.1775270787819337\n",
      "48000/49000 loss: 0.13527167640747903\n",
      "epoch 10: valid acc = 0.891, new learning rate = 0.00029936846961918924\n",
      "number of batches for training: 61\n",
      "8000/49000 loss: 0.12380293574275911\n",
      "16000/49000 loss: 0.13770047977805572\n",
      "24000/49000 loss: 0.13841464438448772\n",
      "32000/49000 loss: 0.15444998489747847\n",
      "40000/49000 loss: 0.14182828219668758\n",
      "48000/49000 loss: 0.14797637657888654\n",
      "epoch 1: valid acc = 0.888, new learning rate = 0.000475\n",
      "8000/49000 loss: 0.12528977233974184\n",
      "16000/49000 loss: 0.13578132306812632\n",
      "24000/49000 loss: 0.15354379755043862\n",
      "32000/49000 loss: 0.16043573031649602\n",
      "40000/49000 loss: 0.14496720939327282\n",
      "48000/49000 loss: 0.11930279078406188\n",
      "epoch 2: valid acc = 0.89, new learning rate = 0.00045125\n",
      "8000/49000 loss: 0.14379310725514266\n",
      "16000/49000 loss: 0.1409638678766595\n",
      "24000/49000 loss: 0.14026149816349687\n",
      "32000/49000 loss: 0.16913967272421143\n",
      "40000/49000 loss: 0.1709732315764646\n",
      "48000/49000 loss: 0.14412540173066182\n",
      "epoch 3: valid acc = 0.89, new learning rate = 0.0004286875\n",
      "8000/49000 loss: 0.12883464876225478\n",
      "16000/49000 loss: 0.1316259865041249\n",
      "24000/49000 loss: 0.16367081034915307\n",
      "32000/49000 loss: 0.13737711322609683\n",
      "40000/49000 loss: 0.13021982836017737\n",
      "48000/49000 loss: 0.1423576804286368\n",
      "epoch 4: valid acc = 0.888, new learning rate = 0.00040725312499999993\n",
      "8000/49000 loss: 0.13946286733813051\n",
      "16000/49000 loss: 0.13722396270147313\n",
      "24000/49000 loss: 0.13064443367846748\n",
      "32000/49000 loss: 0.14497042714347594\n",
      "40000/49000 loss: 0.14566189747554517\n",
      "48000/49000 loss: 0.13643442404828204\n",
      "epoch 5: valid acc = 0.888, new learning rate = 0.0003868904687499999\n",
      "8000/49000 loss: 0.12876090750085814\n",
      "16000/49000 loss: 0.12979346023019855\n",
      "24000/49000 loss: 0.12058467151822315\n",
      "32000/49000 loss: 0.14661949772046667\n",
      "40000/49000 loss: 0.13118351591253988\n",
      "48000/49000 loss: 0.14898258442589218\n",
      "epoch 6: valid acc = 0.893, new learning rate = 0.0003675459453124999\n",
      "8000/49000 loss: 0.1290629360839032\n",
      "16000/49000 loss: 0.13597597862884156\n",
      "24000/49000 loss: 0.12787595361109838\n",
      "32000/49000 loss: 0.13180017311226955\n",
      "40000/49000 loss: 0.15026464677856974\n",
      "48000/49000 loss: 0.13535893444027952\n",
      "epoch 7: valid acc = 0.891, new learning rate = 0.00034916864804687486\n",
      "8000/49000 loss: 0.11727599134967968\n",
      "16000/49000 loss: 0.13170705582990597\n",
      "24000/49000 loss: 0.10547509751195117\n",
      "32000/49000 loss: 0.12144299787063642\n",
      "40000/49000 loss: 0.11195024808138464\n",
      "48000/49000 loss: 0.14289217246080718\n",
      "epoch 8: valid acc = 0.889, new learning rate = 0.0003317102156445311\n",
      "8000/49000 loss: 0.10813140474623205\n",
      "16000/49000 loss: 0.12281890497367222\n",
      "24000/49000 loss: 0.1302667893640153\n",
      "32000/49000 loss: 0.11831231284587125\n",
      "40000/49000 loss: 0.14840214904659216\n",
      "48000/49000 loss: 0.14779223949794482\n",
      "epoch 9: valid acc = 0.886, new learning rate = 0.0003151247048623045\n",
      "8000/49000 loss: 0.1394769991876182\n",
      "16000/49000 loss: 0.1245420584611454\n",
      "24000/49000 loss: 0.14829367386977543\n",
      "32000/49000 loss: 0.12774762581704935\n",
      "40000/49000 loss: 0.11716190457688976\n",
      "48000/49000 loss: 0.12577368443976125\n",
      "epoch 10: valid acc = 0.886, new learning rate = 0.00029936846961918924\n",
      "number of batches for training: 61\n",
      "8000/49000 loss: 0.1316242975718935\n",
      "16000/49000 loss: 0.13748182567262293\n",
      "24000/49000 loss: 0.13325609626875654\n",
      "32000/49000 loss: 0.12292944357946062\n",
      "40000/49000 loss: 0.15299263430335555\n",
      "48000/49000 loss: 0.131499513550779\n",
      "epoch 1: valid acc = 0.891, new learning rate = 0.000475\n",
      "8000/49000 loss: 0.11831886597275683\n",
      "16000/49000 loss: 0.133189121248567\n",
      "24000/49000 loss: 0.11707810669728848\n",
      "32000/49000 loss: 0.1300610632068368\n",
      "40000/49000 loss: 0.12604167628490526\n",
      "48000/49000 loss: 0.1381447469463697\n",
      "epoch 2: valid acc = 0.896, new learning rate = 0.00045125\n",
      "8000/49000 loss: 0.13852343961581923\n",
      "16000/49000 loss: 0.1482268639429238\n",
      "24000/49000 loss: 0.1278041562020138\n",
      "32000/49000 loss: 0.11689075334779554\n",
      "40000/49000 loss: 0.11103355801306859\n",
      "48000/49000 loss: 0.1181980484382781\n",
      "epoch 3: valid acc = 0.887, new learning rate = 0.0004286875\n",
      "8000/49000 loss: 0.15401730760991128\n",
      "16000/49000 loss: 0.13310920308082733\n",
      "24000/49000 loss: 0.16323126173671648\n",
      "32000/49000 loss: 0.13010848308599066\n",
      "40000/49000 loss: 0.1385741646147325\n",
      "48000/49000 loss: 0.1449003021893791\n",
      "epoch 4: valid acc = 0.891, new learning rate = 0.00040725312499999993\n",
      "8000/49000 loss: 0.1386228758820249\n",
      "16000/49000 loss: 0.11844096521429254\n",
      "24000/49000 loss: 0.15267483632279724\n",
      "32000/49000 loss: 0.11648305816401108\n",
      "40000/49000 loss: 0.1280909019271409\n",
      "48000/49000 loss: 0.15058548420577658\n",
      "epoch 5: valid acc = 0.889, new learning rate = 0.0003868904687499999\n",
      "8000/49000 loss: 0.1250535572095974\n",
      "16000/49000 loss: 0.12661768484961414\n",
      "24000/49000 loss: 0.1434287402922833\n",
      "32000/49000 loss: 0.13049082236233892\n",
      "40000/49000 loss: 0.13044561635078425\n",
      "48000/49000 loss: 0.1361614688430963\n",
      "epoch 6: valid acc = 0.885, new learning rate = 0.0003675459453124999\n",
      "8000/49000 loss: 0.12819748677499296\n",
      "16000/49000 loss: 0.12243318804109343\n",
      "24000/49000 loss: 0.11174654333233819\n",
      "32000/49000 loss: 0.11564533338172613\n",
      "40000/49000 loss: 0.11875742472650527\n",
      "48000/49000 loss: 0.11187318346135366\n",
      "epoch 7: valid acc = 0.89, new learning rate = 0.00034916864804687486\n",
      "8000/49000 loss: 0.1416062419870815\n",
      "16000/49000 loss: 0.13566878723311396\n",
      "24000/49000 loss: 0.11326032216236179\n",
      "32000/49000 loss: 0.10625186617830502\n",
      "40000/49000 loss: 0.12667025118475414\n",
      "48000/49000 loss: 0.1810623248799044\n",
      "epoch 8: valid acc = 0.889, new learning rate = 0.0003317102156445311\n",
      "8000/49000 loss: 0.1262250450733492\n",
      "16000/49000 loss: 0.12457648427298017\n",
      "24000/49000 loss: 0.1192806091839796\n",
      "32000/49000 loss: 0.1175688527453383\n",
      "40000/49000 loss: 0.11592838259250969\n",
      "48000/49000 loss: 0.1165794094072971\n",
      "epoch 9: valid acc = 0.89, new learning rate = 0.0003151247048623045\n",
      "8000/49000 loss: 0.1285392509299422\n",
      "16000/49000 loss: 0.11987785589536176\n",
      "24000/49000 loss: 0.12045391733462014\n",
      "32000/49000 loss: 0.12847451071833513\n",
      "40000/49000 loss: 0.13794699016643855\n",
      "48000/49000 loss: 0.11838523447707289\n",
      "epoch 10: valid acc = 0.892, new learning rate = 0.00029936846961918924\n",
      "number of batches for training: 61\n",
      "8000/49000 loss: 0.13456407345471363\n",
      "16000/49000 loss: 0.10639853442138729\n",
      "24000/49000 loss: 0.11424221468710277\n",
      "32000/49000 loss: 0.11681466458283238\n",
      "40000/49000 loss: 0.12931084793114864\n",
      "48000/49000 loss: 0.11363577753498615\n",
      "epoch 1: valid acc = 0.883, new learning rate = 0.000475\n",
      "8000/49000 loss: 0.13164097259464014\n",
      "16000/49000 loss: 0.12527625016777538\n",
      "24000/49000 loss: 0.12362718973865025\n",
      "32000/49000 loss: 0.11971251713344962\n",
      "40000/49000 loss: 0.1352032117740225\n",
      "48000/49000 loss: 0.12537443475073165\n",
      "epoch 2: valid acc = 0.889, new learning rate = 0.00045125\n",
      "8000/49000 loss: 0.11407413407278219\n",
      "16000/49000 loss: 0.13731914604445794\n",
      "24000/49000 loss: 0.12019227456091962\n",
      "32000/49000 loss: 0.12734366534158933\n",
      "40000/49000 loss: 0.09056470461705102\n",
      "48000/49000 loss: 0.1257180103830777\n",
      "epoch 3: valid acc = 0.889, new learning rate = 0.0004286875\n",
      "8000/49000 loss: 0.154974693664013\n",
      "16000/49000 loss: 0.11967049170945392\n",
      "24000/49000 loss: 0.15253950312639297\n",
      "32000/49000 loss: 0.13096552094221972\n",
      "40000/49000 loss: 0.14319981790857617\n",
      "48000/49000 loss: 0.12793821867804073\n",
      "epoch 4: valid acc = 0.888, new learning rate = 0.00040725312499999993\n",
      "8000/49000 loss: 0.12932427734296667\n",
      "16000/49000 loss: 0.10880375710533702\n",
      "24000/49000 loss: 0.10657037827830443\n",
      "32000/49000 loss: 0.12593118059974498\n",
      "40000/49000 loss: 0.12880630086898617\n",
      "48000/49000 loss: 0.1406944090243248\n",
      "epoch 5: valid acc = 0.889, new learning rate = 0.0003868904687499999\n",
      "8000/49000 loss: 0.12866498115568503\n",
      "16000/49000 loss: 0.10418991785771833\n",
      "24000/49000 loss: 0.15109154814722564\n",
      "32000/49000 loss: 0.11509980712405775\n",
      "40000/49000 loss: 0.11419274681707778\n",
      "48000/49000 loss: 0.11141395319523882\n",
      "epoch 6: valid acc = 0.89, new learning rate = 0.0003675459453124999\n",
      "8000/49000 loss: 0.10493640392602244\n",
      "16000/49000 loss: 0.12730274998019328\n",
      "24000/49000 loss: 0.11413747968998074\n",
      "32000/49000 loss: 0.12027172105693364\n",
      "40000/49000 loss: 0.11951033564990167\n",
      "48000/49000 loss: 0.13607998637044275\n",
      "epoch 7: valid acc = 0.891, new learning rate = 0.00034916864804687486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/49000 loss: 0.10309121281212295\n",
      "16000/49000 loss: 0.12350073609809767\n",
      "24000/49000 loss: 0.1268544736051294\n",
      "32000/49000 loss: 0.11450380347679318\n",
      "40000/49000 loss: 0.1210324121026182\n",
      "48000/49000 loss: 0.13277894239793256\n",
      "epoch 8: valid acc = 0.889, new learning rate = 0.0003317102156445311\n",
      "8000/49000 loss: 0.12386074779954684\n",
      "16000/49000 loss: 0.13106876831909187\n",
      "24000/49000 loss: 0.11618865524777928\n",
      "32000/49000 loss: 0.11661085105203396\n",
      "40000/49000 loss: 0.10645579349514546\n",
      "48000/49000 loss: 0.11935609647126219\n",
      "epoch 9: valid acc = 0.894, new learning rate = 0.0003151247048623045\n",
      "8000/49000 loss: 0.11781628799945819\n",
      "16000/49000 loss: 0.10504588554497758\n",
      "24000/49000 loss: 0.1305208884555352\n",
      "32000/49000 loss: 0.10581989707374083\n",
      "40000/49000 loss: 0.14099502660259985\n",
      "48000/49000 loss: 0.09373453778896788\n",
      "epoch 10: valid acc = 0.89, new learning rate = 0.00029936846961918924\n",
      "number of batches for training: 61\n",
      "8000/49000 loss: 0.12217542265506631\n",
      "16000/49000 loss: 0.1388320694238746\n",
      "24000/49000 loss: 0.11566700635950927\n",
      "32000/49000 loss: 0.10312902534149675\n",
      "40000/49000 loss: 0.1223616098869526\n",
      "48000/49000 loss: 0.10905399622019202\n",
      "epoch 1: valid acc = 0.892, new learning rate = 0.000475\n",
      "8000/49000 loss: 0.11732672526955604\n",
      "16000/49000 loss: 0.11569121114630641\n",
      "24000/49000 loss: 0.12476282446162897\n",
      "32000/49000 loss: 0.13921182046204286\n",
      "40000/49000 loss: 0.12169191669929968\n",
      "48000/49000 loss: 0.12594305009790932\n",
      "epoch 2: valid acc = 0.881, new learning rate = 0.00045125\n",
      "8000/49000 loss: 0.15799500212684423\n",
      "16000/49000 loss: 0.13045516642958785\n",
      "24000/49000 loss: 0.10675792935972603\n",
      "32000/49000 loss: 0.11013163681030187\n",
      "40000/49000 loss: 0.13413928528014066\n",
      "48000/49000 loss: 0.11105384351020028\n",
      "epoch 3: valid acc = 0.887, new learning rate = 0.0004286875\n",
      "8000/49000 loss: 0.11745836361944884\n",
      "16000/49000 loss: 0.12058263016047534\n",
      "24000/49000 loss: 0.13305023303366875\n",
      "32000/49000 loss: 0.14889659536003635\n",
      "40000/49000 loss: 0.11158684967286353\n",
      "48000/49000 loss: 0.13761513833983238\n",
      "epoch 4: valid acc = 0.883, new learning rate = 0.00040725312499999993\n",
      "8000/49000 loss: 0.12467135001815448\n",
      "16000/49000 loss: 0.14063700941188106\n",
      "24000/49000 loss: 0.12840660796778525\n",
      "32000/49000 loss: 0.1256277412944003\n",
      "40000/49000 loss: 0.13005846723035872\n",
      "48000/49000 loss: 0.14429094247757573\n",
      "epoch 5: valid acc = 0.889, new learning rate = 0.0003868904687499999\n",
      "8000/49000 loss: 0.14284060756824865\n",
      "16000/49000 loss: 0.12787943280576558\n",
      "24000/49000 loss: 0.11627242842138608\n",
      "32000/49000 loss: 0.11258545820889425\n",
      "40000/49000 loss: 0.11486589832133258\n",
      "48000/49000 loss: 0.10539351204194153\n",
      "epoch 6: valid acc = 0.888, new learning rate = 0.0003675459453124999\n",
      "8000/49000 loss: 0.11895559722002479\n",
      "16000/49000 loss: 0.10172505440453679\n",
      "24000/49000 loss: 0.12042186418461934\n",
      "32000/49000 loss: 0.112465049301403\n",
      "40000/49000 loss: 0.12380295513832743\n",
      "48000/49000 loss: 0.1097891989883157\n",
      "epoch 7: valid acc = 0.89, new learning rate = 0.00034916864804687486\n",
      "8000/49000 loss: 0.12583396146875836\n",
      "16000/49000 loss: 0.10935539980378306\n",
      "24000/49000 loss: 0.12874777110790198\n",
      "32000/49000 loss: 0.11942363909299628\n",
      "40000/49000 loss: 0.12731324911810926\n",
      "48000/49000 loss: 0.10552141410033072\n",
      "epoch 8: valid acc = 0.89, new learning rate = 0.0003317102156445311\n",
      "8000/49000 loss: 0.09984573847624961\n",
      "16000/49000 loss: 0.1240563760024964\n",
      "24000/49000 loss: 0.09984204582852775\n",
      "32000/49000 loss: 0.13192058003421273\n",
      "40000/49000 loss: 0.12595365852948964\n",
      "48000/49000 loss: 0.12701076562332178\n",
      "epoch 9: valid acc = 0.89, new learning rate = 0.0003151247048623045\n",
      "8000/49000 loss: 0.12833788146174813\n",
      "16000/49000 loss: 0.11674350109629439\n",
      "24000/49000 loss: 0.12621534610918267\n",
      "32000/49000 loss: 0.1285487842097612\n",
      "40000/49000 loss: 0.10882491854880308\n",
      "48000/49000 loss: 0.11444760716853074\n",
      "epoch 10: valid acc = 0.892, new learning rate = 0.00029936846961918924\n",
      "number of batches for training: 49\n",
      "10000/49000 loss: 0.11768298755217903\n",
      "20000/49000 loss: 0.13081893249446158\n",
      "30000/49000 loss: 0.13817756541021972\n",
      "40000/49000 loss: 0.14476528754636098\n",
      "epoch 1: valid acc = 0.89, new learning rate = 0.000475\n",
      "10000/49000 loss: 0.1305842596642136\n",
      "20000/49000 loss: 0.10777096585803547\n",
      "30000/49000 loss: 0.12657964775760436\n",
      "40000/49000 loss: 0.12333295907460254\n",
      "epoch 2: valid acc = 0.891, new learning rate = 0.00045125\n",
      "10000/49000 loss: 0.1158458571332205\n",
      "20000/49000 loss: 0.11787525852112177\n",
      "30000/49000 loss: 0.11526162177605334\n",
      "40000/49000 loss: 0.09965400352519259\n",
      "epoch 3: valid acc = 0.89, new learning rate = 0.0004286875\n",
      "10000/49000 loss: 0.10295799309556589\n",
      "20000/49000 loss: 0.13685400469642792\n",
      "30000/49000 loss: 0.11323617653841189\n",
      "40000/49000 loss: 0.13182079195163077\n",
      "epoch 4: valid acc = 0.887, new learning rate = 0.00040725312499999993\n",
      "10000/49000 loss: 0.10901502702174866\n",
      "20000/49000 loss: 0.11122300728687333\n",
      "30000/49000 loss: 0.1136070839993726\n",
      "40000/49000 loss: 0.12429504541572417\n",
      "epoch 5: valid acc = 0.877, new learning rate = 0.0003868904687499999\n",
      "10000/49000 loss: 0.13689701187999187\n",
      "20000/49000 loss: 0.12321594655035305\n",
      "30000/49000 loss: 0.10633353507531867\n",
      "40000/49000 loss: 0.08705179889234319\n",
      "epoch 6: valid acc = 0.887, new learning rate = 0.0003675459453124999\n",
      "10000/49000 loss: 0.11833983066280236\n",
      "20000/49000 loss: 0.10715599433560213\n",
      "30000/49000 loss: 0.11316324269317088\n",
      "40000/49000 loss: 0.1039329925509971\n",
      "epoch 7: valid acc = 0.889, new learning rate = 0.00034916864804687486\n",
      "10000/49000 loss: 0.1113036976016248\n",
      "20000/49000 loss: 0.11043604678304408\n",
      "30000/49000 loss: 0.11253522786502573\n",
      "40000/49000 loss: 0.12185110866764587\n",
      "epoch 8: valid acc = 0.886, new learning rate = 0.0003317102156445311\n",
      "10000/49000 loss: 0.12520275237896705\n",
      "20000/49000 loss: 0.11150067034927826\n",
      "30000/49000 loss: 0.11693305804292468\n",
      "40000/49000 loss: 0.12350288553874814\n",
      "epoch 9: valid acc = 0.89, new learning rate = 0.0003151247048623045\n",
      "10000/49000 loss: 0.12792114009420405\n",
      "20000/49000 loss: 0.14447324797851807\n",
      "30000/49000 loss: 0.10632843936326822\n",
      "40000/49000 loss: 0.1347885841073432\n",
      "epoch 10: valid acc = 0.891, new learning rate = 0.00029936846961918924\n",
      "number of batches for training: 49\n",
      "10000/49000 loss: 0.11848539944074825\n",
      "20000/49000 loss: 0.09178226610346786\n",
      "30000/49000 loss: 0.09500124779381551\n",
      "40000/49000 loss: 0.11905497569276224\n",
      "epoch 1: valid acc = 0.881, new learning rate = 0.000475\n",
      "10000/49000 loss: 0.11823906352187652\n",
      "20000/49000 loss: 0.11284035887124969\n",
      "30000/49000 loss: 0.13397577896866206\n",
      "40000/49000 loss: 0.11459144479275706\n",
      "epoch 2: valid acc = 0.883, new learning rate = 0.00045125\n",
      "10000/49000 loss: 0.11643118746366943\n",
      "20000/49000 loss: 0.11140130852859902\n",
      "30000/49000 loss: 0.11780376222953433\n",
      "40000/49000 loss: 0.08553612316674465\n",
      "epoch 3: valid acc = 0.892, new learning rate = 0.0004286875\n",
      "10000/49000 loss: 0.1213622119211486\n",
      "20000/49000 loss: 0.10162667152182878\n",
      "30000/49000 loss: 0.10110895434513835\n",
      "40000/49000 loss: 0.10778193826742609\n",
      "epoch 4: valid acc = 0.888, new learning rate = 0.00040725312499999993\n",
      "10000/49000 loss: 0.12144570465321972\n",
      "20000/49000 loss: 0.105624004444217\n",
      "30000/49000 loss: 0.12064418082305346\n",
      "40000/49000 loss: 0.100872279742064\n",
      "epoch 5: valid acc = 0.882, new learning rate = 0.0003868904687499999\n",
      "10000/49000 loss: 0.11827731627600128\n",
      "20000/49000 loss: 0.12910752511427434\n",
      "30000/49000 loss: 0.0989672507709701\n",
      "40000/49000 loss: 0.14803111616310943\n",
      "epoch 6: valid acc = 0.886, new learning rate = 0.0003675459453124999\n",
      "10000/49000 loss: 0.11145278676110408\n",
      "20000/49000 loss: 0.13408443581354368\n",
      "30000/49000 loss: 0.0995433674884586\n",
      "40000/49000 loss: 0.11153963211230114\n",
      "epoch 7: valid acc = 0.889, new learning rate = 0.00034916864804687486\n",
      "10000/49000 loss: 0.1061410953912967\n",
      "20000/49000 loss: 0.12805507508906958\n",
      "30000/49000 loss: 0.11297204742575641\n",
      "40000/49000 loss: 0.12634908672123002\n",
      "epoch 8: valid acc = 0.89, new learning rate = 0.0003317102156445311\n",
      "10000/49000 loss: 0.10743514284876048\n",
      "20000/49000 loss: 0.14193115169357173\n",
      "30000/49000 loss: 0.12267656012277724\n",
      "40000/49000 loss: 0.10272430089682433\n",
      "epoch 9: valid acc = 0.892, new learning rate = 0.0003151247048623045\n",
      "10000/49000 loss: 0.10611253084876203\n",
      "20000/49000 loss: 0.11225367413193293\n",
      "30000/49000 loss: 0.0929025711198453\n",
      "40000/49000 loss: 0.09802392151327238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10: valid acc = 0.892, new learning rate = 0.00029936846961918924\n",
      "number of batches for training: 49\n",
      "10000/49000 loss: 0.12485353864875398\n",
      "20000/49000 loss: 0.10723407709524019\n",
      "30000/49000 loss: 0.11295584824420862\n",
      "40000/49000 loss: 0.1238412962285848\n",
      "epoch 1: valid acc = 0.888, new learning rate = 0.000475\n",
      "10000/49000 loss: 0.10908425234941417\n",
      "20000/49000 loss: 0.0941285774489643\n",
      "30000/49000 loss: 0.11829654677663023\n",
      "40000/49000 loss: 0.10554658321199037\n",
      "epoch 2: valid acc = 0.889, new learning rate = 0.00045125\n",
      "10000/49000 loss: 0.09659816910545511\n",
      "20000/49000 loss: 0.10955582370982232\n",
      "30000/49000 loss: 0.1419536096209933\n",
      "40000/49000 loss: 0.13754063005155612\n",
      "epoch 3: valid acc = 0.887, new learning rate = 0.0004286875\n",
      "10000/49000 loss: 0.10810095604585031\n",
      "20000/49000 loss: 0.1350184895643145\n",
      "30000/49000 loss: 0.1320971606695457\n",
      "40000/49000 loss: 0.10027927678481881\n",
      "epoch 4: valid acc = 0.886, new learning rate = 0.00040725312499999993\n",
      "10000/49000 loss: 0.11330879249778181\n",
      "20000/49000 loss: 0.11263498853089288\n",
      "30000/49000 loss: 0.11257443859532755\n",
      "40000/49000 loss: 0.10183815889908952\n",
      "epoch 5: valid acc = 0.889, new learning rate = 0.0003868904687499999\n",
      "10000/49000 loss: 0.09847868341682778\n",
      "20000/49000 loss: 0.110509171690514\n",
      "30000/49000 loss: 0.10559806668980484\n",
      "40000/49000 loss: 0.09046769204493424\n",
      "epoch 6: valid acc = 0.888, new learning rate = 0.0003675459453124999\n",
      "10000/49000 loss: 0.09440519752174778\n",
      "20000/49000 loss: 0.09732021686023082\n",
      "30000/49000 loss: 0.11196810225736033\n",
      "40000/49000 loss: 0.10853210199456355\n",
      "epoch 7: valid acc = 0.886, new learning rate = 0.00034916864804687486\n",
      "10000/49000 loss: 0.10972635295598855\n",
      "20000/49000 loss: 0.10640299373132514\n",
      "30000/49000 loss: 0.10090707987101515\n",
      "40000/49000 loss: 0.09398784236373776\n",
      "epoch 8: valid acc = 0.887, new learning rate = 0.0003317102156445311\n",
      "10000/49000 loss: 0.10542248210987878\n",
      "20000/49000 loss: 0.11152689845682076\n",
      "30000/49000 loss: 0.08698599518975418\n",
      "40000/49000 loss: 0.11728661539093284\n",
      "epoch 9: valid acc = 0.884, new learning rate = 0.0003151247048623045\n",
      "10000/49000 loss: 0.11236827313851891\n",
      "20000/49000 loss: 0.11026804621689498\n",
      "30000/49000 loss: 0.10535727096213254\n",
      "40000/49000 loss: 0.11433749152884654\n",
      "epoch 10: valid acc = 0.887, new learning rate = 0.00029936846961918924\n",
      "number of batches for training: 49\n",
      "10000/49000 loss: 0.1068502222742175\n",
      "20000/49000 loss: 0.09216066690953563\n",
      "30000/49000 loss: 0.09126613555324625\n",
      "40000/49000 loss: 0.12020311007836841\n",
      "epoch 1: valid acc = 0.89, new learning rate = 0.000475\n",
      "10000/49000 loss: 0.1066103574735773\n",
      "20000/49000 loss: 0.11061496529097929\n",
      "30000/49000 loss: 0.12181463740340154\n",
      "40000/49000 loss: 0.11544218373604362\n",
      "epoch 2: valid acc = 0.893, new learning rate = 0.00045125\n",
      "10000/49000 loss: 0.10717679975349637\n",
      "20000/49000 loss: 0.1157508854357363\n",
      "30000/49000 loss: 0.1039149115610426\n",
      "40000/49000 loss: 0.12266341128796057\n",
      "epoch 3: valid acc = 0.889, new learning rate = 0.0004286875\n",
      "10000/49000 loss: 0.10560615233994053\n",
      "20000/49000 loss: 0.10734883422797988\n",
      "30000/49000 loss: 0.12368196408128611\n",
      "40000/49000 loss: 0.0997460734827689\n",
      "epoch 4: valid acc = 0.885, new learning rate = 0.00040725312499999993\n",
      "10000/49000 loss: 0.09168030930615317\n",
      "20000/49000 loss: 0.11838905706840357\n",
      "30000/49000 loss: 0.11298608013490756\n",
      "40000/49000 loss: 0.12054629852687405\n",
      "epoch 5: valid acc = 0.886, new learning rate = 0.0003868904687499999\n",
      "10000/49000 loss: 0.10514826062481869\n",
      "20000/49000 loss: 0.11121016959973201\n",
      "30000/49000 loss: 0.10673359749673961\n",
      "40000/49000 loss: 0.1010861164332508\n",
      "epoch 6: valid acc = 0.888, new learning rate = 0.0003675459453124999\n",
      "10000/49000 loss: 0.10484876920121194\n",
      "20000/49000 loss: 0.10402708344679655\n",
      "30000/49000 loss: 0.11097196214948649\n",
      "40000/49000 loss: 0.11615120765626873\n",
      "epoch 7: valid acc = 0.882, new learning rate = 0.00034916864804687486\n",
      "10000/49000 loss: 0.09499369020725174\n",
      "20000/49000 loss: 0.1024452121060345\n",
      "30000/49000 loss: 0.08673055666689226\n",
      "40000/49000 loss: 0.10556586934416923\n",
      "epoch 8: valid acc = 0.886, new learning rate = 0.0003317102156445311\n",
      "10000/49000 loss: 0.10258656827444328\n",
      "20000/49000 loss: 0.10685346891425825\n",
      "30000/49000 loss: 0.09268338857759449\n",
      "40000/49000 loss: 0.0982646127421319\n",
      "epoch 9: valid acc = 0.888, new learning rate = 0.0003151247048623045\n",
      "10000/49000 loss: 0.11341681515687287\n",
      "20000/49000 loss: 0.08640833108252917\n",
      "30000/49000 loss: 0.0956822140944447\n",
      "40000/49000 loss: 0.13211165072200312\n",
      "epoch 10: valid acc = 0.884, new learning rate = 0.00029936846961918924\n",
      "test acc: 0.8862\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8862"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.classifiers.twolayernet import TwoLayerNet\n",
    "\n",
    "## TODO: Use previous layers to create a two layer neural network\n",
    "## input->(affine->activation)->(affine->softmax)->output\n",
    "## The recommended activation function is ReLU. And you can \n",
    "## also make a comparison with other activation function to see\n",
    "## any difference.\n",
    "\n",
    "model = TwoLayerNet(input_dim=X_train.shape[1], hidden_dim=500, num_classes=20, reg=1e-4, weight_scale=1e-3)\n",
    "\n",
    "num_epoch = 10\n",
    "batch_size = 200\n",
    "lr = 5e-4\n",
    "verbose = True\n",
    "\n",
    "batch_sizes = [200,600,1000] \n",
    "lr_rates = [0.01,0.001,0.0001]\n",
    "\n",
    "results_pool = []\n",
    "\n",
    "i = 0\n",
    "for batch_size in batch_sizes:\n",
    "    for lr_rate in lr_rates:\n",
    "        scores = [train(model, X_train, y_train, X_val, y_val, \n",
    "                  num_epoch=num_epoch, batch_size=batch_size, learning_rate=lr, verbose=verbose)]\n",
    "        results_pool.append(scores)\n",
    "        i = i+1\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "# resutls_pool = np.array(results_pool)\n",
    "# print(results_pool)\n",
    "# max_index = np.argmax(results_pool, axis = 0)[1]\n",
    "\n",
    "test(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qI9Zc9cItrqJ"
   },
   "source": [
    "#### <span style=\"color:red\"><strong>TODO</strong></span>: Show your best results, including training plot of accuracy and loss and a visualization of weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>Solution</strong></span>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-54-27619bc4d3fb>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-54-27619bc4d3fb>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    max_index = np.argmax(results_array[])\u001b[0m\n\u001b[1;37m                                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "results_array = np.array(results)\n",
    "max_index = np.argmax(results_array[:,1])\n",
    "\n",
    "best_model_accuracy = results_array(max_index,2)\n",
    "\n",
    "print(max_index, best_model_accuracy)\n",
    "# for i in range(results_array.shape[0]):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "resutls_pool = np.array(results_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-298-0ffddee96677>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults_pool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmax_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "print(results_pool.shape)\n",
    "max_index = np.argmax(results_pool, axis = 0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "EWZ1E8xFtrqK",
    "outputId": "c477b649-e4f2-4179-be71-484b79eb8eab"
   },
   "outputs": [],
   "source": [
    "# training and accuracy plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "XMGUL4aLtrqM",
    "outputId": "879c2f02-1e77-4183-ef9f-3285ee723190",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualize weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C7SAy1J1trqP"
   },
   "source": [
    "### Save your best model in a dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B9iMeOm1trqQ"
   },
   "outputs": [],
   "source": [
    "## Create \"save_model\" folder if it does not exist\n",
    "save_dir = \"./save_models/\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "## Save your model\n",
    "save_params = model.save_model()\n",
    "with open(\"./save_models/best_model.pkl\", \"wb\") as output_file:\n",
    "    pickle.dump(save_params, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2XzKlA_1trqT"
   },
   "outputs": [],
   "source": [
    "## Load your model\n",
    "#with open(\"./save_models/best_model.pkl\", \"rb\") as input_file:\n",
    "#    load_params = pickle.load(input_file)\n",
    "    \n",
    "#model.update_model(load_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W6DUwt8OtrqV"
   },
   "source": [
    "## Part 3: Multilayer Network\n",
    "\n",
    "Complete the class **MLP** in **./utils/classifiers/network.py**. It should allow arbitrary settings for the number of hidden layers as well as the number of hidden neurons in each layer. **MLP** has a similar structure as a **TwoLayerNet** network.\n",
    "\n",
    "```\n",
    "class MLP:\n",
    "    functions: __init__, loss, step, predict, check_accuracy\n",
    "    variables: layers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO</strong></span>: Complete the class **MLP** in **./utils/classifiers/mlp.py**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>NOTE</strong></span>: Please do not change the code in the cell below, The cell below will run correctly if your code is right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "1C7yH-HBtrqW",
    "outputId": "718a8627-54d1-456f-afb4-3239c3564191",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches for training: 490\n",
      "epoch 1: valid acc = 0.194, new learning rate = 0.0095\n",
      "epoch 2: valid acc = 0.2, new learning rate = 0.009025\n",
      "epoch 3: valid acc = 0.193, new learning rate = 0.00857375\n",
      "epoch 4: valid acc = 0.204, new learning rate = 0.0081450625\n",
      "epoch 5: valid acc = 0.194, new learning rate = 0.007737809374999999\n",
      "epoch 6: valid acc = 0.193, new learning rate = 0.007350918906249998\n",
      "epoch 7: valid acc = 0.204, new learning rate = 0.006983372960937498\n",
      "epoch 8: valid acc = 0.19, new learning rate = 0.006634204312890623\n",
      "epoch 9: valid acc = 0.199, new learning rate = 0.006302494097246091\n",
      "epoch 10: valid acc = 0.199, new learning rate = 0.005987369392383786\n",
      "test acc: 0.1879\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1879"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# THE FOLLOWING IS THE VERIFICATION CODE     #\n",
    "# DO NOT CHANGE IT.                          #\n",
    "\n",
    "from utils.classifiers.mlp import MLP\n",
    "\n",
    "## Use a sequence of layers to create a multiple layer neural network\n",
    "## input->(affine->activation)-> ... ->(affine->activation)->(affine->softmax)->output\n",
    "model = MLP(input_dim=X_train.shape[1], hidden_dims=[100, 100], num_classes=20, reg=0.1, weight_scale=1e-3)\n",
    "\n",
    "num_epoch = 10\n",
    "batch_size = 100\n",
    "lr = 1e-2\n",
    "verbose = False\n",
    "train_acc_hist, val_acc_hist = train(model, X_train, y_train, X_val, y_val, \n",
    "                  num_epoch=num_epoch, batch_size=batch_size, learning_rate=lr, verbose=verbose)\n",
    "test(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO</strong></span>: Plot training and validation accuracy history of each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>Solution</strong></span>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#TODO: plot training and validation accuracy\n",
    "\n",
    "plt.plot(train_acc_hist, label='Train')\n",
    "plt.plot(val_acc_hist, label = 'Validation' )\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "task2-mlp_eager.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
